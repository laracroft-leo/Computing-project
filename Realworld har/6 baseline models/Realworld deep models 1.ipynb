{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyPgJwekD+jxZV2nFW9qfyUy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Install compatible versions\n","print(\"Installing dependencies...\")\n","!pip install -q numpy==1.26.4\n","!pip install -q scikit-learn==1.4.2\n","!pip install -q sktime==0.30.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vm72pVEkosbD","executionInfo":{"status":"ok","timestamp":1762894381879,"user_tz":0,"elapsed":30027,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"675021d4-fb41-4429-a2ea-acd3ed036604"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing dependencies...\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m140.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\n","cuml-cu12 25.6.0 requires scikit-learn>=1.5, but you have scikit-learn 1.4.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.9/23.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.2/136.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\n","cuml-cu12 25.6.0 requires scikit-learn>=1.5, but you have scikit-learn 1.4.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8B4Y992afqi","executionInfo":{"status":"ok","timestamp":1762894655278,"user_tz":0,"elapsed":247950,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"8a71b4a4-6267-4d3d-9bc2-8a6a69b039fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/data/rwhar\n","--2025-11-11 20:53:28--  https://wifo5-14.informatik.uni-mannheim.de/sensor/dataset/realworld2016/realworld2016_dataset.zip\n","Resolving wifo5-14.informatik.uni-mannheim.de (wifo5-14.informatik.uni-mannheim.de)... 134.155.98.56\n","Connecting to wifo5-14.informatik.uni-mannheim.de (wifo5-14.informatik.uni-mannheim.de)|134.155.98.56|:443... connected.\n","WARNING: no certificate subject alternative name matches\n","\trequested host name ‘wifo5-14.informatik.uni-mannheim.de’.\n","HTTP request sent, awaiting response... 403 Forbidden\n","2025-11-11 20:53:29 ERROR 403: Forbidden.\n","\n","--2025-11-11 20:53:29--  http://wifo5-14.informatik.uni-mannheim.de/sensor/dataset/realworld2016/realworld2016_dataset.zip\n","Resolving wifo5-14.informatik.uni-mannheim.de (wifo5-14.informatik.uni-mannheim.de)... 134.155.98.56\n","Connecting to wifo5-14.informatik.uni-mannheim.de (wifo5-14.informatik.uni-mannheim.de)|134.155.98.56|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3721016476 (3.5G) [application/zip]\n","Saving to: ‘realworld2016_dataset.zip’\n","\n","realworld2016_datas 100%[===================>]   3.46G  17.5MB/s    in 3m 26s  \n","\n","2025-11-11 20:56:55 (17.2 MB/s) - ‘realworld2016_dataset.zip’ saved [3721016476/3721016476]\n","\n","=== top-level ===\n","total 3.5G\n","drwxr-xr-x 17 root root 4.0K Nov 11 20:57 .\n","drwxr-xr-x  3 root root 4.0K Nov 11 20:53 ..\n","drwxr-xr-x  5 root root 4.0K Apr 27  2016 proband1\n","drwxr-xr-x  5 root root 4.0K Jul 21  2015 proband10\n","drwxr-xr-x  5 root root 4.0K Jul 22  2015 proband11\n","drwxr-xr-x  5 root root 4.0K Jul 28  2015 proband12\n","drwxr-xr-x  5 root root 4.0K Jul 27  2015 proband13\n","drwxr-xr-x  5 root root 4.0K Jul 29  2015 proband14\n","drwxr-xr-x  5 root root 4.0K Jul 28  2015 proband15\n","drwxr-xr-x  5 root root 4.0K Sep 22  2015 proband2\n","drwxr-xr-x  5 root root 4.0K Jul 27  2015 proband3\n","drwxr-xr-x  5 root root 4.0K Jul 28  2015 proband4\n","drwxr-xr-x  5 root root 4.0K Jul 27  2015 proband5\n","drwxr-xr-x  5 root root 4.0K Jul 27  2015 proband6\n","drwxr-xr-x  5 root root 4.0K Jul 27  2015 proband7\n","drwxr-xr-x  5 root root 4.0K Jul 28  2015 proband8\n","drwxr-xr-x  5 root root 4.0K Jul 27  2015 proband9\n","-rw-r--r--  1 root root 3.5G Jan 18  2022 realworld2016_dataset.zip\n","=== dirs (depth<=2) ===\n",".\n","./proband1\n","./proband10\n","./proband10/data\n","./proband10/images\n","./proband10/videos\n","./proband11\n","./proband11/data\n","./proband11/images\n","./proband11/videos\n","./proband12\n","./proband12/data\n","./proband12/images\n","./proband12/videos\n","./proband13\n","./proband13/data\n","./proband13/images\n","./proband13/videos\n","./proband14\n","./proband14/data\n"]}],"source":["# RealWorld-HAR (RealWorld2016, University of Mannheim)\n","!mkdir -p /content/data/rwhar\n","%cd /content/data/rwhar\n","\n","# Attempt HTTPS first (disabling certificate verification due to an SNI mismatch on the host); on failure, fall back to HTTP\n","!wget -c --no-check-certificate \"https://wifo5-14.informatik.uni-mannheim.de/sensor/dataset/realworld2016/realworld2016_dataset.zip\" -O realworld2016_dataset.zip || wget -c \"http://wifo5-14.informatik.uni-mannheim.de/sensor/dataset/realworld2016/realworld2016_dataset.zip\" -O realworld2016_dataset.zip\n","\n","# Decompress and perform a brief inspection\n","!unzip -q -o realworld2016_dataset.zip\n","!echo \"=== top-level ===\"\n","!ls -lah\n","!echo \"=== dirs (depth<=2) ===\"\n","!find . -maxdepth 2 -type d | sort | head -n 20"]},{"cell_type":"code","source":["# ================ Step 0: Project Initialization ================\n","import os\n","from datetime import datetime\n","\n","# Create directory structure\n","dirs = ['data/raw', 'interim', 'proc', 'features', 'models', 'logs', 'figures', 'configs']\n","for d in dirs:\n","    os.makedirs(f'/content/{d}', exist_ok=True)\n","print(\"✓ Directory structure created\")\n","\n","# Git Initialization\n","%cd /content\n","!git init\n","!git config user.name \"HAR-Project\"\n","!git config user.email \"har@project.local\"\n","print(\"✓ Git repository initialized\")\n","\n","# Persist environment information\n","!pip freeze > logs/env.txt\n","print(\"✓ Environment dependencies saved to logs/env.txt\")\n","\n","# Persist random seed list and hardware information\n","import json\n","import subprocess\n","\n","meta = {\n","    \"timestamp\": datetime.now().isoformat(),\n","    \"random_seeds\": [42, 123, 456, 789, 2024],  # predefined seeds\n","    \"hardware\": {\n","        \"gpu\": subprocess.getoutput(\"nvidia-smi --query-gpu=name --format=csv,noheader\"),\n","        \"cpu\": subprocess.getoutput(\"cat /proc/cpuinfo | grep 'model name' | head -1\").split(':')[1].strip(),\n","    }\n","}\n","\n","with open('logs/init_meta.json', 'w') as f:\n","    json.dump(meta, f, indent=2)\n","print(\"✓ Metadata saved to logs/init_meta.json\")\n","\n","# Initial commit\n","!git add .\n","!git commit -m \"init: project structure and environment\"\n","git_hash = subprocess.getoutput(\"git rev-parse HEAD\")\n","print(f\"✓ Git commit hash: {git_hash[:8]}\")\n","\n","\n","# ================ Step 1: Data Acquisition (Compliance) ================\n","# Move raw data to data/raw/ and retain structure\n","!mv /content/data/rwhar/* /content/data/raw/ 2>/dev/null || true\n","!rm -rf /content/data/rwhar\n","print(\"✓ Raw data moved to data/raw/\")\n","\n","# Compute checksums\n","import hashlib\n","\n","def calc_checksum(filepath):\n","    h = hashlib.sha256()\n","    with open(filepath, 'rb') as f:\n","        for chunk in iter(lambda: f.read(8192), b\"\"):\n","            h.update(chunk)\n","    return h.hexdigest()\n","\n","checksums = {}\n","for root, _, files in os.walk('/content/data/raw'):\n","    for f in files:\n","        path = os.path.join(root, f)\n","        rel_path = os.path.relpath(path, '/content/data/raw')\n","        checksums[rel_path] = calc_checksum(path)\n","\n","with open('/content/logs/checksums.txt', 'w') as f:\n","    f.write(f\"# RealWorld2016 dataset checksums (SHA256)\\n\")\n","    f.write(f\"# Generated at: {datetime.now().isoformat()}\\n\\n\")\n","    for path, sha in sorted(checksums.items()):\n","        f.write(f\"{sha}  {path}\\n\")\n","\n","print(f\"✓ Computed checksums for {len(checksums)} files → logs/checksums.txt\")\n","\n","# Record data source\n","with open('/content/logs/data_source.txt', 'w') as f:\n","    f.write(\"RealWorld2016 Human Activity Recognition Dataset\\n\")\n","    f.write(\"=\" * 50 + \"\\n\")\n","    f.write(\"Source: University of Mannheim\\n\")\n","    f.write(\"URL: https://wifo5-14.informatik.uni-mannheim.de/sensor/dataset/realworld2016/\\n\")\n","    f.write(\"Citation: Sztyler, T., & Stuckenschmidt, H. (2016). On-body localization of wearable devices.\\n\")\n","    f.write(f\"Downloaded: {datetime.now().isoformat()}\\n\")\n","\n","print(\"✓ Data source recorded to logs/data_source.txt\")\n","\n","# Commit data acquisition records\n","!git add logs/\n","!git commit -m \"data: add RealWorld2016 checksums and source\"\n","print(f\"\\n{'='*60}\\nProject initialization and data acquisition completed\\n{'='*60}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qoEB5mcDalte","outputId":"e5818835-6ec1-4b62-f9e5-90690ff01aeb","executionInfo":{"status":"ok","timestamp":1762895042340,"user_tz":0,"elapsed":387060,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✓ Directory structure created\n","/content\n","\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n","\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n","\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n","\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: \tgit branch -m <name>\u001b[m\n","Initialized empty Git repository in /content/.git/\n","✓ Git repository initialized\n","✓ Environment dependencies saved to logs/env.txt\n","✓ Metadata saved to logs/init_meta.json\n","[master (root-commit) e133110] init: project structure and environment\n"," 1837 files changed, 51723 insertions(+)\n"," create mode 100644 .config/.last_opt_in_prompt.yaml\n"," create mode 100644 .config/.last_survey_prompt.yaml\n"," create mode 100644 .config/.last_update_check.json\n"," create mode 100644 .config/active_config\n"," create mode 100644 .config/config_sentinel\n"," create mode 100644 .config/configurations/config_default\n"," create mode 100644 .config/default_configs.db\n"," create mode 100644 .config/gce\n"," create mode 100644 .config/hidden_gcloud_config_universe_descriptor_data_cache_configs.db\n"," create mode 100644 .config/logs/2025.11.07/14.29.37.882815.log\n"," create mode 100644 .config/logs/2025.11.07/14.30.06.191154.log\n"," create mode 100644 .config/logs/2025.11.07/14.30.17.651067.log\n"," create mode 100644 .config/logs/2025.11.07/14.30.19.353226.log\n"," create mode 100644 .config/logs/2025.11.07/14.30.29.487903.log\n"," create mode 100644 .config/logs/2025.11.07/14.30.30.258639.log\n"," create mode 100644 data/rwhar/proband1/data/acc_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/acc_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/acc_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/acc_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/acc_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/acc_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/acc_lying_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/acc_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/acc_running_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/acc_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/acc_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/acc_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/acc_standing_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/acc_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/acc_walking_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/acc_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/gps_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/gps_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/gps_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/gps_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/gps_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/gps_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/gps_lying_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/gps_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/gps_running_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/gps_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/gps_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/gps_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/gps_standing_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/gps_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/gps_walking_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/gps_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/gyr_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/gyr_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/gyr_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/gyr_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/gyr_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/gyr_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/gyr_lying_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/gyr_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/gyr_running_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/gyr_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/gyr_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/gyr_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/gyr_standing_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/gyr_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/gyr_walking_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/gyr_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/lig_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/lig_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/lig_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/lig_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/lig_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/lig_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/lig_lying_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/lig_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/lig_running_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/lig_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/lig_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/lig_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/lig_standing_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/lig_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/lig_walking_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/lig_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/mag_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/mag_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/mag_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/mag_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/mag_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/mag_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/mag_lying_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/mag_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/mag_running_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/mag_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/mag_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/mag_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/mag_standing_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/mag_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/mag_walking_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/mag_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/mic_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/mic_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/mic_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/mic_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/mic_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/mic_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/mic_lying_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/mic_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/mic_running_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/mic_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/mic_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/mic_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/mic_standing_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/mic_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/data/mic_walking_csv.zip\n"," create mode 100644 data/rwhar/proband1/data/mic_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband1/images/chest.png\n"," create mode 100644 data/rwhar/proband1/images/chest_thumb.png\n"," create mode 100644 data/rwhar/proband1/images/forearm.png\n"," create mode 100644 data/rwhar/proband1/images/forearm_thumb.png\n"," create mode 100644 data/rwhar/proband1/images/head.png\n"," create mode 100644 data/rwhar/proband1/images/head_thumb.png\n"," create mode 100644 data/rwhar/proband1/images/overview.png\n"," create mode 100644 data/rwhar/proband1/images/overview_thumb.png\n"," create mode 100644 data/rwhar/proband1/images/preview.png\n"," create mode 100644 data/rwhar/proband1/images/shin.png\n"," create mode 100644 data/rwhar/proband1/images/shin_thumb.png\n"," create mode 100644 data/rwhar/proband1/images/thigh.png\n"," create mode 100644 data/rwhar/proband1/images/thigh_thumb.png\n"," create mode 100644 data/rwhar/proband1/images/upperarm.png\n"," create mode 100644 data/rwhar/proband1/images/upperarm_thumb.png\n"," create mode 100644 data/rwhar/proband1/images/waist.png\n"," create mode 100644 data/rwhar/proband1/images/waist_thumb.png\n"," create mode 100644 data/rwhar/proband1/videos/video_climbing down_thumb.png\n"," create mode 100644 data/rwhar/proband1/videos/video_climbing up_thumb.png\n"," create mode 100644 data/rwhar/proband1/videos/video_jumping_thumb.png\n"," create mode 100644 data/rwhar/proband1/videos/video_lying_thumb.png\n"," create mode 100644 data/rwhar/proband1/videos/video_running_thumb.png\n"," create mode 100644 data/rwhar/proband1/videos/video_sitting_thumb.png\n"," create mode 100644 data/rwhar/proband1/videos/video_standing_thumb.png\n"," create mode 100644 data/rwhar/proband1/videos/video_walking_thumb.png\n"," create mode 100644 data/rwhar/proband10/data/acc_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/acc_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/acc_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/acc_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/acc_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/acc_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/acc_lying_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/acc_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/acc_running_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/acc_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/acc_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/acc_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/acc_standing_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/acc_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/acc_walking_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/acc_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/gps_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/gps_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/gps_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/gps_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/gps_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/gps_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/gps_lying_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/gps_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/gps_running_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/gps_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/gps_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/gps_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/gps_standing_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/gps_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/gps_walking_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/gps_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/gyr_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/gyr_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/gyr_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/gyr_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/gyr_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/gyr_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/gyr_lying_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/gyr_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/gyr_running_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/gyr_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/gyr_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/gyr_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/gyr_standing_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/gyr_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/gyr_walking_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/gyr_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/lig_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/lig_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/lig_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/lig_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/lig_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/lig_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/lig_lying_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/lig_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/lig_running_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/lig_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/lig_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/lig_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/lig_standing_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/lig_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/lig_walking_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/lig_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/mag_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/mag_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/mag_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/mag_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/mag_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/mag_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/mag_lying_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/mag_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/mag_running_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/mag_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/mag_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/mag_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/mag_standing_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/mag_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/mag_walking_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/mag_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/mic_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/mic_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/mic_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/mic_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/mic_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/mic_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/mic_lying_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/mic_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/mic_running_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/mic_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/mic_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/mic_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/mic_standing_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/mic_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/data/mic_walking_csv.zip\n"," create mode 100644 data/rwhar/proband10/data/mic_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband10/images/chest.png\n"," create mode 100644 data/rwhar/proband10/images/chest_thumb.png\n"," create mode 100644 data/rwhar/proband10/images/forearm.png\n"," create mode 100644 data/rwhar/proband10/images/forearm_thumb.png\n"," create mode 100644 data/rwhar/proband10/images/head.png\n"," create mode 100644 data/rwhar/proband10/images/head_thumb.png\n"," create mode 100644 data/rwhar/proband10/images/overview.png\n"," create mode 100644 data/rwhar/proband10/images/overview_thumb.png\n"," create mode 100644 data/rwhar/proband10/images/preview.png\n"," create mode 100644 data/rwhar/proband10/images/shin.png\n"," create mode 100644 data/rwhar/proband10/images/shin_thumb.png\n"," create mode 100644 data/rwhar/proband10/images/thigh.png\n"," create mode 100644 data/rwhar/proband10/images/thigh_thumb.png\n"," create mode 100644 data/rwhar/proband10/images/upperarm.png\n"," create mode 100644 data/rwhar/proband10/images/upperarm_thumb.png\n"," create mode 100644 data/rwhar/proband10/images/waist.png\n"," create mode 100644 data/rwhar/proband10/images/waist_thumb.png\n"," create mode 100644 data/rwhar/proband10/videos/video_climbing down_thumb.png\n"," create mode 100644 data/rwhar/proband10/videos/video_climbing up_thumb.png\n"," create mode 100644 data/rwhar/proband10/videos/video_jumping_thumb.png\n"," create mode 100644 data/rwhar/proband10/videos/video_lying_thumb.png\n"," create mode 100644 data/rwhar/proband10/videos/video_running_thumb.png\n"," create mode 100644 data/rwhar/proband10/videos/video_sitting_thumb.png\n"," create mode 100644 data/rwhar/proband10/videos/video_standing_thumb.png\n"," create mode 100644 data/rwhar/proband10/videos/video_walking_thumb.png\n"," create mode 100644 data/rwhar/proband11/data/acc_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/acc_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/acc_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/acc_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/acc_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/acc_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/acc_lying_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/acc_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/acc_running_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/acc_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/acc_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/acc_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/acc_standing_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/acc_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/acc_walking_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/acc_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/gps_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/gps_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/gps_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/gps_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/gps_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/gps_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/gps_lying_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/gps_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/gps_running_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/gps_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/gps_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/gps_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/gps_standing_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/gps_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/gps_walking_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/gps_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/gyr_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/gyr_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/gyr_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/gyr_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/gyr_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/gyr_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/gyr_lying_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/gyr_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/gyr_running_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/gyr_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/gyr_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/gyr_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/gyr_standing_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/gyr_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/gyr_walking_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/gyr_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/lig_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/lig_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/lig_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/lig_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/lig_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/lig_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/lig_lying_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/lig_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/lig_running_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/lig_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/lig_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/lig_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/lig_standing_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/lig_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/lig_walking_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/lig_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/mag_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/mag_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/mag_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/mag_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/mag_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/mag_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/mag_lying_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/mag_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/mag_running_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/mag_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/mag_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/mag_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/mag_standing_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/mag_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/mag_walking_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/mag_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/mic_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/mic_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/mic_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/mic_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/mic_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/mic_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/mic_lying_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/mic_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/mic_running_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/mic_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/mic_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/mic_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/mic_standing_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/mic_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/data/mic_walking_csv.zip\n"," create mode 100644 data/rwhar/proband11/data/mic_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband11/images/chest.png\n"," create mode 100644 data/rwhar/proband11/images/chest_thumb.png\n"," create mode 100644 data/rwhar/proband11/images/forearm.png\n"," create mode 100644 data/rwhar/proband11/images/forearm_thumb.png\n"," create mode 100644 data/rwhar/proband11/images/head.png\n"," create mode 100644 data/rwhar/proband11/images/head_thumb.png\n"," create mode 100644 data/rwhar/proband11/images/overview.png\n"," create mode 100644 data/rwhar/proband11/images/overview_thumb.png\n"," create mode 100644 data/rwhar/proband11/images/preview.png\n"," create mode 100644 data/rwhar/proband11/images/shin.png\n"," create mode 100644 data/rwhar/proband11/images/shin_thumb.png\n"," create mode 100644 data/rwhar/proband11/images/thigh.png\n"," create mode 100644 data/rwhar/proband11/images/thigh_thumb.png\n"," create mode 100644 data/rwhar/proband11/images/upperarm.png\n"," create mode 100644 data/rwhar/proband11/images/upperarm_thumb.png\n"," create mode 100644 data/rwhar/proband11/images/waist.png\n"," create mode 100644 data/rwhar/proband11/images/waist_thumb.png\n"," create mode 100644 data/rwhar/proband11/videos/video_climbing down_thumb.png\n"," create mode 100644 data/rwhar/proband11/videos/video_climbing up_thumb.png\n"," create mode 100644 data/rwhar/proband11/videos/video_jumping_thumb.png\n"," create mode 100644 data/rwhar/proband11/videos/video_lying_thumb.png\n"," create mode 100644 data/rwhar/proband11/videos/video_running_thumb.png\n"," create mode 100644 data/rwhar/proband11/videos/video_sitting_thumb.png\n"," create mode 100644 data/rwhar/proband11/videos/video_standing_thumb.png\n"," create mode 100644 data/rwhar/proband11/videos/video_walking_thumb.png\n"," create mode 100644 data/rwhar/proband12/data/acc_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/acc_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/acc_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/acc_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/acc_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/acc_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/acc_lying_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/acc_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/acc_running_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/acc_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/acc_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/acc_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/acc_standing_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/acc_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/acc_walking_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/acc_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/gps_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/gps_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/gps_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/gps_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/gps_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/gps_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/gps_lying_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/gps_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/gps_running_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/gps_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/gps_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/gps_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/gps_standing_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/gps_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/gps_walking_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/gps_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/gyr_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/gyr_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/gyr_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/gyr_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/gyr_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/gyr_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/gyr_lying_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/gyr_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/gyr_running_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/gyr_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/gyr_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/gyr_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/gyr_standing_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/gyr_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/gyr_walking_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/gyr_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/lig_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/lig_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/lig_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/lig_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/lig_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/lig_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/lig_lying_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/lig_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/lig_running_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/lig_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/lig_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/lig_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/lig_standing_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/lig_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/lig_walking_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/lig_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/mag_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/mag_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/mag_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/mag_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/mag_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/mag_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/mag_lying_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/mag_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/mag_running_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/mag_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/mag_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/mag_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/mag_standing_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/mag_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/mag_walking_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/mag_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/mic_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/mic_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/mic_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/mic_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/mic_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/mic_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/mic_lying_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/mic_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/mic_running_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/mic_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/mic_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/mic_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/mic_standing_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/mic_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/data/mic_walking_csv.zip\n"," create mode 100644 data/rwhar/proband12/data/mic_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband12/images/chest.png\n"," create mode 100644 data/rwhar/proband12/images/chest_thumb.png\n"," create mode 100644 data/rwhar/proband12/images/forearm.png\n"," create mode 100644 data/rwhar/proband12/images/forearm_thumb.png\n"," create mode 100644 data/rwhar/proband12/images/head.png\n"," create mode 100644 data/rwhar/proband12/images/head_thumb.png\n"," create mode 100644 data/rwhar/proband12/images/overview.png\n"," create mode 100644 data/rwhar/proband12/images/overview_thumb.png\n"," create mode 100644 data/rwhar/proband12/images/preview.png\n"," create mode 100644 data/rwhar/proband12/images/shin.png\n"," create mode 100644 data/rwhar/proband12/images/shin_thumb.png\n"," create mode 100644 data/rwhar/proband12/images/thigh.png\n"," create mode 100644 data/rwhar/proband12/images/thigh_thumb.png\n"," create mode 100644 data/rwhar/proband12/images/upperarm.png\n"," create mode 100644 data/rwhar/proband12/images/upperarm_thumb.png\n"," create mode 100644 data/rwhar/proband12/images/waist.png\n"," create mode 100644 data/rwhar/proband12/images/waist_thumb.png\n"," create mode 100644 data/rwhar/proband12/videos/video_climbing down_thumb.png\n"," create mode 100644 data/rwhar/proband12/videos/video_climbing up_thumb.png\n"," create mode 100644 data/rwhar/proband12/videos/video_jumping_thumb.png\n"," create mode 100644 data/rwhar/proband12/videos/video_lying_thumb.png\n"," create mode 100644 data/rwhar/proband12/videos/video_running_thumb.png\n"," create mode 100644 data/rwhar/proband12/videos/video_sitting_thumb.png\n"," create mode 100644 data/rwhar/proband12/videos/video_standing_thumb.png\n"," create mode 100644 data/rwhar/proband12/videos/video_walking_thumb.png\n"," create mode 100644 data/rwhar/proband13/data/acc_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/acc_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/acc_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/acc_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/acc_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/acc_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/acc_lying_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/acc_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/acc_running_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/acc_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/acc_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/acc_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/acc_standing_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/acc_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/acc_walking_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/acc_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/gps_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/gps_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/gps_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/gps_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/gps_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/gps_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/gps_lying_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/gps_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/gps_running_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/gps_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/gps_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/gps_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/gps_standing_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/gps_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/gps_walking_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/gps_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/gyr_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/gyr_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/gyr_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/gyr_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/gyr_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/gyr_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/gyr_lying_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/gyr_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/gyr_running_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/gyr_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/gyr_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/gyr_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/gyr_standing_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/gyr_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/gyr_walking_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/gyr_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/lig_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/lig_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/lig_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/lig_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/lig_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/lig_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/lig_lying_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/lig_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/lig_running_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/lig_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/lig_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/lig_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/lig_standing_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/lig_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/lig_walking_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/lig_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/mag_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/mag_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/mag_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/mag_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/mag_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/mag_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/mag_lying_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/mag_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/mag_running_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/mag_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/mag_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/mag_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/mag_standing_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/mag_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/mag_walking_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/mag_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/mic_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/mic_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/mic_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/mic_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/mic_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/mic_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/mic_lying_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/mic_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/mic_running_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/mic_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/mic_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/mic_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/mic_standing_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/mic_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/data/mic_walking_csv.zip\n"," create mode 100644 data/rwhar/proband13/data/mic_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband13/images/chest.png\n"," create mode 100644 data/rwhar/proband13/images/chest_thumb.png\n"," create mode 100644 data/rwhar/proband13/images/forearm.png\n"," create mode 100644 data/rwhar/proband13/images/forearm_thumb.png\n"," create mode 100644 data/rwhar/proband13/images/head.png\n"," create mode 100644 data/rwhar/proband13/images/head_thumb.png\n"," create mode 100644 data/rwhar/proband13/images/overview.png\n"," create mode 100644 data/rwhar/proband13/images/overview_thumb.png\n"," create mode 100644 data/rwhar/proband13/images/preview.png\n"," create mode 100644 data/rwhar/proband13/images/shin.png\n"," create mode 100644 data/rwhar/proband13/images/shin_thumb.png\n"," create mode 100644 data/rwhar/proband13/images/thigh.png\n"," create mode 100644 data/rwhar/proband13/images/thigh_thumb.png\n"," create mode 100644 data/rwhar/proband13/images/upperarm.png\n"," create mode 100644 data/rwhar/proband13/images/upperarm_thumb.png\n"," create mode 100644 data/rwhar/proband13/images/waist.png\n"," create mode 100644 data/rwhar/proband13/images/waist_thumb.png\n"," create mode 100644 data/rwhar/proband13/videos/video_climbing down_thumb.png\n"," create mode 100644 data/rwhar/proband13/videos/video_climbing up_thumb.png\n"," create mode 100644 data/rwhar/proband13/videos/video_jumping_thumb.png\n"," create mode 100644 data/rwhar/proband13/videos/video_lying_thumb.png\n"," create mode 100644 data/rwhar/proband13/videos/video_running_thumb.png\n"," create mode 100644 data/rwhar/proband13/videos/video_sitting_thumb.png\n"," create mode 100644 data/rwhar/proband13/videos/video_standing_thumb.png\n"," create mode 100644 data/rwhar/proband13/videos/video_walking_thumb.png\n"," create mode 100644 data/rwhar/proband14/data/acc_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/acc_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/acc_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/acc_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/acc_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/acc_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/acc_lying_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/acc_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/acc_running_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/acc_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/acc_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/acc_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/acc_standing_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/acc_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/acc_walking_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/acc_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/gps_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/gps_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/gps_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/gps_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/gps_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/gps_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/gps_lying_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/gps_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/gps_running_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/gps_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/gps_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/gps_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/gps_standing_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/gps_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/gps_walking_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/gps_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/gyr_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/gyr_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/gyr_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/gyr_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/gyr_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/gyr_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/gyr_lying_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/gyr_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/gyr_running_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/gyr_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/gyr_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/gyr_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/gyr_standing_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/gyr_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/gyr_walking_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/gyr_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/lig_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/lig_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/lig_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/lig_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/lig_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/lig_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/lig_lying_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/lig_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/lig_running_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/lig_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/lig_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/lig_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/lig_standing_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/lig_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/lig_walking_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/lig_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/mag_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/mag_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/mag_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/mag_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/mag_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/mag_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/mag_lying_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/mag_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/mag_running_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/mag_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/mag_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/mag_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/mag_standing_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/mag_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/mag_walking_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/mag_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/mic_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/mic_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/mic_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/mic_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/mic_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/mic_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/mic_lying_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/mic_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/mic_running_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/mic_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/mic_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/mic_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/mic_standing_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/mic_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/data/mic_walking_csv.zip\n"," create mode 100644 data/rwhar/proband14/data/mic_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband14/images/.directory\n"," create mode 100644 data/rwhar/proband14/images/chest.png\n"," create mode 100644 data/rwhar/proband14/images/chest_thumb.png\n"," create mode 100644 data/rwhar/proband14/images/forearm.png\n"," create mode 100644 data/rwhar/proband14/images/forearm_thumb.png\n"," create mode 100644 data/rwhar/proband14/images/head.png\n"," create mode 100644 data/rwhar/proband14/images/head_thumb.png\n"," create mode 100644 data/rwhar/proband14/images/overview.png\n"," create mode 100644 data/rwhar/proband14/images/overview_thumb.png\n"," create mode 100644 data/rwhar/proband14/images/preview.png\n"," create mode 100644 data/rwhar/proband14/images/shin.png\n"," create mode 100644 data/rwhar/proband14/images/shin_thumb.png\n"," create mode 100644 data/rwhar/proband14/images/thigh.png\n"," create mode 100644 data/rwhar/proband14/images/thigh_thumb.png\n"," create mode 100644 data/rwhar/proband14/images/upperarm.png\n"," create mode 100644 data/rwhar/proband14/images/upperarm_thumb.png\n"," create mode 100644 data/rwhar/proband14/images/waist.png\n"," create mode 100644 data/rwhar/proband14/images/waist_thumb.png\n"," create mode 100644 data/rwhar/proband14/videos/video_climbing down_thumb.png\n"," create mode 100644 data/rwhar/proband14/videos/video_climbing up_thumb.png\n"," create mode 100644 data/rwhar/proband14/videos/video_jumping_thumb.png\n"," create mode 100644 data/rwhar/proband14/videos/video_lying_thumb.png\n"," create mode 100644 data/rwhar/proband14/videos/video_running_thumb.png\n"," create mode 100644 data/rwhar/proband14/videos/video_sitting_thumb.png\n"," create mode 100644 data/rwhar/proband14/videos/video_standing_thumb.png\n"," create mode 100644 data/rwhar/proband14/videos/video_walking_thumb.png\n"," create mode 100644 data/rwhar/proband15/data/acc_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/acc_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/acc_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/acc_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/acc_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/acc_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/acc_lying_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/acc_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/acc_running_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/acc_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/acc_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/acc_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/acc_standing_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/acc_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/acc_walking_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/acc_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/gps_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/gps_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/gps_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/gps_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/gps_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/gps_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/gps_lying_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/gps_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/gps_running_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/gps_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/gps_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/gps_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/gps_standing_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/gps_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/gps_walking_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/gps_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/gyr_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/gyr_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/gyr_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/gyr_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/gyr_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/gyr_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/gyr_lying_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/gyr_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/gyr_running_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/gyr_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/gyr_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/gyr_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/gyr_standing_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/gyr_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/gyr_walking_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/gyr_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/lig_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/lig_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/lig_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/lig_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/lig_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/lig_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/lig_lying_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/lig_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/lig_running_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/lig_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/lig_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/lig_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/lig_standing_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/lig_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/lig_walking_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/lig_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/mag_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/mag_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/mag_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/mag_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/mag_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/mag_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/mag_lying_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/mag_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/mag_running_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/mag_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/mag_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/mag_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/mag_standing_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/mag_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/mag_walking_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/mag_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/mic_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/mic_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/mic_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/mic_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/mic_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/mic_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/mic_lying_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/mic_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/mic_running_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/mic_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/mic_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/mic_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/mic_standing_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/mic_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/data/mic_walking_csv.zip\n"," create mode 100644 data/rwhar/proband15/data/mic_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband15/images/chest.png\n"," create mode 100644 data/rwhar/proband15/images/chest_thumb.png\n"," create mode 100644 data/rwhar/proband15/images/forearm.png\n"," create mode 100644 data/rwhar/proband15/images/forearm_thumb.png\n"," create mode 100644 data/rwhar/proband15/images/head.png\n"," create mode 100644 data/rwhar/proband15/images/head_thumb.png\n"," create mode 100644 data/rwhar/proband15/images/overview.png\n"," create mode 100644 data/rwhar/proband15/images/overview_thumb.png\n"," create mode 100644 data/rwhar/proband15/images/preview.png\n"," create mode 100644 data/rwhar/proband15/images/shin.png\n"," create mode 100644 data/rwhar/proband15/images/shin_thumb.png\n"," create mode 100644 data/rwhar/proband15/images/thigh.png\n"," create mode 100644 data/rwhar/proband15/images/thigh_thumb.png\n"," create mode 100644 data/rwhar/proband15/images/upperarm.png\n"," create mode 100644 data/rwhar/proband15/images/upperarm_thumb.png\n"," create mode 100644 data/rwhar/proband15/images/waist.png\n"," create mode 100644 data/rwhar/proband15/images/waist_thumb.png\n"," create mode 100644 data/rwhar/proband15/videos/video_climbing down_thumb.png\n"," create mode 100644 data/rwhar/proband15/videos/video_climbing up_thumb.png\n"," create mode 100644 data/rwhar/proband15/videos/video_jumping_thumb.png\n"," create mode 100644 data/rwhar/proband15/videos/video_lying_thumb.png\n"," create mode 100644 data/rwhar/proband15/videos/video_running_thumb.png\n"," create mode 100644 data/rwhar/proband15/videos/video_sitting_thumb.png\n"," create mode 100644 data/rwhar/proband15/videos/video_standing_thumb.png\n"," create mode 100644 data/rwhar/proband15/videos/video_walking_thumb.png\n"," create mode 100644 data/rwhar/proband2/data/acc_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/acc_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/acc_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/acc_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/acc_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/acc_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/acc_lying_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/acc_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/acc_running_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/acc_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/acc_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/acc_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/acc_standing_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/acc_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/acc_walking_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/acc_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/gps_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/gps_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/gps_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/gps_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/gps_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/gps_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/gps_lying_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/gps_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/gps_running_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/gps_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/gps_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/gps_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/gps_standing_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/gps_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/gps_walking_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/gps_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/gyr_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/gyr_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/gyr_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/gyr_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/gyr_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/gyr_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/gyr_lying_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/gyr_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/gyr_running_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/gyr_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/gyr_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/gyr_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/gyr_standing_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/gyr_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/gyr_walking_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/gyr_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/lig_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/lig_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/lig_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/lig_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/lig_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/lig_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/lig_lying_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/lig_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/lig_running_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/lig_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/lig_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/lig_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/lig_standing_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/lig_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/lig_walking_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/lig_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/mag_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/mag_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/mag_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/mag_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/mag_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/mag_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/mag_lying_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/mag_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/mag_running_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/mag_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/mag_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/mag_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/mag_standing_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/mag_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/mag_walking_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/mag_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/mic_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/mic_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/mic_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/mic_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/mic_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/mic_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/mic_lying_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/mic_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/mic_running_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/mic_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/mic_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/mic_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/mic_standing_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/mic_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/data/mic_walking_csv.zip\n"," create mode 100644 data/rwhar/proband2/data/mic_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband2/images/chest.png\n"," create mode 100644 data/rwhar/proband2/images/chest_thumb.png\n"," create mode 100644 data/rwhar/proband2/images/forearm.png\n"," create mode 100644 data/rwhar/proband2/images/forearm_thumb.png\n"," create mode 100644 data/rwhar/proband2/images/head.png\n"," create mode 100644 data/rwhar/proband2/images/head_thumb.png\n"," create mode 100644 data/rwhar/proband2/images/overview.png\n"," create mode 100644 data/rwhar/proband2/images/overview_thumb.png\n"," create mode 100644 data/rwhar/proband2/images/preview.png\n"," create mode 100644 data/rwhar/proband2/images/shin.png\n"," create mode 100644 data/rwhar/proband2/images/shin_thumb.png\n"," create mode 100644 data/rwhar/proband2/images/thigh.png\n"," create mode 100644 data/rwhar/proband2/images/thigh_thumb.png\n"," create mode 100644 data/rwhar/proband2/images/upperarm.png\n"," create mode 100644 data/rwhar/proband2/images/upperarm_thumb.png\n"," create mode 100644 data/rwhar/proband2/images/waist.png\n"," create mode 100644 data/rwhar/proband2/images/waist_thumb.png\n"," create mode 100644 data/rwhar/proband2/videos/video_climbing down_thumb.png\n"," create mode 100644 data/rwhar/proband2/videos/video_climbing up_thumb.png\n"," create mode 100644 data/rwhar/proband2/videos/video_jumping_thumb.png\n"," create mode 100644 data/rwhar/proband2/videos/video_lying_thumb.png\n"," create mode 100644 data/rwhar/proband2/videos/video_running_thumb.png\n"," create mode 100644 data/rwhar/proband2/videos/video_sitting_thumb.png\n"," create mode 100644 data/rwhar/proband2/videos/video_standing_thumb.png\n"," create mode 100644 data/rwhar/proband2/videos/video_walking_thumb.png\n"," create mode 100644 data/rwhar/proband3/data/acc_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/acc_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/acc_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/acc_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/acc_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/acc_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/acc_lying_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/acc_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/acc_running_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/acc_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/acc_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/acc_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/acc_standing_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/acc_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/acc_walking_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/acc_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/gps_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/gps_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/gps_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/gps_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/gps_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/gps_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/gps_lying_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/gps_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/gps_running_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/gps_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/gps_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/gps_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/gps_standing_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/gps_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/gps_walking_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/gps_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/gyr_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/gyr_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/gyr_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/gyr_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/gyr_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/gyr_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/gyr_lying_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/gyr_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/gyr_running_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/gyr_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/gyr_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/gyr_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/gyr_standing_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/gyr_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/gyr_walking_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/gyr_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/lig_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/lig_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/lig_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/lig_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/lig_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/lig_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/lig_lying_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/lig_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/lig_running_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/lig_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/lig_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/lig_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/lig_standing_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/lig_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/lig_walking_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/lig_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/mag_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/mag_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/mag_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/mag_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/mag_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/mag_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/mag_lying_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/mag_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/mag_running_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/mag_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/mag_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/mag_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/mag_standing_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/mag_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/mag_walking_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/mag_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/mic_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/mic_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/mic_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/mic_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/mic_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/mic_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/mic_lying_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/mic_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/mic_running_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/mic_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/mic_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/mic_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/mic_standing_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/mic_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/data/mic_walking_csv.zip\n"," create mode 100644 data/rwhar/proband3/data/mic_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband3/images/chest.png\n"," create mode 100644 data/rwhar/proband3/images/chest_thumb.png\n"," create mode 100644 data/rwhar/proband3/images/forearm.png\n"," create mode 100644 data/rwhar/proband3/images/forearm_thumb.png\n"," create mode 100644 data/rwhar/proband3/images/head.png\n"," create mode 100644 data/rwhar/proband3/images/head_thumb.png\n"," create mode 100644 data/rwhar/proband3/images/overview.png\n"," create mode 100644 data/rwhar/proband3/images/overview_thumb.png\n"," create mode 100644 data/rwhar/proband3/images/preview.png\n"," create mode 100644 data/rwhar/proband3/images/shin.png\n"," create mode 100644 data/rwhar/proband3/images/shin_thumb.png\n"," create mode 100644 data/rwhar/proband3/images/thigh.png\n"," create mode 100644 data/rwhar/proband3/images/thigh_thumb.png\n"," create mode 100644 data/rwhar/proband3/images/upperarm.png\n"," create mode 100644 data/rwhar/proband3/images/upperarm_thumb.png\n"," create mode 100644 data/rwhar/proband3/images/waist.png\n"," create mode 100644 data/rwhar/proband3/images/waist_thumb.png\n"," create mode 100644 data/rwhar/proband3/videos/video_climbing down_thumb.png\n"," create mode 100644 data/rwhar/proband3/videos/video_climbing up_thumb.png\n"," create mode 100644 data/rwhar/proband3/videos/video_jumping_thumb.png\n"," create mode 100644 data/rwhar/proband3/videos/video_lying_thumb.png\n"," create mode 100644 data/rwhar/proband3/videos/video_running_thumb.png\n"," create mode 100644 data/rwhar/proband3/videos/video_sitting_thumb.png\n"," create mode 100644 data/rwhar/proband3/videos/video_standing_thumb.png\n"," create mode 100644 data/rwhar/proband3/videos/video_walking_thumb.png\n"," create mode 100644 data/rwhar/proband4/data/acc_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/acc_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/acc_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/acc_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/acc_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/acc_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/acc_lying_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/acc_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/acc_running_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/acc_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/acc_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/acc_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/acc_standing_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/acc_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/acc_walking_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/acc_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/gps_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/gps_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/gps_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/gps_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/gps_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/gps_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/gps_lying_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/gps_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/gps_running_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/gps_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/gps_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/gps_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/gps_standing_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/gps_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/gps_walking_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/gps_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/gyr_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/gyr_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/gyr_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/gyr_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/gyr_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/gyr_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/gyr_lying_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/gyr_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/gyr_running_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/gyr_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/gyr_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/gyr_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/gyr_standing_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/gyr_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/gyr_walking_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/gyr_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/lig_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/lig_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/lig_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/lig_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/lig_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/lig_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/lig_lying_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/lig_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/lig_running_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/lig_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/lig_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/lig_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/lig_standing_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/lig_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/lig_walking_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/lig_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/mag_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/mag_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/mag_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/mag_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/mag_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/mag_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/mag_lying_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/mag_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/mag_running_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/mag_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/mag_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/mag_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/mag_standing_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/mag_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/mag_walking_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/mag_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/mic_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/mic_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/mic_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/mic_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/mic_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/mic_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/mic_lying_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/mic_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/mic_running_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/mic_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/mic_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/mic_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/mic_standing_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/mic_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/data/mic_walking_csv.zip\n"," create mode 100644 data/rwhar/proband4/data/mic_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband4/images/DSC_0342.JPG\n"," create mode 100644 data/rwhar/proband4/images/chest.png\n"," create mode 100644 data/rwhar/proband4/images/chest_thumb.png\n"," create mode 100644 data/rwhar/proband4/images/forearm.png\n"," create mode 100644 data/rwhar/proband4/images/forearm_thumb.png\n"," create mode 100644 data/rwhar/proband4/images/head.png\n"," create mode 100644 data/rwhar/proband4/images/head_thumb.png\n"," create mode 100644 data/rwhar/proband4/images/overview.png\n"," create mode 100644 data/rwhar/proband4/images/overview_thumb.png\n"," create mode 100644 data/rwhar/proband4/images/preview.png\n"," create mode 100644 data/rwhar/proband4/images/shin.png\n"," create mode 100644 data/rwhar/proband4/images/shin_thumb.png\n"," create mode 100644 data/rwhar/proband4/images/thigh.png\n"," create mode 100644 data/rwhar/proband4/images/thigh_thumb.png\n"," create mode 100644 data/rwhar/proband4/images/upperarm.png\n"," create mode 100644 data/rwhar/proband4/images/upperarm_thumb.png\n"," create mode 100644 data/rwhar/proband4/images/waist.png\n"," create mode 100644 data/rwhar/proband4/images/waist_thumb.png\n"," create mode 100644 data/rwhar/proband4/videos/video_climbing down_thumb.png\n"," create mode 100644 data/rwhar/proband4/videos/video_climbing up_thumb.png\n"," create mode 100644 data/rwhar/proband4/videos/video_jumping_thumb.png\n"," create mode 100644 data/rwhar/proband4/videos/video_lying_thumb.png\n"," create mode 100644 data/rwhar/proband4/videos/video_running_thumb.png\n"," create mode 100644 data/rwhar/proband4/videos/video_sitting_thumb.png\n"," create mode 100644 data/rwhar/proband4/videos/video_standing_thumb.png\n"," create mode 100644 data/rwhar/proband4/videos/video_walking_thumb.png\n"," create mode 100644 data/rwhar/proband5/data/acc_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/acc_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/acc_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/acc_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/acc_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/acc_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/acc_lying_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/acc_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/acc_running_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/acc_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/acc_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/acc_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/acc_standing_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/acc_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/acc_walking_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/acc_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/gps_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/gps_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/gps_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/gps_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/gps_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/gps_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/gps_lying_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/gps_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/gps_running_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/gps_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/gps_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/gps_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/gps_standing_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/gps_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/gps_walking_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/gps_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/gyr_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/gyr_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/gyr_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/gyr_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/gyr_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/gyr_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/gyr_lying_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/gyr_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/gyr_running_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/gyr_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/gyr_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/gyr_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/gyr_standing_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/gyr_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/gyr_walking_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/gyr_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/lig_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/lig_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/lig_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/lig_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/lig_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/lig_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/lig_lying_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/lig_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/lig_running_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/lig_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/lig_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/lig_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/lig_standing_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/lig_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/lig_walking_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/lig_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/mag_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/mag_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/mag_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/mag_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/mag_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/mag_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/mag_lying_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/mag_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/mag_running_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/mag_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/mag_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/mag_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/mag_standing_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/mag_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/mag_walking_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/mag_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/mic_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/mic_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/mic_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/mic_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/mic_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/mic_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/mic_lying_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/mic_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/mic_running_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/mic_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/mic_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/mic_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/mic_standing_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/mic_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/data/mic_walking_csv.zip\n"," create mode 100644 data/rwhar/proband5/data/mic_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband5/images/chest.png\n"," create mode 100644 data/rwhar/proband5/images/chest_thumb.png\n"," create mode 100644 data/rwhar/proband5/images/forearm.png\n"," create mode 100644 data/rwhar/proband5/images/forearm_thumb.png\n"," create mode 100644 data/rwhar/proband5/images/head.png\n"," create mode 100644 data/rwhar/proband5/images/head_thumb.png\n"," create mode 100644 data/rwhar/proband5/images/overview.png\n"," create mode 100644 data/rwhar/proband5/images/overview_thumb.png\n"," create mode 100644 data/rwhar/proband5/images/preview.png\n"," create mode 100644 data/rwhar/proband5/images/shin.png\n"," create mode 100644 data/rwhar/proband5/images/shin_thumb.png\n"," create mode 100644 data/rwhar/proband5/images/upperarm.png\n"," create mode 100644 data/rwhar/proband5/images/upperarm_thumb.png\n"," create mode 100644 data/rwhar/proband5/images/waist.png\n"," create mode 100644 data/rwhar/proband5/images/waist_thumb.png\n"," create mode 100644 data/rwhar/proband5/videos/video_climbing down_thumb.png\n"," create mode 100644 data/rwhar/proband5/videos/video_climbing up_thumb.png\n"," create mode 100644 data/rwhar/proband5/videos/video_jumping_thumb.png\n"," create mode 100644 data/rwhar/proband5/videos/video_lying_thumb.png\n"," create mode 100644 data/rwhar/proband5/videos/video_running_thumb.png\n"," create mode 100644 data/rwhar/proband5/videos/video_sitting_thumb.png\n"," create mode 100644 data/rwhar/proband5/videos/video_standing_thumb.png\n"," create mode 100644 data/rwhar/proband5/videos/video_walking_thumb.png\n"," create mode 100644 data/rwhar/proband6/data/acc_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/acc_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/acc_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/acc_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/acc_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/acc_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/acc_lying_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/acc_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/acc_running_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/acc_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/acc_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/acc_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/acc_standing_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/acc_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/acc_walking_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/acc_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/gps_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/gps_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/gps_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/gps_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/gps_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/gps_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/gps_lying_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/gps_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/gps_running_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/gps_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/gps_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/gps_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/gps_standing_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/gps_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/gps_walking_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/gps_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/gyr_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/gyr_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/gyr_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/gyr_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/gyr_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/gyr_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/gyr_lying_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/gyr_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/gyr_running_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/gyr_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/gyr_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/gyr_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/gyr_standing_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/gyr_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/gyr_walking_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/gyr_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/lig_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/lig_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/lig_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/lig_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/lig_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/lig_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/lig_lying_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/lig_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/lig_running_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/lig_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/lig_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/lig_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/lig_standing_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/lig_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/lig_walking_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/lig_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/mag_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/mag_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/mag_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/mag_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/mag_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/mag_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/mag_lying_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/mag_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/mag_running_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/mag_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/mag_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/mag_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/mag_standing_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/mag_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/mag_walking_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/mag_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/mic_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/mic_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/mic_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/mic_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/mic_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/mic_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/mic_lying_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/mic_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/mic_running_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/mic_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/mic_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/mic_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/mic_standing_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/mic_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/data/mic_walking_csv.zip\n"," create mode 100644 data/rwhar/proband6/data/mic_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband6/images/chest.png\n"," create mode 100644 data/rwhar/proband6/images/chest_thumb.png\n"," create mode 100644 data/rwhar/proband6/images/forearm.png\n"," create mode 100644 data/rwhar/proband6/images/forearm_thumb.png\n"," create mode 100644 data/rwhar/proband6/images/head.png\n"," create mode 100644 data/rwhar/proband6/images/head_thumb.png\n"," create mode 100644 data/rwhar/proband6/images/overview.png\n"," create mode 100644 data/rwhar/proband6/images/overview_thumb.png\n"," create mode 100644 data/rwhar/proband6/images/preview.png\n"," create mode 100644 data/rwhar/proband6/images/shin.png\n"," create mode 100644 data/rwhar/proband6/images/shin_thumb.png\n"," create mode 100644 data/rwhar/proband6/images/upperarm.png\n"," create mode 100644 data/rwhar/proband6/images/upperarm_thumb.png\n"," create mode 100644 data/rwhar/proband6/images/waist.png\n"," create mode 100644 data/rwhar/proband6/images/waist_thumb.png\n"," create mode 100644 data/rwhar/proband6/videos/video_climbing down_thumb.png\n"," create mode 100644 data/rwhar/proband6/videos/video_climbing up_thumb.png\n"," create mode 100644 data/rwhar/proband6/videos/video_jumping_thumb.png\n"," create mode 100644 data/rwhar/proband6/videos/video_lying_thumb.png\n"," create mode 100644 data/rwhar/proband6/videos/video_running_thumb.png\n"," create mode 100644 data/rwhar/proband6/videos/video_sitting_thumb.png\n"," create mode 100644 data/rwhar/proband6/videos/video_standing_thumb.png\n"," create mode 100644 data/rwhar/proband6/videos/video_walking_thumb.png\n"," create mode 100644 data/rwhar/proband7/data/acc_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/acc_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/acc_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/acc_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/acc_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/acc_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/acc_lying_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/acc_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/acc_running_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/acc_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/acc_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/acc_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/acc_standing_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/acc_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/acc_walking_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/acc_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/gps_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/gps_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/gps_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/gps_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/gps_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/gps_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/gps_lying_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/gps_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/gps_running_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/gps_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/gps_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/gps_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/gps_standing_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/gps_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/gps_walking_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/gps_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/gyr_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/gyr_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/gyr_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/gyr_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/gyr_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/gyr_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/gyr_lying_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/gyr_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/gyr_running_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/gyr_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/gyr_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/gyr_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/gyr_standing_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/gyr_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/gyr_walking_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/gyr_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/lig_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/lig_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/lig_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/lig_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/lig_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/lig_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/lig_lying_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/lig_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/lig_running_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/lig_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/lig_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/lig_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/lig_standing_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/lig_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/lig_walking_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/lig_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/mag_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/mag_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/mag_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/mag_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/mag_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/mag_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/mag_lying_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/mag_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/mag_running_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/mag_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/mag_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/mag_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/mag_standing_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/mag_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/mag_walking_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/mag_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/mic_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/mic_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/mic_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/mic_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/mic_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/mic_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/mic_lying_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/mic_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/mic_running_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/mic_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/mic_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/mic_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/mic_standing_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/mic_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/data/mic_walking_csv.zip\n"," create mode 100644 data/rwhar/proband7/data/mic_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband7/images/chest.png\n"," create mode 100644 data/rwhar/proband7/images/chest_thumb.png\n"," create mode 100644 data/rwhar/proband7/images/forearm.png\n"," create mode 100644 data/rwhar/proband7/images/forearm_thumb.png\n"," create mode 100644 data/rwhar/proband7/images/head.png\n"," create mode 100644 data/rwhar/proband7/images/head_thumb.png\n"," create mode 100644 data/rwhar/proband7/images/overview.png\n"," create mode 100644 data/rwhar/proband7/images/overview_thumb.png\n"," create mode 100644 data/rwhar/proband7/images/preview.png\n"," create mode 100644 data/rwhar/proband7/images/shin.png\n"," create mode 100644 data/rwhar/proband7/images/shin_thumb.png\n"," create mode 100644 data/rwhar/proband7/images/thigh.png\n"," create mode 100644 data/rwhar/proband7/images/thigh_thumb.png\n"," create mode 100644 data/rwhar/proband7/images/upperarm.png\n"," create mode 100644 data/rwhar/proband7/images/upperarm_thumb.png\n"," create mode 100644 data/rwhar/proband7/images/waist.png\n"," create mode 100644 data/rwhar/proband7/images/waist_thumb.png\n"," create mode 100644 data/rwhar/proband7/videos/video_climbing down_thumb.png\n"," create mode 100644 data/rwhar/proband7/videos/video_climbing up_thumb.png\n"," create mode 100644 data/rwhar/proband7/videos/video_jumping_thumb.png\n"," create mode 100644 data/rwhar/proband7/videos/video_lying_thumb.png\n"," create mode 100644 data/rwhar/proband7/videos/video_running_thumb.png\n"," create mode 100644 data/rwhar/proband7/videos/video_sitting_thumb.png\n"," create mode 100644 data/rwhar/proband7/videos/video_standing_thumb.png\n"," create mode 100644 data/rwhar/proband7/videos/video_walking_thumb.png\n"," create mode 100644 data/rwhar/proband8/data/acc_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/acc_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/acc_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/acc_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/acc_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/acc_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/acc_lying_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/acc_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/acc_running_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/acc_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/acc_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/acc_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/acc_standing_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/acc_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/acc_walking_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/acc_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/gps_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/gps_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/gps_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/gps_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/gps_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/gps_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/gps_lying_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/gps_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/gps_running_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/gps_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/gps_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/gps_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/gps_standing_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/gps_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/gps_walking_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/gps_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/gyr_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/gyr_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/gyr_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/gyr_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/gyr_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/gyr_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/gyr_lying_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/gyr_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/gyr_running_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/gyr_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/gyr_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/gyr_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/gyr_standing_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/gyr_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/gyr_walking_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/gyr_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/lig_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/lig_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/lig_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/lig_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/lig_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/lig_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/lig_lying_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/lig_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/lig_running_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/lig_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/lig_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/lig_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/lig_standing_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/lig_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/lig_walking_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/lig_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/mag_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/mag_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/mag_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/mag_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/mag_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/mag_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/mag_lying_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/mag_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/mag_running_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/mag_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/mag_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/mag_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/mag_standing_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/mag_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/mag_walking_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/mag_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/mic_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/mic_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/mic_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/mic_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/mic_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/mic_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/mic_lying_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/mic_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/mic_running_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/mic_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/mic_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/mic_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/mic_standing_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/mic_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/data/mic_walking_csv.zip\n"," create mode 100644 data/rwhar/proband8/data/mic_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband8/images/chest.png\n"," create mode 100644 data/rwhar/proband8/images/chest_thumb.png\n"," create mode 100644 data/rwhar/proband8/images/forearm.png\n"," create mode 100644 data/rwhar/proband8/images/forearm_thumb.png\n"," create mode 100644 data/rwhar/proband8/images/head.png\n"," create mode 100644 data/rwhar/proband8/images/head_thumb.png\n"," create mode 100644 data/rwhar/proband8/images/overview.png\n"," create mode 100644 data/rwhar/proband8/images/overview_thumb.png\n"," create mode 100644 data/rwhar/proband8/images/preview.png\n"," create mode 100644 data/rwhar/proband8/images/shin.png\n"," create mode 100644 data/rwhar/proband8/images/shin_thumb.png\n"," create mode 100644 data/rwhar/proband8/images/thigh.png\n"," create mode 100644 data/rwhar/proband8/images/thigh_thumb.png\n"," create mode 100644 data/rwhar/proband8/images/upperarm.png\n"," create mode 100644 data/rwhar/proband8/images/upperarm_thumb.png\n"," create mode 100644 data/rwhar/proband8/images/waist.png\n"," create mode 100644 data/rwhar/proband8/images/waist_thumb.png\n"," create mode 100644 data/rwhar/proband8/videos/video_climbing down_thumb.png\n"," create mode 100644 data/rwhar/proband8/videos/video_climbing up_thumb.png\n"," create mode 100644 data/rwhar/proband8/videos/video_jumping_thumb.png\n"," create mode 100644 data/rwhar/proband8/videos/video_lying_thumb.png\n"," create mode 100644 data/rwhar/proband8/videos/video_running_thumb.png\n"," create mode 100644 data/rwhar/proband8/videos/video_sitting_thumb.png\n"," create mode 100644 data/rwhar/proband8/videos/video_standing_thumb.png\n"," create mode 100644 data/rwhar/proband8/videos/video_walking_thumb.png\n"," create mode 100644 data/rwhar/proband9/data/acc_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/acc_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/acc_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/acc_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/acc_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/acc_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/acc_lying_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/acc_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/acc_running_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/acc_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/acc_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/acc_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/acc_standing_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/acc_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/acc_walking_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/acc_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/gps_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/gps_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/gps_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/gps_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/gps_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/gps_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/gps_lying_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/gps_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/gps_running_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/gps_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/gps_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/gps_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/gps_standing_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/gps_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/gps_walking_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/gps_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/gyr_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/gyr_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/gyr_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/gyr_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/gyr_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/gyr_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/gyr_lying_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/gyr_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/gyr_running_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/gyr_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/gyr_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/gyr_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/gyr_standing_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/gyr_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/gyr_walking_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/gyr_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/lig_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/lig_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/lig_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/lig_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/lig_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/lig_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/lig_lying_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/lig_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/lig_running_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/lig_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/lig_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/lig_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/lig_standing_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/lig_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/lig_walking_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/lig_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/mag_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/mag_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/mag_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/mag_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/mag_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/mag_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/mag_lying_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/mag_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/mag_running_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/mag_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/mag_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/mag_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/mag_standing_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/mag_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/mag_walking_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/mag_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/mic_climbingdown_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/mic_climbingdown_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/mic_climbingup_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/mic_climbingup_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/mic_jumping_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/mic_jumping_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/mic_lying_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/mic_lying_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/mic_running_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/mic_running_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/mic_sitting_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/mic_sitting_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/mic_standing_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/mic_standing_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/data/mic_walking_csv.zip\n"," create mode 100644 data/rwhar/proband9/data/mic_walking_sqlite.zip\n"," create mode 100644 data/rwhar/proband9/images/chest.png\n"," create mode 100644 data/rwhar/proband9/images/chest_thumb.png\n"," create mode 100644 data/rwhar/proband9/images/forearm.png\n"," create mode 100644 data/rwhar/proband9/images/forearm_thumb.png\n"," create mode 100644 data/rwhar/proband9/images/head.png\n"," create mode 100644 data/rwhar/proband9/images/head_thumb.png\n"," create mode 100644 data/rwhar/proband9/images/overview.png\n"," create mode 100644 data/rwhar/proband9/images/overview_thumb.png\n"," create mode 100644 data/rwhar/proband9/images/preview.png\n"," create mode 100644 data/rwhar/proband9/images/shin.png\n"," create mode 100644 data/rwhar/proband9/images/shin_thumb.png\n"," create mode 100644 data/rwhar/proband9/images/thigh.png\n"," create mode 100644 data/rwhar/proband9/images/thigh_thumb.png\n"," create mode 100644 data/rwhar/proband9/images/upperarm.png\n"," create mode 100644 data/rwhar/proband9/images/upperarm_thumb.png\n"," create mode 100644 data/rwhar/proband9/images/waist.png\n"," create mode 100644 data/rwhar/proband9/images/waist_thumb.png\n"," create mode 100644 data/rwhar/proband9/videos/video_climbing down_thumb.png\n"," create mode 100644 data/rwhar/proband9/videos/video_climbing up_thumb.png\n"," create mode 100644 data/rwhar/proband9/videos/video_jumping_thumb.png\n"," create mode 100644 data/rwhar/proband9/videos/video_lying_thumb.png\n"," create mode 100644 data/rwhar/proband9/videos/video_running_thumb.png\n"," create mode 100644 data/rwhar/proband9/videos/video_sitting_thumb.png\n"," create mode 100644 data/rwhar/proband9/videos/video_standing_thumb.png\n"," create mode 100644 data/rwhar/proband9/videos/video_walking_thumb.png\n"," create mode 100644 data/rwhar/realworld2016_dataset.zip\n"," create mode 100644 logs/env.txt\n"," create mode 100644 logs/init_meta.json\n"," create mode 100755 sample_data/README.md\n"," create mode 100755 sample_data/anscombe.json\n"," create mode 100644 sample_data/california_housing_test.csv\n"," create mode 100644 sample_data/california_housing_train.csv\n"," create mode 100644 sample_data/mnist_test.csv\n"," create mode 100644 sample_data/mnist_train_small.csv\n","✓ Git commit hash: e1331102\n","✓ Raw data moved to data/raw/\n","✓ Computed checksums for 1814 files → logs/checksums.txt\n","✓ Data source recorded to logs/data_source.txt\n","[master 5c8df8e] data: add RealWorld2016 checksums and source\n"," 2 files changed, 1823 insertions(+)\n"," create mode 100644 logs/checksums.txt\n"," create mode 100644 logs/data_source.txt\n","\n","============================================================\n","Project initialization and data acquisition completed\n","============================================================\n"]}]},{"cell_type":"code","source":["# ================ Step 2: Sensor/Location Selection (Revised) ================\n","import pandas as pd\n","from pathlib import Path\n","import json\n","import zipfile\n","\n","print(\"Step 2: Sensor/Location Selection\")\n","print(\"=\" * 60)\n","\n","raw_dir = Path('/content/data/raw')\n","\n","# Decompress all zip files first\n","print(\"Extracting sensor data...\")\n","zip_files = list(raw_dir.rglob('*.zip'))\n","print(f\"Found {len(zip_files)} zip files\")\n","\n","for zip_path in zip_files:\n","    if 'csv.zip' in zip_path.name:\n","        extract_dir = zip_path.parent / zip_path.stem\n","        if not extract_dir.exists():\n","            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","                zip_ref.extractall(extract_dir)\n","\n","print(\"✓ Extraction complete\")\n","\n","# Search for CSV files under acc and gyr directories\n","print(\"\\nSearching for sensor directories...\")\n","acc_dirs = list(raw_dir.rglob('acc_*_csv'))\n","gyr_dirs = list(raw_dir.rglob('gyr_*_csv'))\n","\n","print(f\"✓ Found {len(acc_dirs)} ACC directories\")\n","print(f\"✓ Found {len(gyr_dirs)} GYR directories\")\n","\n","if acc_dirs:\n","    print(f\"\\nExample ACC directory: {acc_dirs[0].relative_to(raw_dir)}\")\n","    sample_files = list(acc_dirs[0].glob('*.csv'))\n","    print(f\"Number of files under {acc_dirs[0].name}: {len(sample_files)}\")\n","    if sample_files:\n","        print(f\"Example file: {sample_files[0].name}\")\n","\n","# Find all files containing \"waist\"\n","waist_files = {'acc': [], 'gyr': []}\n","\n","for acc_dir in acc_dirs:\n","    for f in acc_dir.glob('*waist*.csv'):\n","        waist_files['acc'].append(f)\n","\n","for gyr_dir in gyr_dirs:\n","    for f in gyr_dir.glob('*waist*.csv'):\n","        waist_files['gyr'].append(f)\n","\n","print(f\"\\n✓ Found Waist-ACC files: {len(waist_files['acc'])}\")\n","print(f\"✓ Found Waist-GYR files: {len(waist_files['gyr'])}\")\n","\n","# Display example files\n","if waist_files['acc']:\n","    print(f\"\\nExample ACC file: {waist_files['acc'][0].relative_to(raw_dir)}\")\n","    sample_acc = pd.read_csv(waist_files['acc'][0])\n","    print(f\"Columns: {list(sample_acc.columns)}\")\n","    print(f\"Shape: {sample_acc.shape}\")\n","    print(sample_acc.head(3))\n","\n","if waist_files['gyr']:\n","    print(f\"\\nExample GYR file: {waist_files['gyr'][0].relative_to(raw_dir)}\")\n","    sample_gyr = pd.read_csv(waist_files['gyr'][0])\n","    print(f\"Columns: {list(sample_gyr.columns)}\")\n","    print(f\"Shape: {sample_gyr.shape}\")\n","    print(sample_gyr.head(3))\n","\n","# Collect metadata\n","waist_metadata = []\n","for sensor_type in ['acc', 'gyr']:\n","    for filepath in waist_files[sensor_type]:\n","        parts = filepath.parts\n","        subject = [p for p in parts if p.startswith('proband')][0]\n","        activity = filepath.parent.name.split('_')[1]\n","\n","        df = pd.read_csv(filepath)\n","        waist_metadata.append({\n","            'subject': subject,\n","            'activity': activity,\n","            'sensor': sensor_type,\n","            'original_path': str(filepath.relative_to(raw_dir)),\n","            'shape': list(df.shape),\n","            'columns': list(df.columns)\n","        })\n","\n","# Persist selection report\n","with open('/content/logs/sensor_selection.json', 'w') as f:\n","    json.dump({\n","        'selection': {\n","            'position': 'waist',\n","            'sensors': ['acc', 'gyr'],\n","            'channels': 6,\n","            'rationale': 'Single position to avoid domain shift; ACC+GYRO is the standard configuration for HAR'\n","        },\n","        'files_found': {\n","            'acc': len(waist_files['acc']),\n","            'gyr': len(waist_files['gyr'])\n","        },\n","        'metadata': waist_metadata[:10]\n","    }, f, indent=2)\n","\n","print(f\"\\n✓ Selection report saved: logs/sensor_selection.json\")\n","\n","!git add logs/sensor_selection.json\n","!git commit -m \"data: select waist position with acc+gyr sensors\"\n","\n","\n","# ================ Step 3: Column Alignment and Naming ================\n","print(\"\\n\\nStep 3: Column Alignment and Naming\")\n","print(\"=\" * 60)\n","\n","# Analyze column names\n","acc_cols = set()\n","gyr_cols = set()\n","\n","for filepath in waist_files['acc'][:3]:\n","    df = pd.read_csv(filepath)\n","    acc_cols.update(df.columns)\n","\n","for filepath in waist_files['gyr'][:3]:\n","    df = pd.read_csv(filepath)\n","    gyr_cols.update(df.columns)\n","\n","print(f\"ACC column names: {sorted(acc_cols)}\")\n","print(f\"GYR column names: {sorted(gyr_cols)}\")\n","\n","# Define standard mapping\n","standard_mapping = {\n","    'acc': {\n","        'attr_x': 'acc_x',\n","        'attr_y': 'acc_y',\n","        'attr_z': 'acc_z',\n","        'attr_time': 'timestamp'\n","    },\n","    'gyr': {\n","        'attr_x': 'gyro_x',\n","        'attr_y': 'gyro_y',\n","        'attr_z': 'gyro_z',\n","        'attr_time': 'timestamp'\n","    }\n","}\n","\n","cols_config = {\n","    'standard_columns': ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z'],\n","    'units': {\n","        'acc_x': 'm/s²', 'acc_y': 'm/s²', 'acc_z': 'm/s²',\n","        'gyro_x': 'rad/s', 'gyro_y': 'rad/s', 'gyro_z': 'rad/s'\n","    },\n","    'mapping': standard_mapping,\n","    'timestamp_col': 'timestamp'\n","}\n","\n","with open('/content/configs/cols.json', 'w') as f:\n","    json.dump(cols_config, f, indent=2)\n","\n","print(\"\\n✓ Column mapping configuration saved: configs/cols.json\")\n","\n","# Generate schema report\n","report = [\n","    \"# RealWorld2016 Data Schema Report\\n\\n\",\n","    f\"Generated at: {datetime.now().isoformat()}\\n\\n\",\n","    \"## Standard column definitions\\n\\n\",\n","    \"| Column | Unit | Description |\\n|------|------|------|\\n\"\n","]\n","\n","for col in cols_config['standard_columns']:\n","    unit = cols_config['units'][col]\n","    sensor = 'Accelerometer' if 'acc' in col else 'Gyroscope'\n","    axis = col.split('_')[1].upper()\n","    report.append(f\"| {col} | {unit} | {sensor} {axis}-axis |\\n\")\n","\n","report.append(\"\\n## Original column mapping\\n\\n### Accelerometer\\n\")\n","for orig, std in standard_mapping['acc'].items():\n","    report.append(f\"- `{orig}` → `{std}`\\n\")\n","\n","report.append(\"\\n### Gyroscope\\n\")\n","for orig, std in standard_mapping['gyr'].items():\n","    report.append(f\"- `{orig}` → `{std}`\\n\")\n","\n","# Missing-value statistics\n","report.append(\"\\n## Data quality checks\\n\\n\")\n","for sensor in ['acc', 'gyr']:\n","    report.append(f\"### {sensor.upper()} Missing values (sample of 5 files)\\n\\n\")\n","    has_missing = False\n","    for fp in waist_files[sensor][:5]:\n","        df = pd.read_csv(fp)\n","        missing = df.isnull().sum()\n","        if missing.sum() > 0:\n","            report.append(f\"- {fp.name}: {missing[missing > 0].to_dict()}\\n\")\n","            has_missing = True\n","    if not has_missing:\n","        report.append(\"- No missing values ✓\\n\")\n","    report.append(\"\\n\")\n","\n","with open('/content/logs/schema_report.md', 'w') as f:\n","    f.writelines(report)\n","\n","print(\"✓ Schema report saved: logs/schema_report.md\")\n","print(\"\\n\" + \"\".join(report))\n","\n","!git add configs/cols.json logs/schema_report.md\n","!git commit -m \"data: standardize column names and units\"\n","\n","print(f\"\\n{'='*60}\")\n","print(\"Steps 2–3 completed\")\n","print(f\"{'='*60}\")"],"metadata":{"id":"e40d668gam_J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762895069713,"user_tz":0,"elapsed":27337,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"c25e7c03-d194-47ec-f576-cc5286ec7076"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Step 2: Sensor/Location Selection\n","============================================================\n","Extracting sensor data...\n","Found 1441 zip files\n","✓ Extraction complete\n","\n","Searching for sensor directories...\n","✓ Found 120 ACC directories\n","✓ Found 120 GYR directories\n","\n","Example ACC directory: proband4/data/acc_climbingdown_csv\n","Number of files under acc_climbingdown_csv: 0\n","\n","✓ Found Waist-ACC files: 114\n","✓ Found Waist-GYR files: 114\n","\n","Example ACC file: proband4/data/acc_walking_csv/acc_walking_2_waist.csv\n","Columns: ['id', 'attr_time', 'attr_x', 'attr_y', 'attr_z']\n","Shape: (31031, 5)\n","   id      attr_time    attr_x    attr_y    attr_z\n","0   1  1436291534801  9.831789 -0.413000  1.091157\n","1   2  1436291534820  9.919178 -0.492607  1.191714\n","2   3  1436291534839  9.939528 -0.612317  1.142034\n","\n","Example GYR file: proband4/data/gyr_jumping_csv/Gyroscope_jumping_waist.csv\n","Columns: ['id', 'attr_time', 'attr_x', 'attr_y', 'attr_z']\n","Shape: (4292, 5)\n","   id      attr_time    attr_x    attr_y    attr_z\n","0   1  1436295094022 -0.002988 -0.022363 -0.004502\n","1   2  1436295094041  0.003121 -0.036413 -0.002364\n","2   3  1436295094062  0.016865 -0.037329 -0.006945\n","\n","✓ Selection report saved: logs/sensor_selection.json\n","[master d0cdb28] data: select waist position with acc+gyr sensors\n"," 1 file changed, 187 insertions(+)\n"," create mode 100644 logs/sensor_selection.json\n","\n","\n","Step 3: Column Alignment and Naming\n","============================================================\n","ACC column names: ['attr_time', 'attr_x', 'attr_y', 'attr_z', 'id']\n","GYR column names: ['attr_time', 'attr_x', 'attr_y', 'attr_z', 'id']\n","\n","✓ Column mapping configuration saved: configs/cols.json\n","✓ Schema report saved: logs/schema_report.md\n","\n","# RealWorld2016 Data Schema Report\n","\n","Generated at: 2025-11-11T21:04:29.921673\n","\n","## Standard column definitions\n","\n","| Column | Unit | Description |\n","|------|------|------|\n","| acc_x | m/s² | Accelerometer X-axis |\n","| acc_y | m/s² | Accelerometer Y-axis |\n","| acc_z | m/s² | Accelerometer Z-axis |\n","| gyro_x | rad/s | Gyroscope X-axis |\n","| gyro_y | rad/s | Gyroscope Y-axis |\n","| gyro_z | rad/s | Gyroscope Z-axis |\n","\n","## Original column mapping\n","\n","### Accelerometer\n","- `attr_x` → `acc_x`\n","- `attr_y` → `acc_y`\n","- `attr_z` → `acc_z`\n","- `attr_time` → `timestamp`\n","\n","### Gyroscope\n","- `attr_x` → `gyro_x`\n","- `attr_y` → `gyro_y`\n","- `attr_z` → `gyro_z`\n","- `attr_time` → `timestamp`\n","\n","## Data quality checks\n","\n","### ACC Missing values (sample of 5 files)\n","\n","- No missing values ✓\n","\n","### GYR Missing values (sample of 5 files)\n","\n","- No missing values ✓\n","\n","\n","[master 406b391] data: standardize column names and units\n"," 2 files changed, 72 insertions(+)\n"," create mode 100644 configs/cols.json\n"," create mode 100644 logs/schema_report.md\n","\n","============================================================\n","Steps 2–3 completed\n","============================================================\n"]}]},{"cell_type":"code","source":["# ================ Step 4: Timeline Normalization (Final) ================\n","import numpy as np\n","import pandas as pd\n","from scipy import interpolate\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import json\n","import zipfile\n","\n","print(\"\\n\\nStep 4: Timeline Normalization\")\n","print(\"=\" * 60)\n","\n","raw_dir = Path('/content/data/raw')\n","\n","# Decompression\n","print(\"Extracting waist data...\")\n","for proband_dir in raw_dir.glob('proband*'):\n","    data_dir = proband_dir / 'data'\n","    if data_dir.exists():\n","        for zip_file in data_dir.glob('*_csv.zip'):\n","            if zip_file.stem.startswith(('acc_', 'gyr_')):\n","                extract_dir = zip_file.parent / zip_file.stem\n","                if not extract_dir.exists():\n","                    with zipfile.ZipFile(zip_file, 'r') as zf:\n","                        if any('waist' in f.lower() for f in zf.namelist()):\n","                            zf.extractall(extract_dir)\n","\n","# Scan\n","waist_files = {'acc': [], 'gyr': []}\n","for csv_file in raw_dir.rglob('*.csv'):\n","    if 'waist' in csv_file.name.lower():\n","        if csv_file.parent.name.startswith('acc_'):\n","            waist_files['acc'].append(csv_file)\n","        elif csv_file.parent.name.startswith('gyr_'):\n","            waist_files['gyr'].append(csv_file)\n","\n","print(f\"✓ ACC: {len(waist_files['acc'])}, GYR: {len(waist_files['gyr'])}\")\n","\n","# Improved pairing: directory mapping + same-name preference\n","def find_gyr_for_acc(acc_path):\n","    gyr_dir = acc_path.parent.parent / acc_path.parent.name.replace('acc_', 'gyr_')\n","    if not gyr_dir.exists():\n","        return None\n","    cand = gyr_dir / acc_path.name.replace('acc_', 'gyr_')\n","    if cand.exists():\n","        return cand\n","    cands = sorted(gyr_dir.glob('*waist*.csv'))\n","    return cands[0] if cands else None\n","\n","file_pairs = []\n","for acc_path in waist_files['acc']:\n","    gyr_path = find_gyr_for_acc(acc_path)\n","    if not gyr_path:\n","        continue\n","    proband = next(p for p in acc_path.parts if p.startswith('proband'))\n","    activity = acc_path.parent.name.split('_')[1]\n","    file_pairs.append((acc_path, gyr_path, proband, activity))\n","\n","print(f\"✓ File pairs: {len(file_pairs)}\")\n","\n","with open('/content/configs/cols.json', 'r') as f:\n","    cols_config = json.load(f)\n","\n","TARGET_FS = 50\n","MAX_GAP_MS = 200\n","MIN_DURATION_S = 1.0\n","interim_dir = Path('/content/interim')\n","interim_dir.mkdir(exist_ok=True)\n","\n","def detect_time_unit(df, col='timestamp'):\n","    ts = df[col].sort_values().iloc[:200].values\n","    diffs = np.diff(ts)\n","    diffs = diffs[diffs > 0]\n","    if len(diffs) == 0:\n","        return None, None\n","    dt = np.median(diffs)\n","\n","    if 0.01 < dt < 5:\n","        return df[col] * 1e9, 's'\n","    elif 10 < dt < 100:\n","        return df[col] * 1e6, 'ms'\n","    elif 10000 < dt < 100000:\n","        return df[col] * 1e3, 'us'\n","    elif 1e7 < dt < 1e8:\n","        return df[col], 'ns'\n","    else:\n","        return None, None\n","\n","all_stats = []\n","skipped = []\n","\n","for idx, (acc_path, gyr_path, proband, activity) in enumerate(file_pairs):\n","    print(f\"\\n[{idx+1}/{len(file_pairs)}] {proband}/{activity}\")\n","\n","    acc_df = pd.read_csv(acc_path).rename(columns=cols_config['mapping']['acc'])\n","    gyr_df = pd.read_csv(gyr_path).rename(columns=cols_config['mapping']['gyr'])\n","\n","    acc_ts_ns, acc_unit = detect_time_unit(acc_df)\n","    gyr_ts_ns, gyr_unit = detect_time_unit(gyr_df)\n","\n","    if acc_ts_ns is None or gyr_ts_ns is None:\n","        print(f\"  ⚠️ Skipped: unable to determine timestamp unit\")\n","        skipped.append(f\"{proband}_{activity}\")\n","        continue\n","\n","    acc_df['timestamp_ns'] = acc_ts_ns\n","    gyr_df['timestamp_ns'] = gyr_ts_ns\n","    acc_df = acc_df[['timestamp_ns', 'acc_x', 'acc_y', 'acc_z']].sort_values('timestamp_ns').drop_duplicates('timestamp_ns')\n","    gyr_df = gyr_df[['timestamp_ns', 'gyro_x', 'gyro_y', 'gyro_z']].sort_values('timestamp_ns').drop_duplicates('timestamp_ns')\n","\n","    df = None\n","    merge_mode = 'absolute'\n","    merge_tol = None\n","    offset_ns = 0\n","\n","    # Adaptive tolerance\n","    for tol_ms in [10, 30, 50, 100]:\n","        tol_ns = int(tol_ms * 1e6)\n","        df_try = pd.merge_asof(acc_df, gyr_df, on='timestamp_ns', direction='nearest', tolerance=tol_ns).dropna()\n","        if len(df_try) >= TARGET_FS:\n","            df = df_try\n","            merge_tol = tol_ms\n","            break\n","\n","    # Fallback 1: relative time (relaxed thresholds)\n","    if df is None:\n","        for tol_ms in [10, 30, 50]:\n","            acc_tmp = acc_df.copy()\n","            gyr_tmp = gyr_df.copy()\n","            acc_tmp['t_rel'] = acc_tmp['timestamp_ns'] - acc_tmp['timestamp_ns'].iloc[0]\n","            gyr_tmp['t_rel'] = gyr_tmp['timestamp_ns'] - gyr_tmp['timestamp_ns'].iloc[0]\n","\n","            df_try = pd.merge_asof(acc_tmp.sort_values('t_rel'), gyr_tmp.sort_values('t_rel'),\n","                                   on='t_rel', direction='nearest', tolerance=int(tol_ms*1e6)).dropna()\n","\n","            if len(df_try) > 1:\n","                p99 = (df_try['t_rel'].diff() / 1e6).quantile(0.99)\n","                match_rate = len(df_try) / max(1, min(len(acc_df), len(gyr_df)))\n","\n","                if len(df_try) >= TARGET_FS and p99 <= 40 and match_rate >= 0.5:\n","                    df = df_try.rename(columns={'t_rel': 'timestamp_ns'})\n","                    merge_mode = 'relative'\n","                    merge_tol = tol_ms\n","                    break\n","\n","    # Fallback 2: offset search (broaden range and thresholds)\n","    if df is None:\n","        best_df, best_matches, best_offset = None, -1, 0\n","        for offset_ms in range(-3000, 3001, 50):\n","            gyr_shift = gyr_df.copy()\n","            gyr_shift['timestamp_ns'] = gyr_shift['timestamp_ns'] + int(offset_ms * 1e6)\n","            df_try = pd.merge_asof(acc_df, gyr_shift, on='timestamp_ns',\n","                                   direction='nearest', tolerance=int(30*1e6)).dropna()\n","            if len(df_try) > best_matches:\n","                best_df, best_matches, best_offset = df_try, len(df_try), offset_ms\n","\n","        if best_matches >= TARGET_FS and best_df is not None and len(best_df) > 1:\n","            p99 = (best_df['timestamp_ns'].diff() / 1e6).quantile(0.99)\n","            match_rate = best_matches / max(1, min(len(acc_df), len(gyr_df)))\n","\n","            if p99 <= 40 and match_rate >= 0.5:\n","                df = best_df\n","                merge_mode = 'offset_search'\n","                merge_tol = 30\n","                offset_ns = int(best_offset * 1e6)\n","\n","    # Fallback 3: intersection window resampling\n","    if df is None:\n","        t0 = max(acc_df['timestamp_ns'].iloc[0], gyr_df['timestamp_ns'].iloc[0])\n","        t1 = min(acc_df['timestamp_ns'].iloc[-1], gyr_df['timestamp_ns'].iloc[-1])\n","\n","        if t1 - t0 >= 1e9:\n","            STEP_NS = int(1e9 / TARGET_FS)\n","            t_grid = np.arange(t0, t1, STEP_NS, dtype=np.int64)\n","\n","            acc_interp = interpolate.interp1d(acc_df['timestamp_ns'].values,\n","                                              acc_df[['acc_x', 'acc_y', 'acc_z']].values,\n","                                              axis=0, kind='linear', bounds_error=True)\n","            gyr_interp = interpolate.interp1d(gyr_df['timestamp_ns'].values,\n","                                              gyr_df[['gyro_x', 'gyro_y', 'gyro_z']].values,\n","                                              axis=0, kind='linear', bounds_error=True)\n","\n","            acc_vals = acc_interp(t_grid)\n","            gyr_vals = gyr_interp(t_grid)\n","\n","            df = pd.DataFrame({\n","                'timestamp': t_grid,\n","                'segment_id': 0,\n","                'proband': proband,\n","                'activity': activity,\n","                'acc_x': acc_vals[:, 0], 'acc_y': acc_vals[:, 1], 'acc_z': acc_vals[:, 2],\n","                'gyro_x': gyr_vals[:, 0], 'gyro_y': gyr_vals[:, 1], 'gyro_z': gyr_vals[:, 2]\n","            })\n","\n","            out_name = f\"{proband}_{activity}_waist.csv\"\n","            df.to_csv(interim_dir / out_name, index=False)\n","\n","            all_stats.append({\n","                'file': out_name,\n","                'proband': proband,\n","                'activity': activity,\n","                'acc_unit': acc_unit,\n","                'gyr_unit': gyr_unit,\n","                'merge_mode': 'intersection',\n","                'segments': 1,\n","                'samples': len(df)\n","            })\n","\n","            print(f\"  {acc_unit}/{gyr_unit}, intersection, 1 segment, {len(df)} samples\")\n","            continue\n","\n","    if df is None or len(df) < TARGET_FS:\n","        print(f\"  ⚠️ Skipped: merge failed\")\n","        skipped.append(f\"{proband}_{activity}\")\n","        continue\n","\n","    df = df.reset_index(drop=True)\n","    df['dt_ms'] = df['timestamp_ns'].diff() / 1e6\n","\n","    # Segmentation\n","    gaps = df['dt_ms'].values\n","    large_gap_idx = np.where(gaps > MAX_GAP_MS)[0]\n","    split_points = [0] + large_gap_idx.tolist() + [len(df)]\n","\n","    segments = []\n","    for i in range(len(split_points) - 1):\n","        seg = df.iloc[split_points[i]:split_points[i + 1]].copy()\n","        if len(seg) > 1:\n","            duration_s = (seg['timestamp_ns'].iloc[-1] - seg['timestamp_ns'].iloc[0]) / 1e9\n","            if duration_s >= MIN_DURATION_S:\n","                segments.append(seg)\n","\n","    if len(segments) == 0:\n","        print(f\"  ⚠️ Skipped: no valid segments\")\n","        skipped.append(f\"{proband}_{activity}\")\n","        continue\n","\n","    # Resampling\n","    STEP_NS = int(1e9 / TARGET_FS)\n","    all_resampled = []\n","    for seg_id, seg in enumerate(segments):\n","        t_start = seg['timestamp_ns'].iloc[0]\n","        t_end = seg['timestamp_ns'].iloc[-1]\n","        t_grid = np.arange(t_start, t_end + 1, STEP_NS, dtype=np.int64)\n","\n","        df_seg = pd.DataFrame({\n","            'timestamp': t_grid,\n","            'segment_id': seg_id,\n","            'proband': proband,\n","            'activity': activity\n","        })\n","        for col in ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']:\n","            f = interpolate.interp1d(seg['timestamp_ns'], seg[col], kind='linear', bounds_error=True)\n","            df_seg[col] = f(t_grid)\n","\n","        all_resampled.append(df_seg)\n","\n","    df_final = pd.concat(all_resampled, ignore_index=True)\n","\n","    out_name = f\"{proband}_{activity}_waist.csv\"\n","    df_final.to_csv(interim_dir / out_name, index=False)\n","\n","    stat = {\n","        'file': out_name,\n","        'proband': proband,\n","        'activity': activity,\n","        'acc_unit': acc_unit,\n","        'gyr_unit': gyr_unit,\n","        'merge_mode': merge_mode,\n","        'merge_tolerance_ms': merge_tol,\n","        'segments': len(segments),\n","        'samples': len(df_final)\n","    }\n","    if merge_mode == 'offset_search':\n","        stat['offset_ns'] = offset_ns\n","\n","    all_stats.append(stat)\n","\n","    mode_str = f\"{merge_mode}\" + (f\"(Δ={offset_ns/1e6:.0f}ms)\" if merge_mode=='offset_search' else '')\n","    print(f\"  {acc_unit}/{gyr_unit}, {mode_str}, {len(segments)} segments, {len(df_final)} samples\")\n","\n","print(f\"\\n✓ Completed {len(all_stats)} files\")\n","if skipped:\n","    print(f\"⚠️ Skipped {len(skipped)}: {skipped}\")\n","\n","# Plotting\n","if all_stats:\n","    first_file = all_stats[0]\n","    first_pair = [(p[0], p[1], p[2], p[3]) for p in file_pairs if p[2] == first_file['proband'] and p[3] == first_file['activity']][0]\n","\n","    acc_df = pd.read_csv(first_pair[0]).rename(columns=cols_config['mapping']['acc'])\n","    gyr_df = pd.read_csv(first_pair[1]).rename(columns=cols_config['mapping']['gyr'])\n","    acc_ts_ns, _ = detect_time_unit(acc_df)\n","    gyr_ts_ns, _ = detect_time_unit(gyr_df)\n","    acc_df['timestamp_ns'] = acc_ts_ns\n","    gyr_df['timestamp_ns'] = gyr_ts_ns\n","    acc_df = acc_df[['timestamp_ns', 'acc_x', 'acc_y', 'acc_z']].sort_values('timestamp_ns').drop_duplicates('timestamp_ns')\n","    gyr_df = gyr_df[['timestamp_ns', 'gyro_x', 'gyro_y', 'gyro_z']].sort_values('timestamp_ns').drop_duplicates('timestamp_ns')\n","\n","    df = pd.merge_asof(acc_df, gyr_df, on='timestamp_ns', direction='nearest', tolerance=int(100*1e6)).dropna()\n","    intervals = df['timestamp_ns'].diff() / 1e6\n","\n","    fig, ax = plt.subplots(figsize=(10, 4))\n","    ax.hist(intervals[intervals < 100], bins=100, edgecolor='black', linewidth=0.5)\n","    ax.axvline(20, color='red', linestyle='--', label='Ideal (50Hz=20ms)')\n","    ax.axvline(MAX_GAP_MS, color='orange', linestyle='--', label=f'Threshold ({MAX_GAP_MS}ms)')\n","    ax.set_xlabel('Sampling Interval (ms)')\n","    ax.set_ylabel('Count')\n","    ax.set_title(f'Sampling Interval Distribution - {first_pair[2]}/{first_pair[3]}')\n","    ax.legend()\n","    ax.grid(alpha=0.3)\n","    plt.tight_layout()\n","    plt.savefig('/content/figures/step4_interval_hist.png', dpi=150)\n","    plt.close()\n","\n","with open('/content/logs/step4_summary.json', 'w') as f:\n","    json.dump({'files': all_stats, 'skipped': skipped}, f, indent=2)\n","\n","!git add figures/ logs/step4_*.json interim/\n","!git commit -m \"preproc: final time normalization with all fallbacks\"\n","\n","print(f\"\\n{'='*60}\\nStep 4 completed\\n{'='*60}\")"],"metadata":{"id":"NBnsnPzRaoJo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762895129382,"user_tz":0,"elapsed":59591,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"43f44ef5-5392-4b87-c2f2-099d931b59dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Step 4: Timeline Normalization\n","============================================================\n","Extracting waist data...\n","✓ ACC: 114, GYR: 114\n","✓ File pairs: 114\n","\n","[1/114] proband4/walking\n","  ms/ms, absolute, 13 segments, 30482 samples\n","\n","[2/114] proband4/standing\n","  ms/ms, absolute, 12 segments, 29741 samples\n","\n","[3/114] proband4/jumping\n","  ms/ms, absolute, 3 segments, 4148 samples\n","\n","[4/114] proband4/lying\n","  ms/ms, absolute, 16 segments, 33106 samples\n","\n","[5/114] proband4/running\n","  ms/ms, absolute, 40 segments, 50541 samples\n","\n","[6/114] proband4/sitting\n","  ms/ms, absolute, 14 segments, 31248 samples\n","\n","[7/114] proband14/walking\n","  ms/ms, absolute, 50 segments, 32007 samples\n","\n","[8/114] proband14/standing\n","  ms/ms, absolute, 34 segments, 30114 samples\n","\n","[9/114] proband14/jumping\n","  ms/ms, absolute, 5 segments, 4642 samples\n","\n","[10/114] proband14/lying\n","  ms/ms, absolute, 27 segments, 30563 samples\n","\n","[11/114] proband14/running\n","  ms/ms, absolute, 33 segments, 29786 samples\n","\n","[12/114] proband14/sitting\n","  ms/ms, absolute, 31 segments, 30562 samples\n","\n","[13/114] proband7/walking\n","  ms/ms, absolute, 27 segments, 29987 samples\n","\n","[14/114] proband7/standing\n","  ms/ms, absolute, 24 segments, 32005 samples\n","\n","[15/114] proband7/jumping\n","  ms/ms, absolute, 6 segments, 4853 samples\n","\n","[16/114] proband7/lying\n","  ms/ms, absolute, 29 segments, 31520 samples\n","\n","[17/114] proband7/running\n","  ms/ms, absolute, 27 segments, 35636 samples\n","\n","[18/114] proband7/sitting\n","  ms/ms, absolute, 23 segments, 31417 samples\n","\n","[19/114] proband1/climbingdown\n","  ms/ms, absolute, 17 segments, 24607 samples\n","\n","[20/114] proband1/walking\n","  ms/ms, absolute, 14 segments, 31332 samples\n","\n","[21/114] proband1/standing\n","  ms/ms, absolute, 22 segments, 30988 samples\n","\n","[22/114] proband1/jumping\n","  ms/ms, absolute, 4 segments, 4214 samples\n","\n","[23/114] proband1/climbingup\n","  ms/ms, absolute, 25 segments, 31752 samples\n","\n","[24/114] proband1/lying\n","  ⚠️ Skipped: unable to determine timestamp unit\n","\n","[25/114] proband1/running\n","  ms/ms, absolute, 14 segments, 30013 samples\n","\n","[26/114] proband1/sitting\n","  ⚠️ Skipped: merge failed\n","\n","[27/114] proband9/climbingdown\n","  ms/ms, absolute, 18 segments, 24302 samples\n","\n","[28/114] proband9/walking\n","  ms/ms, absolute, 25 segments, 30358 samples\n","\n","[29/114] proband9/standing\n","  ms/ms, absolute, 18 segments, 31296 samples\n","\n","[30/114] proband9/jumping\n","  ms/ms, absolute, 4 segments, 4976 samples\n","\n","[31/114] proband9/climbingup\n","  ms/ms, absolute, 21 segments, 26388 samples\n","\n","[32/114] proband9/lying\n","  ms/ms, absolute, 15 segments, 30587 samples\n","\n","[33/114] proband9/running\n","  ms/ms, absolute, 41 segments, 39416 samples\n","\n","[34/114] proband9/sitting\n","  ms/ms, absolute, 24 segments, 31473 samples\n","\n","[35/114] proband8/climbingdown\n","  ms/ms, absolute, 26 segments, 21304 samples\n","\n","[36/114] proband8/walking\n","  ms/ms, absolute, 31 segments, 31652 samples\n","\n","[37/114] proband8/standing\n","  ms/ms, absolute, 13 segments, 31386 samples\n","\n","[38/114] proband8/jumping\n","  ms/ms, absolute, 5 segments, 4694 samples\n","\n","[39/114] proband8/climbingup\n","  ms/ms, absolute, 47 segments, 55851 samples\n","\n","[40/114] proband8/lying\n","  ms/ms, absolute, 20 segments, 30683 samples\n","\n","[41/114] proband8/running\n","  ms/ms, absolute, 20 segments, 29937 samples\n","\n","[42/114] proband8/sitting\n","  ms/ms, absolute, 15 segments, 31855 samples\n","\n","[43/114] proband5/climbingdown\n","  ms/ms, absolute, 19 segments, 24376 samples\n","\n","[44/114] proband5/walking\n","  ms/ms, absolute, 24 segments, 33814 samples\n","\n","[45/114] proband5/standing\n","  ms/ms, absolute, 21 segments, 29060 samples\n","\n","[46/114] proband5/jumping\n","  ms/ms, absolute, 3 segments, 4715 samples\n","\n","[47/114] proband5/climbingup\n","  ms/ms, absolute, 19 segments, 29417 samples\n","\n","[48/114] proband5/lying\n","  ms/ms, absolute, 29 segments, 31975 samples\n","\n","[49/114] proband5/running\n","  ms/ms, absolute, 41 segments, 53805 samples\n","\n","[50/114] proband5/sitting\n","  ms/ms, absolute, 16 segments, 31973 samples\n","\n","[51/114] proband3/climbingdown\n","  ms/ms, absolute, 18 segments, 26974 samples\n","\n","[52/114] proband3/walking\n","  ms/ms, absolute, 21 segments, 33355 samples\n","\n","[53/114] proband3/standing\n","  ms/ms, absolute, 25 segments, 30327 samples\n","\n","[54/114] proband3/jumping\n","  ms/ms, absolute, 6 segments, 4966 samples\n","\n","[55/114] proband3/climbingup\n","  ms/ms, absolute, 22 segments, 28553 samples\n","\n","[56/114] proband3/lying\n","  ms/ms, absolute, 23 segments, 30629 samples\n","\n","[57/114] proband3/running\n","  ms/ms, absolute, 33 segments, 36762 samples\n","\n","[58/114] proband3/sitting\n","  ms/ms, absolute, 24 segments, 30493 samples\n","\n","[59/114] proband13/climbingdown\n","  ms/ms, absolute, 20 segments, 21127 samples\n","\n","[60/114] proband13/walking\n","  ms/ms, absolute, 20 segments, 31882 samples\n","\n","[61/114] proband13/standing\n","  ms/ms, absolute, 35 segments, 32877 samples\n","\n","[62/114] proband13/jumping\n","  ms/ms, absolute, 2 segments, 5370 samples\n","\n","[63/114] proband13/climbingup\n","  ms/ms, absolute, 23 segments, 29031 samples\n","\n","[64/114] proband13/lying\n","  ms/ms, absolute, 23 segments, 31336 samples\n","\n","[65/114] proband13/running\n","  ms/ms, absolute, 21 segments, 29961 samples\n","\n","[66/114] proband13/sitting\n","  ms/ms, absolute, 24 segments, 31261 samples\n","\n","[67/114] proband12/climbingdown\n","  ms/ms, absolute, 17 segments, 23499 samples\n","\n","[68/114] proband12/walking\n","  ms/ms, absolute, 50 segments, 29476 samples\n","\n","[69/114] proband12/standing\n","  ms/ms, absolute, 28 segments, 29789 samples\n","\n","[70/114] proband12/jumping\n","  ms/ms, absolute, 12 segments, 4279 samples\n","\n","[71/114] proband12/climbingup\n","  ms/ms, absolute, 13 segments, 27393 samples\n","\n","[72/114] proband12/lying\n","  ms/ms, absolute, 22 segments, 30306 samples\n","\n","[73/114] proband12/running\n","  ms/ms, absolute, 23 segments, 28742 samples\n","\n","[74/114] proband12/sitting\n","  ms/ms, absolute, 62 segments, 29317 samples\n","\n","[75/114] proband10/climbingdown\n","  ms/ms, absolute, 20 segments, 21216 samples\n","\n","[76/114] proband10/walking\n","  ms/ms, absolute, 26 segments, 30684 samples\n","\n","[77/114] proband10/standing\n","  ms/ms, absolute, 27 segments, 31946 samples\n","\n","[78/114] proband10/jumping\n","  ms/ms, absolute, 1 segments, 5193 samples\n","\n","[79/114] proband10/climbingup\n","  ms/ms, absolute, 21 segments, 22201 samples\n","\n","[80/114] proband10/lying\n","  ms/ms, absolute, 22 segments, 31164 samples\n","\n","[81/114] proband10/running\n","  ms/ms, absolute, 31 segments, 31071 samples\n","\n","[82/114] proband10/sitting\n","  ms/ms, absolute, 32 segments, 30836 samples\n","\n","[83/114] proband11/climbingdown\n","  ms/ms, absolute, 19 segments, 24032 samples\n","\n","[84/114] proband11/walking\n","  ms/ms, absolute, 30 segments, 32123 samples\n","\n","[85/114] proband11/standing\n","  ms/ms, absolute, 20 segments, 30514 samples\n","\n","[86/114] proband11/jumping\n","  ms/ms, absolute, 7 segments, 4796 samples\n","\n","[87/114] proband11/climbingup\n","  ms/ms, absolute, 25 segments, 30391 samples\n","\n","[88/114] proband11/lying\n","  ms/ms, absolute, 19 segments, 31836 samples\n","\n","[89/114] proband11/running\n","  ms/ms, absolute, 34 segments, 29843 samples\n","\n","[90/114] proband11/sitting\n","  ms/ms, absolute, 26 segments, 30245 samples\n","\n","[91/114] proband15/climbingdown\n","  ms/ms, absolute, 23 segments, 24728 samples\n","\n","[92/114] proband15/walking\n","  ms/ms, absolute, 27 segments, 32152 samples\n","\n","[93/114] proband15/standing\n","  ms/ms, absolute, 21 segments, 30457 samples\n","\n","[94/114] proband15/jumping\n","  ms/ms, absolute, 6 segments, 4568 samples\n","\n","[95/114] proband15/climbingup\n","  ms/ms, absolute, 22 segments, 28145 samples\n","\n","[96/114] proband15/lying\n","  ms/ms, absolute, 23 segments, 32001 samples\n","\n","[97/114] proband15/running\n","  ms/ms, absolute, 27 segments, 32719 samples\n","\n","[98/114] proband15/sitting\n","  ms/ms, absolute, 29 segments, 31196 samples\n","\n","[99/114] proband2/climbingdown\n","  ms/ms, absolute, 24 segments, 23939 samples\n","\n","[100/114] proband2/walking\n","  ms/ms, absolute, 38 segments, 28946 samples\n","\n","[101/114] proband2/standing\n","  ms/ms, absolute, 49 segments, 28717 samples\n","\n","[102/114] proband2/jumping\n","  ms/ms, absolute, 3 segments, 4512 samples\n","\n","[103/114] proband2/climbingup\n","  ms/ms, absolute, 37 segments, 23484 samples\n","\n","[104/114] proband2/lying\n","  ms/ms, absolute, 28 segments, 30002 samples\n","\n","[105/114] proband2/running\n","  ms/ms, absolute, 42 segments, 28701 samples\n","\n","[106/114] proband2/sitting\n","  ms/ms, absolute, 17 segments, 29887 samples\n","\n","[107/114] proband6/climbingdown\n","  ms/ms, absolute, 19 segments, 24014 samples\n","\n","[108/114] proband6/walking\n","  ms/ms, absolute, 30 segments, 30436 samples\n","\n","[109/114] proband6/standing\n","  ms/ms, absolute, 32 segments, 30664 samples\n","\n","[110/114] proband6/jumping\n","  ms/ms, absolute, 2 segments, 4737 samples\n","\n","[111/114] proband6/climbingup\n","  ms/ms, absolute, 16 segments, 24999 samples\n","\n","[112/114] proband6/lying\n","  ms/ms, absolute, 26 segments, 31199 samples\n","\n","[113/114] proband6/running\n","  ms/ms, absolute, 38 segments, 31910 samples\n","\n","[114/114] proband6/sitting\n","  ms/ms, absolute, 20 segments, 32055 samples\n","\n","✓ Completed 112 files\n","⚠️ Skipped 2: ['proband1_lying', 'proband1_sitting']\n","[master 929a822] preproc: final time normalization with all fallbacks\n"," 114 files changed, 3031873 insertions(+)\n"," create mode 100644 figures/step4_interval_hist.png\n"," create mode 100644 interim/proband10_climbingdown_waist.csv\n"," create mode 100644 interim/proband10_climbingup_waist.csv\n"," create mode 100644 interim/proband10_jumping_waist.csv\n"," create mode 100644 interim/proband10_lying_waist.csv\n"," create mode 100644 interim/proband10_running_waist.csv\n"," create mode 100644 interim/proband10_sitting_waist.csv\n"," create mode 100644 interim/proband10_standing_waist.csv\n"," create mode 100644 interim/proband10_walking_waist.csv\n"," create mode 100644 interim/proband11_climbingdown_waist.csv\n"," create mode 100644 interim/proband11_climbingup_waist.csv\n"," create mode 100644 interim/proband11_jumping_waist.csv\n"," create mode 100644 interim/proband11_lying_waist.csv\n"," create mode 100644 interim/proband11_running_waist.csv\n"," create mode 100644 interim/proband11_sitting_waist.csv\n"," create mode 100644 interim/proband11_standing_waist.csv\n"," create mode 100644 interim/proband11_walking_waist.csv\n"," create mode 100644 interim/proband12_climbingdown_waist.csv\n"," create mode 100644 interim/proband12_climbingup_waist.csv\n"," create mode 100644 interim/proband12_jumping_waist.csv\n"," create mode 100644 interim/proband12_lying_waist.csv\n"," create mode 100644 interim/proband12_running_waist.csv\n"," create mode 100644 interim/proband12_sitting_waist.csv\n"," create mode 100644 interim/proband12_standing_waist.csv\n"," create mode 100644 interim/proband12_walking_waist.csv\n"," create mode 100644 interim/proband13_climbingdown_waist.csv\n"," create mode 100644 interim/proband13_climbingup_waist.csv\n"," create mode 100644 interim/proband13_jumping_waist.csv\n"," create mode 100644 interim/proband13_lying_waist.csv\n"," create mode 100644 interim/proband13_running_waist.csv\n"," create mode 100644 interim/proband13_sitting_waist.csv\n"," create mode 100644 interim/proband13_standing_waist.csv\n"," create mode 100644 interim/proband13_walking_waist.csv\n"," create mode 100644 interim/proband14_jumping_waist.csv\n"," create mode 100644 interim/proband14_lying_waist.csv\n"," create mode 100644 interim/proband14_running_waist.csv\n"," create mode 100644 interim/proband14_sitting_waist.csv\n"," create mode 100644 interim/proband14_standing_waist.csv\n"," create mode 100644 interim/proband14_walking_waist.csv\n"," create mode 100644 interim/proband15_climbingdown_waist.csv\n"," create mode 100644 interim/proband15_climbingup_waist.csv\n"," create mode 100644 interim/proband15_jumping_waist.csv\n"," create mode 100644 interim/proband15_lying_waist.csv\n"," create mode 100644 interim/proband15_running_waist.csv\n"," create mode 100644 interim/proband15_sitting_waist.csv\n"," create mode 100644 interim/proband15_standing_waist.csv\n"," create mode 100644 interim/proband15_walking_waist.csv\n"," create mode 100644 interim/proband1_climbingdown_waist.csv\n"," create mode 100644 interim/proband1_climbingup_waist.csv\n"," create mode 100644 interim/proband1_jumping_waist.csv\n"," create mode 100644 interim/proband1_running_waist.csv\n"," create mode 100644 interim/proband1_standing_waist.csv\n"," create mode 100644 interim/proband1_walking_waist.csv\n"," create mode 100644 interim/proband2_climbingdown_waist.csv\n"," create mode 100644 interim/proband2_climbingup_waist.csv\n"," create mode 100644 interim/proband2_jumping_waist.csv\n"," create mode 100644 interim/proband2_lying_waist.csv\n"," create mode 100644 interim/proband2_running_waist.csv\n"," create mode 100644 interim/proband2_sitting_waist.csv\n"," create mode 100644 interim/proband2_standing_waist.csv\n"," create mode 100644 interim/proband2_walking_waist.csv\n"," create mode 100644 interim/proband3_climbingdown_waist.csv\n"," create mode 100644 interim/proband3_climbingup_waist.csv\n"," create mode 100644 interim/proband3_jumping_waist.csv\n"," create mode 100644 interim/proband3_lying_waist.csv\n"," create mode 100644 interim/proband3_running_waist.csv\n"," create mode 100644 interim/proband3_sitting_waist.csv\n"," create mode 100644 interim/proband3_standing_waist.csv\n"," create mode 100644 interim/proband3_walking_waist.csv\n"," create mode 100644 interim/proband4_jumping_waist.csv\n"," create mode 100644 interim/proband4_lying_waist.csv\n"," create mode 100644 interim/proband4_running_waist.csv\n"," create mode 100644 interim/proband4_sitting_waist.csv\n"," create mode 100644 interim/proband4_standing_waist.csv\n"," create mode 100644 interim/proband4_walking_waist.csv\n"," create mode 100644 interim/proband5_climbingdown_waist.csv\n"," create mode 100644 interim/proband5_climbingup_waist.csv\n"," create mode 100644 interim/proband5_jumping_waist.csv\n"," create mode 100644 interim/proband5_lying_waist.csv\n"," create mode 100644 interim/proband5_running_waist.csv\n"," create mode 100644 interim/proband5_sitting_waist.csv\n"," create mode 100644 interim/proband5_standing_waist.csv\n"," create mode 100644 interim/proband5_walking_waist.csv\n"," create mode 100644 interim/proband6_climbingdown_waist.csv\n"," create mode 100644 interim/proband6_climbingup_waist.csv\n"," create mode 100644 interim/proband6_jumping_waist.csv\n"," create mode 100644 interim/proband6_lying_waist.csv\n"," create mode 100644 interim/proband6_running_waist.csv\n"," create mode 100644 interim/proband6_sitting_waist.csv\n"," create mode 100644 interim/proband6_standing_waist.csv\n"," create mode 100644 interim/proband6_walking_waist.csv\n"," create mode 100644 interim/proband7_jumping_waist.csv\n"," create mode 100644 interim/proband7_lying_waist.csv\n"," create mode 100644 interim/proband7_running_waist.csv\n"," create mode 100644 interim/proband7_sitting_waist.csv\n"," create mode 100644 interim/proband7_standing_waist.csv\n"," create mode 100644 interim/proband7_walking_waist.csv\n"," create mode 100644 interim/proband8_climbingdown_waist.csv\n"," create mode 100644 interim/proband8_climbingup_waist.csv\n"," create mode 100644 interim/proband8_jumping_waist.csv\n"," create mode 100644 interim/proband8_lying_waist.csv\n"," create mode 100644 interim/proband8_running_waist.csv\n"," create mode 100644 interim/proband8_sitting_waist.csv\n"," create mode 100644 interim/proband8_standing_waist.csv\n"," create mode 100644 interim/proband8_walking_waist.csv\n"," create mode 100644 interim/proband9_climbingdown_waist.csv\n"," create mode 100644 interim/proband9_climbingup_waist.csv\n"," create mode 100644 interim/proband9_jumping_waist.csv\n"," create mode 100644 interim/proband9_lying_waist.csv\n"," create mode 100644 interim/proband9_running_waist.csv\n"," create mode 100644 interim/proband9_sitting_waist.csv\n"," create mode 100644 interim/proband9_standing_waist.csv\n"," create mode 100644 interim/proband9_walking_waist.csv\n"," create mode 100644 logs/step4_summary.json\n","\n","============================================================\n","Step 4 completed\n","============================================================\n"]}]},{"cell_type":"code","source":["# ================ Step 5: Gravity Removal / Detrending (Batch Processing) ================\n","import numpy as np\n","import pandas as pd\n","from scipy.signal import butter, filtfilt\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import json\n","\n","print(\"\\n\\nStep 5: Gravity Removal / Detrending\")\n","print(\"=\" * 60)\n","\n","interim_dir = Path('/content/interim')\n","proc_dir = Path('/content/proc')\n","proc_dir.mkdir(exist_ok=True)\n","\n","TARGET_FS = 50\n","CUTOFF_HZ = 0.3\n","\n","def highpass_filter(data, cutoff, fs, order=3):\n","    \"\"\"Third-order Butterworth high-pass filter\"\"\"\n","    nyq = 0.5 * fs\n","    normal_cutoff = cutoff / nyq\n","    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n","    return filtfilt(b, a, data)\n","\n","# Process all files\n","interim_files = sorted(interim_dir.glob('*.csv'))\n","print(f\"Found {len(interim_files)} files\")\n","\n","all_static_means = []\n","\n","for idx, filepath in enumerate(interim_files):\n","    print(f\"\\n[{idx+1}/{len(interim_files)}] {filepath.name}\")\n","\n","    df = pd.read_csv(filepath)\n","    print(f\"  Original: {df.shape}, {df['segment_id'].nunique()} segments\")\n","\n","    processed_segments = []\n","\n","    # Filter per segment\n","    for seg_id, seg_df in df.groupby('segment_id'):\n","        seg_df = seg_df.copy()\n","\n","        # Accelerometer high-pass filtering\n","        for axis in ['x', 'y', 'z']:\n","            col = f'acc_{axis}'\n","            seg_df[col] = highpass_filter(seg_df[col].values, CUTOFF_HZ, TARGET_FS, order=3)\n","\n","        # Gyroscope mean removal\n","        for axis in ['x', 'y', 'z']:\n","            col = f'gyro_{axis}'\n","            seg_df[col] = seg_df[col] - seg_df[col].mean()\n","\n","        processed_segments.append(seg_df)\n","\n","    df_filtered = pd.concat(processed_segments, ignore_index=True)\n","\n","    # Validate static segment (from the longest segment)\n","    longest_seg = df_filtered.groupby('segment_id').size().idxmax()\n","    seg_for_verify = df_filtered[df_filtered['segment_id'] == longest_seg].reset_index(drop=True)\n","\n","    window_size = TARGET_FS * 2\n","    acc_mag = np.sqrt(seg_for_verify['acc_x']**2 + seg_for_verify['acc_y']**2 + seg_for_verify['acc_z']**2)\n","    static_idx = acc_mag.rolling(window_size).std().idxmin()\n","    static_seg = seg_for_verify.iloc[static_idx:static_idx+window_size]\n","\n","    static_means = {f'acc_{ax}': static_seg[f'acc_{ax}'].mean() for ax in ['x', 'y', 'z']}\n","    all_static_means.append({'file': filepath.name, **static_means})\n","\n","    # Save\n","    df_filtered.to_csv(proc_dir / filepath.name, index=False)\n","    print(f\"  ✓ {len(df_filtered)} samples → proc/{filepath.name}\")\n","\n","print(f\"\\n✓ Completed {len(interim_files)} files\")\n","\n","# Plot verification figure for the first file\n","if interim_files:\n","    first_file = interim_files[0]\n","    df = pd.read_csv(proc_dir / first_file.name)\n","    longest_seg = df.groupby('segment_id').size().idxmax()\n","    seg = df[df['segment_id'] == longest_seg].reset_index(drop=True)\n","\n","    window_size = TARGET_FS * 2\n","    acc_mag = np.sqrt(seg['acc_x']**2 + seg['acc_y']**2 + seg['acc_z']**2)\n","    static_idx = acc_mag.rolling(window_size).std().idxmin()\n","    static_seg = seg.iloc[static_idx:static_idx+window_size]\n","\n","    fig, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=True)\n","    time_sec = np.arange(len(seg)) / TARGET_FS\n","\n","    for i, axis in enumerate(['x', 'y', 'z']):\n","        ax = axes[i]\n","        col = f'acc_{axis}'\n","        ax.plot(time_sec, seg[col], linewidth=0.5, alpha=0.7)\n","        ax.axhline(0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n","\n","        static_t = static_idx / TARGET_FS\n","        static_mean = static_seg[col].mean()\n","        ax.axvspan(static_t, static_t + 2, color='green', alpha=0.2,\n","                   label=f'Static (mean={static_mean:.4f})')\n","\n","        ax.set_ylabel(f'ACC {axis.upper()} (m/s²)')\n","        ax.grid(alpha=0.3)\n","        ax.legend(loc='upper right')\n","\n","    axes[-1].set_xlabel('Time (s)')\n","    axes[0].set_title(f'Detrended Signal - {first_file.name} (segment {longest_seg})')\n","    plt.tight_layout()\n","    plt.savefig('/content/figures/step5_detrend_verify.png', dpi=150)\n","    plt.close()\n","    print(f\"\\n✓ Verification figure: figures/step5_detrend_verify.png\")\n","\n","# Save parameters\n","filter_params = {\n","    'acc_highpass': {'cutoff_hz': CUTOFF_HZ, 'order': 3, 'filter_type': 'Butterworth'},\n","    'gyro_detrend': 'mean_removal',\n","    'sampling_rate': TARGET_FS,\n","    'filtering_method': 'per_segment',\n","    'files_processed': len(interim_files),\n","    'static_means_samples': all_static_means[:5]\n","}\n","\n","with open('/content/logs/step5_filter_params.json', 'w') as f:\n","    json.dump(filter_params, f, indent=2)\n","\n","get_ipython().system('git add figures/step5_detrend_verify.png logs/step5_filter_params.json proc/')\n","get_ipython().system('git commit -m \"preproc: batch filtering for all files\"')\n","\n","print(f\"\\n{'='*60}\\nStep 5 completed\\n{'='*60}\")"],"metadata":{"id":"zSpAZx7oapfD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762895191832,"user_tz":0,"elapsed":62446,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"c81b282f-669d-4416-da1c-b9fe37f6883e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Step 5: Gravity Removal / Detrending\n","============================================================\n","Found 112 files\n","\n","[1/112] proband10_climbingdown_waist.csv\n","  Original: (21216, 10), 20 segments\n","  ✓ 21216 samples → proc/proband10_climbingdown_waist.csv\n","\n","[2/112] proband10_climbingup_waist.csv\n","  Original: (22201, 10), 21 segments\n","  ✓ 22201 samples → proc/proband10_climbingup_waist.csv\n","\n","[3/112] proband10_jumping_waist.csv\n","  Original: (5193, 10), 1 segments\n","  ✓ 5193 samples → proc/proband10_jumping_waist.csv\n","\n","[4/112] proband10_lying_waist.csv\n","  Original: (31164, 10), 22 segments\n","  ✓ 31164 samples → proc/proband10_lying_waist.csv\n","\n","[5/112] proband10_running_waist.csv\n","  Original: (31071, 10), 31 segments\n","  ✓ 31071 samples → proc/proband10_running_waist.csv\n","\n","[6/112] proband10_sitting_waist.csv\n","  Original: (30836, 10), 32 segments\n","  ✓ 30836 samples → proc/proband10_sitting_waist.csv\n","\n","[7/112] proband10_standing_waist.csv\n","  Original: (31946, 10), 27 segments\n","  ✓ 31946 samples → proc/proband10_standing_waist.csv\n","\n","[8/112] proband10_walking_waist.csv\n","  Original: (30684, 10), 26 segments\n","  ✓ 30684 samples → proc/proband10_walking_waist.csv\n","\n","[9/112] proband11_climbingdown_waist.csv\n","  Original: (24032, 10), 19 segments\n","  ✓ 24032 samples → proc/proband11_climbingdown_waist.csv\n","\n","[10/112] proband11_climbingup_waist.csv\n","  Original: (30391, 10), 25 segments\n","  ✓ 30391 samples → proc/proband11_climbingup_waist.csv\n","\n","[11/112] proband11_jumping_waist.csv\n","  Original: (4796, 10), 7 segments\n","  ✓ 4796 samples → proc/proband11_jumping_waist.csv\n","\n","[12/112] proband11_lying_waist.csv\n","  Original: (31836, 10), 19 segments\n","  ✓ 31836 samples → proc/proband11_lying_waist.csv\n","\n","[13/112] proband11_running_waist.csv\n","  Original: (29843, 10), 34 segments\n","  ✓ 29843 samples → proc/proband11_running_waist.csv\n","\n","[14/112] proband11_sitting_waist.csv\n","  Original: (30245, 10), 26 segments\n","  ✓ 30245 samples → proc/proband11_sitting_waist.csv\n","\n","[15/112] proband11_standing_waist.csv\n","  Original: (30514, 10), 20 segments\n","  ✓ 30514 samples → proc/proband11_standing_waist.csv\n","\n","[16/112] proband11_walking_waist.csv\n","  Original: (32123, 10), 30 segments\n","  ✓ 32123 samples → proc/proband11_walking_waist.csv\n","\n","[17/112] proband12_climbingdown_waist.csv\n","  Original: (23499, 10), 17 segments\n","  ✓ 23499 samples → proc/proband12_climbingdown_waist.csv\n","\n","[18/112] proband12_climbingup_waist.csv\n","  Original: (27393, 10), 13 segments\n","  ✓ 27393 samples → proc/proband12_climbingup_waist.csv\n","\n","[19/112] proband12_jumping_waist.csv\n","  Original: (4279, 10), 12 segments\n","  ✓ 4279 samples → proc/proband12_jumping_waist.csv\n","\n","[20/112] proband12_lying_waist.csv\n","  Original: (30306, 10), 22 segments\n","  ✓ 30306 samples → proc/proband12_lying_waist.csv\n","\n","[21/112] proband12_running_waist.csv\n","  Original: (28742, 10), 23 segments\n","  ✓ 28742 samples → proc/proband12_running_waist.csv\n","\n","[22/112] proband12_sitting_waist.csv\n","  Original: (29317, 10), 62 segments\n","  ✓ 29317 samples → proc/proband12_sitting_waist.csv\n","\n","[23/112] proband12_standing_waist.csv\n","  Original: (29789, 10), 28 segments\n","  ✓ 29789 samples → proc/proband12_standing_waist.csv\n","\n","[24/112] proband12_walking_waist.csv\n","  Original: (29476, 10), 50 segments\n","  ✓ 29476 samples → proc/proband12_walking_waist.csv\n","\n","[25/112] proband13_climbingdown_waist.csv\n","  Original: (21127, 10), 20 segments\n","  ✓ 21127 samples → proc/proband13_climbingdown_waist.csv\n","\n","[26/112] proband13_climbingup_waist.csv\n","  Original: (29031, 10), 23 segments\n","  ✓ 29031 samples → proc/proband13_climbingup_waist.csv\n","\n","[27/112] proband13_jumping_waist.csv\n","  Original: (5370, 10), 2 segments\n","  ✓ 5370 samples → proc/proband13_jumping_waist.csv\n","\n","[28/112] proband13_lying_waist.csv\n","  Original: (31336, 10), 23 segments\n","  ✓ 31336 samples → proc/proband13_lying_waist.csv\n","\n","[29/112] proband13_running_waist.csv\n","  Original: (29961, 10), 21 segments\n","  ✓ 29961 samples → proc/proband13_running_waist.csv\n","\n","[30/112] proband13_sitting_waist.csv\n","  Original: (31261, 10), 24 segments\n","  ✓ 31261 samples → proc/proband13_sitting_waist.csv\n","\n","[31/112] proband13_standing_waist.csv\n","  Original: (32877, 10), 35 segments\n","  ✓ 32877 samples → proc/proband13_standing_waist.csv\n","\n","[32/112] proband13_walking_waist.csv\n","  Original: (31882, 10), 20 segments\n","  ✓ 31882 samples → proc/proband13_walking_waist.csv\n","\n","[33/112] proband14_jumping_waist.csv\n","  Original: (4642, 10), 5 segments\n","  ✓ 4642 samples → proc/proband14_jumping_waist.csv\n","\n","[34/112] proband14_lying_waist.csv\n","  Original: (30563, 10), 27 segments\n","  ✓ 30563 samples → proc/proband14_lying_waist.csv\n","\n","[35/112] proband14_running_waist.csv\n","  Original: (29786, 10), 33 segments\n","  ✓ 29786 samples → proc/proband14_running_waist.csv\n","\n","[36/112] proband14_sitting_waist.csv\n","  Original: (30562, 10), 31 segments\n","  ✓ 30562 samples → proc/proband14_sitting_waist.csv\n","\n","[37/112] proband14_standing_waist.csv\n","  Original: (30114, 10), 34 segments\n","  ✓ 30114 samples → proc/proband14_standing_waist.csv\n","\n","[38/112] proband14_walking_waist.csv\n","  Original: (32007, 10), 50 segments\n","  ✓ 32007 samples → proc/proband14_walking_waist.csv\n","\n","[39/112] proband15_climbingdown_waist.csv\n","  Original: (24728, 10), 23 segments\n","  ✓ 24728 samples → proc/proband15_climbingdown_waist.csv\n","\n","[40/112] proband15_climbingup_waist.csv\n","  Original: (28145, 10), 22 segments\n","  ✓ 28145 samples → proc/proband15_climbingup_waist.csv\n","\n","[41/112] proband15_jumping_waist.csv\n","  Original: (4568, 10), 6 segments\n","  ✓ 4568 samples → proc/proband15_jumping_waist.csv\n","\n","[42/112] proband15_lying_waist.csv\n","  Original: (32001, 10), 23 segments\n","  ✓ 32001 samples → proc/proband15_lying_waist.csv\n","\n","[43/112] proband15_running_waist.csv\n","  Original: (32719, 10), 27 segments\n","  ✓ 32719 samples → proc/proband15_running_waist.csv\n","\n","[44/112] proband15_sitting_waist.csv\n","  Original: (31196, 10), 29 segments\n","  ✓ 31196 samples → proc/proband15_sitting_waist.csv\n","\n","[45/112] proband15_standing_waist.csv\n","  Original: (30457, 10), 21 segments\n","  ✓ 30457 samples → proc/proband15_standing_waist.csv\n","\n","[46/112] proband15_walking_waist.csv\n","  Original: (32152, 10), 27 segments\n","  ✓ 32152 samples → proc/proband15_walking_waist.csv\n","\n","[47/112] proband1_climbingdown_waist.csv\n","  Original: (24607, 10), 17 segments\n","  ✓ 24607 samples → proc/proband1_climbingdown_waist.csv\n","\n","[48/112] proband1_climbingup_waist.csv\n","  Original: (31752, 10), 25 segments\n","  ✓ 31752 samples → proc/proband1_climbingup_waist.csv\n","\n","[49/112] proband1_jumping_waist.csv\n","  Original: (4214, 10), 4 segments\n","  ✓ 4214 samples → proc/proband1_jumping_waist.csv\n","\n","[50/112] proband1_running_waist.csv\n","  Original: (30013, 10), 14 segments\n","  ✓ 30013 samples → proc/proband1_running_waist.csv\n","\n","[51/112] proband1_standing_waist.csv\n","  Original: (30988, 10), 22 segments\n","  ✓ 30988 samples → proc/proband1_standing_waist.csv\n","\n","[52/112] proband1_walking_waist.csv\n","  Original: (31332, 10), 14 segments\n","  ✓ 31332 samples → proc/proband1_walking_waist.csv\n","\n","[53/112] proband2_climbingdown_waist.csv\n","  Original: (23939, 10), 24 segments\n","  ✓ 23939 samples → proc/proband2_climbingdown_waist.csv\n","\n","[54/112] proband2_climbingup_waist.csv\n","  Original: (23484, 10), 37 segments\n","  ✓ 23484 samples → proc/proband2_climbingup_waist.csv\n","\n","[55/112] proband2_jumping_waist.csv\n","  Original: (4512, 10), 3 segments\n","  ✓ 4512 samples → proc/proband2_jumping_waist.csv\n","\n","[56/112] proband2_lying_waist.csv\n","  Original: (30002, 10), 28 segments\n","  ✓ 30002 samples → proc/proband2_lying_waist.csv\n","\n","[57/112] proband2_running_waist.csv\n","  Original: (28701, 10), 42 segments\n","  ✓ 28701 samples → proc/proband2_running_waist.csv\n","\n","[58/112] proband2_sitting_waist.csv\n","  Original: (29887, 10), 17 segments\n","  ✓ 29887 samples → proc/proband2_sitting_waist.csv\n","\n","[59/112] proband2_standing_waist.csv\n","  Original: (28717, 10), 49 segments\n","  ✓ 28717 samples → proc/proband2_standing_waist.csv\n","\n","[60/112] proband2_walking_waist.csv\n","  Original: (28946, 10), 38 segments\n","  ✓ 28946 samples → proc/proband2_walking_waist.csv\n","\n","[61/112] proband3_climbingdown_waist.csv\n","  Original: (26974, 10), 18 segments\n","  ✓ 26974 samples → proc/proband3_climbingdown_waist.csv\n","\n","[62/112] proband3_climbingup_waist.csv\n","  Original: (28553, 10), 22 segments\n","  ✓ 28553 samples → proc/proband3_climbingup_waist.csv\n","\n","[63/112] proband3_jumping_waist.csv\n","  Original: (4966, 10), 6 segments\n","  ✓ 4966 samples → proc/proband3_jumping_waist.csv\n","\n","[64/112] proband3_lying_waist.csv\n","  Original: (30629, 10), 23 segments\n","  ✓ 30629 samples → proc/proband3_lying_waist.csv\n","\n","[65/112] proband3_running_waist.csv\n","  Original: (36762, 10), 33 segments\n","  ✓ 36762 samples → proc/proband3_running_waist.csv\n","\n","[66/112] proband3_sitting_waist.csv\n","  Original: (30493, 10), 24 segments\n","  ✓ 30493 samples → proc/proband3_sitting_waist.csv\n","\n","[67/112] proband3_standing_waist.csv\n","  Original: (30327, 10), 25 segments\n","  ✓ 30327 samples → proc/proband3_standing_waist.csv\n","\n","[68/112] proband3_walking_waist.csv\n","  Original: (33355, 10), 21 segments\n","  ✓ 33355 samples → proc/proband3_walking_waist.csv\n","\n","[69/112] proband4_jumping_waist.csv\n","  Original: (4148, 10), 3 segments\n","  ✓ 4148 samples → proc/proband4_jumping_waist.csv\n","\n","[70/112] proband4_lying_waist.csv\n","  Original: (33106, 10), 16 segments\n","  ✓ 33106 samples → proc/proband4_lying_waist.csv\n","\n","[71/112] proband4_running_waist.csv\n","  Original: (50541, 10), 40 segments\n","  ✓ 50541 samples → proc/proband4_running_waist.csv\n","\n","[72/112] proband4_sitting_waist.csv\n","  Original: (31248, 10), 14 segments\n","  ✓ 31248 samples → proc/proband4_sitting_waist.csv\n","\n","[73/112] proband4_standing_waist.csv\n","  Original: (29741, 10), 12 segments\n","  ✓ 29741 samples → proc/proband4_standing_waist.csv\n","\n","[74/112] proband4_walking_waist.csv\n","  Original: (30482, 10), 13 segments\n","  ✓ 30482 samples → proc/proband4_walking_waist.csv\n","\n","[75/112] proband5_climbingdown_waist.csv\n","  Original: (24376, 10), 19 segments\n","  ✓ 24376 samples → proc/proband5_climbingdown_waist.csv\n","\n","[76/112] proband5_climbingup_waist.csv\n","  Original: (29417, 10), 19 segments\n","  ✓ 29417 samples → proc/proband5_climbingup_waist.csv\n","\n","[77/112] proband5_jumping_waist.csv\n","  Original: (4715, 10), 3 segments\n","  ✓ 4715 samples → proc/proband5_jumping_waist.csv\n","\n","[78/112] proband5_lying_waist.csv\n","  Original: (31975, 10), 29 segments\n","  ✓ 31975 samples → proc/proband5_lying_waist.csv\n","\n","[79/112] proband5_running_waist.csv\n","  Original: (53805, 10), 41 segments\n","  ✓ 53805 samples → proc/proband5_running_waist.csv\n","\n","[80/112] proband5_sitting_waist.csv\n","  Original: (31973, 10), 16 segments\n","  ✓ 31973 samples → proc/proband5_sitting_waist.csv\n","\n","[81/112] proband5_standing_waist.csv\n","  Original: (29060, 10), 21 segments\n","  ✓ 29060 samples → proc/proband5_standing_waist.csv\n","\n","[82/112] proband5_walking_waist.csv\n","  Original: (33814, 10), 24 segments\n","  ✓ 33814 samples → proc/proband5_walking_waist.csv\n","\n","[83/112] proband6_climbingdown_waist.csv\n","  Original: (24014, 10), 19 segments\n","  ✓ 24014 samples → proc/proband6_climbingdown_waist.csv\n","\n","[84/112] proband6_climbingup_waist.csv\n","  Original: (24999, 10), 16 segments\n","  ✓ 24999 samples → proc/proband6_climbingup_waist.csv\n","\n","[85/112] proband6_jumping_waist.csv\n","  Original: (4737, 10), 2 segments\n","  ✓ 4737 samples → proc/proband6_jumping_waist.csv\n","\n","[86/112] proband6_lying_waist.csv\n","  Original: (31199, 10), 26 segments\n","  ✓ 31199 samples → proc/proband6_lying_waist.csv\n","\n","[87/112] proband6_running_waist.csv\n","  Original: (31910, 10), 38 segments\n","  ✓ 31910 samples → proc/proband6_running_waist.csv\n","\n","[88/112] proband6_sitting_waist.csv\n","  Original: (32055, 10), 20 segments\n","  ✓ 32055 samples → proc/proband6_sitting_waist.csv\n","\n","[89/112] proband6_standing_waist.csv\n","  Original: (30664, 10), 32 segments\n","  ✓ 30664 samples → proc/proband6_standing_waist.csv\n","\n","[90/112] proband6_walking_waist.csv\n","  Original: (30436, 10), 30 segments\n","  ✓ 30436 samples → proc/proband6_walking_waist.csv\n","\n","[91/112] proband7_jumping_waist.csv\n","  Original: (4853, 10), 6 segments\n","  ✓ 4853 samples → proc/proband7_jumping_waist.csv\n","\n","[92/112] proband7_lying_waist.csv\n","  Original: (31520, 10), 29 segments\n","  ✓ 31520 samples → proc/proband7_lying_waist.csv\n","\n","[93/112] proband7_running_waist.csv\n","  Original: (35636, 10), 27 segments\n","  ✓ 35636 samples → proc/proband7_running_waist.csv\n","\n","[94/112] proband7_sitting_waist.csv\n","  Original: (31417, 10), 23 segments\n","  ✓ 31417 samples → proc/proband7_sitting_waist.csv\n","\n","[95/112] proband7_standing_waist.csv\n","  Original: (32005, 10), 24 segments\n","  ✓ 32005 samples → proc/proband7_standing_waist.csv\n","\n","[96/112] proband7_walking_waist.csv\n","  Original: (29987, 10), 27 segments\n","  ✓ 29987 samples → proc/proband7_walking_waist.csv\n","\n","[97/112] proband8_climbingdown_waist.csv\n","  Original: (21304, 10), 26 segments\n","  ✓ 21304 samples → proc/proband8_climbingdown_waist.csv\n","\n","[98/112] proband8_climbingup_waist.csv\n","  Original: (55851, 10), 47 segments\n","  ✓ 55851 samples → proc/proband8_climbingup_waist.csv\n","\n","[99/112] proband8_jumping_waist.csv\n","  Original: (4694, 10), 5 segments\n","  ✓ 4694 samples → proc/proband8_jumping_waist.csv\n","\n","[100/112] proband8_lying_waist.csv\n","  Original: (30683, 10), 20 segments\n","  ✓ 30683 samples → proc/proband8_lying_waist.csv\n","\n","[101/112] proband8_running_waist.csv\n","  Original: (29937, 10), 20 segments\n","  ✓ 29937 samples → proc/proband8_running_waist.csv\n","\n","[102/112] proband8_sitting_waist.csv\n","  Original: (31855, 10), 15 segments\n","  ✓ 31855 samples → proc/proband8_sitting_waist.csv\n","\n","[103/112] proband8_standing_waist.csv\n","  Original: (31386, 10), 13 segments\n","  ✓ 31386 samples → proc/proband8_standing_waist.csv\n","\n","[104/112] proband8_walking_waist.csv\n","  Original: (31652, 10), 31 segments\n","  ✓ 31652 samples → proc/proband8_walking_waist.csv\n","\n","[105/112] proband9_climbingdown_waist.csv\n","  Original: (24302, 10), 18 segments\n","  ✓ 24302 samples → proc/proband9_climbingdown_waist.csv\n","\n","[106/112] proband9_climbingup_waist.csv\n","  Original: (26388, 10), 21 segments\n","  ✓ 26388 samples → proc/proband9_climbingup_waist.csv\n","\n","[107/112] proband9_jumping_waist.csv\n","  Original: (4976, 10), 4 segments\n","  ✓ 4976 samples → proc/proband9_jumping_waist.csv\n","\n","[108/112] proband9_lying_waist.csv\n","  Original: (30587, 10), 15 segments\n","  ✓ 30587 samples → proc/proband9_lying_waist.csv\n","\n","[109/112] proband9_running_waist.csv\n","  Original: (39416, 10), 41 segments\n","  ✓ 39416 samples → proc/proband9_running_waist.csv\n","\n","[110/112] proband9_sitting_waist.csv\n","  Original: (31473, 10), 24 segments\n","  ✓ 31473 samples → proc/proband9_sitting_waist.csv\n","\n","[111/112] proband9_standing_waist.csv\n","  Original: (31296, 10), 18 segments\n","  ✓ 31296 samples → proc/proband9_standing_waist.csv\n","\n","[112/112] proband9_walking_waist.csv\n","  Original: (30358, 10), 25 segments\n","  ✓ 30358 samples → proc/proband9_walking_waist.csv\n","\n","✓ Completed 112 files\n","\n","✓ Verification figure: figures/step5_detrend_verify.png\n","[master 5dbb255] preproc: batch filtering for all files\n"," 114 files changed, 3030676 insertions(+)\n"," create mode 100644 figures/step5_detrend_verify.png\n"," create mode 100644 logs/step5_filter_params.json\n"," create mode 100644 proc/proband10_climbingdown_waist.csv\n"," create mode 100644 proc/proband10_climbingup_waist.csv\n"," create mode 100644 proc/proband10_jumping_waist.csv\n"," create mode 100644 proc/proband10_lying_waist.csv\n"," create mode 100644 proc/proband10_running_waist.csv\n"," create mode 100644 proc/proband10_sitting_waist.csv\n"," create mode 100644 proc/proband10_standing_waist.csv\n"," create mode 100644 proc/proband10_walking_waist.csv\n"," create mode 100644 proc/proband11_climbingdown_waist.csv\n"," create mode 100644 proc/proband11_climbingup_waist.csv\n"," create mode 100644 proc/proband11_jumping_waist.csv\n"," create mode 100644 proc/proband11_lying_waist.csv\n"," create mode 100644 proc/proband11_running_waist.csv\n"," create mode 100644 proc/proband11_sitting_waist.csv\n"," create mode 100644 proc/proband11_standing_waist.csv\n"," create mode 100644 proc/proband11_walking_waist.csv\n"," create mode 100644 proc/proband12_climbingdown_waist.csv\n"," create mode 100644 proc/proband12_climbingup_waist.csv\n"," create mode 100644 proc/proband12_jumping_waist.csv\n"," create mode 100644 proc/proband12_lying_waist.csv\n"," create mode 100644 proc/proband12_running_waist.csv\n"," create mode 100644 proc/proband12_sitting_waist.csv\n"," create mode 100644 proc/proband12_standing_waist.csv\n"," create mode 100644 proc/proband12_walking_waist.csv\n"," create mode 100644 proc/proband13_climbingdown_waist.csv\n"," create mode 100644 proc/proband13_climbingup_waist.csv\n"," create mode 100644 proc/proband13_jumping_waist.csv\n"," create mode 100644 proc/proband13_lying_waist.csv\n"," create mode 100644 proc/proband13_running_waist.csv\n"," create mode 100644 proc/proband13_sitting_waist.csv\n"," create mode 100644 proc/proband13_standing_waist.csv\n"," create mode 100644 proc/proband13_walking_waist.csv\n"," create mode 100644 proc/proband14_jumping_waist.csv\n"," create mode 100644 proc/proband14_lying_waist.csv\n"," create mode 100644 proc/proband14_running_waist.csv\n"," create mode 100644 proc/proband14_sitting_waist.csv\n"," create mode 100644 proc/proband14_standing_waist.csv\n"," create mode 100644 proc/proband14_walking_waist.csv\n"," create mode 100644 proc/proband15_climbingdown_waist.csv\n"," create mode 100644 proc/proband15_climbingup_waist.csv\n"," create mode 100644 proc/proband15_jumping_waist.csv\n"," create mode 100644 proc/proband15_lying_waist.csv\n"," create mode 100644 proc/proband15_running_waist.csv\n"," create mode 100644 proc/proband15_sitting_waist.csv\n"," create mode 100644 proc/proband15_standing_waist.csv\n"," create mode 100644 proc/proband15_walking_waist.csv\n"," create mode 100644 proc/proband1_climbingdown_waist.csv\n"," create mode 100644 proc/proband1_climbingup_waist.csv\n"," create mode 100644 proc/proband1_jumping_waist.csv\n"," create mode 100644 proc/proband1_running_waist.csv\n"," create mode 100644 proc/proband1_standing_waist.csv\n"," create mode 100644 proc/proband1_walking_waist.csv\n"," create mode 100644 proc/proband2_climbingdown_waist.csv\n"," create mode 100644 proc/proband2_climbingup_waist.csv\n"," create mode 100644 proc/proband2_jumping_waist.csv\n"," create mode 100644 proc/proband2_lying_waist.csv\n"," create mode 100644 proc/proband2_running_waist.csv\n"," create mode 100644 proc/proband2_sitting_waist.csv\n"," create mode 100644 proc/proband2_standing_waist.csv\n"," create mode 100644 proc/proband2_walking_waist.csv\n"," create mode 100644 proc/proband3_climbingdown_waist.csv\n"," create mode 100644 proc/proband3_climbingup_waist.csv\n"," create mode 100644 proc/proband3_jumping_waist.csv\n"," create mode 100644 proc/proband3_lying_waist.csv\n"," create mode 100644 proc/proband3_running_waist.csv\n"," create mode 100644 proc/proband3_sitting_waist.csv\n"," create mode 100644 proc/proband3_standing_waist.csv\n"," create mode 100644 proc/proband3_walking_waist.csv\n"," create mode 100644 proc/proband4_jumping_waist.csv\n"," create mode 100644 proc/proband4_lying_waist.csv\n"," create mode 100644 proc/proband4_running_waist.csv\n"," create mode 100644 proc/proband4_sitting_waist.csv\n"," create mode 100644 proc/proband4_standing_waist.csv\n"," create mode 100644 proc/proband4_walking_waist.csv\n"," create mode 100644 proc/proband5_climbingdown_waist.csv\n"," create mode 100644 proc/proband5_climbingup_waist.csv\n"," create mode 100644 proc/proband5_jumping_waist.csv\n"," create mode 100644 proc/proband5_lying_waist.csv\n"," create mode 100644 proc/proband5_running_waist.csv\n"," create mode 100644 proc/proband5_sitting_waist.csv\n"," create mode 100644 proc/proband5_standing_waist.csv\n"," create mode 100644 proc/proband5_walking_waist.csv\n"," create mode 100644 proc/proband6_climbingdown_waist.csv\n"," create mode 100644 proc/proband6_climbingup_waist.csv\n"," create mode 100644 proc/proband6_jumping_waist.csv\n"," create mode 100644 proc/proband6_lying_waist.csv\n"," create mode 100644 proc/proband6_running_waist.csv\n"," create mode 100644 proc/proband6_sitting_waist.csv\n"," create mode 100644 proc/proband6_standing_waist.csv\n"," create mode 100644 proc/proband6_walking_waist.csv\n"," create mode 100644 proc/proband7_jumping_waist.csv\n"," create mode 100644 proc/proband7_lying_waist.csv\n"," create mode 100644 proc/proband7_running_waist.csv\n"," create mode 100644 proc/proband7_sitting_waist.csv\n"," create mode 100644 proc/proband7_standing_waist.csv\n"," create mode 100644 proc/proband7_walking_waist.csv\n"," create mode 100644 proc/proband8_climbingdown_waist.csv\n"," create mode 100644 proc/proband8_climbingup_waist.csv\n"," create mode 100644 proc/proband8_jumping_waist.csv\n"," create mode 100644 proc/proband8_lying_waist.csv\n"," create mode 100644 proc/proband8_running_waist.csv\n"," create mode 100644 proc/proband8_sitting_waist.csv\n"," create mode 100644 proc/proband8_standing_waist.csv\n"," create mode 100644 proc/proband8_walking_waist.csv\n"," create mode 100644 proc/proband9_climbingdown_waist.csv\n"," create mode 100644 proc/proband9_climbingup_waist.csv\n"," create mode 100644 proc/proband9_jumping_waist.csv\n"," create mode 100644 proc/proband9_lying_waist.csv\n"," create mode 100644 proc/proband9_running_waist.csv\n"," create mode 100644 proc/proband9_sitting_waist.csv\n"," create mode 100644 proc/proband9_standing_waist.csv\n"," create mode 100644 proc/proband9_walking_waist.csv\n","\n","============================================================\n","Step 5 completed\n","============================================================\n"]}]},{"cell_type":"code","source":["# ================ Step 6: Class Mapping ================\n","import pandas as pd\n","from pathlib import Path\n","import json\n","\n","print(\"\\n\\nStep 6: Class Mapping\")\n","print(\"=\" * 60)\n","\n","proc_dir = Path('/content/proc')\n","TARGET_FS = 50\n","\n","# Fixed order of 8 standard classes (consistent across folds)\n","STANDARD_CLASSES = ['walking', 'running', 'sitting', 'standing',\n","                    'lying', 'stairs_up', 'stairs_down', 'jumping']\n","\n","# Mapping from original activity names\n","activity_mapping = {\n","    'climbingdown': 'stairs_down',\n","    'climbingup': 'stairs_up',\n","    'jumping': 'jumping',\n","    'lying': 'lying',\n","    'running': 'running',\n","    'sitting': 'sitting',\n","    'standing': 'standing',\n","    'walking': 'walking'\n","}\n","\n","# Sliding-window parameters (aligned with subsequent feature extraction)\n","WINDOW_SEC = 3\n","OVERLAP = 0.5\n","WINDOW_SAMPLES = int(TARGET_FS * WINDOW_SEC)\n","STRIDE_SAMPLES = int(WINDOW_SAMPLES * (1 - OVERLAP))\n","MIN_WINDOWS_THRESHOLD = 50\n","\n","print(f\"Sliding window: {WINDOW_SEC}s ({WINDOW_SAMPLES} samples), overlap {OVERLAP*100:.0f}%, stride {STRIDE_SAMPLES}\")\n","\n","# Scan files and count windows per segment\n","proc_files = sorted(proc_dir.glob('*.csv'))\n","print(f\"\\nFound {len(proc_files)} files\")\n","\n","activity_stats = {}\n","proband_class_matrix = {}\n","\n","for filepath in proc_files:\n","    df = pd.read_csv(filepath)\n","\n","    # Prefer reading from columns\n","    activity = df['activity'].iloc[0] if 'activity' in df.columns else filepath.stem.split('_')[1]\n","    proband = df['proband'].iloc[0] if 'proband' in df.columns else filepath.stem.split('_')[0]\n","\n","    # Count windows per segment (without crossing segments)\n","    n_windows = 0\n","    for _, seg in df.groupby('segment_id'):\n","        seg_len = len(seg)\n","        if seg_len >= WINDOW_SAMPLES:\n","            n_windows += 1 + (seg_len - WINDOW_SAMPLES) // STRIDE_SAMPLES\n","\n","    # Accumulate statistics for original activities\n","    if activity not in activity_stats:\n","        activity_stats[activity] = {'samples': 0, 'windows': 0, 'files': 0}\n","    activity_stats[activity]['samples'] += len(df)\n","    activity_stats[activity]['windows'] += n_windows\n","    activity_stats[activity]['files'] += 1\n","\n","    # Build proband × class matrix\n","    if activity in activity_mapping:\n","        std_act = activity_mapping[activity]\n","        if proband not in proband_class_matrix:\n","            proband_class_matrix[proband] = {c: 0 for c in STANDARD_CLASSES}\n","        proband_class_matrix[proband][std_act] += n_windows\n","\n","print(\"\\nOriginal activity statistics:\")\n","for act in sorted(activity_stats.keys()):\n","    stats = activity_stats[act]\n","    print(f\"  {act:15s}: {stats['files']:2d} files, {stats['samples']:6d} samples, {stats['windows']:4d} windows\")\n","\n","# Map to the 8 standard classes\n","mapped_stats = {c: {'windows': 0, 'samples': 0, 'files': 0, 'original_names': []}\n","                for c in STANDARD_CLASSES}\n","tail_classes_original = []\n","\n","for orig_act, stats in activity_stats.items():\n","    if orig_act in activity_mapping:\n","        std_act = activity_mapping[orig_act]\n","        mapped_stats[std_act]['windows'] += stats['windows']\n","        mapped_stats[std_act]['samples'] += stats['samples']\n","        mapped_stats[std_act]['files'] += stats['files']\n","        if orig_act not in mapped_stats[std_act]['original_names']:\n","            mapped_stats[std_act]['original_names'].append(orig_act)\n","\n","        if stats['windows'] < MIN_WINDOWS_THRESHOLD:\n","            tail_classes_original.append({'original': orig_act, 'mapped': std_act, 'windows': stats['windows']})\n","\n","# Tail-class determination at the standard-class level\n","tail_standard_classes = [c for c in STANDARD_CLASSES if mapped_stats[c]['windows'] < MIN_WINDOWS_THRESHOLD]\n","included_flags = {c: (mapped_stats[c]['windows'] >= MIN_WINDOWS_THRESHOLD) for c in STANDARD_CLASSES}\n","\n","print(\"\\nStatistics for the 8 standard classes:\")\n","for std_act in STANDARD_CLASSES:\n","    stats = mapped_stats[std_act]\n","    status = \" [TAIL]\" if std_act in tail_standard_classes else \"\"\n","    status = \" [MISSING]\" if stats['windows'] == 0 else status\n","    print(f\"  {std_act:15s}: {stats['files']:2d} files, {stats['samples']:6d} samples, {stats['windows']:4d} windows{status}\")\n","\n","# Fixed encoding\n","label_to_id = {c: i for i, c in enumerate(STANDARD_CLASSES)}\n","id_to_label = {i: c for c, i in label_to_id.items()}\n","\n","print(\"\\nLabel encoding:\")\n","for i, c in id_to_label.items():\n","    print(f\"  {i}: {c}\")\n","\n","# Proband coverage matrix\n","print(\"\\nProband × Class coverage (number of windows):\")\n","print(f\"{'Proband':<12}\", end='')\n","for c in STANDARD_CLASSES:\n","    print(f\"{c[:4]:>6}\", end='')\n","print()\n","for p in sorted(proband_class_matrix.keys()):\n","    print(f\"{p:<12}\", end='')\n","    for c in STANDARD_CLASSES:\n","        cnt = proband_class_matrix[p][c]\n","        print(f\"{cnt:>6}\", end='')\n","    print()\n","\n","# Save configuration\n","classes_config = {\n","    'standard_classes': STANDARD_CLASSES,\n","    'num_classes': len(STANDARD_CLASSES),\n","    'label_to_id': label_to_id,\n","    'id_to_label': id_to_label,\n","    'activity_mapping': activity_mapping,\n","    'window_config': {\n","        'window_size_sec': WINDOW_SEC,\n","        'window_samples': WINDOW_SAMPLES,\n","        'overlap': OVERLAP,\n","        'stride_samples': STRIDE_SAMPLES,\n","        'sampling_rate_hz': TARGET_FS\n","    },\n","    'statistics': {\n","        'per_class': {c: {**mapped_stats[c], 'id': label_to_id[c]} for c in STANDARD_CLASSES},\n","        'tail_classes_original': tail_classes_original,\n","        'tail_standard_classes': tail_standard_classes,\n","        'included_flags': included_flags,\n","        'min_windows_threshold': MIN_WINDOWS_THRESHOLD,\n","        'proband_coverage': proband_class_matrix\n","    }\n","}\n","\n","with open('/content/configs/classes.json', 'w') as f:\n","    json.dump(classes_config, f, indent=2)\n","\n","print(f\"\\n✓ Class configuration saved: configs/classes.json\")\n","\n","if tail_standard_classes:\n","    print(f\"\\n⚠️ Tail classes at the standard level (windows < {MIN_WINDOWS_THRESHOLD}): {tail_standard_classes}\")\n","\n","included_classes = [c for c in STANDARD_CLASSES if included_flags[c]]\n","print(f\"✓ Classes included for training ({len(included_classes)}/{len(STANDARD_CLASSES)}): {included_classes}\")\n","\n","get_ipython().system('git add configs/classes.json')\n","get_ipython().system('git commit -m \"data: add standard-level tail classes and inclusion flags\"')\n","\n","print(f\"\\n{'='*60}\\nStep 6 completed\\n{'='*60}\")"],"metadata":{"id":"Oheuo5dvaqpH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762895198012,"user_tz":0,"elapsed":6177,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"d296117d-15a5-4fb7-d93a-8f9208e4f0bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Step 6: Class Mapping\n","============================================================\n","Sliding window: 3s (150 samples), overlap 50%, stride 75\n","\n","Found 112 files\n","\n","Original activity statistics:\n","  climbingdown   : 12 files, 284118 samples, 3425 windows\n","  climbingup     : 12 files, 357605 samples, 4331 windows\n","  jumping        : 15 files,  70663 samples,  842 windows\n","  lying          : 14 files, 436907 samples, 5343 windows\n","  running        : 15 files, 518843 samples, 6230 windows\n","  sitting        : 14 files, 433818 samples, 5259 windows\n","  standing       : 15 files, 459881 samples, 5574 windows\n","  walking        : 15 files, 468686 samples, 5618 windows\n","\n","Statistics for the 8 standard classes:\n","  walking        : 15 files, 468686 samples, 5618 windows\n","  running        : 15 files, 518843 samples, 6230 windows\n","  sitting        : 14 files, 433818 samples, 5259 windows\n","  standing       : 15 files, 459881 samples, 5574 windows\n","  lying          : 14 files, 436907 samples, 5343 windows\n","  stairs_up      : 12 files, 357605 samples, 4331 windows\n","  stairs_down    : 12 files, 284118 samples, 3425 windows\n","  jumping        : 15 files,  70663 samples,  842 windows\n","\n","Label encoding:\n","  0: walking\n","  1: running\n","  2: sitting\n","  3: standing\n","  4: lying\n","  5: stairs_up\n","  6: stairs_down\n","  7: jumping\n","\n","Proband × Class coverage (number of windows):\n","Proband       walk  runn  sitt  stan  lyin  stai  stai  jump\n","proband1       396   379     0   382     0   385   303    50\n","proband10      372   367   366   388   384   264   254    68\n","proband11      382   347   364   378   396   367   293    53\n","proband12      318   349   299   353   373   345   289    41\n","proband13      398   368   380   384   385   353   252    69\n","proband14      351   346   362   352   369     0     0    55\n","proband15      388   395   375   374   392   343   296    51\n","proband2       331   322   373   312   358   259   281    56\n","proband3       415   443   370   368   371   348   332    58\n","proband4       388   615   395   380   418     0     0    51\n","proband5       413   659   404   358   382   363   294    59\n","proband6       362   370   399   363   378   309   290    60\n","proband7       360   434   386   391   375     0     0    56\n","proband8       375   368   401   399   378   674   245    54\n","proband9       369   468   385   392   384   321   296    61\n","\n","✓ Class configuration saved: configs/classes.json\n","✓ Classes included for training (8/8): ['walking', 'running', 'sitting', 'standing', 'lying', 'stairs_up', 'stairs_down', 'jumping']\n","[master 983d4e9] data: add standard-level tail classes and inclusion flags\n"," 1 file changed, 291 insertions(+)\n"," create mode 100644 configs/classes.json\n","\n","============================================================\n","Step 6 completed\n","============================================================\n"]}]},{"cell_type":"code","source":["# ================ Step 7: LOSO Subject Splits ================\n","import pandas as pd\n","from pathlib import Path\n","import json\n","\n","print(\"\\n\\nStep 7: LOSO Subject Splits\")\n","print(\"=\" * 60)\n","\n","proc_dir = Path('/content/proc')\n","\n","# Scan all files and extract subjects\n","proc_files = sorted(proc_dir.glob('*.csv'))\n","print(f\"Found {len(proc_files)} files\")\n","\n","subjects = set()\n","file_subject_map = {}\n","\n","for filepath in proc_files:\n","    df = pd.read_csv(filepath)\n","    subject = df['proband'].iloc[0] if 'proband' in df.columns else filepath.stem.split('_')[0]\n","    subjects.add(subject)\n","    file_subject_map[filepath.name] = subject\n","\n","subjects = sorted(subjects)\n","print(f\"\\n✓ Total subjects: {len(subjects)}\")\n","print(f\"Subject list: {subjects}\")\n","\n","# Create LOSO folds\n","loso_splits = []\n","\n","for fold_id, test_subject in enumerate(subjects):\n","    train_subjects = [s for s in subjects if s != test_subject]\n","\n","    loso_splits.append({\n","        'fold': fold_id,\n","        'test_subject': test_subject,\n","        'train_subjects': train_subjects,\n","        'n_train': len(train_subjects),\n","        'n_test': 1\n","    })\n","\n","    print(f\"\\nFold {fold_id}: Test={test_subject}, Train={train_subjects}\")\n","\n","# Save as CSV\n","splits_csv = []\n","for split in loso_splits:\n","    splits_csv.append({\n","        'fold': split['fold'],\n","        'test_subject': split['test_subject'],\n","        'train_subjects': ','.join(split['train_subjects']),\n","        'n_train': split['n_train'],\n","        'n_test': split['n_test']\n","    })\n","\n","df_splits = pd.DataFrame(splits_csv)\n","df_splits.to_csv('/content/logs/splits.csv', index=False)\n","print(f\"\\n✓ Splits saved: logs/splits.csv\")\n","print(\"\\n\" + df_splits.to_string(index=False))\n","\n","# Save as JSON (for convenient downstream loading)\n","splits_config = {\n","    'split_method': 'LOSO',\n","    'n_folds': len(subjects),\n","    'subjects': subjects,\n","    'file_subject_map': file_subject_map,\n","    'folds': loso_splits\n","}\n","\n","with open('/content/configs/splits.json', 'w') as f:\n","    json.dump(splits_config, f, indent=2)\n","\n","print(f\"\\n✓ Split configuration saved: configs/splits.json\")\n","\n","# Validation: each subject is used exactly once as test set\n","test_subjects_count = pd.Series([s['test_subject'] for s in loso_splits]).value_counts()\n","assert (test_subjects_count == 1).all(), \"Each subject should appear exactly once as the test set\"\n","print(f\"\\n✓ Validation passed: each subject appears exactly once as the test set\")\n","\n","get_ipython().system('git add logs/splits.csv configs/splits.json')\n","get_ipython().system('git commit -m \"split: create LOSO folds (leave-one-subject-out)\"')\n","\n","print(f\"\\n{'='*60}\\nStep 7 completed\\n{'='*60}\")"],"metadata":{"id":"T0WKJkW0ar5J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762895203925,"user_tz":0,"elapsed":5909,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"5a276406-c5ca-4420-f81c-86b4afd8e965"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Step 7: LOSO Subject Splits\n","============================================================\n","Found 112 files\n","\n","✓ Total subjects: 15\n","Subject list: ['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","\n","Fold 0: Test=proband1, Train=['proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","\n","Fold 1: Test=proband10, Train=['proband1', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","\n","Fold 2: Test=proband11, Train=['proband1', 'proband10', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","\n","Fold 3: Test=proband12, Train=['proband1', 'proband10', 'proband11', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","\n","Fold 4: Test=proband13, Train=['proband1', 'proband10', 'proband11', 'proband12', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","\n","Fold 5: Test=proband14, Train=['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","\n","Fold 6: Test=proband15, Train=['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","\n","Fold 7: Test=proband2, Train=['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","\n","Fold 8: Test=proband3, Train=['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","\n","Fold 9: Test=proband4, Train=['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","\n","Fold 10: Test=proband5, Train=['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband6', 'proband7', 'proband8', 'proband9']\n","\n","Fold 11: Test=proband6, Train=['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband7', 'proband8', 'proband9']\n","\n","Fold 12: Test=proband7, Train=['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband8', 'proband9']\n","\n","Fold 13: Test=proband8, Train=['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband9']\n","\n","Fold 14: Test=proband9, Train=['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8']\n","\n","✓ Splits saved: logs/splits.csv\n","\n"," fold test_subject                                                                                                                      train_subjects  n_train  n_test\n","    0     proband1 proband10,proband11,proband12,proband13,proband14,proband15,proband2,proband3,proband4,proband5,proband6,proband7,proband8,proband9       14       1\n","    1    proband10  proband1,proband11,proband12,proband13,proband14,proband15,proband2,proband3,proband4,proband5,proband6,proband7,proband8,proband9       14       1\n","    2    proband11  proband1,proband10,proband12,proband13,proband14,proband15,proband2,proband3,proband4,proband5,proband6,proband7,proband8,proband9       14       1\n","    3    proband12  proband1,proband10,proband11,proband13,proband14,proband15,proband2,proband3,proband4,proband5,proband6,proband7,proband8,proband9       14       1\n","    4    proband13  proband1,proband10,proband11,proband12,proband14,proband15,proband2,proband3,proband4,proband5,proband6,proband7,proband8,proband9       14       1\n","    5    proband14  proband1,proband10,proband11,proband12,proband13,proband15,proband2,proband3,proband4,proband5,proband6,proband7,proband8,proband9       14       1\n","    6    proband15  proband1,proband10,proband11,proband12,proband13,proband14,proband2,proband3,proband4,proband5,proband6,proband7,proband8,proband9       14       1\n","    7     proband2 proband1,proband10,proband11,proband12,proband13,proband14,proband15,proband3,proband4,proband5,proband6,proband7,proband8,proband9       14       1\n","    8     proband3 proband1,proband10,proband11,proband12,proband13,proband14,proband15,proband2,proband4,proband5,proband6,proband7,proband8,proband9       14       1\n","    9     proband4 proband1,proband10,proband11,proband12,proband13,proband14,proband15,proband2,proband3,proband5,proband6,proband7,proband8,proband9       14       1\n","   10     proband5 proband1,proband10,proband11,proband12,proband13,proband14,proband15,proband2,proband3,proband4,proband6,proband7,proband8,proband9       14       1\n","   11     proband6 proband1,proband10,proband11,proband12,proband13,proband14,proband15,proband2,proband3,proband4,proband5,proband7,proband8,proband9       14       1\n","   12     proband7 proband1,proband10,proband11,proband12,proband13,proband14,proband15,proband2,proband3,proband4,proband5,proband6,proband8,proband9       14       1\n","   13     proband8 proband1,proband10,proband11,proband12,proband13,proband14,proband15,proband2,proband3,proband4,proband5,proband6,proband7,proband9       14       1\n","   14     proband9 proband1,proband10,proband11,proband12,proband13,proband14,proband15,proband2,proband3,proband4,proband5,proband6,proband7,proband8       14       1\n","\n","✓ Split configuration saved: configs/splits.json\n","\n","✓ Validation passed: each subject appears exactly once as the test set\n","[master 584799f] split: create LOSO folds (leave-one-subject-out)\n"," 2 files changed, 483 insertions(+)\n"," create mode 100644 configs/splits.json\n"," create mode 100644 logs/splits.csv\n","\n","============================================================\n","Step 7 completed\n","============================================================\n"]}]},{"cell_type":"code","source":["# ================ Step 8: Sliding Windowing and Label Assignment ================\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import json\n","from collections import defaultdict\n","\n","print(\"\\n\\nStep 8: Sliding Windowing and Label Assignment\")\n","print(\"=\" * 60)\n","\n","# Load configuration\n","with open('/content/configs/classes.json', 'r') as f:\n","    classes_cfg = json.load(f)\n","\n","with open('/content/configs/splits.json', 'r') as f:\n","    splits_cfg = json.load(f)\n","\n","proc_dir = Path('/content/proc')\n","features_dir = Path('/content/features')\n","features_dir.mkdir(exist_ok=True)\n","\n","# Window parameters\n","WINDOW_SEC = 3\n","OVERLAP = 0.5\n","TARGET_FS = 50\n","WINDOW_SAMPLES = int(TARGET_FS * WINDOW_SEC)\n","STRIDE_SAMPLES = int(WINDOW_SAMPLES * (1 - OVERLAP))\n","DOMINANT_THRESHOLD = 0.8\n","\n","label_to_id = classes_cfg['label_to_id']\n","\n","print(f\"Window parameters: {WINDOW_SEC}s ({WINDOW_SAMPLES} samples), overlap {OVERLAP*100:.0f}%, stride {STRIDE_SAMPLES}\")\n","print(f\"Dominant-label threshold: {DOMINANT_THRESHOLD*100:.0f}%\\n\")\n","\n","# Process each file to generate all windows\n","proc_files = sorted(proc_dir.glob('*.csv'))\n","print(f\"Processing {len(proc_files)} files...\\n\")\n","\n","all_windows = []\n","discarded_windows = 0\n","\n","for file_idx, filepath in enumerate(proc_files):\n","    df = pd.read_csv(filepath)\n","\n","    subject = df['proband'].iloc[0]\n","    activity = df['activity'].iloc[0]\n","    std_label = classes_cfg['activity_mapping'].get(activity, activity)\n","    label_id = label_to_id[std_label]\n","\n","    file_windows = 0\n","    for seg_id, seg_df in df.groupby('segment_id'):\n","        seg_df = seg_df.reset_index(drop=True)\n","        seg_len = len(seg_df)\n","\n","        if seg_len < WINDOW_SAMPLES:\n","            continue\n","\n","        for start_idx in range(0, seg_len - WINDOW_SAMPLES + 1, STRIDE_SAMPLES):\n","            end_idx = start_idx + WINDOW_SAMPLES\n","            window = seg_df.iloc[start_idx:end_idx]\n","\n","            # Check dominant label\n","            window_labels = window['activity'].values\n","            unique_labels, counts = np.unique(window_labels, return_counts=True)\n","            dominant_idx = counts.argmax()\n","            dominant_label = unique_labels[dominant_idx]\n","            dominant_ratio = counts[dominant_idx] / len(window_labels)\n","\n","            if dominant_ratio < DOMINANT_THRESHOLD:\n","                discarded_windows += 1\n","                continue\n","\n","            # Save window\n","            window_data = {\n","                'subject': subject,\n","                'activity': std_label,\n","                'label': label_id,\n","                'file': filepath.name,\n","                'segment_id': seg_id,\n","                'start_idx': start_idx,\n","                'dominant_ratio': dominant_ratio\n","            }\n","\n","            for col in ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']:\n","                window_data[col] = window[col].values.tolist()\n","\n","            all_windows.append(window_data)\n","            file_windows += 1\n","\n","    print(f\"[{file_idx+1}/{len(proc_files)}] {filepath.name}: {file_windows} windows ({std_label}, {subject})\")\n","\n","print(f\"\\n✓ Total windows: {len(all_windows)}\")\n","print(f\"✓ Discarded windows: {discarded_windows} (dominant label < {DOMINANT_THRESHOLD*100:.0f}%)\")\n","\n","# Save window metadata (excluding sensor data)\n","windows_meta = pd.DataFrame([{k: v for k, v in w.items()\n","                              if k not in ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']}\n","                             for w in all_windows])\n","\n","# Add window IDs\n","windows_meta['window_id'] = (windows_meta['file'] + ':' +\n","                              windows_meta['segment_id'].astype(str) + ':' +\n","                              windows_meta['start_idx'].astype(str))\n","\n","windows_meta.to_csv(features_dir / 'windows_meta.csv', index=False)\n","print(f\"\\n✓ Global window metadata: features/windows_meta.csv\")\n","\n","# Save complete window data\n","with open(features_dir / 'windows_raw.json', 'w') as f:\n","    json.dump(all_windows, f)\n","print(f\"✓ Raw window data: features/windows_raw.json\")\n","\n","# Generate train/test split per fold\n","print(\"\\n\" + \"=\"*60)\n","print(\"Generate train/test splits per fold:\")\n","print(\"=\"*60)\n","\n","per_fold_totals = []\n","\n","for fold in splits_cfg['folds']:\n","    k = fold['fold']\n","    test_subj = fold['test_subject']\n","\n","    # Mark train/test\n","    fold_meta = windows_meta.copy()\n","    fold_meta['fold'] = k\n","    fold_meta['split'] = np.where(fold_meta['subject'] == test_subj, 'test', 'train')\n","\n","    # Save metadata for this fold\n","    fold_meta.to_csv(features_dir / f'windows_meta_fold{k}.csv', index=False)\n","\n","    # Per-fold statistics\n","    stats = fold_meta.groupby(['split', 'activity', 'subject']).size().reset_index(name='windows')\n","    stats.to_csv(f'/content/logs/window_stats_fold{k}.csv', index=False)\n","\n","    n_train = int((fold_meta['split'] == 'train').sum())\n","    n_test = int((fold_meta['split'] == 'test').sum())\n","\n","    per_fold_totals.append({\n","        'fold': k,\n","        'test_subject': test_subj,\n","        'n_train_windows': n_train,\n","        'n_test_windows': n_test,\n","        'n_total': n_train + n_test\n","    })\n","\n","    print(f\"Fold {k}: Train={n_train}, Test={n_test}, test subject={test_subj}\")\n","\n","# Save fold-level summary\n","df_fold_totals = pd.DataFrame(per_fold_totals)\n","df_fold_totals.to_csv('/content/logs/window_fold_totals.csv', index=False)\n","print(f\"\\n✓ Fold-level summary: logs/window_fold_totals.csv\")\n","\n","# Global summary\n","summary = {\n","    'total_windows': len(all_windows),\n","    'discarded_windows': discarded_windows,\n","    'window_params': {\n","        'window_size_sec': WINDOW_SEC,\n","        'window_samples': WINDOW_SAMPLES,\n","        'overlap': OVERLAP,\n","        'stride_samples': STRIDE_SAMPLES,\n","        'dominant_threshold': DOMINANT_THRESHOLD\n","    },\n","    'per_class_totals': windows_meta.groupby('activity')['window_id'].count().to_dict(),\n","    'per_subject_totals': windows_meta.groupby('subject')['window_id'].count().to_dict()\n","}\n","\n","with open('/content/logs/window_summary.json', 'w') as f:\n","    json.dump(summary, f, indent=2)\n","\n","print(\"\\nGlobal statistics:\")\n","print(f\"  Per class: {summary['per_class_totals']}\")\n","print(f\"  Per subject: {summary['per_subject_totals']}\")\n","\n","get_ipython().system('git add features/ logs/window_*.csv logs/window_*.json')\n","get_ipython().system('git commit -m \"feature: windowing with per-fold train/test splits\"')\n","\n","print(f\"\\n{'='*60}\\nStep 8 completed\\n{'='*60}\")"],"metadata":{"id":"q0ESTSQ0atL6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762895300107,"user_tz":0,"elapsed":96163,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"10b63f01-dd68-4545-9c15-9f147118435d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Step 8: Sliding Windowing and Label Assignment\n","============================================================\n","Window parameters: 3s (150 samples), overlap 50%, stride 75\n","Dominant-label threshold: 80%\n","\n","Processing 112 files...\n","\n","[1/112] proband10_climbingdown_waist.csv: 254 windows (stairs_down, proband10)\n","[2/112] proband10_climbingup_waist.csv: 264 windows (stairs_up, proband10)\n","[3/112] proband10_jumping_waist.csv: 68 windows (jumping, proband10)\n","[4/112] proband10_lying_waist.csv: 384 windows (lying, proband10)\n","[5/112] proband10_running_waist.csv: 367 windows (running, proband10)\n","[6/112] proband10_sitting_waist.csv: 366 windows (sitting, proband10)\n","[7/112] proband10_standing_waist.csv: 388 windows (standing, proband10)\n","[8/112] proband10_walking_waist.csv: 372 windows (walking, proband10)\n","[9/112] proband11_climbingdown_waist.csv: 293 windows (stairs_down, proband11)\n","[10/112] proband11_climbingup_waist.csv: 367 windows (stairs_up, proband11)\n","[11/112] proband11_jumping_waist.csv: 53 windows (jumping, proband11)\n","[12/112] proband11_lying_waist.csv: 396 windows (lying, proband11)\n","[13/112] proband11_running_waist.csv: 347 windows (running, proband11)\n","[14/112] proband11_sitting_waist.csv: 364 windows (sitting, proband11)\n","[15/112] proband11_standing_waist.csv: 378 windows (standing, proband11)\n","[16/112] proband11_walking_waist.csv: 382 windows (walking, proband11)\n","[17/112] proband12_climbingdown_waist.csv: 289 windows (stairs_down, proband12)\n","[18/112] proband12_climbingup_waist.csv: 345 windows (stairs_up, proband12)\n","[19/112] proband12_jumping_waist.csv: 41 windows (jumping, proband12)\n","[20/112] proband12_lying_waist.csv: 373 windows (lying, proband12)\n","[21/112] proband12_running_waist.csv: 349 windows (running, proband12)\n","[22/112] proband12_sitting_waist.csv: 299 windows (sitting, proband12)\n","[23/112] proband12_standing_waist.csv: 353 windows (standing, proband12)\n","[24/112] proband12_walking_waist.csv: 318 windows (walking, proband12)\n","[25/112] proband13_climbingdown_waist.csv: 252 windows (stairs_down, proband13)\n","[26/112] proband13_climbingup_waist.csv: 353 windows (stairs_up, proband13)\n","[27/112] proband13_jumping_waist.csv: 69 windows (jumping, proband13)\n","[28/112] proband13_lying_waist.csv: 385 windows (lying, proband13)\n","[29/112] proband13_running_waist.csv: 368 windows (running, proband13)\n","[30/112] proband13_sitting_waist.csv: 380 windows (sitting, proband13)\n","[31/112] proband13_standing_waist.csv: 384 windows (standing, proband13)\n","[32/112] proband13_walking_waist.csv: 398 windows (walking, proband13)\n","[33/112] proband14_jumping_waist.csv: 55 windows (jumping, proband14)\n","[34/112] proband14_lying_waist.csv: 369 windows (lying, proband14)\n","[35/112] proband14_running_waist.csv: 346 windows (running, proband14)\n","[36/112] proband14_sitting_waist.csv: 362 windows (sitting, proband14)\n","[37/112] proband14_standing_waist.csv: 352 windows (standing, proband14)\n","[38/112] proband14_walking_waist.csv: 351 windows (walking, proband14)\n","[39/112] proband15_climbingdown_waist.csv: 296 windows (stairs_down, proband15)\n","[40/112] proband15_climbingup_waist.csv: 343 windows (stairs_up, proband15)\n","[41/112] proband15_jumping_waist.csv: 51 windows (jumping, proband15)\n","[42/112] proband15_lying_waist.csv: 392 windows (lying, proband15)\n","[43/112] proband15_running_waist.csv: 395 windows (running, proband15)\n","[44/112] proband15_sitting_waist.csv: 375 windows (sitting, proband15)\n","[45/112] proband15_standing_waist.csv: 374 windows (standing, proband15)\n","[46/112] proband15_walking_waist.csv: 388 windows (walking, proband15)\n","[47/112] proband1_climbingdown_waist.csv: 303 windows (stairs_down, proband1)\n","[48/112] proband1_climbingup_waist.csv: 385 windows (stairs_up, proband1)\n","[49/112] proband1_jumping_waist.csv: 50 windows (jumping, proband1)\n","[50/112] proband1_running_waist.csv: 379 windows (running, proband1)\n","[51/112] proband1_standing_waist.csv: 382 windows (standing, proband1)\n","[52/112] proband1_walking_waist.csv: 396 windows (walking, proband1)\n","[53/112] proband2_climbingdown_waist.csv: 281 windows (stairs_down, proband2)\n","[54/112] proband2_climbingup_waist.csv: 259 windows (stairs_up, proband2)\n","[55/112] proband2_jumping_waist.csv: 56 windows (jumping, proband2)\n","[56/112] proband2_lying_waist.csv: 358 windows (lying, proband2)\n","[57/112] proband2_running_waist.csv: 322 windows (running, proband2)\n","[58/112] proband2_sitting_waist.csv: 373 windows (sitting, proband2)\n","[59/112] proband2_standing_waist.csv: 312 windows (standing, proband2)\n","[60/112] proband2_walking_waist.csv: 331 windows (walking, proband2)\n","[61/112] proband3_climbingdown_waist.csv: 332 windows (stairs_down, proband3)\n","[62/112] proband3_climbingup_waist.csv: 348 windows (stairs_up, proband3)\n","[63/112] proband3_jumping_waist.csv: 58 windows (jumping, proband3)\n","[64/112] proband3_lying_waist.csv: 371 windows (lying, proband3)\n","[65/112] proband3_running_waist.csv: 443 windows (running, proband3)\n","[66/112] proband3_sitting_waist.csv: 370 windows (sitting, proband3)\n","[67/112] proband3_standing_waist.csv: 368 windows (standing, proband3)\n","[68/112] proband3_walking_waist.csv: 415 windows (walking, proband3)\n","[69/112] proband4_jumping_waist.csv: 51 windows (jumping, proband4)\n","[70/112] proband4_lying_waist.csv: 418 windows (lying, proband4)\n","[71/112] proband4_running_waist.csv: 615 windows (running, proband4)\n","[72/112] proband4_sitting_waist.csv: 395 windows (sitting, proband4)\n","[73/112] proband4_standing_waist.csv: 380 windows (standing, proband4)\n","[74/112] proband4_walking_waist.csv: 388 windows (walking, proband4)\n","[75/112] proband5_climbingdown_waist.csv: 294 windows (stairs_down, proband5)\n","[76/112] proband5_climbingup_waist.csv: 363 windows (stairs_up, proband5)\n","[77/112] proband5_jumping_waist.csv: 59 windows (jumping, proband5)\n","[78/112] proband5_lying_waist.csv: 382 windows (lying, proband5)\n","[79/112] proband5_running_waist.csv: 659 windows (running, proband5)\n","[80/112] proband5_sitting_waist.csv: 404 windows (sitting, proband5)\n","[81/112] proband5_standing_waist.csv: 358 windows (standing, proband5)\n","[82/112] proband5_walking_waist.csv: 413 windows (walking, proband5)\n","[83/112] proband6_climbingdown_waist.csv: 290 windows (stairs_down, proband6)\n","[84/112] proband6_climbingup_waist.csv: 309 windows (stairs_up, proband6)\n","[85/112] proband6_jumping_waist.csv: 60 windows (jumping, proband6)\n","[86/112] proband6_lying_waist.csv: 378 windows (lying, proband6)\n","[87/112] proband6_running_waist.csv: 370 windows (running, proband6)\n","[88/112] proband6_sitting_waist.csv: 399 windows (sitting, proband6)\n","[89/112] proband6_standing_waist.csv: 363 windows (standing, proband6)\n","[90/112] proband6_walking_waist.csv: 362 windows (walking, proband6)\n","[91/112] proband7_jumping_waist.csv: 56 windows (jumping, proband7)\n","[92/112] proband7_lying_waist.csv: 375 windows (lying, proband7)\n","[93/112] proband7_running_waist.csv: 434 windows (running, proband7)\n","[94/112] proband7_sitting_waist.csv: 386 windows (sitting, proband7)\n","[95/112] proband7_standing_waist.csv: 391 windows (standing, proband7)\n","[96/112] proband7_walking_waist.csv: 360 windows (walking, proband7)\n","[97/112] proband8_climbingdown_waist.csv: 245 windows (stairs_down, proband8)\n","[98/112] proband8_climbingup_waist.csv: 674 windows (stairs_up, proband8)\n","[99/112] proband8_jumping_waist.csv: 54 windows (jumping, proband8)\n","[100/112] proband8_lying_waist.csv: 378 windows (lying, proband8)\n","[101/112] proband8_running_waist.csv: 368 windows (running, proband8)\n","[102/112] proband8_sitting_waist.csv: 401 windows (sitting, proband8)\n","[103/112] proband8_standing_waist.csv: 399 windows (standing, proband8)\n","[104/112] proband8_walking_waist.csv: 375 windows (walking, proband8)\n","[105/112] proband9_climbingdown_waist.csv: 296 windows (stairs_down, proband9)\n","[106/112] proband9_climbingup_waist.csv: 321 windows (stairs_up, proband9)\n","[107/112] proband9_jumping_waist.csv: 61 windows (jumping, proband9)\n","[108/112] proband9_lying_waist.csv: 384 windows (lying, proband9)\n","[109/112] proband9_running_waist.csv: 468 windows (running, proband9)\n","[110/112] proband9_sitting_waist.csv: 385 windows (sitting, proband9)\n","[111/112] proband9_standing_waist.csv: 392 windows (standing, proband9)\n","[112/112] proband9_walking_waist.csv: 369 windows (walking, proband9)\n","\n","✓ Total windows: 36622\n","✓ Discarded windows: 0 (dominant label < 80%)\n","\n","✓ Global window metadata: features/windows_meta.csv\n","✓ Raw window data: features/windows_raw.json\n","\n","============================================================\n","Generate train/test splits per fold:\n","============================================================\n","Fold 0: Train=34727, Test=1895, test subject=proband1\n","Fold 1: Train=34159, Test=2463, test subject=proband10\n","Fold 2: Train=34042, Test=2580, test subject=proband11\n","Fold 3: Train=34255, Test=2367, test subject=proband12\n","Fold 4: Train=34033, Test=2589, test subject=proband13\n","Fold 5: Train=34787, Test=1835, test subject=proband14\n","Fold 6: Train=34008, Test=2614, test subject=proband15\n","Fold 7: Train=34330, Test=2292, test subject=proband2\n","Fold 8: Train=33917, Test=2705, test subject=proband3\n","Fold 9: Train=34375, Test=2247, test subject=proband4\n","Fold 10: Train=33690, Test=2932, test subject=proband5\n","Fold 11: Train=34091, Test=2531, test subject=proband6\n","Fold 12: Train=34620, Test=2002, test subject=proband7\n","Fold 13: Train=33728, Test=2894, test subject=proband8\n","Fold 14: Train=33946, Test=2676, test subject=proband9\n","\n","✓ Fold-level summary: logs/window_fold_totals.csv\n","\n","Global statistics:\n","  Per class: {'jumping': 842, 'lying': 5343, 'running': 6230, 'sitting': 5259, 'stairs_down': 3425, 'stairs_up': 4331, 'standing': 5574, 'walking': 5618}\n","  Per subject: {'proband1': 1895, 'proband10': 2463, 'proband11': 2580, 'proband12': 2367, 'proband13': 2589, 'proband14': 1835, 'proband15': 2614, 'proband2': 2292, 'proband3': 2705, 'proband4': 2247, 'proband5': 2932, 'proband6': 2531, 'proband7': 2002, 'proband8': 2894, 'proband9': 2676}\n","[master 30a9e75] feature: windowing with per-fold train/test splits\n"," 34 files changed, 587717 insertions(+)\n"," create mode 100644 features/windows_meta.csv\n"," create mode 100644 features/windows_meta_fold0.csv\n"," create mode 100644 features/windows_meta_fold1.csv\n"," create mode 100644 features/windows_meta_fold10.csv\n"," create mode 100644 features/windows_meta_fold11.csv\n"," create mode 100644 features/windows_meta_fold12.csv\n"," create mode 100644 features/windows_meta_fold13.csv\n"," create mode 100644 features/windows_meta_fold14.csv\n"," create mode 100644 features/windows_meta_fold2.csv\n"," create mode 100644 features/windows_meta_fold3.csv\n"," create mode 100644 features/windows_meta_fold4.csv\n"," create mode 100644 features/windows_meta_fold5.csv\n"," create mode 100644 features/windows_meta_fold6.csv\n"," create mode 100644 features/windows_meta_fold7.csv\n"," create mode 100644 features/windows_meta_fold8.csv\n"," create mode 100644 features/windows_meta_fold9.csv\n"," create mode 100644 features/windows_raw.json\n"," create mode 100644 logs/window_fold_totals.csv\n"," create mode 100644 logs/window_stats_fold0.csv\n"," create mode 100644 logs/window_stats_fold1.csv\n"," create mode 100644 logs/window_stats_fold10.csv\n"," create mode 100644 logs/window_stats_fold11.csv\n"," create mode 100644 logs/window_stats_fold12.csv\n"," create mode 100644 logs/window_stats_fold13.csv\n"," create mode 100644 logs/window_stats_fold14.csv\n"," create mode 100644 logs/window_stats_fold2.csv\n"," create mode 100644 logs/window_stats_fold3.csv\n"," create mode 100644 logs/window_stats_fold4.csv\n"," create mode 100644 logs/window_stats_fold5.csv\n"," create mode 100644 logs/window_stats_fold6.csv\n"," create mode 100644 logs/window_stats_fold7.csv\n"," create mode 100644 logs/window_stats_fold8.csv\n"," create mode 100644 logs/window_stats_fold9.csv\n"," create mode 100644 logs/window_summary.json\n","\n","============================================================\n","Step 8 completed\n","============================================================\n"]}]},{"cell_type":"code","source":["# ================ Step 9: Per-Fold Standardization (Performance-Optimized) ================\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import json\n","\n","print(\"\\n\\nStep 9: Per-Fold Standardization (z-score)\")\n","print(\"=\" * 60)\n","\n","# Load configuration\n","with open('/content/configs/splits.json', 'r') as f:\n","    splits_cfg = json.load(f)\n","\n","# Load window data\n","with open('/content/features/windows_raw.json', 'r') as f:\n","    all_windows = json.load(f)\n","\n","features_dir = Path('/content/features')\n","proc_dir = Path('/content/proc')\n","\n","CHANNELS = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']\n","EPS = 1e-8\n","\n","print(f\"Channels: {CHANNELS}\")\n","print(f\"Total windows: {len(all_windows)}\\n\")\n","\n","scaler_summary = []\n","\n","for fold in splits_cfg['folds']:\n","    k = fold['fold']\n","    test_subj = fold['test_subject']\n","\n","    print(f\"\\nFold {k}: test subject={test_subj}\")\n","\n","    fold_meta = pd.read_csv(features_dir / f'windows_meta_fold{k}.csv')\n","    assert len(all_windows) == len(fold_meta), f\"Window count mismatch: {len(all_windows)} vs {len(fold_meta)}\"\n","\n","    train_indices = set(fold_meta[fold_meta['split'] == 'train'].index.tolist())\n","    test_indices = set(fold_meta[fold_meta['split'] == 'test'].index.tolist())\n","\n","    print(f\"  Train windows: {len(train_indices)}, Test windows: {len(test_indices)}\")\n","\n","    # Vectorized collection of training data\n","    train_data = {ch: [] for ch in CHANNELS}\n","    for idx in train_indices:\n","        window = all_windows[idx]\n","        for ch in CHANNELS:\n","            train_data[ch].extend(window[ch])\n","\n","    # Convert to NumPy arrays and compute parameters\n","    scaler_params = {}\n","    for ch in CHANNELS:\n","        data = np.array(train_data[ch], dtype=np.float32)\n","        mean = float(data.mean())\n","        std = float(max(data.std(), EPS))\n","        scaler_params[ch] = {'mean': mean, 'std': std}\n","\n","    print(f\"  Scaler parameters:\")\n","    for ch in CHANNELS:\n","        print(f\"    {ch}: mean={scaler_params[ch]['mean']:.4f}, std={scaler_params[ch]['std']:.4f}\")\n","\n","    # Vectorized standardization and save as NPZ\n","    norm_data = {\n","        'window_ids': [],\n","        'subjects': [],\n","        'activities': [],\n","        'labels': [],\n","        'splits': []\n","    }\n","    for ch in CHANNELS:\n","        norm_data[ch] = []\n","\n","    train_norm = {ch: [] for ch in CHANNELS}\n","    test_norm = {ch: [] for ch in CHANNELS}\n","\n","    for idx in range(len(all_windows)):\n","        window = all_windows[idx]\n","\n","        if idx in train_indices:\n","            split = 'train'\n","        elif idx in test_indices:\n","            split = 'test'\n","        else:\n","            continue\n","\n","        norm_data['window_ids'].append(fold_meta.loc[idx, 'window_id'])\n","        norm_data['subjects'].append(window['subject'])\n","        norm_data['activities'].append(window['activity'])\n","        norm_data['labels'].append(window['label'])\n","        norm_data['splits'].append(split)\n","\n","        for ch in CHANNELS:\n","            data = np.array(window[ch], dtype=np.float32)\n","            normalized = (data - scaler_params[ch]['mean']) / scaler_params[ch]['std']\n","            norm_data[ch].append(normalized)\n","\n","            # Collect statistics for validation\n","            if split == 'train':\n","                train_norm[ch].extend(normalized)\n","            else:\n","                test_norm[ch].extend(normalized)\n","\n","    # Post-standardization validation: training set\n","    print(f\"  Training-set validation after standardization:\")\n","    for ch in CHANNELS:\n","        mean_val = np.mean(train_norm[ch])\n","        std_val = np.std(train_norm[ch])\n","        print(f\"    {ch}: mean={mean_val:.6f}, std={std_val:.6f}\")\n","\n","    # Post-standardization validation: test set\n","    print(f\"  Test-set validation after standardization:\")\n","    for ch in CHANNELS:\n","        if test_norm[ch]:\n","            mean_val = np.mean(test_norm[ch])\n","            print(f\"    {ch}: mean={mean_val:.6f}\")\n","\n","    # Persist scaler parameters\n","    scaler_file = proc_dir / f'scaler_fold{k}.npz'\n","    np.savez(scaler_file, **{f'{ch}_mean': scaler_params[ch]['mean'] for ch in CHANNELS},\n","                          **{f'{ch}_std': scaler_params[ch]['std'] for ch in CHANNELS})\n","\n","    # Persist standardized windows as NPZ (float32)\n","    norm_file = features_dir / f'windows_normalized_fold{k}.npz'\n","    np.savez_compressed(norm_file,\n","                       window_ids=np.array(norm_data['window_ids']),\n","                       subjects=np.array(norm_data['subjects']),\n","                       activities=np.array(norm_data['activities']),\n","                       labels=np.array(norm_data['labels'], dtype=np.int32),\n","                       splits=np.array(norm_data['splits']),\n","                       **{ch: np.array(norm_data[ch], dtype=np.float32) for ch in CHANNELS})\n","\n","    print(f\"  ✓ Saved: {scaler_file.name}, {norm_file.name}\")\n","\n","    scaler_summary.append({\n","        'fold': k,\n","        'test_subject': test_subj,\n","        'n_train': len(train_indices),\n","        'n_test': len(test_indices),\n","        'scaler_params': scaler_params\n","    })\n","\n","with open('/content/logs/scaler_summary.json', 'w') as f:\n","    json.dump(scaler_summary, f, indent=2)\n","\n","print(f\"\\n{'='*60}\")\n","print(f\"✓ Completed standardization across {len(splits_cfg['folds'])} folds\")\n","print(f\"✓ Scaler parameters: proc/scaler_fold*.npz\")\n","print(f\"✓ Standardized data: features/windows_normalized_fold*.npz (NPZ/float32)\")\n","print(f\"✓ Summary: logs/scaler_summary.json\")\n","\n","get_ipython().system('git add proc/scaler_fold*.npz features/windows_normalized_fold*.npz logs/scaler_summary.json')\n","get_ipython().system('git commit -m \"preproc: optimized z-score with NPZ storage and validation\"')\n","\n","print(f\"\\n{'='*60}\\nStep 9 completed\\n{'='*60}\")"],"metadata":{"id":"mSgweW8faufK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762895596340,"user_tz":0,"elapsed":296227,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"d0e24bd6-ee56-4c20-ba7f-a9ebdbf4edd1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Step 9: Per-Fold Standardization (z-score)\n","============================================================\n","Channels: ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']\n","Total windows: 36622\n","\n","\n","Fold 0: test subject=proband1\n","  Train windows: 34727, Test windows: 1895\n","  Scaler parameters:\n","    acc_x: mean=-0.0001, std=3.8156\n","    acc_y: mean=0.0000, std=1.8273\n","    acc_z: mean=0.0001, std=2.0051\n","    gyro_x: mean=-0.0001, std=0.5433\n","    gyro_y: mean=-0.0000, std=0.6868\n","    gyro_z: mean=-0.0001, std=0.3573\n","  Training-set validation after standardization:\n","    acc_x: mean=0.000000, std=1.000000\n","    acc_y: mean=0.000000, std=1.000000\n","    acc_z: mean=-0.000000, std=1.000000\n","    gyro_x: mean=0.000000, std=1.000000\n","    gyro_y: mean=0.000000, std=1.000000\n","    gyro_z: mean=0.000000, std=1.000000\n","  Test-set validation after standardization:\n","    acc_x: mean=-0.000124\n","    acc_y: mean=0.000252\n","    acc_z: mean=0.000556\n","    gyro_x: mean=0.001704\n","    gyro_y: mean=-0.000234\n","    gyro_z: mean=0.000859\n","  ✓ Saved: scaler_fold0.npz, windows_normalized_fold0.npz\n","\n","Fold 1: test subject=proband10\n","  Train windows: 34159, Test windows: 2463\n","  Scaler parameters:\n","    acc_x: mean=-0.0003, std=3.7910\n","    acc_y: mean=0.0001, std=1.7633\n","    acc_z: mean=0.0002, std=1.9672\n","    gyro_x: mean=-0.0001, std=0.5352\n","    gyro_y: mean=-0.0000, std=0.6743\n","    gyro_z: mean=-0.0001, std=0.3485\n","  Training-set validation after standardization:\n","    acc_x: mean=-0.000000, std=1.000000\n","    acc_y: mean=-0.000000, std=1.000000\n","    acc_z: mean=-0.000000, std=1.000000\n","    gyro_x: mean=-0.000000, std=1.000000\n","    gyro_y: mean=-0.000000, std=1.000000\n","    gyro_z: mean=0.000000, std=1.000000\n","  Test-set validation after standardization:\n","    acc_x: mean=0.000563\n","    acc_y: mean=-0.000574\n","    acc_z: mean=-0.000248\n","    gyro_x: mean=-0.001031\n","    gyro_y: mean=0.000080\n","    gyro_z: mean=-0.000325\n","  ✓ Saved: scaler_fold1.npz, windows_normalized_fold1.npz\n","\n","Fold 2: test subject=proband11\n","  Train windows: 34042, Test windows: 2580\n","  Scaler parameters:\n","    acc_x: mean=-0.0003, std=3.8414\n","    acc_y: mean=-0.0000, std=1.8385\n","    acc_z: mean=0.0003, std=2.0168\n","    gyro_x: mean=-0.0000, std=0.5485\n","    gyro_y: mean=-0.0000, std=0.6836\n","    gyro_z: mean=-0.0001, std=0.3500\n","  Training-set validation after standardization:\n","    acc_x: mean=0.000000, std=1.000000\n","    acc_y: mean=0.000000, std=1.000000\n","    acc_z: mean=0.000000, std=1.000000\n","    gyro_x: mean=0.000000, std=1.000000\n","    gyro_y: mean=0.000000, std=1.000000\n","    gyro_z: mean=-0.000000, std=1.000000\n","  Test-set validation after standardization:\n","    acc_x: mean=0.000306\n","    acc_y: mean=0.000449\n","    acc_z: mean=-0.000364\n","    gyro_x: mean=-0.002474\n","    gyro_y: mean=-0.000271\n","    gyro_z: mean=0.000058\n","  ✓ Saved: scaler_fold2.npz, windows_normalized_fold2.npz\n","\n","Fold 3: test subject=proband12\n","  Train windows: 34255, Test windows: 2367\n","  Scaler parameters:\n","    acc_x: mean=0.0001, std=3.8025\n","    acc_y: mean=0.0001, std=1.8310\n","    acc_z: mean=0.0002, std=2.0121\n","    gyro_x: mean=-0.0001, std=0.5402\n","    gyro_y: mean=-0.0000, std=0.6889\n","    gyro_z: mean=-0.0001, std=0.3552\n","  Training-set validation after standardization:\n","    acc_x: mean=0.000000, std=1.000000\n","    acc_y: mean=0.000000, std=1.000000\n","    acc_z: mean=-0.000000, std=1.000000\n","    gyro_x: mean=0.000000, std=1.000000\n","    gyro_y: mean=0.000000, std=1.000000\n","    gyro_z: mean=0.000000, std=1.000000\n","  Test-set validation after standardization:\n","    acc_x: mean=-0.000945\n","    acc_y: mean=-0.000499\n","    acc_z: mean=-0.000188\n","    gyro_x: mean=-0.000246\n","    gyro_y: mean=-0.000541\n","    gyro_z: mean=0.000470\n","  ✓ Saved: scaler_fold3.npz, windows_normalized_fold3.npz\n","\n","Fold 4: test subject=proband13\n","  Train windows: 34033, Test windows: 2589\n","  Scaler parameters:\n","    acc_x: mean=0.0000, std=3.8211\n","    acc_y: mean=0.0001, std=1.8294\n","    acc_z: mean=0.0003, std=2.0081\n","    gyro_x: mean=-0.0001, std=0.5273\n","    gyro_y: mean=-0.0000, std=0.6912\n","    gyro_z: mean=-0.0001, std=0.3575\n","  Training-set validation after standardization:\n","    acc_x: mean=0.000000, std=1.000000\n","    acc_y: mean=0.000000, std=1.000000\n","    acc_z: mean=-0.000000, std=1.000000\n","    gyro_x: mean=-0.000000, std=1.000000\n","    gyro_y: mean=-0.000000, std=1.000000\n","    gyro_z: mean=0.000000, std=1.000000\n","  Test-set validation after standardization:\n","    acc_x: mean=-0.000733\n","    acc_y: mean=-0.000498\n","    acc_z: mean=-0.000663\n","    gyro_x: mean=0.000413\n","    gyro_y: mean=-0.000658\n","    gyro_z: mean=-0.000576\n","  ✓ Saved: scaler_fold4.npz, windows_normalized_fold4.npz\n","\n","Fold 5: test subject=proband14\n","  Train windows: 34787, Test windows: 1835\n","  Scaler parameters:\n","    acc_x: mean=-0.0003, std=3.7730\n","    acc_y: mean=-0.0001, std=1.7359\n","    acc_z: mean=0.0001, std=1.9575\n","    gyro_x: mean=-0.0002, std=0.5440\n","    gyro_y: mean=-0.0000, std=0.6808\n","    gyro_z: mean=-0.0001, std=0.3577\n","  Training-set validation after standardization:\n","    acc_x: mean=-0.000000, std=1.000000\n","    acc_y: mean=-0.000000, std=1.000000\n","    acc_z: mean=-0.000000, std=1.000000\n","    gyro_x: mean=-0.000000, std=1.000000\n","    gyro_y: mean=-0.000000, std=1.000000\n","    gyro_z: mean=-0.000000, std=1.000000\n","  Test-set validation after standardization:\n","    acc_x: mean=0.000918\n","    acc_y: mean=0.001726\n","    acc_z: mean=0.000907\n","    gyro_x: mean=0.001931\n","    gyro_y: mean=0.000358\n","    gyro_z: mean=0.000059\n","  ✓ Saved: scaler_fold5.npz, windows_normalized_fold5.npz\n","\n","Fold 6: test subject=proband15\n","  Train windows: 34008, Test windows: 2614\n","  Scaler parameters:\n","    acc_x: mean=0.0001, std=3.8238\n","    acc_y: mean=0.0001, std=1.8297\n","    acc_z: mean=0.0003, std=2.0170\n","    gyro_x: mean=-0.0002, std=0.5455\n","    gyro_y: mean=-0.0000, std=0.6655\n","    gyro_z: mean=-0.0001, std=0.3476\n","  Training-set validation after standardization:\n","    acc_x: mean=0.000000, std=1.000000\n","    acc_y: mean=-0.000000, std=1.000000\n","    acc_z: mean=0.000000, std=1.000000\n","    gyro_x: mean=0.000000, std=1.000000\n","    gyro_y: mean=-0.000000, std=1.000000\n","    gyro_z: mean=0.000000, std=1.000000\n","  Test-set validation after standardization:\n","    acc_x: mean=-0.001051\n","    acc_y: mean=-0.000419\n","    acc_z: mean=-0.000410\n","    gyro_x: mean=0.002585\n","    gyro_y: mean=0.000004\n","    gyro_z: mean=0.000445\n","  ✓ Saved: scaler_fold6.npz, windows_normalized_fold6.npz\n","\n","Fold 7: test subject=proband2\n","  Train windows: 34330, Test windows: 2292\n","  Scaler parameters:\n","    acc_x: mean=-0.0001, std=3.7488\n","    acc_y: mean=0.0000, std=1.7805\n","    acc_z: mean=0.0001, std=2.0074\n","    gyro_x: mean=-0.0001, std=0.5394\n","    gyro_y: mean=-0.0001, std=0.6910\n","    gyro_z: mean=-0.0001, std=0.3587\n","  Training-set validation after standardization:\n","    acc_x: mean=0.000000, std=1.000000\n","    acc_y: mean=-0.000000, std=1.000000\n","    acc_z: mean=-0.000000, std=1.000000\n","    gyro_x: mean=-0.000000, std=1.000000\n","    gyro_y: mean=-0.000000, std=1.000000\n","    gyro_z: mean=0.000000, std=1.000000\n","  Test-set validation after standardization:\n","    acc_x: mean=-0.000466\n","    acc_y: mean=0.000190\n","    acc_z: mean=0.000869\n","    gyro_x: mean=0.000025\n","    gyro_y: mean=0.000310\n","    gyro_z: mean=0.000471\n","  ✓ Saved: scaler_fold7.npz, windows_normalized_fold7.npz\n","\n","Fold 8: test subject=proband3\n","  Train windows: 33917, Test windows: 2705\n","  Scaler parameters:\n","    acc_x: mean=-0.0003, std=3.7974\n","    acc_y: mean=0.0001, std=1.8102\n","    acc_z: mean=0.0002, std=2.0051\n","    gyro_x: mean=-0.0000, std=0.5456\n","    gyro_y: mean=-0.0001, std=0.6930\n","    gyro_z: mean=-0.0001, std=0.3576\n","  Training-set validation after standardization:\n","    acc_x: mean=-0.000000, std=1.000000\n","    acc_y: mean=0.000000, std=1.000000\n","    acc_z: mean=-0.000000, std=1.000000\n","    gyro_x: mean=0.000000, std=1.000000\n","    gyro_y: mean=0.000000, std=1.000000\n","    gyro_z: mean=0.000000, std=1.000000\n","  Test-set validation after standardization:\n","    acc_x: mean=0.000550\n","    acc_y: mean=-0.000506\n","    acc_z: mean=-0.000216\n","    gyro_x: mean=-0.001845\n","    gyro_y: mean=0.000419\n","    gyro_z: mean=-0.001126\n","  ✓ Saved: scaler_fold8.npz, windows_normalized_fold8.npz\n","\n","Fold 9: test subject=proband4\n","  Train windows: 34375, Test windows: 2247\n","  Scaler parameters:\n","    acc_x: mean=0.0000, std=3.8402\n","    acc_y: mean=-0.0000, std=1.8165\n","    acc_z: mean=0.0001, std=2.0122\n","    gyro_x: mean=-0.0001, std=0.5447\n","    gyro_y: mean=-0.0000, std=0.6843\n","    gyro_z: mean=-0.0001, std=0.3604\n","  Training-set validation after standardization:\n","    acc_x: mean=-0.000000, std=1.000000\n","    acc_y: mean=-0.000000, std=1.000000\n","    acc_z: mean=0.000000, std=1.000000\n","    gyro_x: mean=-0.000000, std=1.000000\n","    gyro_y: mean=0.000000, std=1.000000\n","    gyro_z: mean=0.000000, std=1.000000\n","  Test-set validation after standardization:\n","    acc_x: mean=-0.000841\n","    acc_y: mean=0.000295\n","    acc_z: mean=0.000621\n","    gyro_x: mean=0.000432\n","    gyro_y: mean=-0.000098\n","    gyro_z: mean=-0.000370\n","  ✓ Saved: scaler_fold9.npz, windows_normalized_fold9.npz\n","\n","Fold 10: test subject=proband5\n","  Train windows: 33690, Test windows: 2932\n","  Scaler parameters:\n","    acc_x: mean=-0.0002, std=3.8540\n","    acc_y: mean=0.0001, std=1.8298\n","    acc_z: mean=0.0003, std=2.0192\n","    gyro_x: mean=-0.0001, std=0.5492\n","    gyro_y: mean=-0.0001, std=0.6960\n","    gyro_z: mean=-0.0001, std=0.3567\n","  Training-set validation after standardization:\n","    acc_x: mean=0.000000, std=1.000000\n","    acc_y: mean=0.000000, std=1.000000\n","    acc_z: mean=0.000000, std=1.000000\n","    gyro_x: mean=-0.000000, std=1.000000\n","    gyro_y: mean=-0.000000, std=1.000000\n","    gyro_z: mean=0.000000, std=1.000000\n","  Test-set validation after standardization:\n","    acc_x: mean=0.000081\n","    acc_y: mean=-0.000413\n","    acc_z: mean=-0.000730\n","    gyro_x: mean=-0.000069\n","    gyro_y: mean=0.000244\n","    gyro_z: mean=0.000133\n","  ✓ Saved: scaler_fold10.npz, windows_normalized_fold10.npz\n","\n","Fold 11: test subject=proband6\n","  Train windows: 34091, Test windows: 2531\n","  Scaler parameters:\n","    acc_x: mean=-0.0002, std=3.8036\n","    acc_y: mean=-0.0001, std=1.8262\n","    acc_z: mean=0.0002, std=2.0039\n","    gyro_x: mean=-0.0001, std=0.5322\n","    gyro_y: mean=-0.0000, std=0.6875\n","    gyro_z: mean=-0.0001, std=0.3539\n","  Training-set validation after standardization:\n","    acc_x: mean=-0.000000, std=1.000000\n","    acc_y: mean=-0.000000, std=1.000000\n","    acc_z: mean=-0.000000, std=1.000000\n","    gyro_x: mean=-0.000000, std=1.000000\n","    gyro_y: mean=0.000000, std=1.000000\n","    gyro_z: mean=-0.000000, std=1.000000\n","  Test-set validation after standardization:\n","    acc_x: mean=0.000054\n","    acc_y: mean=0.000872\n","    acc_z: mean=-0.000180\n","    gyro_x: mean=-0.000310\n","    gyro_y: mean=0.000153\n","    gyro_z: mean=-0.000844\n","  ✓ Saved: scaler_fold11.npz, windows_normalized_fold11.npz\n","\n","Fold 12: test subject=proband7\n","  Train windows: 34620, Test windows: 2002\n","  Scaler parameters:\n","    acc_x: mean=-0.0002, std=3.7881\n","    acc_y: mean=-0.0000, std=1.8145\n","    acc_z: mean=0.0001, std=2.0035\n","    gyro_x: mean=-0.0001, std=0.5509\n","    gyro_y: mean=-0.0000, std=0.6887\n","    gyro_z: mean=-0.0001, std=0.3561\n","  Training-set validation after standardization:\n","    acc_x: mean=-0.000000, std=1.000000\n","    acc_y: mean=0.000000, std=1.000000\n","    acc_z: mean=0.000000, std=1.000000\n","    gyro_x: mean=0.000000, std=1.000000\n","    gyro_y: mean=-0.000000, std=1.000000\n","    gyro_z: mean=-0.000000, std=1.000000\n","  Test-set validation after standardization:\n","    acc_x: mean=0.000188\n","    acc_y: mean=0.000302\n","    acc_z: mean=0.000549\n","    gyro_x: mean=0.000852\n","    gyro_y: mean=-0.000142\n","    gyro_z: mean=0.000597\n","  ✓ Saved: scaler_fold12.npz, windows_normalized_fold12.npz\n","\n","Fold 13: test subject=proband8\n","  Train windows: 33728, Test windows: 2894\n","  Scaler parameters:\n","    acc_x: mean=-0.0001, std=3.8546\n","    acc_y: mean=0.0001, std=1.8390\n","    acc_z: mean=0.0002, std=1.9551\n","    gyro_x: mean=-0.0001, std=0.5487\n","    gyro_y: mean=-0.0001, std=0.5670\n","    gyro_z: mean=-0.0001, std=0.3456\n","  Training-set validation after standardization:\n","    acc_x: mean=-0.000000, std=1.000000\n","    acc_y: mean=0.000000, std=1.000000\n","    acc_z: mean=-0.000000, std=1.000000\n","    gyro_x: mean=0.000000, std=1.000000\n","    gyro_y: mean=-0.000000, std=1.000000\n","    gyro_z: mean=0.000000, std=1.000000\n","  Test-set validation after standardization:\n","    acc_x: mean=-0.000107\n","    acc_y: mean=-0.000520\n","    acc_z: mean=0.000311\n","    gyro_x: mean=0.000248\n","    gyro_y: mean=0.000853\n","    gyro_z: mean=-0.000173\n","  ✓ Saved: scaler_fold13.npz, windows_normalized_fold13.npz\n","\n","Fold 14: test subject=proband9\n","  Train windows: 33946, Test windows: 2676\n","  Scaler parameters:\n","    acc_x: mean=-0.0006, std=3.8054\n","    acc_y: mean=0.0000, std=1.7538\n","    acc_z: mean=0.0002, std=2.0116\n","    gyro_x: mean=-0.0001, std=0.5406\n","    gyro_y: mean=-0.0000, std=0.6713\n","    gyro_z: mean=-0.0001, std=0.3401\n","  Training-set validation after standardization:\n","    acc_x: mean=0.000000, std=1.000000\n","    acc_y: mean=-0.000000, std=1.000000\n","    acc_z: mean=0.000000, std=1.000000\n","    gyro_x: mean=0.000000, std=1.000000\n","    gyro_y: mean=-0.000000, std=1.000000\n","    gyro_z: mean=0.000000, std=1.000000\n","  Test-set validation after standardization:\n","    acc_x: mean=0.001619\n","    acc_y: mean=0.000139\n","    acc_z: mean=-0.000070\n","    gyro_x: mean=-0.001037\n","    gyro_y: mean=-0.000455\n","    gyro_z: mean=0.000751\n","  ✓ Saved: scaler_fold14.npz, windows_normalized_fold14.npz\n","\n","============================================================\n","✓ Completed standardization across 15 folds\n","✓ Scaler parameters: proc/scaler_fold*.npz\n","✓ Standardized data: features/windows_normalized_fold*.npz (NPZ/float32)\n","✓ Summary: logs/scaler_summary.json\n","[master bbcde42] preproc: optimized z-score with NPZ storage and validation\n"," 31 files changed, 482 insertions(+)\n"," create mode 100644 features/windows_normalized_fold0.npz\n"," create mode 100644 features/windows_normalized_fold1.npz\n"," create mode 100644 features/windows_normalized_fold10.npz\n"," create mode 100644 features/windows_normalized_fold11.npz\n"," create mode 100644 features/windows_normalized_fold12.npz\n"," create mode 100644 features/windows_normalized_fold13.npz\n"," create mode 100644 features/windows_normalized_fold14.npz\n"," create mode 100644 features/windows_normalized_fold2.npz\n"," create mode 100644 features/windows_normalized_fold3.npz\n"," create mode 100644 features/windows_normalized_fold4.npz\n"," create mode 100644 features/windows_normalized_fold5.npz\n"," create mode 100644 features/windows_normalized_fold6.npz\n"," create mode 100644 features/windows_normalized_fold7.npz\n"," create mode 100644 features/windows_normalized_fold8.npz\n"," create mode 100644 features/windows_normalized_fold9.npz\n"," create mode 100644 logs/scaler_summary.json\n"," create mode 100644 proc/scaler_fold0.npz\n"," create mode 100644 proc/scaler_fold1.npz\n"," create mode 100644 proc/scaler_fold10.npz\n"," create mode 100644 proc/scaler_fold11.npz\n"," create mode 100644 proc/scaler_fold12.npz\n"," create mode 100644 proc/scaler_fold13.npz\n"," create mode 100644 proc/scaler_fold14.npz\n"," create mode 100644 proc/scaler_fold2.npz\n"," create mode 100644 proc/scaler_fold3.npz\n"," create mode 100644 proc/scaler_fold4.npz\n"," create mode 100644 proc/scaler_fold5.npz\n"," create mode 100644 proc/scaler_fold6.npz\n"," create mode 100644 proc/scaler_fold7.npz\n"," create mode 100644 proc/scaler_fold8.npz\n"," create mode 100644 proc/scaler_fold9.npz\n","\n","============================================================\n","Step 9 completed\n","============================================================\n"]}]},{"cell_type":"code","source":["# Run once to specify which folds to execute\n","import json, os\n","os.makedirs(\"logs\", exist_ok=True)\n","json.dump({\"folds\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]}, open(\"logs/active_folds.json\",\"w\"), indent=2)"],"metadata":{"id":"WtvNFY4njyPq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================ Step 10: ROCKET Feature Generation (Academic-Compliant Edition) ================\n","from pathlib import Path\n","import json\n","import shutil\n","\n","# ============ Configuration Loading ============\n","def get_active_folds(path=\"logs/active_folds.json\", default_all=None):\n","    p = Path(path)\n","    if p.exists():\n","        return json.loads(p.read_text())[\"folds\"]\n","    return default_all\n","\n","def get_active_rockets(path=\"logs/active_rockets.json\", default_all=None):\n","    \"\"\"Load ROCKET model configurations to run\"\"\"\n","    p = Path(path)\n","    if p.exists():\n","        return json.loads(p.read_text())[\"rockets\"]\n","    return default_all if default_all else ['multirocket', 'minirocket']\n","\n","def scan_available_folds(data_dir=\"/content/features\"):\n","    \"\"\"Scan available folds from standardized window files\"\"\"\n","    available = []\n","    for f in Path(data_dir).glob(\"windows_normalized_fold*.npz\"):\n","        try:\n","            fold_id = int(f.stem.replace(\"windows_normalized_fold\", \"\"))\n","            available.append(fold_id)\n","        except:\n","            continue\n","    return sorted(available)\n","\n","# Fetch configs\n","available_folds = scan_available_folds()\n","print(f\"Available folds (from data files): {available_folds}\")\n","\n","folds_to_run = get_active_folds(default_all=available_folds)\n","print(f\"Running folds (from config): {folds_to_run}\")\n","\n","rockets_to_run = get_active_rockets(default_all=['minirocket'])\n","print(f\"Running rockets (from config): {rockets_to_run}\")\n","\n","if not folds_to_run:\n","    print(\"❌ No folds to run! Please check logs/active_folds.json\")\n","    import sys\n","    sys.exit(1)\n","\n","print(\"\\n\" + \"=\"*60)\n","print(f\"📋 Will process {len(folds_to_run)} fold(s): {folds_to_run}\")\n","print(f\"📋 Will generate {len(rockets_to_run)} rocket(s): {rockets_to_run}\")\n","print(\"=\"*60)\n","\n","# Avoid excessive parallelism / thread oversubscription\n","import os\n","os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n","os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n","os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n","os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n","\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import json\n","import time\n","import pickle\n","import sys\n","from sktime.transformations.panel.rocket import MiniRocketMultivariate, MultiRocketMultivariate\n","from threadpoolctl import threadpool_limits, threadpool_info\n","from numpy.lib.format import open_memmap\n","\n","print(\"\\n\\nStep 10: ROCKET Feature Generation (Academic-Compliant Edition)\")\n","print(\"=\" * 60)\n","\n","# Create directories\n","logs_dir = Path('/content/logs')\n","logs_dir.mkdir(parents=True, exist_ok=True)\n","features_dir = Path('/content/features')\n","features_dir.mkdir(parents=True, exist_ok=True)\n","models_dir = Path('/content/models')\n","models_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Environment fingerprint\n","env_info = {\n","    'numpy': np.__version__,\n","    'pandas': pd.__version__,\n","    'sklearn': __import__('sklearn').__version__,\n","    'sktime': __import__('sktime').__version__,\n","    'python': sys.version,\n","    'OMP_NUM_THREADS': os.environ.get('OMP_NUM_THREADS'),\n","    'MKL_NUM_THREADS': os.environ.get('MKL_NUM_THREADS'),\n","    'OPENBLAS_NUM_THREADS': os.environ.get('OPENBLAS_NUM_THREADS'),\n","    'NUMEXPR_NUM_THREADS': os.environ.get('NUMEXPR_NUM_THREADS'),\n","    'threadpools': threadpool_info()\n","}\n","\n","# Load configuration\n","with open('/content/configs/splits.json', 'r') as f:\n","    splits_cfg = json.load(f)\n","\n","CHANNELS = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']\n","N_CHANNELS = len(CHANNELS)\n","CALIB_MAX_SAMPLES = 4096  # Number of training subsamples to accelerate fit\n","\n","def align_to_84(n):\n","    return n - (n % 84)\n","\n","def stratified_by_subject_indices(subjects, max_n, seed=0):\n","    \"\"\"Stratified sampling by subject to balance proportions across subjects\"\"\"\n","    rng = np.random.default_rng(seed)\n","    uniq = np.unique(subjects)\n","    per = max(1, max_n // len(uniq))\n","    idx = []\n","    for s in uniq:\n","        s_idx = np.flatnonzero(subjects == s)\n","        take = min(per, len(s_idx))\n","        idx.extend(rng.choice(s_idx, size=take, replace=False))\n","    if len(idx) < max_n:\n","        remain = np.setdiff1d(np.arange(len(subjects)), np.array(idx, dtype=int), assume_unique=True)\n","        need = min(max_n - len(idx), len(remain))\n","        if need > 0:\n","            idx.extend(rng.choice(remain, size=need, replace=False))\n","    return np.array(idx[:max_n], dtype=int)\n","\n","# Compatibility helpers for differing sktime versions\n","def _get_used_kernels(tr):\n","    return int(getattr(tr, \"num_kernels_\", getattr(tr, \"n_kernels_\", None))\n","               or tr.get_params().get(\"num_kernels\")\n","               or tr.get_params().get(\"n_kernels\"))\n","\n","def _get_n_fpk(tr, default=4):\n","    return int(getattr(tr, \"n_features_per_kernel\", getattr(tr, \"n_features_per_kernel_\", default)))\n","\n","# ROCKET parameter configurations\n","ROCKET_CONFIGS = {\n","    'minirocket': {\n","        'class': MiniRocketMultivariate,\n","        'params': {\n","            'num_kernels': align_to_84(10_000),\n","            'max_dilations_per_kernel': 32,\n","            'n_jobs': -1,\n","            'random_state': 0\n","        },\n","        'batch_size': 16384\n","    },\n","    'multirocket': {\n","        'class': MultiRocketMultivariate,\n","        'params': {\n","            'num_kernels': align_to_84(6_250),\n","            'n_jobs': -1,\n","            'random_state': 0\n","        },\n","        'batch_size': 16384\n","    }\n","}\n","\n","print(\"Configuration:\")\n","for name, cfg in ROCKET_CONFIGS.items():\n","    print(f\"  {name.upper()}: {cfg['params']}, batch_size={cfg['batch_size']}\")\n","print(f\"  Parallelization: n_jobs=-1, BLAS threads=1\")\n","print(f\"  I/O optimizations: memmap streaming writes, sampled statistics\")\n","print(f\"  Fit strategy: per-fold independent (stratified sampling of {CALIB_MAX_SAMPLES} samples)\")\n","print(f\"\\nEnvironment fingerprint:\")\n","for k, v in env_info.items():\n","    if k != 'threadpools':\n","        print(f\"  {k}: {v}\")\n","print()\n","\n","# Dynamic batch auto-tuning + streaming memmap writes\n","def transform_to_memmap(transformer, X, output_file, batch_size, rocket_type,\n","                        target_mem_mb=512, probe=256):\n","    \"\"\"Probe feature dimension, adapt batch size to memory budget, stream to memmap\"\"\"\n","    n_samples = len(X)\n","    batch_times = []\n","\n","    # 1) Probe with a small batch (JIT warmup)\n","    probe_n = min(probe, n_samples, max(1, batch_size//8))\n","    t0 = time.time()\n","    probe_batch = X[:probe_n]\n","    # MultiROCKET requires float64\n","    if rocket_type == 'multirocket':\n","        probe_batch = probe_batch.astype(np.float64, copy=False)\n","    first_probe = transformer.transform(probe_batch)\n","    if hasattr(first_probe, 'values'):\n","        first_probe = first_probe.values\n","    n_features = int(first_probe.shape[1])\n","    batch_times.append(time.time() - t0)\n","\n","    # 2) Determine safe batch size given memory budget\n","    bytes_per_row = n_features * 4\n","    safe_bs = max(128, min(batch_size, int((target_mem_mb * 1024**2) // bytes_per_row)))\n","    safe_bs = min(safe_bs, n_samples)\n","    if safe_bs < batch_size:\n","        print(f\"  ⚙️  auto-tune batch_size: {batch_size} → {safe_bs} (target≈{target_mem_mb}MB)\")\n","\n","    # 3) Recompute first batch and create memmap\n","    t1 = time.time()\n","    batch = X[:safe_bs]\n","    # MultiROCKET: convert to float64 if needed\n","    if rocket_type == 'multirocket':\n","        batch = batch.astype(np.float64, copy=False)\n","\n","    first = transformer.transform(batch)\n","    if hasattr(first, 'values'):\n","        first = first.values\n","    first = first.astype(np.float32, copy=False)\n","\n","    mm = open_memmap(output_file, mode='w+', dtype=np.float32, shape=(n_samples, n_features))\n","    mm[:len(first)] = first\n","    batch_times.append(time.time() - t1)\n","    total_batches = (n_samples - 1) // safe_bs + 1\n","    print(f\"  Batch 1/{total_batches}: {len(first)} samples, {batch_times[-1]:.2f}s\")\n","\n","    # 4) Continue streaming writes\n","    for i in range(len(first), n_samples, safe_bs):\n","        s = time.time()\n","        end = min(i + safe_bs, n_samples)\n","        batch = X[i:end]\n","        # MultiROCKET: convert to float64 if needed\n","        if rocket_type == 'multirocket':\n","            batch = batch.astype(np.float64, copy=False)\n","\n","        b = transformer.transform(batch)\n","        if hasattr(b, 'values'):\n","            b = b.values\n","        mm[i:end] = b.astype(np.float32, copy=False)\n","        bt = time.time() - s\n","        batch_times.append(bt)\n","        print(f\"  Batch {i//safe_bs+1}/{total_batches}: {end - i} samples, {bt:.2f}s\")\n","\n","    mm.flush()\n","    del mm\n","    return n_features, batch_times\n","\n","# Sampled statistics\n","def sample_statistics(file_path, sample_rate=0.01):\n","    \"\"\"Sampled statistics over memmap file\"\"\"\n","    X = np.load(file_path, mmap_mode='r')\n","    n_samples = X.shape[0]\n","    n_sample = max(int(n_samples * sample_rate), 1000)\n","\n","    indices = np.random.choice(n_samples, size=min(n_sample, n_samples), replace=False)\n","    sample = X[indices]\n","\n","    return {\n","        'min': float(sample.min()),\n","        'max': float(sample.max()),\n","        'sparsity_pct': float((sample == 0).sum() / sample.size * 100)\n","    }\n","\n","# Main loop\n","all_summaries = {}\n","\n","for rocket_type in rockets_to_run:\n","    rocket_cfg = ROCKET_CONFIGS[rocket_type]\n","    print(f\"\\n{'='*60}\")\n","    print(f\"Generate {rocket_type.upper()} features\")\n","    print(f\"{'='*60}\")\n","\n","    rocket_summary = []\n","\n","    for fold in splits_cfg['folds']:\n","        k = fold['fold']\n","\n","        if k not in folds_to_run:\n","            print(f\"⏭️  Skipping fold {k} (not in active_folds.json)\")\n","            continue\n","\n","        test_subj = fold['test_subject']\n","\n","        print(f\"\\n{'='*60}\")\n","        print(f\"Fold {k}: test_subject={test_subj}\")\n","        print(f\"{'='*60}\")\n","\n","        # Disk space check\n","        free_gb = shutil.disk_usage(str(features_dir)).free / (1024**3)\n","        if free_gb < 5:\n","            print(f\"⚠️  Warning: only {free_gb:.2f} GB of free disk space\")\n","        assert free_gb > 2, f\"❌ Insufficient disk space! Remaining {free_gb:.2f} GB < 2 GB\"\n","\n","        # Load standardized data\n","        norm_file = features_dir / f'windows_normalized_fold{k}.npz'\n","        print(f\"Loading: {norm_file.name}\")\n","\n","        data = np.load(norm_file, allow_pickle=False)\n","\n","        # Extract arrays\n","        window_ids = data['window_ids']\n","        subjects = data['subjects']\n","        labels = data['labels']\n","        splits = data['splits']\n","\n","        # Build (n_samples, n_channels, n_timesteps) format\n","        X_all = np.stack([data[ch] for ch in CHANNELS], axis=1).astype(np.float32)\n","\n","        # Release npz handle\n","        if hasattr(data, \"close\"):\n","            data.close()\n","\n","        n_samples, n_channels, n_timesteps = X_all.shape\n","        print(f\"Data shape: {X_all.shape} (samples, channels, timesteps)\")\n","\n","        # Build train/test masks\n","        train_mask = splits == 'train'\n","        test_mask = splits == 'test'\n","\n","        X_train = X_all[train_mask]\n","        X_test = X_all[test_mask]\n","\n","        # Ensure contiguous memory\n","        X_train = np.ascontiguousarray(X_train)\n","        X_test = np.ascontiguousarray(X_test)\n","\n","        y_train = labels[train_mask]\n","        y_test = labels[test_mask]\n","\n","        train_ids = window_ids[train_mask]\n","        test_ids = window_ids[test_mask]\n","        train_subjs = subjects[train_mask]\n","        test_subjs = subjects[test_mask]\n","\n","        print(f\"Training set: {X_train.shape[0]} samples\")\n","        print(f\"Test set: {X_test.shape[0]} samples\")\n","\n","        # Anti-leakage assertion\n","        train_subj_set = set(train_subjs)\n","        test_subj_set = set(test_subjs)\n","        intersection = train_subj_set & test_subj_set\n","\n","        assert len(intersection) == 0, f\"Leakage detection failed! Overlap between train and test subjects: {intersection}\"\n","        print(f\"✓ Leakage check passed: train subjects ∩ test subjects = ∅\")\n","\n","        # Per-fold independent fit (accelerated via stratified sampling)\n","        transformer = rocket_cfg['class'](**rocket_cfg['params'])\n","\n","        # Stratified calibration subset\n","        calib_n = min(CALIB_MAX_SAMPLES, len(X_train))\n","        calib_idx = stratified_by_subject_indices(train_subjs, calib_n, seed=0)\n","        X_calib = X_train[calib_idx]\n","\n","        print(f\"\\nFitting {rocket_type.upper()} on {len(calib_idx)} calibration samples...\")\n","        fit_start = time.time()\n","        transformer.fit(X_calib)\n","        fit_time = time.time() - fit_start\n","\n","        used_kernels = _get_used_kernels(transformer)\n","        print(f\"✓ Fit completed: {fit_time:.2f}s (actual kernels: {used_kernels}, calibration samples: {len(calib_idx)})\")\n","\n","        # JIT warmup (MultiROCKET requires float64)\n","        warmup_batch = X_calib[:min(256, len(X_calib))]\n","        if rocket_type == 'multirocket':\n","            warmup_batch = warmup_batch.astype(np.float64, copy=False)\n","        _ = transformer.transform(warmup_batch)\n","\n","        # Fetch batch size\n","        BATCH_SIZE = rocket_cfg['batch_size']\n","\n","        # Transform training set\n","        print(f\"\\nTransforming training data (batch_size={BATCH_SIZE}, streaming to disk)...\")\n","        train_feat_file = features_dir / f'X_{rocket_type}_train_fold{k}.npy'\n","        train_start = time.time()\n","\n","        with threadpool_limits(limits=1, user_api='blas'):\n","            n_features, train_batch_times = transform_to_memmap(\n","                transformer, X_train, train_feat_file, BATCH_SIZE, rocket_type\n","            )\n","\n","        train_time = time.time() - train_start\n","        train_feat_size_mb = train_feat_file.stat().st_size / (1024 ** 2)\n","        print(f\"✓ Train transform: {train_time:.2f}s, shape: ({X_train.shape[0]}, {n_features})\")\n","        print(f\"  Batch time p50={np.median(train_batch_times):.2f}s, p90={np.percentile(train_batch_times, 90):.2f}s\")\n","        print(f\"  File size: {train_feat_size_mb:.2f} MB\")\n","\n","        # Persist training-set metadata\n","        train_meta_file = features_dir / f'meta_{rocket_type}_train_fold{k}.npz'\n","        np.savez(train_meta_file, y=y_train, window_ids=train_ids, subjects=train_subjs)\n","\n","        # Transform test set\n","        print(f\"\\nTransforming test data (batch_size={BATCH_SIZE}, streaming to disk)...\")\n","        test_feat_file = features_dir / f'X_{rocket_type}_test_fold{k}.npy'\n","        test_start = time.time()\n","\n","        with threadpool_limits(limits=1, user_api='blas'):\n","            _, test_batch_times = transform_to_memmap(\n","                transformer, X_test, test_feat_file, BATCH_SIZE, rocket_type\n","            )\n","\n","        test_time = time.time() - test_start\n","        test_feat_size_mb = test_feat_file.stat().st_size / (1024 ** 2)\n","        print(f\"✓ Test transform: {test_time:.2f}s, shape: ({X_test.shape[0]}, {n_features})\")\n","        print(f\"  Batch time p50={np.median(test_batch_times):.2f}s, p90={np.percentile(test_batch_times, 90):.2f}s\")\n","        print(f\"  File size: {test_feat_size_mb:.2f} MB\")\n","\n","        # Persist test-set metadata\n","        test_meta_file = features_dir / f'meta_{rocket_type}_test_fold{k}.npz'\n","        np.savez(test_meta_file, y=y_test, window_ids=test_ids, subjects=test_subjs)\n","\n","        # Feature-dimension assertion (MultiROCKET)\n","        if rocket_type == 'multirocket':\n","            n_fpk = _get_n_fpk(transformer, 4)\n","            expected_features = 2 * n_fpk * used_kernels\n","            assert n_features == expected_features, f\"Feature dimension mismatch: {n_features} != {expected_features}\"\n","            print(f\"✓ Feature-dimension validation passed: {n_features} = 2 × {n_fpk} × {used_kernels}\")\n","\n","        # Persist per-fold transformer\n","        transformer_file = models_dir / f'transformer_{rocket_type}_fold{k}.pkl'\n","        with open(transformer_file, 'wb') as f:\n","            pickle.dump(transformer, f, protocol=4)\n","        transformer_size_mb = transformer_file.stat().st_size / (1024 ** 2)\n","        print(f\"\\n✓ Transformer saved: {transformer_file.name} ({transformer_size_mb:.2f} MB)\")\n","\n","        total_size_mb = train_feat_size_mb + test_feat_size_mb + transformer_size_mb\n","\n","        print(f\"✓ Train features: {train_feat_file.name} ({train_feat_size_mb:.2f} MB)\")\n","        print(f\"✓ Train metadata: {train_meta_file.name}\")\n","        print(f\"✓ Test features: {test_feat_file.name} ({test_feat_size_mb:.2f} MB)\")\n","        print(f\"✓ Test metadata: {test_meta_file.name}\")\n","        print(f\"✓ Total disk usage: {total_size_mb:.2f} MB\")\n","\n","        # Sampled statistics\n","        print(f\"\\nFeature statistics (1% sample):\")\n","        train_stats = sample_statistics(train_feat_file)\n","        test_stats = sample_statistics(test_feat_file)\n","\n","        print(f\"  Number of features: {n_features}\")\n","        print(f\"  Train value range: [{train_stats['min']:.4f}, {train_stats['max']:.4f}]\")\n","        print(f\"  Test value range: [{test_stats['min']:.4f}, {test_stats['max']:.4f}]\")\n","        print(f\"  Train sparsity: {train_stats['sparsity_pct']:.2f}%\")\n","\n","        # Release memory\n","        del X_all, X_train, X_test, X_calib\n","\n","        # Record summary\n","        rocket_summary.append({\n","            'fold': k,\n","            'test_subject': test_subj,\n","            'rocket_type': rocket_type,\n","            'batch_size': rocket_cfg['batch_size'],\n","            'n_features': n_features,\n","            'actual_kernels': used_kernels,\n","            'calib_samples': len(calib_idx),\n","            'n_train_samples': int(len(y_train)),\n","            'n_test_samples': int(len(y_test)),\n","            'fit_time_sec': round(fit_time, 2),\n","            'train_transform_time_sec': round(train_time, 2),\n","            'test_transform_time_sec': round(test_time, 2),\n","            'total_time_sec': round(fit_time + train_time + test_time, 2),\n","            'train_batch_p50_sec': round(np.median(train_batch_times), 2),\n","            'train_batch_p90_sec': round(np.percentile(train_batch_times, 90), 2),\n","            'disk_usage_mb': round(total_size_mb, 2),\n","            'train_feat_size_mb': round(train_feat_size_mb, 2),\n","            'test_feat_size_mb': round(test_feat_size_mb, 2),\n","            'transformer_size_mb': round(transformer_size_mb, 2),\n","            'leak_check_passed': True,\n","            'independent_fit_per_fold': True\n","        })\n","\n","    # Persist summary\n","    summary_df = pd.DataFrame(rocket_summary)\n","    summary_df.to_csv(logs_dir / f'rocket_{rocket_type}_summary.csv', index=False)\n","\n","    with open(logs_dir / f'rocket_{rocket_type}_summary.json', 'w') as f:\n","        json.dump({\n","            'rocket_type': rocket_type,\n","            'parameters': rocket_cfg['params'],\n","            'batch_size': rocket_cfg['batch_size'],\n","            'optimization': {\n","                'blas_threads': 1,\n","                'n_jobs': -1,\n","                'streaming_memmap_write': True,\n","                'sampled_statistics': True,\n","                'adaptive_batch_size': True,\n","                'jit_warmup': True,\n","                'independent_fit_per_fold': True,\n","                'stratified_calibration': True,\n","                'calib_max_samples': CALIB_MAX_SAMPLES,\n","                'float64_on_demand': True,\n","                'contiguous_memory': True,\n","                'disk_space_check': True,\n","                'separate_train_test_transform': True,\n","                'npy_format_for_mmap': True\n","            },\n","            'n_folds': len(rocket_summary),\n","            'per_fold_stats': rocket_summary,\n","            'aggregated_stats': {\n","                'avg_n_features': int(summary_df['n_features'].mean()),\n","                'avg_fit_time_sec': round(summary_df['fit_time_sec'].mean(), 2),\n","                'avg_train_transform_time_sec': round(summary_df['train_transform_time_sec'].mean(), 2),\n","                'avg_test_transform_time_sec': round(summary_df['test_transform_time_sec'].mean(), 2),\n","                'total_disk_usage_mb': round(summary_df['disk_usage_mb'].sum(), 2)\n","            }\n","        }, f, indent=2)\n","\n","    all_summaries[rocket_type] = rocket_summary\n","\n","    print(f\"\\n{'='*60}\")\n","    print(f\"{rocket_type.upper()} completed\")\n","    print(f\"✓ Summary CSV: {logs_dir / f'rocket_{rocket_type}_summary.csv'}\")\n","    print(f\"✓ Summary JSON: {logs_dir / f'rocket_{rocket_type}_summary.json'}\")\n","    print(f\"{'='*60}\")\n","\n","# Persist environment fingerprint\n","with open(logs_dir / 'rocket_env.json', 'w') as f:\n","    json.dump(env_info, f, indent=2)\n","\n","# Final summary\n","print(f\"\\n{'='*60}\")\n","print(\"All ROCKET feature generation completed\")\n","print(f\"{'='*60}\")\n","\n","for rocket_type in rockets_to_run:\n","    if rocket_type in all_summaries:\n","        summary_df = pd.DataFrame(all_summaries[rocket_type])\n","        print(f\"\\n{rocket_type.upper()} 汇总:\")  # keep consistent with earlier semantics\n","        print(f\"  Avg. number of features: {summary_df['n_features'].mean():.0f}\")\n","        print(f\"  Avg. fit time: {summary_df['fit_time_sec'].mean():.2f}s\")\n","        print(f\"  Avg. train transform time: {summary_df['train_transform_time_sec'].mean():.2f}s\")\n","        print(f\"  Avg. test transform time: {summary_df['test_transform_time_sec'].mean():.2f}s\")\n","        print(f\"  Avg. total time: {summary_df['total_time_sec'].mean():.2f}s\")\n","        print(f\"  Total disk usage: {summary_df['disk_usage_mb'].sum():.2f} MB\")\n","\n","print(f\"\\n✓ Environment fingerprint: {logs_dir / 'rocket_env.json'}\")\n","print(f\"✓ All folds passed anti-leakage validation\")\n","print(f\"✓ Academic compliance: per-fold independent fit (stratified sampling), no cross-fold information leakage\")\n","print(f\"✓ Optimizations: BLAS single-thread, adaptive batching, memmap streaming writes, sampled statistics, disk checks\")\n","print(f\"✓ MultiROCKET: float64 on demand, JIT warmup, contiguous memory\")\n","print(f\"\\n⚠️  Large files are Git-ignored; commit commands:\")\n","print(f\"   git add logs/rocket_*.json configs/ models/transformer_*_fold*.pkl\")\n","print(f\"   git commit -m 'feature: academic-compliant ROCKET (independent fit per fold)'\")\n","\n","print(f\"\\n{'='*60}\\nStep 10 completed\\n{'='*60}\")"],"metadata":{"id":"BiflW_qt5iW2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762896060651,"user_tz":0,"elapsed":461579,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"0f3a7197-431e-48bb-b305-49167a9e9ebe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Available folds (from data files): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n","Running folds (from config): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n","Running rockets (from config): ['minirocket']\n","\n","============================================================\n","📋 Will process 15 fold(s): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n","📋 Will generate 1 rocket(s): ['minirocket']\n","============================================================\n","\n","\n","Step 10: ROCKET Feature Generation (Academic-Compliant Edition)\n","============================================================\n","Configuration:\n","  MINIROCKET: {'num_kernels': 9996, 'max_dilations_per_kernel': 32, 'n_jobs': -1, 'random_state': 0}, batch_size=16384\n","  MULTIROCKET: {'num_kernels': 6216, 'n_jobs': -1, 'random_state': 0}, batch_size=16384\n","  Parallelization: n_jobs=-1, BLAS threads=1\n","  I/O optimizations: memmap streaming writes, sampled statistics\n","  Fit strategy: per-fold independent (stratified sampling of 4096 samples)\n","\n","Environment fingerprint:\n","  numpy: 1.26.4\n","  pandas: 2.2.2\n","  sklearn: 1.4.2\n","  sktime: 0.30.0\n","  python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n","  OMP_NUM_THREADS: 1\n","  MKL_NUM_THREADS: 1\n","  OPENBLAS_NUM_THREADS: 1\n","  NUMEXPR_NUM_THREADS: 1\n","\n","\n","============================================================\n","Generate MINIROCKET features\n","============================================================\n","\n","============================================================\n","Fold 0: test_subject=proband1\n","============================================================\n","Loading: windows_normalized_fold0.npz\n","Data shape: (36622, 6, 150) (samples, channels, timesteps)\n","Training set: 34727 samples\n","Test set: 1895 samples\n","✓ Leakage check passed: train subjects ∩ test subjects = ∅\n","\n","Fitting MINIROCKET on 4096 calibration samples...\n","✓ Fit completed: 29.63s (actual kernels: 9996, calibration samples: 4096)\n","\n","Transforming training data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 13427 (target≈512MB)\n","  Batch 1/3: 13427 samples, 9.52s\n","  Batch 2/3: 13427 samples, 10.15s\n","  Batch 3/3: 7873 samples, 5.37s\n","✓ Train transform: 26.22s, shape: (34727, 9996)\n","  Batch time p50=7.45s, p90=9.96s\n","  File size: 1324.20 MB\n","\n","Transforming test data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 1895 (target≈512MB)\n","  Batch 1/1: 1895 samples, 1.21s\n","✓ Test transform: 1.58s, shape: (1895, 9996)\n","  Batch time p50=0.69s, p90=1.11s\n","  File size: 72.26 MB\n","\n","✓ Transformer saved: transformer_minirocket_fold0.pkl (0.06 MB)\n","✓ Train features: X_minirocket_train_fold0.npy (1324.20 MB)\n","✓ Train metadata: meta_minirocket_train_fold0.npz\n","✓ Test features: X_minirocket_test_fold0.npy (72.26 MB)\n","✓ Test metadata: meta_minirocket_test_fold0.npz\n","✓ Total disk usage: 1396.52 MB\n","\n","Feature statistics (1% sample):\n","  Number of features: 9996\n","  Train value range: [0.0000, 1.0000]\n","  Test value range: [0.0000, 1.0000]\n","  Train sparsity: 15.15%\n","\n","============================================================\n","Fold 1: test_subject=proband10\n","============================================================\n","Loading: windows_normalized_fold1.npz\n","Data shape: (36622, 6, 150) (samples, channels, timesteps)\n","Training set: 34159 samples\n","Test set: 2463 samples\n","✓ Leakage check passed: train subjects ∩ test subjects = ∅\n","\n","Fitting MINIROCKET on 4096 calibration samples...\n","✓ Fit completed: 0.07s (actual kernels: 9996, calibration samples: 4096)\n","\n","Transforming training data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 13427 (target≈512MB)\n","  Batch 1/3: 13427 samples, 9.32s\n","  Batch 2/3: 13427 samples, 9.61s\n","  Batch 3/3: 7305 samples, 5.26s\n","✓ Train transform: 25.94s, shape: (34159, 9996)\n","  Batch time p50=7.29s, p90=9.52s\n","  File size: 1302.54 MB\n","\n","Transforming test data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 2463 (target≈512MB)\n","  Batch 1/1: 2463 samples, 1.58s\n","✓ Test transform: 2.04s, shape: (2463, 9996)\n","  Batch time p50=0.88s, p90=1.44s\n","  File size: 93.92 MB\n","\n","✓ Transformer saved: transformer_minirocket_fold1.pkl (0.06 MB)\n","✓ Train features: X_minirocket_train_fold1.npy (1302.54 MB)\n","✓ Train metadata: meta_minirocket_train_fold1.npz\n","✓ Test features: X_minirocket_test_fold1.npy (93.92 MB)\n","✓ Test metadata: meta_minirocket_test_fold1.npz\n","✓ Total disk usage: 1396.52 MB\n","\n","Feature statistics (1% sample):\n","  Number of features: 9996\n","  Train value range: [0.0000, 1.0000]\n","  Test value range: [0.0000, 1.0000]\n","  Train sparsity: 14.35%\n","\n","============================================================\n","Fold 2: test_subject=proband11\n","============================================================\n","Loading: windows_normalized_fold2.npz\n","Data shape: (36622, 6, 150) (samples, channels, timesteps)\n","Training set: 34042 samples\n","Test set: 2580 samples\n","✓ Leakage check passed: train subjects ∩ test subjects = ∅\n","\n","Fitting MINIROCKET on 4096 calibration samples...\n","✓ Fit completed: 0.07s (actual kernels: 9996, calibration samples: 4096)\n","\n","Transforming training data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 13427 (target≈512MB)\n","  Batch 1/3: 13427 samples, 9.33s\n","  Batch 2/3: 13427 samples, 9.61s\n","  Batch 3/3: 7188 samples, 4.66s\n","✓ Train transform: 25.09s, shape: (34042, 9996)\n","  Batch time p50=6.99s, p90=9.53s\n","  File size: 1298.08 MB\n","\n","Transforming test data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 2580 (target≈512MB)\n","  Batch 1/1: 2580 samples, 1.63s\n","✓ Test transform: 2.09s, shape: (2580, 9996)\n","  Batch time p50=0.90s, p90=1.48s\n","  File size: 98.38 MB\n","\n","✓ Transformer saved: transformer_minirocket_fold2.pkl (0.06 MB)\n","✓ Train features: X_minirocket_train_fold2.npy (1298.08 MB)\n","✓ Train metadata: meta_minirocket_train_fold2.npz\n","✓ Test features: X_minirocket_test_fold2.npy (98.38 MB)\n","✓ Test metadata: meta_minirocket_test_fold2.npz\n","✓ Total disk usage: 1396.52 MB\n","\n","Feature statistics (1% sample):\n","  Number of features: 9996\n","  Train value range: [0.0000, 1.0000]\n","  Test value range: [0.0000, 1.0000]\n","  Train sparsity: 15.02%\n","\n","============================================================\n","Fold 3: test_subject=proband12\n","============================================================\n","Loading: windows_normalized_fold3.npz\n","Data shape: (36622, 6, 150) (samples, channels, timesteps)\n","Training set: 34255 samples\n","Test set: 2367 samples\n","✓ Leakage check passed: train subjects ∩ test subjects = ∅\n","\n","Fitting MINIROCKET on 4096 calibration samples...\n","✓ Fit completed: 0.07s (actual kernels: 9996, calibration samples: 4096)\n","\n","Transforming training data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 13427 (target≈512MB)\n","  Batch 1/3: 13427 samples, 9.26s\n","  Batch 2/3: 13427 samples, 9.59s\n","  Batch 3/3: 7401 samples, 5.10s\n","✓ Train transform: 25.05s, shape: (34255, 9996)\n","  Batch time p50=7.18s, p90=9.49s\n","  File size: 1306.20 MB\n","\n","Transforming test data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 2367 (target≈512MB)\n","  Batch 1/1: 2367 samples, 1.58s\n","✓ Test transform: 2.01s, shape: (2367, 9996)\n","  Batch time p50=0.86s, p90=1.44s\n","  File size: 90.26 MB\n","\n","✓ Transformer saved: transformer_minirocket_fold3.pkl (0.06 MB)\n","✓ Train features: X_minirocket_train_fold3.npy (1306.20 MB)\n","✓ Train metadata: meta_minirocket_train_fold3.npz\n","✓ Test features: X_minirocket_test_fold3.npy (90.26 MB)\n","✓ Test metadata: meta_minirocket_test_fold3.npz\n","✓ Total disk usage: 1396.52 MB\n","\n","Feature statistics (1% sample):\n","  Number of features: 9996\n","  Train value range: [0.0000, 1.0000]\n","  Test value range: [0.0000, 1.0000]\n","  Train sparsity: 14.79%\n","\n","============================================================\n","Fold 4: test_subject=proband13\n","============================================================\n","Loading: windows_normalized_fold4.npz\n","Data shape: (36622, 6, 150) (samples, channels, timesteps)\n","Training set: 34033 samples\n","Test set: 2589 samples\n","✓ Leakage check passed: train subjects ∩ test subjects = ∅\n","\n","Fitting MINIROCKET on 4096 calibration samples...\n","✓ Fit completed: 0.07s (actual kernels: 9996, calibration samples: 4096)\n","\n","Transforming training data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 13427 (target≈512MB)\n","  Batch 1/3: 13427 samples, 9.28s\n","  Batch 2/3: 13427 samples, 9.51s\n","  Batch 3/3: 7179 samples, 4.85s\n","✓ Train transform: 24.69s, shape: (34033, 9996)\n","  Batch time p50=7.06s, p90=9.44s\n","  File size: 1297.74 MB\n","\n","Transforming test data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 2589 (target≈512MB)\n","  Batch 1/1: 2589 samples, 1.68s\n","✓ Test transform: 2.13s, shape: (2589, 9996)\n","  Batch time p50=0.91s, p90=1.53s\n","  File size: 98.72 MB\n","\n","✓ Transformer saved: transformer_minirocket_fold4.pkl (0.06 MB)\n","✓ Train features: X_minirocket_train_fold4.npy (1297.74 MB)\n","✓ Train metadata: meta_minirocket_train_fold4.npz\n","✓ Test features: X_minirocket_test_fold4.npy (98.72 MB)\n","✓ Test metadata: meta_minirocket_test_fold4.npz\n","✓ Total disk usage: 1396.52 MB\n","\n","Feature statistics (1% sample):\n","  Number of features: 9996\n","  Train value range: [0.0000, 1.0000]\n","  Test value range: [0.0000, 1.0000]\n","  Train sparsity: 14.59%\n","\n","============================================================\n","Fold 5: test_subject=proband14\n","============================================================\n","Loading: windows_normalized_fold5.npz\n","Data shape: (36622, 6, 150) (samples, channels, timesteps)\n","Training set: 34787 samples\n","Test set: 1835 samples\n","✓ Leakage check passed: train subjects ∩ test subjects = ∅\n","\n","Fitting MINIROCKET on 4096 calibration samples...\n","✓ Fit completed: 0.07s (actual kernels: 9996, calibration samples: 4096)\n","\n","Transforming training data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 13427 (target≈512MB)\n","  Batch 1/3: 13427 samples, 9.34s\n","  Batch 2/3: 13427 samples, 9.53s\n","  Batch 3/3: 7933 samples, 5.55s\n","✓ Train transform: 25.61s, shape: (34787, 9996)\n","  Batch time p50=7.45s, p90=9.47s\n","  File size: 1326.49 MB\n","\n","Transforming test data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 1835 (target≈512MB)\n","  Batch 1/1: 1835 samples, 1.15s\n","✓ Test transform: 1.50s, shape: (1835, 9996)\n","  Batch time p50=0.65s, p90=1.05s\n","  File size: 69.97 MB\n","\n","✓ Transformer saved: transformer_minirocket_fold5.pkl (0.06 MB)\n","✓ Train features: X_minirocket_train_fold5.npy (1326.49 MB)\n","✓ Train metadata: meta_minirocket_train_fold5.npz\n","✓ Test features: X_minirocket_test_fold5.npy (69.97 MB)\n","✓ Test metadata: meta_minirocket_test_fold5.npz\n","✓ Total disk usage: 1396.52 MB\n","\n","Feature statistics (1% sample):\n","  Number of features: 9996\n","  Train value range: [0.0000, 1.0000]\n","  Test value range: [0.0000, 1.0000]\n","  Train sparsity: 15.15%\n","\n","============================================================\n","Fold 6: test_subject=proband15\n","============================================================\n","Loading: windows_normalized_fold6.npz\n","Data shape: (36622, 6, 150) (samples, channels, timesteps)\n","Training set: 34008 samples\n","Test set: 2614 samples\n","✓ Leakage check passed: train subjects ∩ test subjects = ∅\n","\n","Fitting MINIROCKET on 4096 calibration samples...\n","✓ Fit completed: 0.07s (actual kernels: 9996, calibration samples: 4096)\n","\n","Transforming training data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 13427 (target≈512MB)\n","  Batch 1/3: 13427 samples, 9.25s\n","  Batch 2/3: 13427 samples, 9.56s\n","  Batch 3/3: 7154 samples, 4.91s\n","✓ Train transform: 24.79s, shape: (34008, 9996)\n","  Batch time p50=7.08s, p90=9.47s\n","  File size: 1296.78 MB\n","\n","Transforming test data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 2614 (target≈512MB)\n","  Batch 1/1: 2614 samples, 1.67s\n","✓ Test transform: 2.08s, shape: (2614, 9996)\n","  Batch time p50=0.91s, p90=1.52s\n","  File size: 99.68 MB\n","\n","✓ Transformer saved: transformer_minirocket_fold6.pkl (0.06 MB)\n","✓ Train features: X_minirocket_train_fold6.npy (1296.78 MB)\n","✓ Train metadata: meta_minirocket_train_fold6.npz\n","✓ Test features: X_minirocket_test_fold6.npy (99.68 MB)\n","✓ Test metadata: meta_minirocket_test_fold6.npz\n","✓ Total disk usage: 1396.52 MB\n","\n","Feature statistics (1% sample):\n","  Number of features: 9996\n","  Train value range: [0.0000, 1.0000]\n","  Test value range: [0.0000, 1.0000]\n","  Train sparsity: 13.93%\n","\n","============================================================\n","Fold 7: test_subject=proband2\n","============================================================\n","Loading: windows_normalized_fold7.npz\n","Data shape: (36622, 6, 150) (samples, channels, timesteps)\n","Training set: 34330 samples\n","Test set: 2292 samples\n","✓ Leakage check passed: train subjects ∩ test subjects = ∅\n","\n","Fitting MINIROCKET on 4096 calibration samples...\n","✓ Fit completed: 0.07s (actual kernels: 9996, calibration samples: 4096)\n","\n","Transforming training data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 13427 (target≈512MB)\n","  Batch 1/3: 13427 samples, 9.26s\n","  Batch 2/3: 13427 samples, 9.55s\n","  Batch 3/3: 7476 samples, 5.07s\n","✓ Train transform: 25.00s, shape: (34330, 9996)\n","  Batch time p50=7.17s, p90=9.46s\n","  File size: 1309.06 MB\n","\n","Transforming test data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 2292 (target≈512MB)\n","  Batch 1/1: 2292 samples, 1.44s\n","✓ Test transform: 1.84s, shape: (2292, 9996)\n","  Batch time p50=0.79s, p90=1.31s\n","  File size: 87.40 MB\n","\n","✓ Transformer saved: transformer_minirocket_fold7.pkl (0.06 MB)\n","✓ Train features: X_minirocket_train_fold7.npy (1309.06 MB)\n","✓ Train metadata: meta_minirocket_train_fold7.npz\n","✓ Test features: X_minirocket_test_fold7.npy (87.40 MB)\n","✓ Test metadata: meta_minirocket_test_fold7.npz\n","✓ Total disk usage: 1396.52 MB\n","\n","Feature statistics (1% sample):\n","  Number of features: 9996\n","  Train value range: [0.0000, 1.0000]\n","  Test value range: [0.0000, 1.0000]\n","  Train sparsity: 14.59%\n","\n","============================================================\n","Fold 8: test_subject=proband3\n","============================================================\n","Loading: windows_normalized_fold8.npz\n","Data shape: (36622, 6, 150) (samples, channels, timesteps)\n","Training set: 33917 samples\n","Test set: 2705 samples\n","✓ Leakage check passed: train subjects ∩ test subjects = ∅\n","\n","Fitting MINIROCKET on 4096 calibration samples...\n","✓ Fit completed: 0.07s (actual kernels: 9996, calibration samples: 4096)\n","\n","Transforming training data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 13427 (target≈512MB)\n","  Batch 1/3: 13427 samples, 9.31s\n","  Batch 2/3: 13427 samples, 9.55s\n","  Batch 3/3: 7063 samples, 4.81s\n","✓ Train transform: 24.71s, shape: (33917, 9996)\n","  Batch time p50=7.06s, p90=9.47s\n","  File size: 1293.31 MB\n","\n","Transforming test data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 2705 (target≈512MB)\n","  Batch 1/1: 2705 samples, 1.79s\n","✓ Test transform: 2.26s, shape: (2705, 9996)\n","  Batch time p50=0.98s, p90=1.63s\n","  File size: 103.15 MB\n","\n","✓ Transformer saved: transformer_minirocket_fold8.pkl (0.06 MB)\n","✓ Train features: X_minirocket_train_fold8.npy (1293.31 MB)\n","✓ Train metadata: meta_minirocket_train_fold8.npz\n","✓ Test features: X_minirocket_test_fold8.npy (103.15 MB)\n","✓ Test metadata: meta_minirocket_test_fold8.npz\n","✓ Total disk usage: 1396.52 MB\n","\n","Feature statistics (1% sample):\n","  Number of features: 9996\n","  Train value range: [0.0000, 1.0000]\n","  Test value range: [0.0000, 1.0000]\n","  Train sparsity: 14.34%\n","\n","============================================================\n","Fold 9: test_subject=proband4\n","============================================================\n","Loading: windows_normalized_fold9.npz\n","Data shape: (36622, 6, 150) (samples, channels, timesteps)\n","Training set: 34375 samples\n","Test set: 2247 samples\n","✓ Leakage check passed: train subjects ∩ test subjects = ∅\n","\n","Fitting MINIROCKET on 4096 calibration samples...\n","✓ Fit completed: 0.07s (actual kernels: 9996, calibration samples: 4096)\n","\n","Transforming training data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 13427 (target≈512MB)\n","  Batch 1/3: 13427 samples, 9.35s\n","  Batch 2/3: 13427 samples, 9.65s\n","  Batch 3/3: 7521 samples, 5.20s\n","✓ Train transform: 25.34s, shape: (34375, 9996)\n","  Batch time p50=7.28s, p90=9.56s\n","  File size: 1310.78 MB\n","\n","Transforming test data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 2247 (target≈512MB)\n","  Batch 1/1: 2247 samples, 1.45s\n","✓ Test transform: 1.85s, shape: (2247, 9996)\n","  Batch time p50=0.80s, p90=1.32s\n","  File size: 85.68 MB\n","\n","✓ Transformer saved: transformer_minirocket_fold9.pkl (0.06 MB)\n","✓ Train features: X_minirocket_train_fold9.npy (1310.78 MB)\n","✓ Train metadata: meta_minirocket_train_fold9.npz\n","✓ Test features: X_minirocket_test_fold9.npy (85.68 MB)\n","✓ Test metadata: meta_minirocket_test_fold9.npz\n","✓ Total disk usage: 1396.52 MB\n","\n","Feature statistics (1% sample):\n","  Number of features: 9996\n","  Train value range: [0.0000, 1.0000]\n","  Test value range: [0.0000, 1.0000]\n","  Train sparsity: 14.73%\n","\n","============================================================\n","Fold 10: test_subject=proband5\n","============================================================\n","Loading: windows_normalized_fold10.npz\n","Data shape: (36622, 6, 150) (samples, channels, timesteps)\n","Training set: 33690 samples\n","Test set: 2932 samples\n","✓ Leakage check passed: train subjects ∩ test subjects = ∅\n","\n","Fitting MINIROCKET on 4096 calibration samples...\n","✓ Fit completed: 0.07s (actual kernels: 9996, calibration samples: 4096)\n","\n","Transforming training data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 13427 (target≈512MB)\n","  Batch 1/3: 13427 samples, 9.26s\n","  Batch 2/3: 13427 samples, 9.60s\n","  Batch 3/3: 6836 samples, 4.99s\n","✓ Train transform: 25.35s, shape: (33690, 9996)\n","  Batch time p50=7.12s, p90=9.50s\n","  File size: 1284.66 MB\n","\n","Transforming test data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 2932 (target≈512MB)\n","  Batch 1/1: 2932 samples, 1.84s\n","✓ Test transform: 2.36s, shape: (2932, 9996)\n","  Batch time p50=1.00s, p90=1.67s\n","  File size: 111.80 MB\n","\n","✓ Transformer saved: transformer_minirocket_fold10.pkl (0.06 MB)\n","✓ Train features: X_minirocket_train_fold10.npy (1284.66 MB)\n","✓ Train metadata: meta_minirocket_train_fold10.npz\n","✓ Test features: X_minirocket_test_fold10.npy (111.80 MB)\n","✓ Test metadata: meta_minirocket_test_fold10.npz\n","✓ Total disk usage: 1396.52 MB\n","\n","Feature statistics (1% sample):\n","  Number of features: 9996\n","  Train value range: [0.0000, 1.0000]\n","  Test value range: [0.0000, 1.0000]\n","  Train sparsity: 15.07%\n","\n","============================================================\n","Fold 11: test_subject=proband6\n","============================================================\n","Loading: windows_normalized_fold11.npz\n","Data shape: (36622, 6, 150) (samples, channels, timesteps)\n","Training set: 34091 samples\n","Test set: 2531 samples\n","✓ Leakage check passed: train subjects ∩ test subjects = ∅\n","\n","Fitting MINIROCKET on 4096 calibration samples...\n","✓ Fit completed: 0.07s (actual kernels: 9996, calibration samples: 4096)\n","\n","Transforming training data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 13427 (target≈512MB)\n","  Batch 1/3: 13427 samples, 9.42s\n","  Batch 2/3: 13427 samples, 9.83s\n","  Batch 3/3: 7237 samples, 5.39s\n","✓ Train transform: 26.29s, shape: (34091, 9996)\n","  Batch time p50=7.40s, p90=9.70s\n","  File size: 1299.95 MB\n","\n","Transforming test data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 2531 (target≈512MB)\n","  Batch 1/1: 2531 samples, 1.66s\n","✓ Test transform: 2.11s, shape: (2531, 9996)\n","  Batch time p50=0.91s, p90=1.51s\n","  File size: 96.51 MB\n","\n","✓ Transformer saved: transformer_minirocket_fold11.pkl (0.06 MB)\n","✓ Train features: X_minirocket_train_fold11.npy (1299.95 MB)\n","✓ Train metadata: meta_minirocket_train_fold11.npz\n","✓ Test features: X_minirocket_test_fold11.npy (96.51 MB)\n","✓ Test metadata: meta_minirocket_test_fold11.npz\n","✓ Total disk usage: 1396.52 MB\n","\n","Feature statistics (1% sample):\n","  Number of features: 9996\n","  Train value range: [0.0000, 1.0000]\n","  Test value range: [0.0000, 1.0000]\n","  Train sparsity: 14.87%\n","\n","============================================================\n","Fold 12: test_subject=proband7\n","============================================================\n","Loading: windows_normalized_fold12.npz\n","Data shape: (36622, 6, 150) (samples, channels, timesteps)\n","Training set: 34620 samples\n","Test set: 2002 samples\n","✓ Leakage check passed: train subjects ∩ test subjects = ∅\n","\n","Fitting MINIROCKET on 4096 calibration samples...\n","✓ Fit completed: 0.07s (actual kernels: 9996, calibration samples: 4096)\n","\n","Transforming training data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 13427 (target≈512MB)\n","  Batch 1/3: 13427 samples, 9.29s\n","  Batch 2/3: 13427 samples, 9.82s\n","  Batch 3/3: 7766 samples, 4.83s\n","✓ Train transform: 25.09s, shape: (34620, 9996)\n","  Batch time p50=7.06s, p90=9.66s\n","  File size: 1320.12 MB\n","\n","Transforming test data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 2002 (target≈512MB)\n","  Batch 1/1: 2002 samples, 1.13s\n","✓ Test transform: 1.46s, shape: (2002, 9996)\n","  Batch time p50=0.62s, p90=1.03s\n","  File size: 76.34 MB\n","\n","✓ Transformer saved: transformer_minirocket_fold12.pkl (0.06 MB)\n","✓ Train features: X_minirocket_train_fold12.npy (1320.12 MB)\n","✓ Train metadata: meta_minirocket_train_fold12.npz\n","✓ Test features: X_minirocket_test_fold12.npy (76.34 MB)\n","✓ Test metadata: meta_minirocket_test_fold12.npz\n","✓ Total disk usage: 1396.52 MB\n","\n","Feature statistics (1% sample):\n","  Number of features: 9996\n","  Train value range: [0.0000, 1.0000]\n","  Test value range: [0.0000, 1.0000]\n","  Train sparsity: 14.24%\n","\n","============================================================\n","Fold 13: test_subject=proband8\n","============================================================\n","Loading: windows_normalized_fold13.npz\n","Data shape: (36622, 6, 150) (samples, channels, timesteps)\n","Training set: 33728 samples\n","Test set: 2894 samples\n","✓ Leakage check passed: train subjects ∩ test subjects = ∅\n","\n","Fitting MINIROCKET on 4096 calibration samples...\n","✓ Fit completed: 0.07s (actual kernels: 9996, calibration samples: 4096)\n","\n","Transforming training data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 13427 (target≈512MB)\n","  Batch 1/3: 13427 samples, 9.27s\n","  Batch 2/3: 13427 samples, 9.90s\n","  Batch 3/3: 6874 samples, 4.82s\n","✓ Train transform: 25.73s, shape: (33728, 9996)\n","  Batch time p50=7.05s, p90=9.71s\n","  File size: 1286.11 MB\n","\n","Transforming test data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 2894 (target≈512MB)\n","  Batch 1/1: 2894 samples, 1.83s\n","✓ Test transform: 2.32s, shape: (2894, 9996)\n","  Batch time p50=0.99s, p90=1.66s\n","  File size: 110.35 MB\n","\n","✓ Transformer saved: transformer_minirocket_fold13.pkl (0.06 MB)\n","✓ Train features: X_minirocket_train_fold13.npy (1286.11 MB)\n","✓ Train metadata: meta_minirocket_train_fold13.npz\n","✓ Test features: X_minirocket_test_fold13.npy (110.35 MB)\n","✓ Test metadata: meta_minirocket_test_fold13.npz\n","✓ Total disk usage: 1396.52 MB\n","\n","Feature statistics (1% sample):\n","  Number of features: 9996\n","  Train value range: [0.0000, 1.0000]\n","  Test value range: [0.0000, 1.0000]\n","  Train sparsity: 14.95%\n","\n","============================================================\n","Fold 14: test_subject=proband9\n","============================================================\n","Loading: windows_normalized_fold14.npz\n","Data shape: (36622, 6, 150) (samples, channels, timesteps)\n","Training set: 33946 samples\n","Test set: 2676 samples\n","✓ Leakage check passed: train subjects ∩ test subjects = ∅\n","\n","Fitting MINIROCKET on 4096 calibration samples...\n","✓ Fit completed: 0.07s (actual kernels: 9996, calibration samples: 4096)\n","\n","Transforming training data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 13427 (target≈512MB)\n","  Batch 1/3: 13427 samples, 9.32s\n","  Batch 2/3: 13427 samples, 9.90s\n","  Batch 3/3: 7092 samples, 4.89s\n","✓ Train transform: 25.88s, shape: (33946, 9996)\n","  Batch time p50=7.10s, p90=9.73s\n","  File size: 1294.42 MB\n","\n","Transforming test data (batch_size=16384, streaming to disk)...\n","  ⚙️  auto-tune batch_size: 16384 → 2676 (target≈512MB)\n","  Batch 1/1: 2676 samples, 1.71s\n","✓ Test transform: 2.15s, shape: (2676, 9996)\n","  Batch time p50=0.92s, p90=1.55s\n","  File size: 102.04 MB\n","\n","✓ Transformer saved: transformer_minirocket_fold14.pkl (0.06 MB)\n","✓ Train features: X_minirocket_train_fold14.npy (1294.42 MB)\n","✓ Train metadata: meta_minirocket_train_fold14.npz\n","✓ Test features: X_minirocket_test_fold14.npy (102.04 MB)\n","✓ Test metadata: meta_minirocket_test_fold14.npz\n","✓ Total disk usage: 1396.52 MB\n","\n","Feature statistics (1% sample):\n","  Number of features: 9996\n","  Train value range: [0.0000, 1.0000]\n","  Test value range: [0.0000, 1.0000]\n","  Train sparsity: 14.32%\n","\n","============================================================\n","MINIROCKET completed\n","✓ Summary CSV: /content/logs/rocket_minirocket_summary.csv\n","✓ Summary JSON: /content/logs/rocket_minirocket_summary.json\n","============================================================\n","\n","============================================================\n","All ROCKET feature generation completed\n","============================================================\n","\n","MINIROCKET 汇总:\n","  Avg. number of features: 9996\n","  Avg. fit time: 2.04s\n","  Avg. train transform time: 25.39s\n","  Avg. test transform time: 1.99s\n","  Avg. total time: 29.41s\n","  Total disk usage: 20947.80 MB\n","\n","✓ Environment fingerprint: /content/logs/rocket_env.json\n","✓ All folds passed anti-leakage validation\n","✓ Academic compliance: per-fold independent fit (stratified sampling), no cross-fold information leakage\n","✓ Optimizations: BLAS single-thread, adaptive batching, memmap streaming writes, sampled statistics, disk checks\n","✓ MultiROCKET: float64 on demand, JIT warmup, contiguous memory\n","\n","⚠️  Large files are Git-ignored; commit commands:\n","   git add logs/rocket_*.json configs/ models/transformer_*_fold*.pkl\n","   git commit -m 'feature: academic-compliant ROCKET (independent fit per fold)'\n","\n","============================================================\n","Step 10 completed\n","============================================================\n"]}]},{"cell_type":"code","source":["# ================ Step 11: MiniROCKET + Ridge Classifier (Ultimate Optimized Edition) ================\n","# Pin BLAS threads (must be set before imports)\n","import os\n","os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n","os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n","os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n","os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n","\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import json\n","import pickle\n","from sklearn.linear_model import RidgeClassifier\n","from sklearn.model_selection import GroupKFold\n","from sklearn.utils.class_weight import compute_sample_weight\n","from sklearn.metrics import f1_score\n","from sklearn.feature_selection import VarianceThreshold\n","from joblib import Parallel, delayed\n","from threadpoolctl import threadpool_limits\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"\\n\\nStep 11: MiniROCKET + Ridge Classifier (Ultimate Optimized Edition)\")\n","print(\"=\" * 60)\n","\n","# Create output directory\n","Path('preds').mkdir(exist_ok=True)\n","\n","# Load configuration\n","def get_active_folds(path=\"logs/active_folds.json\"):\n","    p = Path(path)\n","    if p.exists():\n","        return json.loads(p.read_text())[\"folds\"]\n","    return []\n","\n","with open('configs/splits.json', 'r') as f:\n","    splits_cfg = json.load(f)\n","\n","with open('configs/classes.json', 'r') as f:\n","    classes_cfg = json.load(f)\n","\n","folds_to_run = get_active_folds()\n","print(f\"Folds to run: {folds_to_run}\\n\")\n","\n","# Ridge hyperparameters (ultimate optimized edition)\n","CV_FOLDS = 5\n","\n","print(f\"Ridge hyperparameters (ultimate optimized edition):\")\n","print(f\"  Two-stage search: coarse (9 points × 3-fold) → fine (9 points × 5-fold)\")\n","print(f\"  Sample weights: balanced (pre-cached)\")\n","print(f\"  solver: lsqr\")\n","print(f\"  Parallelism: thread-based + single-thread BLAS + pre_dispatch control\")\n","print(f\"  Low-variance filter: threshold=1e-6 (train-only fit)\\n\")\n","\n","id_to_label = {int(k): v for k, v in classes_cfg['id_to_label'].items()}\n","label_order = sorted(id_to_label.keys())\n","all_summaries = []\n","\n","# Process each fold\n","for fold in splits_cfg['folds']:\n","    k = fold['fold']\n","\n","    if k not in folds_to_run:\n","        print(f\"⏭️  Skipping Fold {k}\")\n","        continue\n","\n","    test_subj = fold['test_subject']\n","\n","    print(f\"\\n{'='*60}\")\n","    print(f\"Fold {k}: test subject={test_subj}\")\n","    print(f\"{'='*60}\")\n","\n","    # Load MiniROCKET training features\n","    X_train_raw = np.load(f'features/X_minirocket_train_fold{k}.npy', mmap_mode='r')\n","    meta_train = np.load(f'features/meta_minirocket_train_fold{k}.npz', allow_pickle=True)\n","\n","    y_train = meta_train['y']\n","    subjects_train = meta_train['subjects']\n","\n","    print(f\"Training set: {X_train_raw.shape[0]} samples, {X_train_raw.shape[1]} features\")\n","\n","    # Low-variance filtering (train-only fit, no leakage)\n","    vt = VarianceThreshold(threshold=1e-6)\n","    X_train = vt.fit_transform(X_train_raw)\n","    n_features_removed = X_train_raw.shape[1] - X_train.shape[1]\n","    print(f\"Low-variance filter: removed {n_features_removed} features, kept {X_train.shape[1]}\")\n","\n","    # Guardrail: automatically adjust number of CV folds\n","    n_unique_subjects = len(np.unique(subjects_train))\n","    actual_cv_folds = min(CV_FOLDS, n_unique_subjects)\n","    print(f\"Number of subjects: {n_unique_subjects}\")\n","    if actual_cv_folds < CV_FOLDS:\n","        print(f\"⚠️  Insufficient subjects, CV folds adjusted: {CV_FOLDS} → {actual_cv_folds}\")\n","\n","    print(f\"Class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\\n\")\n","\n","    # Precompute splits and weights (shared across all alphas)\n","    print(f\"Stage A: Coarse search (9 points × 3-fold)...\")\n","\n","    # Coarse: 3-fold\n","    splits_coarse = list(GroupKFold(n_splits=3).split(X_train, y_train, groups=subjects_train))\n","    w_coarse_list = [compute_sample_weight('balanced', y_train[tr]) for tr, _ in splits_coarse]\n","\n","    # Fine: 5-fold\n","    splits_fine = list(GroupKFold(n_splits=actual_cv_folds).split(X_train, y_train, groups=subjects_train))\n","    w_fine_list = [compute_sample_weight('balanced', y_train[tr]) for tr, _ in splits_fine]\n","\n","    # Function to compute CV score only (no OOF)\n","    def cv_score_only(alpha, splits, w_list):\n","        with threadpool_limits(limits=1):\n","            scores = []\n","            for (tr, va), w_tr in zip(splits, w_list):\n","                clf = RidgeClassifier(alpha=alpha, solver=\"lsqr\", fit_intercept=True)\n","                clf.fit(X_train[tr], y_train[tr], sample_weight=w_tr)\n","                scores.append(f1_score(y_train[va], clf.predict(X_train[va]),\n","                                      average='macro', zero_division=0))\n","            return float(np.mean(scores))\n","\n","    # Coarse search: 9 alpha points\n","    ALPHAS_COARSE = np.logspace(-6, 6, 9)\n","    n_jobs = min(len(ALPHAS_COARSE), max(1, (os.cpu_count() or 2) - 1))\n","\n","    scores_coarse = Parallel(n_jobs=n_jobs, prefer=\"threads\", pre_dispatch=\"2*n_jobs\")(\n","        delayed(cv_score_only)(alpha, splits_coarse, w_coarse_list)\n","        for alpha in ALPHAS_COARSE\n","    )\n","\n","    best_coarse_idx = int(np.argmax(scores_coarse))\n","    best_coarse_alpha = ALPHAS_COARSE[best_coarse_idx]\n","    best_coarse_score = scores_coarse[best_coarse_idx]\n","\n","    print(f\"  Coarse best: alpha={best_coarse_alpha:.6e}, CV macro F1={best_coarse_score:.4f}\")\n","\n","    # Fine search: ±1 decade around the best alpha\n","    print(f\"\\nStage B: Fine search (9 points × {actual_cv_folds}-fold)...\")\n","    log_alpha = float(np.log10(best_coarse_alpha))\n","    ALPHAS_FINE = np.logspace(log_alpha - 1, log_alpha + 1, 9)\n","\n","    scores_fine = Parallel(n_jobs=n_jobs, prefer=\"threads\", pre_dispatch=\"2*n_jobs\")(\n","        delayed(cv_score_only)(alpha, splits_fine, w_fine_list)\n","        for alpha in ALPHAS_FINE\n","    )\n","\n","    best_fine_idx = int(np.argmax(scores_fine))\n","    best_alpha = float(ALPHAS_FINE[best_fine_idx])\n","    best_score = scores_fine[best_fine_idx]\n","\n","    print(f\"  Fine best: alpha={best_alpha:.6e}, CV macro F1={best_score:.4f}\")\n","\n","    # Save alpha curve (coarse + fine combined)\n","    alpha_grid = list(ALPHAS_COARSE) + list(ALPHAS_FINE)\n","    score_grid = scores_coarse + scores_fine\n","    pd.DataFrame({\"alpha\": alpha_grid, \"cv_macro_f1\": score_grid}).to_csv(\n","        f\"logs/ridge_cv_fold{k}.csv\", index=False\n","    )\n","    print(f\"✓ Alpha curve saved: logs/ridge_cv_fold{k}.csv\")\n","\n","    # Generate OOF predictions only for the best alpha\n","    print(f\"\\nGenerating OOF predictions for the best alpha...\")\n","    def oof_for_best_alpha(alpha, splits, w_list):\n","        with threadpool_limits(limits=1):\n","            y_oof = np.empty_like(y_train)\n","            for (tr, va), w_tr in zip(splits, w_list):\n","                clf = RidgeClassifier(alpha=alpha, solver=\"lsqr\", fit_intercept=True)\n","                clf.fit(X_train[tr], y_train[tr], sample_weight=w_tr)\n","                y_oof[va] = clf.predict(X_train[va])\n","            return y_oof\n","\n","    y_oof_pred = oof_for_best_alpha(best_alpha, splits_fine, w_fine_list)\n","\n","    # OOF validation metrics\n","    per_class_f1_oof = f1_score(y_train, y_oof_pred, labels=label_order,\n","                                 average=None, zero_division=0)\n","    macro_f1_oof = f1_score(y_train, y_oof_pred, average='macro', zero_division=0)\n","\n","    print(f\"\\nTraining OOF validation (out-of-fold, not optimistic):\")\n","    print(f\"  Macro F1: {macro_f1_oof:.4f}\")\n","    print(f\"  Per-class F1:\")\n","    for cid, f1v in zip(label_order, per_class_f1_oof):\n","        n_c = int((y_train == cid).sum())\n","        print(f\"    {id_to_label[cid]:15s} (n={n_c:4d}): {f1v:.4f}\")\n","\n","    # Retrain on the full training set\n","    print(f\"\\nRetraining on the full training set...\")\n","    sample_weights = compute_sample_weight('balanced', y_train)\n","    ridge = RidgeClassifier(alpha=best_alpha, solver=\"lsqr\", fit_intercept=True)\n","    ridge.fit(X_train, y_train, sample_weight=sample_weights)\n","    print(f\"✓ Training completed\")\n","\n","    # Save classifier and variance filter\n","    model_data = {'ridge': ridge, 'variance_filter': vt}\n","    model_file = f'models/ridge_fold{k}.pkl'\n","    with open(model_file, 'wb') as f:\n","        pickle.dump(model_data, f, protocol=4)\n","\n","    model_size_mb = Path(model_file).stat().st_size / (1024 ** 2)\n","    print(f\"\\n✓ Model saved: {model_file} ({model_size_mb:.2f} MB)\")\n","\n","    # Test-set inference\n","    print(f\"\\nTest-set inference...\")\n","    X_test_raw = np.load(f'features/X_minirocket_test_fold{k}.npy', mmap_mode='r')\n","    meta_test = np.load(f'features/meta_minirocket_test_fold{k}.npz', allow_pickle=True)\n","    y_test = meta_test['y']\n","\n","    # Apply variance filter (transform only)\n","    X_test = vt.transform(X_test_raw)\n","    y_test_pred = ridge.predict(X_test)\n","\n","    # Save predictions\n","    np.save(f'preds/preds_fold{k}_minirocket.npy', y_test_pred)\n","    print(f\"✓ Test predictions saved: preds/preds_fold{k}_minirocket.npy\")\n","    print(f\"  Test set: {len(y_test)} samples\")\n","\n","    # Record summary\n","    summary = {\n","        'fold': k,\n","        'test_subject': test_subj,\n","        'n_train_samples': int(len(y_train)),\n","        'n_test_samples': int(len(y_test)),\n","        'n_features_original': int(X_train_raw.shape[1]),\n","        'n_features_filtered': int(X_train.shape[1]),\n","        'n_features_removed': int(n_features_removed),\n","        'actual_cv_folds': actual_cv_folds,\n","        'best_alpha': float(best_alpha),\n","        'best_coarse_alpha': float(best_coarse_alpha),\n","        'oof_macro_f1': float(macro_f1_oof),\n","        'per_class_f1_oof': {id_to_label[cid]: float(f1v) for cid, f1v in zip(label_order, per_class_f1_oof)},\n","        'model_size_mb': float(model_size_mb)\n","    }\n","    all_summaries.append(summary)\n","\n","# Save summary\n","summary_df = pd.DataFrame([{\n","    'fold': s['fold'],\n","    'test_subject': s['test_subject'],\n","    'n_train_samples': s['n_train_samples'],\n","    'n_test_samples': s['n_test_samples'],\n","    'n_features_filtered': s['n_features_filtered'],\n","    'actual_cv_folds': s['actual_cv_folds'],\n","    'best_alpha': s['best_alpha'],\n","    'oof_macro_f1': s['oof_macro_f1'],\n","    'model_size_mb': s['model_size_mb']\n","} for s in all_summaries])\n","\n","summary_df.to_csv('logs/ridge_summary.csv', index=False)\n","\n","with open('logs/ridge_summary.json', 'w') as f:\n","    json.dump({\n","        'ridge_config': {\n","            'two_stage_search': {\n","                'coarse': '9 points × 3-fold',\n","                'fine': '9 points × 5-fold',\n","                'total_evaluations': '≈18 evaluations (vs. original 65)'\n","            },\n","            'sample_weight': 'balanced (pre-cached)',\n","            'solver': 'lsqr',\n","            'variance_threshold': 1e-6,\n","            'parallel': {\n","                'prefer': 'threads',\n","                'blas_threads': 1,\n","                'pre_dispatch': '2*n_jobs'\n","            }\n","        },\n","        'n_folds': len(all_summaries),\n","        'per_fold_stats': all_summaries,\n","        'aggregated_stats': {\n","            'avg_oof_macro_f1': float(summary_df['oof_macro_f1'].mean()),\n","            'std_oof_macro_f1': float(summary_df['oof_macro_f1'].std()),\n","            'avg_best_alpha': float(summary_df['best_alpha'].mean()),\n","            'avg_features_filtered': float(summary_df['n_features_filtered'].mean()),\n","            'total_model_size_mb': float(summary_df['model_size_mb'].sum())\n","        }\n","    }, f, indent=2)\n","\n","print(f\"\\n{'='*60}\")\n","print(f\"Ridge classifier training completed\")\n","print(f\"{'='*60}\")\n","print(f\"\\nSummary:\")\n","print(f\"  Mean OOF Macro F1: {summary_df['oof_macro_f1'].mean():.4f} ± {summary_df['oof_macro_f1'].std():.4f}\")\n","print(f\"  Mean best alpha: {summary_df['best_alpha'].mean():.6e}\")\n","print(f\"  Mean number of features (post-filter): {summary_df['n_features_filtered'].mean():.0f}\")\n","print(f\"  Total model size: {summary_df['model_size_mb'].sum():.2f} MB\")\n","print(f\"\\nUltimate optimization notes:\")\n","print(f\"  1. Two-stage search: coarse→fine, evaluations 65→18 (~3.6×)\")\n","print(f\"  2. Thread-based parallelism: shared memory, avoids process IPC overhead (~1.5–2×)\")\n","print(f\"  3. Pre-caching: splits + weights precomputed (~1.3×)\")\n","print(f\"  4. Separated computation: compute scores first, then OOF only for best α (~1.2×)\")\n","print(f\"  5. Low-variance filtering: reduces feature dimensionality (~1.3–2×)\")\n","print(f\"  6. BLAS limits: avoid oversubscription (stability)\")\n","print(f\"  Overall speedup: 5–15× (depends on CPU cores and data scale)\")\n","print(f\"\\n✓ Summary CSV: logs/ridge_summary.csv\")\n","print(f\"✓ Summary JSON: logs/ridge_summary.json\")\n","print(f\"✓ Alpha curves: logs/ridge_cv_fold*.csv\")\n","print(f\"✓ Test predictions: preds/preds_fold*_minirocket.npy\")\n","print(f\"\\n{'='*60}\\nStep 11 completed\\n{'='*60}\")"],"metadata":{"id":"-Kl0It0L-Z38","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762946499246,"user_tz":0,"elapsed":50438573,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"1567f9c2-52f7-49ed-9046-172efc0ac6f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Step 11: MiniROCKET + Ridge Classifier (Ultimate Optimized Edition)\n","============================================================\n","Folds to run: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n","\n","Ridge hyperparameters (ultimate optimized edition):\n","  Two-stage search: coarse (9 points × 3-fold) → fine (9 points × 5-fold)\n","  Sample weights: balanced (pre-cached)\n","  solver: lsqr\n","  Parallelism: thread-based + single-thread BLAS + pre_dispatch control\n","  Low-variance filter: threshold=1e-6 (train-only fit)\n","\n","\n","============================================================\n","Fold 0: test subject=proband1\n","============================================================\n","Training set: 34727 samples, 9996 features\n","Low-variance filter: removed 0 features, kept 9996\n","Number of subjects: 14\n","Class distribution: {0: 5222, 1: 5851, 2: 5259, 3: 5192, 4: 5343, 5: 3946, 6: 3122, 7: 792}\n","\n","Stage A: Coarse search (9 points × 3-fold)...\n","  Coarse best: alpha=1.000000e+03, CV macro F1=0.8292\n","\n","Stage B: Fine search (9 points × 5-fold)...\n","  Fine best: alpha=1.778279e+02, CV macro F1=0.8330\n","✓ Alpha curve saved: logs/ridge_cv_fold0.csv\n","\n","Generating OOF predictions for the best alpha...\n","\n","Training OOF validation (out-of-fold, not optimistic):\n","  Macro F1: 0.8393\n","  Per-class F1:\n","    walking         (n=5222): 0.8266\n","    running         (n=5851): 0.8883\n","    sitting         (n=5259): 0.7890\n","    standing        (n=5192): 0.7899\n","    lying           (n=5343): 0.8729\n","    stairs_up       (n=3946): 0.7954\n","    stairs_down     (n=3122): 0.7710\n","    jumping         (n= 792): 0.9815\n","\n","Retraining on the full training set...\n","✓ Training completed\n","\n","✓ Model saved: models/ridge_fold0.pkl (0.38 MB)\n","\n","Test-set inference...\n","✓ Test predictions saved: preds/preds_fold0_minirocket.npy\n","  Test set: 1895 samples\n","\n","============================================================\n","Fold 1: test subject=proband10\n","============================================================\n","Training set: 34159 samples, 9996 features\n","Low-variance filter: removed 0 features, kept 9996\n","Number of subjects: 14\n","Class distribution: {0: 5246, 1: 5863, 2: 4893, 3: 5186, 4: 4959, 5: 4067, 6: 3171, 7: 774}\n","\n","Stage A: Coarse search (9 points × 3-fold)...\n","  Coarse best: alpha=3.162278e+01, CV macro F1=0.8311\n","\n","Stage B: Fine search (9 points × 5-fold)...\n","  Fine best: alpha=3.162278e+02, CV macro F1=0.8337\n","✓ Alpha curve saved: logs/ridge_cv_fold1.csv\n","\n","Generating OOF predictions for the best alpha...\n","\n","Training OOF validation (out-of-fold, not optimistic):\n","  Macro F1: 0.8339\n","  Per-class F1:\n","    walking         (n=5246): 0.8186\n","    running         (n=5863): 0.8867\n","    sitting         (n=4893): 0.7755\n","    standing        (n=5186): 0.7830\n","    lying           (n=4959): 0.8605\n","    stairs_up       (n=4067): 0.7993\n","    stairs_down     (n=3171): 0.7668\n","    jumping         (n= 774): 0.9811\n","\n","Retraining on the full training set...\n","✓ Training completed\n","\n","✓ Model saved: models/ridge_fold1.pkl (0.38 MB)\n","\n","Test-set inference...\n","✓ Test predictions saved: preds/preds_fold1_minirocket.npy\n","  Test set: 2463 samples\n","\n","============================================================\n","Fold 2: test subject=proband11\n","============================================================\n","Training set: 34042 samples, 9996 features\n","Low-variance filter: removed 0 features, kept 9996\n","Number of subjects: 14\n","Class distribution: {0: 5236, 1: 5883, 2: 4895, 3: 5196, 4: 4947, 5: 3964, 6: 3132, 7: 789}\n","\n","Stage A: Coarse search (9 points × 3-fold)...\n","  Coarse best: alpha=3.162278e+01, CV macro F1=0.8263\n","\n","Stage B: Fine search (9 points × 5-fold)...\n","  Fine best: alpha=1.778279e+02, CV macro F1=0.8209\n","✓ Alpha curve saved: logs/ridge_cv_fold2.csv\n","\n","Generating OOF predictions for the best alpha...\n","\n","Training OOF validation (out-of-fold, not optimistic):\n","  Macro F1: 0.8258\n","  Per-class F1:\n","    walking         (n=5236): 0.8140\n","    running         (n=5883): 0.8780\n","    sitting         (n=4895): 0.7669\n","    standing        (n=5196): 0.7661\n","    lying           (n=4947): 0.8700\n","    stairs_up       (n=3964): 0.7927\n","    stairs_down     (n=3132): 0.7373\n","    jumping         (n= 789): 0.9815\n","\n","Retraining on the full training set...\n","✓ Training completed\n","\n","✓ Model saved: models/ridge_fold2.pkl (0.38 MB)\n","\n","Test-set inference...\n","✓ Test predictions saved: preds/preds_fold2_minirocket.npy\n","  Test set: 2580 samples\n","\n","============================================================\n","Fold 3: test subject=proband12\n","============================================================\n","Training set: 34255 samples, 9996 features\n","Low-variance filter: removed 0 features, kept 9996\n","Number of subjects: 14\n","Class distribution: {0: 5300, 1: 5881, 2: 4960, 3: 5221, 4: 4970, 5: 3986, 6: 3136, 7: 801}\n","\n","Stage A: Coarse search (9 points × 3-fold)...\n","  Coarse best: alpha=3.162278e+01, CV macro F1=0.8347\n","\n","Stage B: Fine search (9 points × 5-fold)...\n","  Fine best: alpha=3.162278e+02, CV macro F1=0.8269\n","✓ Alpha curve saved: logs/ridge_cv_fold3.csv\n","\n","Generating OOF predictions for the best alpha...\n","\n","Training OOF validation (out-of-fold, not optimistic):\n","  Macro F1: 0.8289\n","  Per-class F1:\n","    walking         (n=5300): 0.8102\n","    running         (n=5881): 0.8848\n","    sitting         (n=4960): 0.7876\n","    standing        (n=5221): 0.7719\n","    lying           (n=4970): 0.8657\n","    stairs_up       (n=3986): 0.7853\n","    stairs_down     (n=3136): 0.7447\n","    jumping         (n= 801): 0.9811\n","\n","Retraining on the full training set...\n","✓ Training completed\n","\n","✓ Model saved: models/ridge_fold3.pkl (0.38 MB)\n","\n","Test-set inference...\n","✓ Test predictions saved: preds/preds_fold3_minirocket.npy\n","  Test set: 2367 samples\n","\n","============================================================\n","Fold 4: test subject=proband13\n","============================================================\n","Training set: 34033 samples, 9996 features\n","Low-variance filter: removed 0 features, kept 9996\n","Number of subjects: 14\n","Class distribution: {0: 5220, 1: 5862, 2: 4879, 3: 5190, 4: 4958, 5: 3978, 6: 3173, 7: 773}\n","\n","Stage A: Coarse search (9 points × 3-fold)...\n","  Coarse best: alpha=3.162278e+01, CV macro F1=0.8263\n","\n","Stage B: Fine search (9 points × 5-fold)...\n","  Fine best: alpha=3.162278e+01, CV macro F1=0.8215\n","✓ Alpha curve saved: logs/ridge_cv_fold4.csv\n","\n","Generating OOF predictions for the best alpha...\n","\n","Training OOF validation (out-of-fold, not optimistic):\n","  Macro F1: 0.8260\n","  Per-class F1:\n","    walking         (n=5220): 0.7867\n","    running         (n=5862): 0.8923\n","    sitting         (n=4879): 0.7772\n","    standing        (n=5190): 0.7778\n","    lying           (n=4958): 0.8545\n","    stairs_up       (n=3978): 0.7811\n","    stairs_down     (n=3173): 0.7562\n","    jumping         (n= 773): 0.9824\n","\n","Retraining on the full training set...\n","✓ Training completed\n","\n","✓ Model saved: models/ridge_fold4.pkl (0.38 MB)\n","\n","Test-set inference...\n","✓ Test predictions saved: preds/preds_fold4_minirocket.npy\n","  Test set: 2589 samples\n","\n","============================================================\n","Fold 5: test subject=proband14\n","============================================================\n","Training set: 34787 samples, 9996 features\n","Low-variance filter: removed 0 features, kept 9996\n","Number of subjects: 14\n","Class distribution: {0: 5267, 1: 5884, 2: 4897, 3: 5222, 4: 4974, 5: 4331, 6: 3425, 7: 787}\n","\n","Stage A: Coarse search (9 points × 3-fold)...\n","  Coarse best: alpha=1.000000e+03, CV macro F1=0.8279\n","\n","Stage B: Fine search (9 points × 5-fold)...\n","  Fine best: alpha=1.778279e+02, CV macro F1=0.8360\n","✓ Alpha curve saved: logs/ridge_cv_fold5.csv\n","\n","Generating OOF predictions for the best alpha...\n","\n","Training OOF validation (out-of-fold, not optimistic):\n","  Macro F1: 0.8440\n","  Per-class F1:\n","    walking         (n=5267): 0.8249\n","    running         (n=5884): 0.8879\n","    sitting         (n=4897): 0.8117\n","    standing        (n=5222): 0.8043\n","    lying           (n=4974): 0.8575\n","    stairs_up       (n=4331): 0.8117\n","    stairs_down     (n=3425): 0.7739\n","    jumping         (n= 787): 0.9801\n","\n","Retraining on the full training set...\n","✓ Training completed\n","\n","✓ Model saved: models/ridge_fold5.pkl (0.38 MB)\n","\n","Test-set inference...\n","✓ Test predictions saved: preds/preds_fold5_minirocket.npy\n","  Test set: 1835 samples\n","\n","============================================================\n","Fold 6: test subject=proband15\n","============================================================\n","Training set: 34008 samples, 9996 features\n","Low-variance filter: removed 0 features, kept 9996\n","Number of subjects: 14\n","Class distribution: {0: 5230, 1: 5835, 2: 4884, 3: 5200, 4: 4951, 5: 3988, 6: 3129, 7: 791}\n","\n","Stage A: Coarse search (9 points × 3-fold)...\n","  Coarse best: alpha=3.162278e+01, CV macro F1=0.8337\n","\n","Stage B: Fine search (9 points × 5-fold)...\n","  Fine best: alpha=5.623413e+01, CV macro F1=0.8270\n","✓ Alpha curve saved: logs/ridge_cv_fold6.csv\n","\n","Generating OOF predictions for the best alpha...\n","\n","Training OOF validation (out-of-fold, not optimistic):\n","  Macro F1: 0.8317\n","  Per-class F1:\n","    walking         (n=5230): 0.8188\n","    running         (n=5835): 0.8878\n","    sitting         (n=4884): 0.7621\n","    standing        (n=5200): 0.7742\n","    lying           (n=4951): 0.8602\n","    stairs_up       (n=3988): 0.7984\n","    stairs_down     (n=3129): 0.7688\n","    jumping         (n= 791): 0.9834\n","\n","Retraining on the full training set...\n","✓ Training completed\n","\n","✓ Model saved: models/ridge_fold6.pkl (0.38 MB)\n","\n","Test-set inference...\n","✓ Test predictions saved: preds/preds_fold6_minirocket.npy\n","  Test set: 2614 samples\n","\n","============================================================\n","Fold 7: test subject=proband2\n","============================================================\n","Training set: 34330 samples, 9996 features\n","Low-variance filter: removed 0 features, kept 9996\n","Number of subjects: 14\n","Class distribution: {0: 5287, 1: 5908, 2: 4886, 3: 5262, 4: 4985, 5: 4072, 6: 3144, 7: 786}\n","\n","Stage A: Coarse search (9 points × 3-fold)...\n","  Coarse best: alpha=3.162278e+01, CV macro F1=0.8240\n","\n","Stage B: Fine search (9 points × 5-fold)...\n","  Fine best: alpha=3.162278e+01, CV macro F1=0.8243\n","✓ Alpha curve saved: logs/ridge_cv_fold7.csv\n","\n","Generating OOF predictions for the best alpha...\n","\n","Training OOF validation (out-of-fold, not optimistic):\n","  Macro F1: 0.8324\n","  Per-class F1:\n","    walking         (n=5287): 0.8177\n","    running         (n=5908): 0.8806\n","    sitting         (n=4886): 0.7741\n","    standing        (n=5262): 0.7723\n","    lying           (n=4985): 0.8688\n","    stairs_up       (n=4072): 0.8122\n","    stairs_down     (n=3144): 0.7506\n","    jumping         (n= 786): 0.9826\n","\n","Retraining on the full training set...\n","✓ Training completed\n","\n","✓ Model saved: models/ridge_fold7.pkl (0.38 MB)\n","\n","Test-set inference...\n","✓ Test predictions saved: preds/preds_fold7_minirocket.npy\n","  Test set: 2292 samples\n","\n","============================================================\n","Fold 8: test subject=proband3\n","============================================================\n","Training set: 33917 samples, 9996 features\n","Low-variance filter: removed 0 features, kept 9996\n","Number of subjects: 14\n","Class distribution: {0: 5203, 1: 5787, 2: 4889, 3: 5206, 4: 4972, 5: 3983, 6: 3093, 7: 784}\n","\n","Stage A: Coarse search (9 points × 3-fold)...\n","  Coarse best: alpha=3.162278e+01, CV macro F1=0.8400\n","\n","Stage B: Fine search (9 points × 5-fold)...\n","  Fine best: alpha=1.000000e+02, CV macro F1=0.8361\n","✓ Alpha curve saved: logs/ridge_cv_fold8.csv\n","\n","Generating OOF predictions for the best alpha...\n","\n","Training OOF validation (out-of-fold, not optimistic):\n","  Macro F1: 0.8405\n","  Per-class F1:\n","    walking         (n=5203): 0.8209\n","    running         (n=5787): 0.8903\n","    sitting         (n=4889): 0.7687\n","    standing        (n=5206): 0.7796\n","    lying           (n=4972): 0.8584\n","    stairs_up       (n=3983): 0.8166\n","    stairs_down     (n=3093): 0.8100\n","    jumping         (n= 784): 0.9794\n","\n","Retraining on the full training set...\n","✓ Training completed\n","\n","✓ Model saved: models/ridge_fold8.pkl (0.38 MB)\n","\n","Test-set inference...\n","✓ Test predictions saved: preds/preds_fold8_minirocket.npy\n","  Test set: 2705 samples\n","\n","============================================================\n","Fold 9: test subject=proband4\n","============================================================\n","Training set: 34375 samples, 9996 features\n","Low-variance filter: removed 0 features, kept 9996\n","Number of subjects: 14\n","Class distribution: {0: 5230, 1: 5615, 2: 4864, 3: 5194, 4: 4925, 5: 4331, 6: 3425, 7: 791}\n","\n","Stage A: Coarse search (9 points × 3-fold)...\n","  Coarse best: alpha=3.162278e+01, CV macro F1=0.8382\n","\n","Stage B: Fine search (9 points × 5-fold)...\n","  Fine best: alpha=1.778279e+02, CV macro F1=0.8412\n","✓ Alpha curve saved: logs/ridge_cv_fold9.csv\n","\n","Generating OOF predictions for the best alpha...\n","\n","Training OOF validation (out-of-fold, not optimistic):\n","  Macro F1: 0.8513\n","  Per-class F1:\n","    walking         (n=5230): 0.8210\n","    running         (n=5615): 0.9161\n","    sitting         (n=4864): 0.8071\n","    standing        (n=5194): 0.8066\n","    lying           (n=4925): 0.8904\n","    stairs_up       (n=4331): 0.8166\n","    stairs_down     (n=3425): 0.7700\n","    jumping         (n= 791): 0.9828\n","\n","Retraining on the full training set...\n","✓ Training completed\n","\n","✓ Model saved: models/ridge_fold9.pkl (0.38 MB)\n","\n","Test-set inference...\n","✓ Test predictions saved: preds/preds_fold9_minirocket.npy\n","  Test set: 2247 samples\n","\n","============================================================\n","Fold 10: test subject=proband5\n","============================================================\n","Training set: 33690 samples, 9996 features\n","Low-variance filter: removed 0 features, kept 9996\n","Number of subjects: 14\n","Class distribution: {0: 5205, 1: 5571, 2: 4855, 3: 5216, 4: 4961, 5: 3968, 6: 3131, 7: 783}\n","\n","Stage A: Coarse search (9 points × 3-fold)...\n","  Coarse best: alpha=3.162278e+01, CV macro F1=0.8513\n","\n","Stage B: Fine search (9 points × 5-fold)...\n","  Fine best: alpha=3.162278e+02, CV macro F1=0.8334\n","✓ Alpha curve saved: logs/ridge_cv_fold10.csv\n","\n","Generating OOF predictions for the best alpha...\n","\n","Training OOF validation (out-of-fold, not optimistic):\n","  Macro F1: 0.8361\n","  Per-class F1:\n","    walking         (n=5205): 0.8447\n","    running         (n=5571): 0.9118\n","    sitting         (n=4855): 0.7471\n","    standing        (n=5216): 0.7771\n","    lying           (n=4961): 0.8434\n","    stairs_up       (n=3968): 0.8086\n","    stairs_down     (n=3131): 0.7727\n","    jumping         (n= 783): 0.9832\n","\n","Retraining on the full training set...\n","✓ Training completed\n","\n","✓ Model saved: models/ridge_fold10.pkl (0.38 MB)\n","\n","Test-set inference...\n","✓ Test predictions saved: preds/preds_fold10_minirocket.npy\n","  Test set: 2932 samples\n","\n","============================================================\n","Fold 11: test subject=proband6\n","============================================================\n","Training set: 34091 samples, 9996 features\n","Low-variance filter: removed 0 features, kept 9996\n","Number of subjects: 14\n","Class distribution: {0: 5256, 1: 5860, 2: 4860, 3: 5211, 4: 4965, 5: 4022, 6: 3135, 7: 782}\n","\n","Stage A: Coarse search (9 points × 3-fold)...\n","  Coarse best: alpha=3.162278e+01, CV macro F1=0.8351\n","\n","Stage B: Fine search (9 points × 5-fold)...\n","  Fine best: alpha=5.623413e+01, CV macro F1=0.8343\n","✓ Alpha curve saved: logs/ridge_cv_fold11.csv\n","\n","Generating OOF predictions for the best alpha...\n","\n","Training OOF validation (out-of-fold, not optimistic):\n","  Macro F1: 0.8386\n","  Per-class F1:\n","    walking         (n=5256): 0.8370\n","    running         (n=5860): 0.8906\n","    sitting         (n=4860): 0.7763\n","    standing        (n=5211): 0.7933\n","    lying           (n=4965): 0.8550\n","    stairs_up       (n=4022): 0.8050\n","    stairs_down     (n=3135): 0.7707\n","    jumping         (n= 782): 0.9806\n","\n","Retraining on the full training set...\n","✓ Training completed\n","\n","✓ Model saved: models/ridge_fold11.pkl (0.38 MB)\n","\n","Test-set inference...\n","✓ Test predictions saved: preds/preds_fold11_minirocket.npy\n","  Test set: 2531 samples\n","\n","============================================================\n","Fold 12: test subject=proband7\n","============================================================\n","Training set: 34620 samples, 9996 features\n","Low-variance filter: removed 0 features, kept 9996\n","Number of subjects: 14\n","Class distribution: {0: 5258, 1: 5796, 2: 4873, 3: 5183, 4: 4968, 5: 4331, 6: 3425, 7: 786}\n","\n","Stage A: Coarse search (9 points × 3-fold)...\n","  Coarse best: alpha=1.000000e+03, CV macro F1=0.8343\n","\n","Stage B: Fine search (9 points × 5-fold)...\n","  Fine best: alpha=1.000000e+03, CV macro F1=0.8232\n","✓ Alpha curve saved: logs/ridge_cv_fold12.csv\n","\n","Generating OOF predictions for the best alpha...\n","\n","Training OOF validation (out-of-fold, not optimistic):\n","  Macro F1: 0.8323\n","  Per-class F1:\n","    walking         (n=5258): 0.8198\n","    running         (n=5796): 0.8819\n","    sitting         (n=4873): 0.7640\n","    standing        (n=5183): 0.7597\n","    lying           (n=4968): 0.8664\n","    stairs_up       (n=4331): 0.8214\n","    stairs_down     (n=3425): 0.7629\n","    jumping         (n= 786): 0.9820\n","\n","Retraining on the full training set...\n","✓ Training completed\n","\n","✓ Model saved: models/ridge_fold12.pkl (0.38 MB)\n","\n","Test-set inference...\n","✓ Test predictions saved: preds/preds_fold12_minirocket.npy\n","  Test set: 2002 samples\n","\n","============================================================\n","Fold 13: test subject=proband8\n","============================================================\n","Training set: 33728 samples, 9996 features\n","Low-variance filter: removed 1 features, kept 9995\n","Number of subjects: 14\n","Class distribution: {0: 5243, 1: 5862, 2: 4858, 3: 5175, 4: 4965, 5: 3657, 6: 3180, 7: 788}\n","\n","Stage A: Coarse search (9 points × 3-fold)...\n","  Coarse best: alpha=3.162278e+01, CV macro F1=0.8542\n","\n","Stage B: Fine search (9 points × 5-fold)...\n","  Fine best: alpha=3.162278e+02, CV macro F1=0.8429\n","✓ Alpha curve saved: logs/ridge_cv_fold13.csv\n","\n","Generating OOF predictions for the best alpha...\n","\n","Training OOF validation (out-of-fold, not optimistic):\n","  Macro F1: 0.8431\n","  Per-class F1:\n","    walking         (n=5243): 0.8527\n","    running         (n=5862): 0.8841\n","    sitting         (n=4858): 0.7588\n","    standing        (n=5175): 0.7588\n","    lying           (n=4965): 0.8744\n","    stairs_up       (n=3657): 0.8386\n","    stairs_down     (n=3180): 0.7991\n","    jumping         (n= 788): 0.9783\n","\n","Retraining on the full training set...\n","✓ Training completed\n","\n","✓ Model saved: models/ridge_fold13.pkl (0.38 MB)\n","\n","Test-set inference...\n","✓ Test predictions saved: preds/preds_fold13_minirocket.npy\n","  Test set: 2894 samples\n","\n","============================================================\n","Fold 14: test subject=proband9\n","============================================================\n","Training set: 33946 samples, 9996 features\n","Low-variance filter: removed 0 features, kept 9996\n","Number of subjects: 14\n","Class distribution: {0: 5249, 1: 5762, 2: 4874, 3: 5182, 4: 4959, 5: 4010, 6: 3129, 7: 781}\n","\n","Stage A: Coarse search (9 points × 3-fold)...\n","  Coarse best: alpha=1.000000e+03, CV macro F1=0.8351\n","\n","Stage B: Fine search (9 points × 5-fold)...\n","  Fine best: alpha=5.623413e+02, CV macro F1=0.8227\n","✓ Alpha curve saved: logs/ridge_cv_fold14.csv\n","\n","Generating OOF predictions for the best alpha...\n","\n","Training OOF validation (out-of-fold, not optimistic):\n","  Macro F1: 0.8299\n","  Per-class F1:\n","    walking         (n=5249): 0.8287\n","    running         (n=5762): 0.8936\n","    sitting         (n=4874): 0.7546\n","    standing        (n=5182): 0.7709\n","    lying           (n=4959): 0.8567\n","    stairs_up       (n=4010): 0.7934\n","    stairs_down     (n=3129): 0.7603\n","    jumping         (n= 781): 0.9813\n","\n","Retraining on the full training set...\n","✓ Training completed\n","\n","✓ Model saved: models/ridge_fold14.pkl (0.38 MB)\n","\n","Test-set inference...\n","✓ Test predictions saved: preds/preds_fold14_minirocket.npy\n","  Test set: 2676 samples\n","\n","============================================================\n","Ridge classifier training completed\n","============================================================\n","\n","Summary:\n","  Mean OOF Macro F1: 0.8356 ± 0.0072\n","  Mean best alpha: 2.542852e+02\n","  Mean number of features (post-filter): 9996\n","  Total model size: 5.73 MB\n","\n","Ultimate optimization notes:\n","  1. Two-stage search: coarse→fine, evaluations 65→18 (~3.6×)\n","  2. Thread-based parallelism: shared memory, avoids process IPC overhead (~1.5–2×)\n","  3. Pre-caching: splits + weights precomputed (~1.3×)\n","  4. Separated computation: compute scores first, then OOF only for best α (~1.2×)\n","  5. Low-variance filtering: reduces feature dimensionality (~1.3–2×)\n","  6. BLAS limits: avoid oversubscription (stability)\n","  Overall speedup: 5–15× (depends on CPU cores and data scale)\n","\n","✓ Summary CSV: logs/ridge_summary.csv\n","✓ Summary JSON: logs/ridge_summary.json\n","✓ Alpha curves: logs/ridge_cv_fold*.csv\n","✓ Test predictions: preds/preds_fold*_minirocket.npy\n","\n","============================================================\n","Step 11 completed\n","============================================================\n"]}]},{"cell_type":"code","source":["# ================ Step 13: TST Preparation ================\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from pathlib import Path\n","import json\n","from sklearn.model_selection import GroupKFold\n","\n","print(\"\\n\\nStep 13: TST Preparation\")\n","print(\"=\" * 60)\n","\n","# Load configuration\n","with open('/content/configs/splits.json', 'r') as f:\n","    splits_cfg = json.load(f)\n","\n","with open('/content/logs/active_folds.json', 'r') as f:\n","    active_folds = json.load(f)['folds']\n","\n","features_dir = Path('/content/features')\n","interim_dir = Path('/content/interim')\n","interim_dir.mkdir(exist_ok=True)\n","\n","# TST parameters\n","N_CHANNELS = 6\n","SEQ_LEN = 150\n","PATCH_LEN = 25\n","BATCH_SIZE = 64\n","NUM_WORKERS = 4\n","N_VAL_SPLITS = 5\n","\n","CHANNELS = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']\n","\n","print(f\"Tensor shape: (C={N_CHANNELS}, L={SEQ_LEN})\")\n","print(f\"Patch length: {PATCH_LEN}\")\n","print(f\"Batch size: {BATCH_SIZE}, Workers: {NUM_WORKERS}\")\n","print(f\"Number of validation splits: {N_VAL_SPLITS}\\n\")\n","\n","class TSTDataset(Dataset):\n","    def __init__(self, data, labels, subjects=None):\n","        self.data = torch.from_numpy(data).float()\n","        self.labels = torch.from_numpy(labels).long()\n","        self.subjects = subjects\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx], self.labels[idx]\n","\n","def prepare_fold(fold_id):\n","    print(f\"\\n{'='*60}\")\n","    print(f\"Fold {fold_id}\")\n","    print(f\"{'='*60}\")\n","\n","    # Load standardized data\n","    norm_file = features_dir / f'windows_normalized_fold{fold_id}.npz'\n","    data = np.load(norm_file)\n","\n","    # Extract data\n","    splits = data['splits']\n","    labels = data['labels']\n","    subjects = data['subjects']\n","\n","    train_mask = (splits == 'train')\n","    test_mask = (splits == 'test')\n","\n","    # Assemble tensor (N, C, L)\n","    sensor_data = np.stack([data[ch] for ch in CHANNELS], axis=1)\n","\n","    X_train_full = sensor_data[train_mask]\n","    y_train_full = labels[train_mask]\n","    subjects_train = subjects[train_mask]\n","\n","    X_test = sensor_data[test_mask]\n","    y_test = labels[test_mask]\n","    subjects_test = subjects[test_mask]\n","\n","    print(f\"Train set: {X_train_full.shape}, Test set: {X_test.shape}\")\n","    print(f\"Training subjects: {np.unique(subjects_train).tolist()}\")\n","    print(f\"Test subjects: {np.unique(subjects_test).tolist()}\")\n","\n","    # Partition validation folds within the training set using GroupKFold\n","    gkf = GroupKFold(n_splits=N_VAL_SPLITS)\n","    val_splits = list(gkf.split(X_train_full, y_train_full, groups=subjects_train))\n","\n","    print(f\"\\nValidation splits (GroupKFold={N_VAL_SPLITS}):\")\n","    for val_idx, (train_idx, val_idx_inner) in enumerate(val_splits):\n","        val_subjects = np.unique(subjects_train[val_idx_inner])\n","        print(f\"  Val Split {val_idx}: Train={len(train_idx)}, Val={len(val_idx_inner)}, Val subjects={val_subjects.tolist()}\")\n","\n","    # Create DataLoaders (using the 0-th validation split as an example)\n","    train_idx, val_idx = val_splits[0]\n","\n","    X_train = X_train_full[train_idx]\n","    y_train = y_train_full[train_idx]\n","    X_val = X_train_full[val_idx]\n","    y_val = y_train_full[val_idx]\n","\n","    print(f\"\\nUsing validation split 0:\")\n","    print(f\"  Train: {X_train.shape}\")\n","    print(f\"  Validation: {X_val.shape}\")\n","    print(f\"  Test: {X_test.shape}\")\n","\n","    train_dataset = TSTDataset(X_train, y_train)\n","    val_dataset = TSTDataset(X_val, y_val)\n","    test_dataset = TSTDataset(X_test, y_test)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n","                             num_workers=NUM_WORKERS, pin_memory=True)\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n","                           num_workers=NUM_WORKERS, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n","                            num_workers=NUM_WORKERS, pin_memory=True)\n","\n","    # Tensor sanity check\n","    print(f\"\\nTensor sanity check:\")\n","    sample_batch = next(iter(train_loader))\n","    print(f\"  Batch shape: {sample_batch[0].shape}\")\n","    print(f\"  Label shape: {sample_batch[1].shape}\")\n","    print(f\"  Dtype: {sample_batch[0].dtype}\")\n","    print(f\"  Label range: [{sample_batch[1].min()}, {sample_batch[1].max()}]\")\n","\n","    # Save tensor data\n","    tensors = {\n","        'fold': fold_id,\n","        'X_train_full': torch.from_numpy(X_train_full).float(),\n","        'y_train_full': torch.from_numpy(y_train_full).long(),\n","        'subjects_train': subjects_train,\n","        'X_test': torch.from_numpy(X_test).float(),\n","        'y_test': torch.from_numpy(y_test).long(),\n","        'subjects_test': subjects_test,\n","        'val_splits_indices': val_splits,\n","        'shape': {\n","            'n_channels': N_CHANNELS,\n","            'seq_len': SEQ_LEN,\n","            'patch_len': PATCH_LEN\n","        },\n","        'config': {\n","            'batch_size': BATCH_SIZE,\n","            'num_workers': NUM_WORKERS,\n","            'n_val_splits': N_VAL_SPLITS\n","        }\n","    }\n","\n","    save_path = interim_dir / f'tensors_fold{fold_id}.pt'\n","    torch.save(tensors, save_path)\n","    print(f\"\\n✓ Saved: {save_path}\")\n","\n","    return {\n","        'fold': fold_id,\n","        'train_full': len(X_train_full),\n","        'test': len(X_test),\n","        'train_subjects': np.unique(subjects_train).tolist(),\n","        'test_subjects': np.unique(subjects_test).tolist(),\n","        'n_val_splits': N_VAL_SPLITS\n","    }\n","\n","# Process all active folds\n","fold_stats = []\n","for fold_id in active_folds:\n","    stats = prepare_fold(fold_id)\n","    fold_stats.append(stats)\n","\n","# Save summary\n","summary = {\n","    'tensor_shape': f'(C={N_CHANNELS}, L={SEQ_LEN})',\n","    'patch_len': PATCH_LEN,\n","    'dataloader_config': {\n","        'batch_size': BATCH_SIZE,\n","        'num_workers': NUM_WORKERS,\n","        'shuffle_train': True,\n","        'pin_memory': True\n","    },\n","    'validation': {\n","        'method': 'GroupKFold',\n","        'n_splits': N_VAL_SPLITS,\n","        'groupby': 'subject'\n","    },\n","    'split_order': 'LOSO outer -> GroupKFold inner',\n","    'dtype': 'torch.float32',\n","    'folds': fold_stats\n","}\n","\n","with open('/content/logs/step13_tst_summary.json', 'w') as f:\n","    json.dump(summary, f, indent=2)\n","\n","print(f\"\\n{'='*60}\")\n","print(f\"✓ Completed TST preparation for {len(active_folds)} folds\")\n","print(f\"✓ Tensors: interim/tensors_fold{{k}}.pt\")\n","print(f\"✓ Summary: logs/step13_tst_summary.json\")\n","print(f\"{'='*60}\\n\")\n","\n","get_ipython().system('git add interim/tensors_fold*.pt logs/step13_tst_summary.json')\n","get_ipython().system('git commit -m \"tst: prepare tensors with GroupKFold validation\"')\n","\n","print(f\"Step 13 completed\\n{'='*60}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xvemWSR0FUcK","executionInfo":{"status":"ok","timestamp":1762946590843,"user_tz":0,"elapsed":91594,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"8aa2a1c7-5c89-411b-e38f-f43d856c41f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Step 13: TST Preparation\n","============================================================\n","Tensor shape: (C=6, L=150)\n","Patch length: 25\n","Batch size: 64, Workers: 4\n","Number of validation splits: 5\n","\n","\n","============================================================\n","Fold 0\n","============================================================\n","Train set: (34727, 6, 150), Test set: (1895, 6, 150)\n","Training subjects: ['proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","Test subjects: ['proband1']\n","\n","Validation splits (GroupKFold=5):\n","  Val Split 0: Train=27593, Val=7134, Val subjects=['proband12', 'proband14', 'proband5']\n","  Val Split 1: Train=29370, Val=5357, Val subjects=['proband10', 'proband8']\n","  Val Split 2: Train=27244, Val=7483, Val subjects=['proband3', 'proband4', 'proband6']\n","  Val Split 3: Train=27469, Val=7258, Val subjects=['proband11', 'proband7', 'proband9']\n","  Val Split 4: Train=27232, Val=7495, Val subjects=['proband13', 'proband15', 'proband2']\n","\n","Using validation split 0:\n","  Train: (27593, 6, 150)\n","  Validation: (7134, 6, 150)\n","  Test: (1895, 6, 150)\n","\n","Tensor sanity check:\n","  Batch shape: torch.Size([64, 6, 150])\n","  Label shape: torch.Size([64])\n","  Dtype: torch.float32\n","  Label range: [0, 7]\n","\n","✓ Saved: /content/interim/tensors_fold0.pt\n","\n","============================================================\n","Fold 1\n","============================================================\n","Train set: (34159, 6, 150), Test set: (2463, 6, 150)\n","Training subjects: ['proband1', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","Test subjects: ['proband10']\n","\n","Validation splits (GroupKFold=5):\n","  Val Split 0: Train=26933, Val=7226, Val subjects=['proband2', 'proband5', 'proband7']\n","  Val Split 1: Train=28898, Val=5261, Val subjects=['proband12', 'proband8']\n","  Val Split 2: Train=27028, Val=7131, Val subjects=['proband1', 'proband3', 'proband6']\n","  Val Split 3: Train=27068, Val=7091, Val subjects=['proband11', 'proband14', 'proband9']\n","  Val Split 4: Train=26709, Val=7450, Val subjects=['proband13', 'proband15', 'proband4']\n","\n","Using validation split 0:\n","  Train: (26933, 6, 150)\n","  Validation: (7226, 6, 150)\n","  Test: (2463, 6, 150)\n","\n","Tensor sanity check:\n","  Batch shape: torch.Size([64, 6, 150])\n","  Label shape: torch.Size([64])\n","  Dtype: torch.float32\n","  Label range: [0, 7]\n","\n","✓ Saved: /content/interim/tensors_fold1.pt\n","\n","============================================================\n","Fold 2\n","============================================================\n","Train set: (34042, 6, 150), Test set: (2580, 6, 150)\n","Training subjects: ['proband1', 'proband10', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","Test subjects: ['proband11']\n","\n","Validation splits (GroupKFold=5):\n","  Val Split 0: Train=26983, Val=7059, Val subjects=['proband14', 'proband2', 'proband5']\n","  Val Split 1: Train=28781, Val=5261, Val subjects=['proband12', 'proband8']\n","  Val Split 2: Train=26627, Val=7415, Val subjects=['proband10', 'proband3', 'proband4']\n","  Val Split 3: Train=26940, Val=7102, Val subjects=['proband1', 'proband6', 'proband9']\n","  Val Split 4: Train=26837, Val=7205, Val subjects=['proband13', 'proband15', 'proband7']\n","\n","Using validation split 0:\n","  Train: (26983, 6, 150)\n","  Validation: (7059, 6, 150)\n","  Test: (2580, 6, 150)\n","\n","Tensor sanity check:\n","  Batch shape: torch.Size([64, 6, 150])\n","  Label shape: torch.Size([64])\n","  Dtype: torch.float32\n","  Label range: [0, 7]\n","\n","✓ Saved: /content/interim/tensors_fold2.pt\n","\n","============================================================\n","Fold 3\n","============================================================\n","Train set: (34255, 6, 150), Test set: (2367, 6, 150)\n","Training subjects: ['proband1', 'proband10', 'proband11', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","Test subjects: ['proband12']\n","\n","Validation splits (GroupKFold=5):\n","  Val Split 0: Train=27029, Val=7226, Val subjects=['proband2', 'proband5', 'proband7']\n","  Val Split 1: Train=28898, Val=5357, Val subjects=['proband10', 'proband8']\n","  Val Split 2: Train=27124, Val=7131, Val subjects=['proband1', 'proband3', 'proband6']\n","  Val Split 3: Train=27164, Val=7091, Val subjects=['proband11', 'proband14', 'proband9']\n","  Val Split 4: Train=26805, Val=7450, Val subjects=['proband13', 'proband15', 'proband4']\n","\n","Using validation split 0:\n","  Train: (27029, 6, 150)\n","  Validation: (7226, 6, 150)\n","  Test: (2367, 6, 150)\n","\n","Tensor sanity check:\n","  Batch shape: torch.Size([64, 6, 150])\n","  Label shape: torch.Size([64])\n","  Dtype: torch.float32\n","  Label range: [0, 6]\n","\n","✓ Saved: /content/interim/tensors_fold3.pt\n","\n","============================================================\n","Fold 4\n","============================================================\n","Train set: (34033, 6, 150), Test set: (2589, 6, 150)\n","Training subjects: ['proband1', 'proband10', 'proband11', 'proband12', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","Test subjects: ['proband13']\n","\n","Validation splits (GroupKFold=5):\n","  Val Split 0: Train=26974, Val=7059, Val subjects=['proband14', 'proband2', 'proband5']\n","  Val Split 1: Train=28772, Val=5261, Val subjects=['proband12', 'proband8']\n","  Val Split 2: Train=26618, Val=7415, Val subjects=['proband10', 'proband3', 'proband4']\n","  Val Split 3: Train=26931, Val=7102, Val subjects=['proband1', 'proband6', 'proband9']\n","  Val Split 4: Train=26837, Val=7196, Val subjects=['proband11', 'proband15', 'proband7']\n","\n","Using validation split 0:\n","  Train: (26974, 6, 150)\n","  Validation: (7059, 6, 150)\n","  Test: (2589, 6, 150)\n","\n","Tensor sanity check:\n","  Batch shape: torch.Size([64, 6, 150])\n","  Label shape: torch.Size([64])\n","  Dtype: torch.float32\n","  Label range: [0, 7]\n","\n","✓ Saved: /content/interim/tensors_fold4.pt\n","\n","============================================================\n","Fold 5\n","============================================================\n","Train set: (34787, 6, 150), Test set: (1835, 6, 150)\n","Training subjects: ['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","Test subjects: ['proband14']\n","\n","Validation splits (GroupKFold=5):\n","  Val Split 0: Train=27593, Val=7194, Val subjects=['proband1', 'proband12', 'proband5']\n","  Val Split 1: Train=29430, Val=5357, Val subjects=['proband10', 'proband8']\n","  Val Split 2: Train=27304, Val=7483, Val subjects=['proband3', 'proband4', 'proband6']\n","  Val Split 3: Train=27529, Val=7258, Val subjects=['proband11', 'proband7', 'proband9']\n","  Val Split 4: Train=27292, Val=7495, Val subjects=['proband13', 'proband15', 'proband2']\n","\n","Using validation split 0:\n","  Train: (27593, 6, 150)\n","  Validation: (7194, 6, 150)\n","  Test: (1835, 6, 150)\n","\n","Tensor sanity check:\n","  Batch shape: torch.Size([64, 6, 150])\n","  Label shape: torch.Size([64])\n","  Dtype: torch.float32\n","  Label range: [0, 7]\n","\n","✓ Saved: /content/interim/tensors_fold5.pt\n","\n","============================================================\n","Fold 6\n","============================================================\n","Train set: (34008, 6, 150), Test set: (2614, 6, 150)\n","Training subjects: ['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","Test subjects: ['proband15']\n","\n","Validation splits (GroupKFold=5):\n","  Val Split 0: Train=26949, Val=7059, Val subjects=['proband14', 'proband2', 'proband5']\n","  Val Split 1: Train=28747, Val=5261, Val subjects=['proband12', 'proband8']\n","  Val Split 2: Train=26593, Val=7415, Val subjects=['proband10', 'proband3', 'proband4']\n","  Val Split 3: Train=26906, Val=7102, Val subjects=['proband1', 'proband6', 'proband9']\n","  Val Split 4: Train=26837, Val=7171, Val subjects=['proband11', 'proband13', 'proband7']\n","\n","Using validation split 0:\n","  Train: (26949, 6, 150)\n","  Validation: (7059, 6, 150)\n","  Test: (2614, 6, 150)\n","\n","Tensor sanity check:\n","  Batch shape: torch.Size([64, 6, 150])\n","  Label shape: torch.Size([64])\n","  Dtype: torch.float32\n","  Label range: [0, 7]\n","\n","✓ Saved: /content/interim/tensors_fold6.pt\n","\n","============================================================\n","Fold 7\n","============================================================\n","Train set: (34330, 6, 150), Test set: (2292, 6, 150)\n","Training subjects: ['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","Test subjects: ['proband2']\n","\n","Validation splits (GroupKFold=5):\n","  Val Split 0: Train=27196, Val=7134, Val subjects=['proband12', 'proband14', 'proband5']\n","  Val Split 1: Train=28973, Val=5357, Val subjects=['proband10', 'proband8']\n","  Val Split 2: Train=27092, Val=7238, Val subjects=['proband3', 'proband6', 'proband7']\n","  Val Split 3: Train=27179, Val=7151, Val subjects=['proband1', 'proband11', 'proband9']\n","  Val Split 4: Train=26880, Val=7450, Val subjects=['proband13', 'proband15', 'proband4']\n","\n","Using validation split 0:\n","  Train: (27196, 6, 150)\n","  Validation: (7134, 6, 150)\n","  Test: (2292, 6, 150)\n","\n","Tensor sanity check:\n","  Batch shape: torch.Size([64, 6, 150])\n","  Label shape: torch.Size([64])\n","  Dtype: torch.float32\n","  Label range: [0, 6]\n","\n","✓ Saved: /content/interim/tensors_fold7.pt\n","\n","============================================================\n","Fold 8\n","============================================================\n","Train set: (33917, 6, 150), Test set: (2705, 6, 150)\n","Training subjects: ['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","Test subjects: ['proband3']\n","\n","Validation splits (GroupKFold=5):\n","  Val Split 0: Train=26858, Val=7059, Val subjects=['proband14', 'proband2', 'proband5']\n","  Val Split 1: Train=28656, Val=5261, Val subjects=['proband12', 'proband8']\n","  Val Split 2: Train=26531, Val=7386, Val subjects=['proband10', 'proband4', 'proband9']\n","  Val Split 3: Train=26770, Val=7147, Val subjects=['proband15', 'proband6', 'proband7']\n","  Val Split 4: Train=26853, Val=7064, Val subjects=['proband1', 'proband11', 'proband13']\n","\n","Using validation split 0:\n","  Train: (26858, 6, 150)\n","  Validation: (7059, 6, 150)\n","  Test: (2705, 6, 150)\n","\n","Tensor sanity check:\n","  Batch shape: torch.Size([64, 6, 150])\n","  Label shape: torch.Size([64])\n","  Dtype: torch.float32\n","  Label range: [0, 7]\n","\n","✓ Saved: /content/interim/tensors_fold8.pt\n","\n","============================================================\n","Fold 9\n","============================================================\n","Train set: (34375, 6, 150), Test set: (2247, 6, 150)\n","Training subjects: ['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband5', 'proband6', 'proband7', 'proband8', 'proband9']\n","Test subjects: ['proband4']\n","\n","Validation splits (GroupKFold=5):\n","  Val Split 0: Train=27241, Val=7134, Val subjects=['proband12', 'proband14', 'proband5']\n","  Val Split 1: Train=29018, Val=5357, Val subjects=['proband10', 'proband8']\n","  Val Split 2: Train=27137, Val=7238, Val subjects=['proband3', 'proband6', 'proband7']\n","  Val Split 3: Train=27224, Val=7151, Val subjects=['proband1', 'proband11', 'proband9']\n","  Val Split 4: Train=26880, Val=7495, Val subjects=['proband13', 'proband15', 'proband2']\n","\n","Using validation split 0:\n","  Train: (27241, 6, 150)\n","  Validation: (7134, 6, 150)\n","  Test: (2247, 6, 150)\n","\n","Tensor sanity check:\n","  Batch shape: torch.Size([64, 6, 150])\n","  Label shape: torch.Size([64])\n","  Dtype: torch.float32\n","  Label range: [0, 6]\n","\n","✓ Saved: /content/interim/tensors_fold9.pt\n","\n","============================================================\n","Fold 10\n","============================================================\n","Train set: (33690, 6, 150), Test set: (2932, 6, 150)\n","Training subjects: ['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband6', 'proband7', 'proband8', 'proband9']\n","Test subjects: ['proband5']\n","\n","Validation splits (GroupKFold=5):\n","  Val Split 0: Train=28504, Val=5186, Val subjects=['proband2', 'proband8']\n","  Val Split 1: Train=26371, Val=7319, Val subjects=['proband12', 'proband3', 'proband4']\n","  Val Split 2: Train=26549, Val=7141, Val subjects=['proband10', 'proband7', 'proband9']\n","  Val Split 3: Train=26650, Val=7040, Val subjects=['proband1', 'proband15', 'proband6']\n","  Val Split 4: Train=26686, Val=7004, Val subjects=['proband11', 'proband13', 'proband14']\n","\n","Using validation split 0:\n","  Train: (28504, 6, 150)\n","  Validation: (5186, 6, 150)\n","  Test: (2932, 6, 150)\n","\n","Tensor sanity check:\n","  Batch shape: torch.Size([64, 6, 150])\n","  Label shape: torch.Size([64])\n","  Dtype: torch.float32\n","  Label range: [0, 7]\n","\n","✓ Saved: /content/interim/tensors_fold10.pt\n","\n","============================================================\n","Fold 11\n","============================================================\n","Train set: (34091, 6, 150), Test set: (2531, 6, 150)\n","Training subjects: ['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband7', 'proband8', 'proband9']\n","Test subjects: ['proband6']\n","\n","Validation splits (GroupKFold=5):\n","  Val Split 0: Train=26972, Val=7119, Val subjects=['proband1', 'proband2', 'proband5']\n","  Val Split 1: Train=28830, Val=5261, Val subjects=['proband12', 'proband8']\n","  Val Split 2: Train=26676, Val=7415, Val subjects=['proband10', 'proband3', 'proband4']\n","  Val Split 3: Train=27000, Val=7091, Val subjects=['proband11', 'proband14', 'proband9']\n","  Val Split 4: Train=26886, Val=7205, Val subjects=['proband13', 'proband15', 'proband7']\n","\n","Using validation split 0:\n","  Train: (26972, 6, 150)\n","  Validation: (7119, 6, 150)\n","  Test: (2531, 6, 150)\n","\n","Tensor sanity check:\n","  Batch shape: torch.Size([64, 6, 150])\n","  Label shape: torch.Size([64])\n","  Dtype: torch.float32\n","  Label range: [0, 7]\n","\n","✓ Saved: /content/interim/tensors_fold11.pt\n","\n","============================================================\n","Fold 12\n","============================================================\n","Train set: (34620, 6, 150), Test set: (2002, 6, 150)\n","Training subjects: ['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband8', 'proband9']\n","Test subjects: ['proband7']\n","\n","Validation splits (GroupKFold=5):\n","  Val Split 0: Train=27486, Val=7134, Val subjects=['proband12', 'proband14', 'proband5']\n","  Val Split 1: Train=29263, Val=5357, Val subjects=['proband10', 'proband8']\n","  Val Split 2: Train=27137, Val=7483, Val subjects=['proband3', 'proband4', 'proband6']\n","  Val Split 3: Train=27469, Val=7151, Val subjects=['proband1', 'proband11', 'proband9']\n","  Val Split 4: Train=27125, Val=7495, Val subjects=['proband13', 'proband15', 'proband2']\n","\n","Using validation split 0:\n","  Train: (27486, 6, 150)\n","  Validation: (7134, 6, 150)\n","  Test: (2002, 6, 150)\n","\n","Tensor sanity check:\n","  Batch shape: torch.Size([64, 6, 150])\n","  Label shape: torch.Size([64])\n","  Dtype: torch.float32\n","  Label range: [0, 7]\n","\n","✓ Saved: /content/interim/tensors_fold12.pt\n","\n","============================================================\n","Fold 13\n","============================================================\n","Train set: (33728, 6, 150), Test set: (2894, 6, 150)\n","Training subjects: ['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband9']\n","Test subjects: ['proband8']\n","\n","Validation splits (GroupKFold=5):\n","  Val Split 0: Train=28504, Val=5224, Val subjects=['proband2', 'proband5']\n","  Val Split 1: Train=26409, Val=7319, Val subjects=['proband12', 'proband3', 'proband4']\n","  Val Split 2: Train=26587, Val=7141, Val subjects=['proband10', 'proband7', 'proband9']\n","  Val Split 3: Train=26688, Val=7040, Val subjects=['proband1', 'proband15', 'proband6']\n","  Val Split 4: Train=26724, Val=7004, Val subjects=['proband11', 'proband13', 'proband14']\n","\n","Using validation split 0:\n","  Train: (28504, 6, 150)\n","  Validation: (5224, 6, 150)\n","  Test: (2894, 6, 150)\n","\n","Tensor sanity check:\n","  Batch shape: torch.Size([64, 6, 150])\n","  Label shape: torch.Size([64])\n","  Dtype: torch.float32\n","  Label range: [0, 7]\n","\n","✓ Saved: /content/interim/tensors_fold13.pt\n","\n","============================================================\n","Fold 14\n","============================================================\n","Train set: (33946, 6, 150), Test set: (2676, 6, 150)\n","Training subjects: ['proband1', 'proband10', 'proband11', 'proband12', 'proband13', 'proband14', 'proband15', 'proband2', 'proband3', 'proband4', 'proband5', 'proband6', 'proband7', 'proband8']\n","Test subjects: ['proband9']\n","\n","Validation splits (GroupKFold=5):\n","  Val Split 0: Train=26887, Val=7059, Val subjects=['proband14', 'proband2', 'proband5']\n","  Val Split 1: Train=28685, Val=5261, Val subjects=['proband12', 'proband8']\n","  Val Split 2: Train=26776, Val=7170, Val subjects=['proband10', 'proband3', 'proband7']\n","  Val Split 3: Train=26554, Val=7392, Val subjects=['proband15', 'proband4', 'proband6']\n","  Val Split 4: Train=26882, Val=7064, Val subjects=['proband1', 'proband11', 'proband13']\n","\n","Using validation split 0:\n","  Train: (26887, 6, 150)\n","  Validation: (7059, 6, 150)\n","  Test: (2676, 6, 150)\n","\n","Tensor sanity check:\n","  Batch shape: torch.Size([64, 6, 150])\n","  Label shape: torch.Size([64])\n","  Dtype: torch.float32\n","  Label range: [0, 7]\n","\n","✓ Saved: /content/interim/tensors_fold14.pt\n","\n","============================================================\n","✓ Completed TST preparation for 15 folds\n","✓ Tensors: interim/tensors_fold{k}.pt\n","✓ Summary: logs/step13_tst_summary.json\n","============================================================\n","\n","[master 567a52e] tst: prepare tensors with GroupKFold validation\n"," 16 files changed, 394 insertions(+)\n"," create mode 100644 interim/tensors_fold0.pt\n"," create mode 100644 interim/tensors_fold1.pt\n"," create mode 100644 interim/tensors_fold10.pt\n"," create mode 100644 interim/tensors_fold11.pt\n"," create mode 100644 interim/tensors_fold12.pt\n"," create mode 100644 interim/tensors_fold13.pt\n"," create mode 100644 interim/tensors_fold14.pt\n"," create mode 100644 interim/tensors_fold2.pt\n"," create mode 100644 interim/tensors_fold3.pt\n"," create mode 100644 interim/tensors_fold4.pt\n"," create mode 100644 interim/tensors_fold5.pt\n"," create mode 100644 interim/tensors_fold6.pt\n"," create mode 100644 interim/tensors_fold7.pt\n"," create mode 100644 interim/tensors_fold8.pt\n"," create mode 100644 interim/tensors_fold9.pt\n"," create mode 100644 logs/step13_tst_summary.json\n","Step 13 completed\n","============================================================\n"]}]},{"cell_type":"code","source":["# ================ Step 14: TST Training ================\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.cuda.amp import autocast, GradScaler\n","from pathlib import Path\n","import json\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n","import random\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"\\n\\nStep 14: TST Training\")\n","print(\"=\" * 60)\n","\n","# Set random seeds\n","random.seed(42)\n","torch.manual_seed(42)\n","torch.cuda.manual_seed_all(42)\n","np.random.seed(42)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","torch.use_deterministic_algorithms(True, warn_only=True)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","AMP_ENABLED = (device.type == 'cuda')\n","amp_dtype = torch.bfloat16 if (AMP_ENABLED and torch.cuda.is_bf16_supported()) else torch.float16\n","print(f\"Device: {device}, AMP: {AMP_ENABLED}, dtype: {amp_dtype}\")\n","\n","# Load configuration\n","with open('/content/configs/classes.json', 'r') as f:\n","    classes_cfg = json.load(f)\n","\n","with open('/content/logs/active_folds.json', 'r') as f:\n","    active_folds = json.load(f)['folds']\n","\n","interim_dir = Path('/content/interim')\n","models_dir = Path('/content/models')\n","figures_dir = Path('/content/figures')\n","models_dir.mkdir(exist_ok=True)\n","figures_dir.mkdir(exist_ok=True)\n","\n","NUM_CLASSES = classes_cfg['num_classes']\n","INCLUDED_CLASSES = [c for c, flag in classes_cfg['statistics']['included_flags'].items() if flag]\n","NUM_INCLUDED = len(INCLUDED_CLASSES)\n","\n","# TST hyperparameters\n","D_MODEL = 64\n","N_HEADS = 4\n","DEPTH = 4\n","DROPOUT = 0.1\n","LR = 1e-3\n","WEIGHT_DECAY = 1e-4\n","GRAD_CLIP = 1.0\n","BATCH_SIZE = 64\n","NUM_WORKERS = 4\n","MAX_EPOCHS = 100\n","PATIENCE = 10\n","\n","print(f\"\\nHyperparameters:\")\n","print(f\"  d_model={D_MODEL}, n_heads={N_HEADS}, depth={DEPTH}, dropout={DROPOUT}\")\n","print(f\"  lr={LR}, weight_decay={WEIGHT_DECAY}, grad_clip={GRAD_CLIP}\")\n","print(f\"  patience={PATIENCE}, max_epochs={MAX_EPOCHS}\")\n","print(f\"  Number of classes: {NUM_CLASSES} (included: {NUM_INCLUDED})\\n\")\n","\n","class PatchEmbedding(nn.Module):\n","    def __init__(self, n_channels, seq_len, patch_len, d_model):\n","        super().__init__()\n","        self.patch_len = patch_len\n","        self.n_patches = seq_len // patch_len\n","        self.proj = nn.Linear(n_channels * patch_len, d_model)\n","\n","    def forward(self, x):\n","        B, C, L = x.shape\n","        x = x.unfold(2, self.patch_len, self.patch_len)\n","        x = x.permute(0, 2, 1, 3).contiguous()\n","        x = x.view(B, self.n_patches, -1)\n","        x = self.proj(x)\n","        return x\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1)]\n","\n","class TST(nn.Module):\n","    def __init__(self, n_channels, seq_len, patch_len, num_classes, d_model, n_heads, depth, dropout):\n","        super().__init__()\n","        self.patch_embedding = PatchEmbedding(n_channels, seq_len, patch_len, d_model)\n","        self.pos_encoding = PositionalEncoding(d_model)\n","\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model,\n","            nhead=n_heads,\n","            dim_feedforward=d_model * 4,\n","            dropout=dropout,\n","            batch_first=True\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(d_model, num_classes)\n","\n","    def forward(self, x):\n","        x = self.patch_embedding(x)\n","        x = self.pos_encoding(x)\n","        x = self.transformer(x)\n","        x = x.mean(dim=1)\n","        x = self.dropout(x)\n","        x = self.fc(x)\n","        return x\n","\n","class TSTDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","class EarlyStopping:\n","    def __init__(self, patience, mode='max'):\n","        self.patience = patience\n","        self.mode = mode\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.best_epoch = 0\n","\n","    def __call__(self, score, epoch):\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.best_epoch = epoch\n","            return True\n","\n","        if self.mode == 'max':\n","            improved = score > self.best_score\n","        else:\n","            improved = score < self.best_score\n","\n","        if improved:\n","            self.best_score = score\n","            self.best_epoch = epoch\n","            self.counter = 0\n","            return True\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","            return False\n","\n","def train_epoch(model, loader, criterion, optimizer, scaler, device, amp_enabled, amp_dtype):\n","    model.train()\n","    total_loss = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    for X, y in loader:\n","        X, y = X.to(device), y.to(device)\n","\n","        optimizer.zero_grad()\n","        with autocast(enabled=amp_enabled, dtype=amp_dtype):\n","            outputs = model(X)\n","            loss = criterion(outputs, y)\n","\n","        scaler.scale(loss).backward()\n","        scaler.unscale_(optimizer)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item() * len(X)\n","        preds = outputs.argmax(dim=1).cpu().numpy()\n","        all_preds.extend(preds)\n","        all_labels.extend(y.cpu().numpy())\n","\n","    avg_loss = total_loss / len(loader.dataset)\n","    acc = accuracy_score(all_labels, all_preds)\n","    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n","    return avg_loss, acc, f1\n","\n","def eval_epoch(model, loader, criterion, device, amp_enabled, amp_dtype):\n","    model.eval()\n","    total_loss = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for X, y in loader:\n","            X, y = X.to(device), y.to(device)\n","            with autocast(enabled=amp_enabled, dtype=amp_dtype):\n","                outputs = model(X)\n","                loss = criterion(outputs, y)\n","\n","            total_loss += loss.item() * len(X)\n","            preds = outputs.argmax(dim=1).cpu().numpy()\n","            all_preds.extend(preds)\n","            all_labels.extend(y.cpu().numpy())\n","\n","    avg_loss = total_loss / len(loader.dataset)\n","    acc = accuracy_score(all_labels, all_preds)\n","    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n","    return avg_loss, acc, f1, all_preds, all_labels\n","\n","def train_fold(fold_id):\n","    print(f\"\\n{'='*60}\")\n","    print(f\"Fold {fold_id}\")\n","    print(f\"{'='*60}\")\n","\n","    # Load tensors\n","    tensors = torch.load(interim_dir / f'tensors_fold{fold_id}.pt', weights_only=False)\n","    X_train_full = tensors['X_train_full']\n","    y_train_full = tensors['y_train_full']\n","    X_test = tensors['X_test']\n","    y_test = tensors['y_test']\n","    val_splits = tensors['val_splits_indices']\n","\n","    n_channels = tensors['shape']['n_channels']\n","    seq_len = tensors['shape']['seq_len']\n","    patch_len = tensors['shape']['patch_len']\n","\n","    print(f\"Data: Train={len(X_train_full)}, Test={len(X_test)}\")\n","    print(f\"Shapes: C={n_channels}, L={seq_len}, Patch={patch_len}\")\n","\n","    # Phase 1: Select best epoch via validation early stopping\n","    print(f\"\\n--- Phase 1: Validation-based early stopping ---\")\n","    train_idx, val_idx = val_splits[0]\n","    X_train = X_train_full[train_idx]\n","    y_train = y_train_full[train_idx]\n","    X_val = X_train_full[val_idx]\n","    y_val = y_train_full[val_idx]\n","\n","    train_dataset = TSTDataset(X_train, y_train)\n","    val_dataset = TSTDataset(X_val, y_val)\n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n","                             num_workers=NUM_WORKERS, pin_memory=True,\n","                             persistent_workers=(NUM_WORKERS > 0))\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n","                           num_workers=NUM_WORKERS, pin_memory=True,\n","                           persistent_workers=(NUM_WORKERS > 0))\n","\n","    model = TST(n_channels, seq_len, patch_len, NUM_CLASSES, D_MODEL, N_HEADS, DEPTH, DROPOUT).to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n","    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n","    scaler = GradScaler(enabled=(AMP_ENABLED and amp_dtype == torch.float16))\n","    early_stopping = EarlyStopping(patience=PATIENCE, mode='max')\n","\n","    history = {'train_loss': [], 'train_acc': [], 'train_f1': [],\n","               'val_loss': [], 'val_acc': [], 'val_f1': []}\n","\n","    print(f\"Train: {len(X_train)}, Val: {len(X_val)}\")\n","\n","    for epoch in range(MAX_EPOCHS):\n","        train_loss, train_acc, train_f1 = train_epoch(model, train_loader, criterion, optimizer, scaler, device, AMP_ENABLED, amp_dtype)\n","        val_loss, val_acc, val_f1, _, _ = eval_epoch(model, val_loader, criterion, device, AMP_ENABLED, amp_dtype)\n","\n","        history['train_loss'].append(train_loss)\n","        history['train_acc'].append(train_acc)\n","        history['train_f1'].append(train_f1)\n","        history['val_loss'].append(val_loss)\n","        history['val_acc'].append(val_acc)\n","        history['val_f1'].append(val_f1)\n","\n","        scheduler.step(val_f1)\n","\n","        improved = early_stopping(val_f1, epoch)\n","\n","        if epoch % 5 == 0 or improved:\n","            print(f\"Epoch {epoch:3d}: Train Loss={train_loss:.4f}, F1={train_f1:.4f} | \"\n","                  f\"Val Loss={val_loss:.4f}, F1={val_f1:.4f}\")\n","\n","        if early_stopping.early_stop:\n","            print(f\"Early stopping at epoch {epoch}\")\n","            break\n","\n","    best_epoch = early_stopping.best_epoch\n","    best_val_f1 = early_stopping.best_score\n","    print(f\"\\nBest epoch: {best_epoch}, Best val_f1: {best_val_f1:.4f}\")\n","\n","    # Phase 2: Retrain on the full training set up to the best epoch\n","    print(f\"\\n--- Phase 2: Retrain on full training set (target epoch={best_epoch}) ---\")\n","    train_full_dataset = TSTDataset(X_train_full, y_train_full)\n","    test_dataset = TSTDataset(X_test, y_test)\n","    train_full_loader = DataLoader(train_full_dataset, batch_size=BATCH_SIZE, shuffle=True,\n","                                   num_workers=NUM_WORKERS, pin_memory=True,\n","                                   persistent_workers=(NUM_WORKERS > 0))\n","    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n","                            num_workers=NUM_WORKERS, pin_memory=True,\n","                            persistent_workers=(NUM_WORKERS > 0))\n","\n","    model_final = TST(n_channels, seq_len, patch_len, NUM_CLASSES, D_MODEL, N_HEADS, DEPTH, DROPOUT).to(device)\n","    optimizer_final = Adam(model_final.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n","    scaler_final = GradScaler(enabled=(AMP_ENABLED and amp_dtype == torch.float16))\n","\n","    final_history = {'train_loss': [], 'train_acc': [], 'train_f1': []}\n","\n","    for epoch in range(best_epoch + 1):\n","        train_loss, train_acc, train_f1 = train_epoch(model_final, train_full_loader, criterion,\n","                                                       optimizer_final, scaler_final, device, AMP_ENABLED, amp_dtype)\n","        final_history['train_loss'].append(train_loss)\n","        final_history['train_acc'].append(train_acc)\n","        final_history['train_f1'].append(train_f1)\n","\n","        if epoch % 10 == 0 or epoch == best_epoch:\n","            print(f\"Epoch {epoch:3d}: Train Loss={train_loss:.4f}, Acc={train_acc:.4f}, F1={train_f1:.4f}\")\n","\n","    # Test-set evaluation\n","    test_loss, test_acc, test_f1, test_preds, test_labels = eval_epoch(model_final, test_loader, criterion, device, AMP_ENABLED, amp_dtype)\n","    print(f\"\\nTest set: Loss={test_loss:.4f}, Acc={test_acc:.4f}, F1={test_f1:.4f}\")\n","\n","    # Save model\n","    model_path = models_dir / f'tst_fold{fold_id}.pt'\n","    torch.save({\n","        'model_state_dict': model_final.state_dict(),\n","        'model_config': {\n","            'n_channels': n_channels,\n","            'seq_len': seq_len,\n","            'patch_len': patch_len,\n","            'num_classes': NUM_CLASSES,\n","            'd_model': D_MODEL,\n","            'n_heads': N_HEADS,\n","            'depth': DEPTH,\n","            'dropout': DROPOUT\n","        },\n","        'fold': fold_id,\n","        'best_epoch': best_epoch,\n","        'best_val_f1': best_val_f1,\n","        'test_metrics': {\n","            'loss': test_loss,\n","            'accuracy': test_acc,\n","            'macro_f1': test_f1\n","        }\n","    }, model_path)\n","    print(f\"✓ Model saved: {model_path}\")\n","\n","    # Plot training curves\n","    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n","\n","    # Validation-phase curves\n","    ax = axes[0]\n","    ax.plot(history['train_loss'], label='Train')\n","    ax.plot(history['val_loss'], label='Val')\n","    ax.axvline(best_epoch, color='red', linestyle='--', label=f'Best Epoch={best_epoch}')\n","    ax.set_xlabel('Epoch')\n","    ax.set_ylabel('Loss')\n","    ax.set_title(f'Fold {fold_id} - Loss (Validation Phase)')\n","    ax.legend()\n","    ax.grid(alpha=0.3)\n","\n","    ax = axes[1]\n","    ax.plot(history['train_f1'], label='Train')\n","    ax.plot(history['val_f1'], label='Val')\n","    ax.axvline(best_epoch, color='red', linestyle='--', label=f'Best Epoch={best_epoch}')\n","    ax.set_xlabel('Epoch')\n","    ax.set_ylabel('Macro F1')\n","    ax.set_title(f'Fold {fold_id} - F1 (Validation Phase)')\n","    ax.legend()\n","    ax.grid(alpha=0.3)\n","\n","    # Retraining-phase curves\n","    ax = axes[2]\n","    ax.plot(final_history['train_loss'], label='Train Loss')\n","    ax.plot(final_history['train_f1'], label='Train F1')\n","    ax.axvline(best_epoch, color='red', linestyle='--', label=f'Stop Epoch={best_epoch}')\n","    ax.set_xlabel('Epoch')\n","    ax.set_ylabel('Metric')\n","    ax.set_title(f'Fold {fold_id} - Retrain on Full Train Set')\n","    ax.legend()\n","    ax.grid(alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.savefig(f'/content/figures/step14_tst_fold{fold_id}.png', dpi=150)\n","    plt.close()\n","\n","    # Save report\n","    report = {\n","        'fold': fold_id,\n","        'phase1_validation': {\n","            'best_epoch': best_epoch,\n","            'best_val_f1': best_val_f1,\n","            'train_size': len(X_train),\n","            'val_size': len(X_val),\n","            'history': {k: [float(v) for v in vals] for k, vals in history.items()}\n","        },\n","        'phase2_retrain': {\n","            'target_epoch': best_epoch,\n","            'train_full_size': len(X_train_full),\n","            'final_train_f1': float(final_history['train_f1'][-1]),\n","            'history': {k: [float(v) for v in vals] for k, vals in final_history.items()}\n","        },\n","        'test_results': {\n","            'test_size': len(X_test),\n","            'loss': float(test_loss),\n","            'accuracy': float(test_acc),\n","            'macro_f1': float(test_f1)\n","        },\n","        'hyperparameters': {\n","            'd_model': D_MODEL,\n","            'n_heads': N_HEADS,\n","            'depth': DEPTH,\n","            'dropout': DROPOUT,\n","            'lr': LR,\n","            'weight_decay': WEIGHT_DECAY,\n","            'grad_clip': GRAD_CLIP,\n","            'batch_size': BATCH_SIZE,\n","            'patience': PATIENCE,\n","            'max_epochs': MAX_EPOCHS\n","        },\n","        'consistency_check': {\n","            'val_stopped_at': best_epoch,\n","            'retrain_stopped_at': best_epoch,\n","            'consistent': True\n","        }\n","    }\n","\n","    with open(f'/content/logs/step14_tst_fold{fold_id}.json', 'w') as f:\n","        json.dump(report, f, indent=2)\n","\n","    return report\n","\n","# Train all active folds\n","all_reports = []\n","for fold_id in active_folds:\n","    report = train_fold(fold_id)\n","    all_reports.append(report)\n","\n","# Aggregate summary\n","summary = {\n","    'method': 'TST',\n","    'training_procedure': 'Two-phase training: (1) Within the training set, use validation-based early stopping to choose the best epoch; (2) Retrain on the full training set up to the best epoch',\n","    'amp_enabled': AMP_ENABLED,\n","    'amp_dtype': str(amp_dtype),\n","    'device': str(device),\n","    'deterministic': True,\n","    'random_seed': 42,\n","    'folds': all_reports,\n","    'average_test_metrics': {\n","        'accuracy': np.mean([r['test_results']['accuracy'] for r in all_reports]),\n","        'macro_f1': np.mean([r['test_results']['macro_f1'] for r in all_reports])\n","    }\n","}\n","\n","with open('/content/logs/step14_tst_summary.json', 'w') as f:\n","    json.dump(summary, f, indent=2)\n","\n","print(f\"\\n{'='*60}\")\n","print(f\"✓ Completed training for {len(active_folds)} folds\")\n","print(f\"✓ Models: models/tst_fold{{k}}.pt\")\n","print(f\"✓ Curves: figures/step14_tst_fold{{k}}.png\")\n","print(f\"✓ Reports: logs/step14_tst_fold{{k}}.json\")\n","print(f\"✓ Summary: logs/step14_tst_summary.json\")\n","print(f\"\\nAverage test-set performance:\")\n","print(f\"  Accuracy: {summary['average_test_metrics']['accuracy']:.4f}\")\n","print(f\"  Macro F1: {summary['average_test_metrics']['macro_f1']:.4f}\")\n","print(f\"{'='*60}\\n\")\n","\n","get_ipython().system('git add models/ figures/step14_*.png logs/step14_*.json')\n","get_ipython().system('git commit -m \"train: TST with two-phase training and early stopping\"')\n","\n","print(f\"Step 14 completed\\n{'='*60}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gELuFDEdGfs-","executionInfo":{"status":"ok","timestamp":1762950740616,"user_tz":0,"elapsed":4149757,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"8ff5132d-52c6-4dd7-e925-dd5c41d84dd4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Step 14: TST Training\n","============================================================\n","Device: cuda, AMP: True, dtype: torch.bfloat16\n","\n","Hyperparameters:\n","  d_model=64, n_heads=4, depth=4, dropout=0.1\n","  lr=0.001, weight_decay=0.0001, grad_clip=1.0\n","  patience=10, max_epochs=100\n","  Number of classes: 8 (included: 8)\n","\n","\n","============================================================\n","Fold 0\n","============================================================\n","Data: Train=34727, Test=1895\n","Shapes: C=6, L=150, Patch=25\n","\n","--- Phase 1: Validation-based early stopping ---\n","Train: 27593, Val: 7134\n","Epoch   0: Train Loss=0.9607, F1=0.6118 | Val Loss=0.8718, F1=0.6166\n","Epoch   1: Train Loss=0.7052, F1=0.7445 | Val Loss=0.8468, F1=0.6422\n","Epoch   2: Train Loss=0.5977, F1=0.8023 | Val Loss=0.6308, F1=0.8022\n","Epoch   3: Train Loss=0.4997, F1=0.8464 | Val Loss=0.6202, F1=0.8041\n","Epoch   5: Train Loss=0.3934, F1=0.8806 | Val Loss=0.5711, F1=0.8059\n","Epoch   6: Train Loss=0.3583, F1=0.8924 | Val Loss=0.5949, F1=0.8076\n","Epoch  10: Train Loss=0.2634, F1=0.9200 | Val Loss=0.5906, F1=0.8135\n","Epoch  15: Train Loss=0.1998, F1=0.9401 | Val Loss=0.6392, F1=0.8117\n","Epoch  20: Train Loss=0.1166, F1=0.9682 | Val Loss=0.8307, F1=0.8142\n","Epoch  21: Train Loss=0.1102, F1=0.9697 | Val Loss=0.8042, F1=0.8170\n","Epoch  25: Train Loss=0.0910, F1=0.9745 | Val Loss=0.9140, F1=0.8093\n","Epoch  30: Train Loss=0.0541, F1=0.9845 | Val Loss=0.9803, F1=0.8139\n","Early stopping at epoch 31\n","\n","Best epoch: 21, Best val_f1: 0.8170\n","\n","--- Phase 2: Retrain on full training set (target epoch=21) ---\n","Epoch   0: Train Loss=0.8947, Acc=0.6207, F1=0.6444\n","Epoch  10: Train Loss=0.2648, Acc=0.9097, F1=0.9197\n","Epoch  20: Train Loss=0.1675, Acc=0.9441, F1=0.9494\n","Epoch  21: Train Loss=0.1631, Acc=0.9453, F1=0.9510\n","\n","Test set: Loss=0.6378, Acc=0.8311, F1=0.6492\n","✓ Model saved: /content/models/tst_fold0.pt\n","\n","============================================================\n","Fold 1\n","============================================================\n","Data: Train=34159, Test=2463\n","Shapes: C=6, L=150, Patch=25\n","\n","--- Phase 1: Validation-based early stopping ---\n","Train: 26933, Val: 7226\n","Epoch   0: Train Loss=0.9441, F1=0.5976 | Val Loss=0.9705, F1=0.5901\n","Epoch   1: Train Loss=0.6874, F1=0.7445 | Val Loss=0.8595, F1=0.6589\n","Epoch   2: Train Loss=0.5882, F1=0.7985 | Val Loss=0.7362, F1=0.7282\n","Epoch   3: Train Loss=0.5099, F1=0.8383 | Val Loss=0.7141, F1=0.7641\n","Epoch   5: Train Loss=0.4244, F1=0.8679 | Val Loss=0.7897, F1=0.7561\n","Epoch   6: Train Loss=0.3917, F1=0.8780 | Val Loss=0.6657, F1=0.7875\n","Epoch  10: Train Loss=0.2840, F1=0.9149 | Val Loss=0.6270, F1=0.7998\n","Epoch  15: Train Loss=0.2118, F1=0.9358 | Val Loss=0.8342, F1=0.7560\n","Epoch  20: Train Loss=0.1230, F1=0.9642 | Val Loss=0.9065, F1=0.7852\n","Early stopping at epoch 20\n","\n","Best epoch: 10, Best val_f1: 0.7998\n","\n","--- Phase 2: Retrain on full training set (target epoch=10) ---\n","Epoch   0: Train Loss=0.8839, Acc=0.6283, F1=0.6393\n","Epoch  10: Train Loss=0.2645, Acc=0.9116, F1=0.9211\n","\n","Test set: Loss=0.5371, Acc=0.8295, F1=0.8403\n","✓ Model saved: /content/models/tst_fold1.pt\n","\n","============================================================\n","Fold 2\n","============================================================\n","Data: Train=34042, Test=2580\n","Shapes: C=6, L=150, Patch=25\n","\n","--- Phase 1: Validation-based early stopping ---\n","Train: 26983, Val: 7059\n","Epoch   0: Train Loss=0.9252, F1=0.6173 | Val Loss=0.9429, F1=0.6037\n","Epoch   1: Train Loss=0.6911, F1=0.7448 | Val Loss=0.7643, F1=0.7381\n","Epoch   2: Train Loss=0.5906, F1=0.8071 | Val Loss=0.6818, F1=0.7813\n","Epoch   5: Train Loss=0.4031, F1=0.8737 | Val Loss=0.6050, F1=0.7973\n","Epoch  10: Train Loss=0.2870, F1=0.9103 | Val Loss=0.7315, F1=0.7811\n","Epoch  13: Train Loss=0.1965, F1=0.9418 | Val Loss=0.7707, F1=0.8034\n","Epoch  15: Train Loss=0.1758, F1=0.9488 | Val Loss=0.8435, F1=0.7850\n","Epoch  20: Train Loss=0.1196, F1=0.9650 | Val Loss=0.8873, F1=0.7967\n","Epoch  21: Train Loss=0.1086, F1=0.9693 | Val Loss=0.8845, F1=0.8069\n","Epoch  25: Train Loss=0.0919, F1=0.9748 | Val Loss=1.1237, F1=0.7610\n","Epoch  30: Train Loss=0.0646, F1=0.9822 | Val Loss=1.0818, F1=0.7949\n","Early stopping at epoch 31\n","\n","Best epoch: 21, Best val_f1: 0.8069\n","\n","--- Phase 2: Retrain on full training set (target epoch=21) ---\n","Epoch   0: Train Loss=0.9126, Acc=0.6091, F1=0.6298\n","Epoch  10: Train Loss=0.2727, Acc=0.9075, F1=0.9174\n","Epoch  20: Train Loss=0.1705, Acc=0.9422, F1=0.9483\n","Epoch  21: Train Loss=0.1685, Acc=0.9436, F1=0.9485\n","\n","Test set: Loss=0.3006, Acc=0.9124, F1=0.9190\n","✓ Model saved: /content/models/tst_fold2.pt\n","\n","============================================================\n","Fold 3\n","============================================================\n","Data: Train=34255, Test=2367\n","Shapes: C=6, L=150, Patch=25\n","\n","--- Phase 1: Validation-based early stopping ---\n","Train: 27029, Val: 7226\n","Epoch   0: Train Loss=0.9391, F1=0.6150 | Val Loss=0.9461, F1=0.6235\n","Epoch   1: Train Loss=0.6895, F1=0.7477 | Val Loss=0.7800, F1=0.6900\n","Epoch   2: Train Loss=0.5785, F1=0.8101 | Val Loss=0.7369, F1=0.7723\n","Epoch   3: Train Loss=0.4792, F1=0.8490 | Val Loss=0.6222, F1=0.7940\n","Epoch   5: Train Loss=0.3757, F1=0.8850 | Val Loss=0.7450, F1=0.7620\n","Epoch  10: Train Loss=0.2251, F1=0.9339 | Val Loss=0.7564, F1=0.7770\n","Early stopping at epoch 13\n","\n","Best epoch: 3, Best val_f1: 0.7940\n","\n","--- Phase 2: Retrain on full training set (target epoch=3) ---\n","Epoch   0: Train Loss=0.9066, Acc=0.6154, F1=0.6311\n","Epoch   3: Train Loss=0.4774, Acc=0.8305, F1=0.8493\n","\n","Test set: Loss=0.5227, Acc=0.8217, F1=0.8064\n","✓ Model saved: /content/models/tst_fold3.pt\n","\n","============================================================\n","Fold 4\n","============================================================\n","Data: Train=34033, Test=2589\n","Shapes: C=6, L=150, Patch=25\n","\n","--- Phase 1: Validation-based early stopping ---\n","Train: 26974, Val: 7059\n","Epoch   0: Train Loss=0.9695, F1=0.5954 | Val Loss=0.8779, F1=0.6350\n","Epoch   1: Train Loss=0.6853, F1=0.7455 | Val Loss=0.8140, F1=0.6598\n","Epoch   2: Train Loss=0.6037, F1=0.7880 | Val Loss=0.7134, F1=0.7005\n","Epoch   3: Train Loss=0.5066, F1=0.8372 | Val Loss=0.6464, F1=0.7815\n","Epoch   4: Train Loss=0.4423, F1=0.8584 | Val Loss=0.5471, F1=0.8117\n","Epoch   5: Train Loss=0.3987, F1=0.8731 | Val Loss=0.6535, F1=0.7776\n","Epoch   6: Train Loss=0.3653, F1=0.8852 | Val Loss=0.5304, F1=0.8220\n","Epoch  10: Train Loss=0.2867, F1=0.9102 | Val Loss=0.5939, F1=0.8122\n","Epoch  15: Train Loss=0.1695, F1=0.9511 | Val Loss=0.7624, F1=0.8159\n","Early stopping at epoch 16\n","\n","Best epoch: 6, Best val_f1: 0.8220\n","\n","--- Phase 2: Retrain on full training set (target epoch=6) ---\n","Epoch   0: Train Loss=0.9051, Acc=0.6144, F1=0.6350\n","Epoch   6: Train Loss=0.3567, Acc=0.8751, F1=0.8894\n","\n","Test set: Loss=0.5438, Acc=0.8235, F1=0.8362\n","✓ Model saved: /content/models/tst_fold4.pt\n","\n","============================================================\n","Fold 5\n","============================================================\n","Data: Train=34787, Test=1835\n","Shapes: C=6, L=150, Patch=25\n","\n","--- Phase 1: Validation-based early stopping ---\n","Train: 27593, Val: 7194\n","Epoch   0: Train Loss=0.9667, F1=0.5911 | Val Loss=0.7764, F1=0.6872\n","Epoch   1: Train Loss=0.6916, F1=0.7458 | Val Loss=0.7178, F1=0.6946\n","Epoch   2: Train Loss=0.6173, F1=0.7872 | Val Loss=0.7052, F1=0.7211\n","Epoch   3: Train Loss=0.5207, F1=0.8363 | Val Loss=0.6385, F1=0.7882\n","Epoch   5: Train Loss=0.4073, F1=0.8747 | Val Loss=0.6360, F1=0.7923\n","Epoch   7: Train Loss=0.3410, F1=0.8960 | Val Loss=0.6527, F1=0.7988\n","Epoch   8: Train Loss=0.3167, F1=0.9032 | Val Loss=0.6088, F1=0.8217\n","Epoch  10: Train Loss=0.2661, F1=0.9201 | Val Loss=0.6420, F1=0.8098\n","Epoch  15: Train Loss=0.1636, F1=0.9532 | Val Loss=0.6528, F1=0.8351\n","Epoch  20: Train Loss=0.1145, F1=0.9682 | Val Loss=0.7908, F1=0.8274\n","Epoch  23: Train Loss=0.0774, F1=0.9790 | Val Loss=0.7852, F1=0.8424\n","Epoch  25: Train Loss=0.0700, F1=0.9807 | Val Loss=0.8615, F1=0.8413\n","Epoch  26: Train Loss=0.0659, F1=0.9819 | Val Loss=0.8559, F1=0.8446\n","Epoch  30: Train Loss=0.0571, F1=0.9832 | Val Loss=0.9623, F1=0.8343\n","Epoch  35: Train Loss=0.0368, F1=0.9906 | Val Loss=1.0430, F1=0.8411\n","Early stopping at epoch 36\n","\n","Best epoch: 26, Best val_f1: 0.8446\n","\n","--- Phase 2: Retrain on full training set (target epoch=26) ---\n","Epoch   0: Train Loss=0.8901, Acc=0.6264, F1=0.6361\n","Epoch  10: Train Loss=0.2742, Acc=0.9066, F1=0.9162\n","Epoch  20: Train Loss=0.1724, Acc=0.9428, F1=0.9488\n","Epoch  26: Train Loss=0.1388, Acc=0.9530, F1=0.9577\n","\n","Test set: Loss=1.3975, Acc=0.7057, F1=0.5161\n","✓ Model saved: /content/models/tst_fold5.pt\n","\n","============================================================\n","Fold 6\n","============================================================\n","Data: Train=34008, Test=2614\n","Shapes: C=6, L=150, Patch=25\n","\n","--- Phase 1: Validation-based early stopping ---\n","Train: 26949, Val: 7059\n","Epoch   0: Train Loss=0.9287, F1=0.6145 | Val Loss=0.8388, F1=0.6481\n","Epoch   1: Train Loss=0.6844, F1=0.7433 | Val Loss=0.8571, F1=0.6726\n","Epoch   2: Train Loss=0.6017, F1=0.7889 | Val Loss=0.6568, F1=0.7751\n","Epoch   3: Train Loss=0.5064, F1=0.8375 | Val Loss=0.6182, F1=0.7992\n","Epoch   4: Train Loss=0.4562, F1=0.8546 | Val Loss=0.5915, F1=0.8147\n","Epoch   5: Train Loss=0.4135, F1=0.8713 | Val Loss=0.6616, F1=0.7858\n","Epoch  10: Train Loss=0.2786, F1=0.9151 | Val Loss=0.6999, F1=0.7813\n","Early stopping at epoch 14\n","\n","Best epoch: 4, Best val_f1: 0.8147\n","\n","--- Phase 2: Retrain on full training set (target epoch=4) ---\n","Epoch   0: Train Loss=0.8894, Acc=0.6238, F1=0.6478\n","Epoch   4: Train Loss=0.4105, Acc=0.8554, F1=0.8722\n","\n","Test set: Loss=0.4840, Acc=0.8439, F1=0.8368\n","✓ Model saved: /content/models/tst_fold6.pt\n","\n","============================================================\n","Fold 7\n","============================================================\n","Data: Train=34330, Test=2292\n","Shapes: C=6, L=150, Patch=25\n","\n","--- Phase 1: Validation-based early stopping ---\n","Train: 27196, Val: 7134\n","Epoch   0: Train Loss=0.9675, F1=0.5917 | Val Loss=0.9035, F1=0.6115\n","Epoch   1: Train Loss=0.7168, F1=0.7357 | Val Loss=0.7853, F1=0.6838\n","Epoch   2: Train Loss=0.5972, F1=0.8067 | Val Loss=0.7207, F1=0.7499\n","Epoch   3: Train Loss=0.5150, F1=0.8354 | Val Loss=0.5806, F1=0.8125\n","Epoch   5: Train Loss=0.3953, F1=0.8750 | Val Loss=0.5818, F1=0.8177\n","Epoch  10: Train Loss=0.2768, F1=0.9159 | Val Loss=0.6616, F1=0.8086\n","Epoch  15: Train Loss=0.1683, F1=0.9513 | Val Loss=0.7888, F1=0.7966\n","Early stopping at epoch 15\n","\n","Best epoch: 5, Best val_f1: 0.8177\n","\n","--- Phase 2: Retrain on full training set (target epoch=5) ---\n","Epoch   0: Train Loss=0.9202, Acc=0.6049, F1=0.6245\n","Epoch   5: Train Loss=0.3691, Acc=0.8710, F1=0.8852\n","\n","Test set: Loss=0.3837, Acc=0.8621, F1=0.8733\n","✓ Model saved: /content/models/tst_fold7.pt\n","\n","============================================================\n","Fold 8\n","============================================================\n","Data: Train=33917, Test=2705\n","Shapes: C=6, L=150, Patch=25\n","\n","--- Phase 1: Validation-based early stopping ---\n","Train: 26858, Val: 7059\n","Epoch   0: Train Loss=0.9206, F1=0.6144 | Val Loss=0.8514, F1=0.6353\n","Epoch   1: Train Loss=0.6792, F1=0.7549 | Val Loss=0.8011, F1=0.6960\n","Epoch   2: Train Loss=0.5648, F1=0.8145 | Val Loss=0.7248, F1=0.7375\n","Epoch   3: Train Loss=0.4974, F1=0.8402 | Val Loss=0.6041, F1=0.7920\n","Epoch   4: Train Loss=0.4554, F1=0.8548 | Val Loss=0.5829, F1=0.8027\n","Epoch   5: Train Loss=0.4172, F1=0.8682 | Val Loss=0.7079, F1=0.7530\n","Epoch   7: Train Loss=0.3675, F1=0.8861 | Val Loss=0.5690, F1=0.8147\n","Epoch  10: Train Loss=0.2898, F1=0.9109 | Val Loss=0.5701, F1=0.8132\n","Epoch  15: Train Loss=0.1648, F1=0.9514 | Val Loss=0.8274, F1=0.7734\n","Early stopping at epoch 17\n","\n","Best epoch: 7, Best val_f1: 0.8147\n","\n","--- Phase 2: Retrain on full training set (target epoch=7) ---\n","Epoch   0: Train Loss=0.9025, Acc=0.6131, F1=0.6359\n","Epoch   7: Train Loss=0.3453, Acc=0.8800, F1=0.8932\n","\n","Test set: Loss=2.1039, Acc=0.5863, F1=0.5684\n","✓ Model saved: /content/models/tst_fold8.pt\n","\n","============================================================\n","Fold 9\n","============================================================\n","Data: Train=34375, Test=2247\n","Shapes: C=6, L=150, Patch=25\n","\n","--- Phase 1: Validation-based early stopping ---\n","Train: 27241, Val: 7134\n","Epoch   0: Train Loss=0.9178, F1=0.6025 | Val Loss=0.9336, F1=0.6180\n","Epoch   1: Train Loss=0.6667, F1=0.7460 | Val Loss=0.7939, F1=0.7089\n","Epoch   2: Train Loss=0.5805, F1=0.7921 | Val Loss=0.7958, F1=0.7282\n","Epoch   3: Train Loss=0.4699, F1=0.8509 | Val Loss=0.6611, F1=0.7826\n","Epoch   5: Train Loss=0.3537, F1=0.8911 | Val Loss=0.5754, F1=0.8136\n","Epoch  10: Train Loss=0.2483, F1=0.9264 | Val Loss=0.6563, F1=0.8068\n","Epoch  15: Train Loss=0.1492, F1=0.9573 | Val Loss=0.8367, F1=0.7862\n","Early stopping at epoch 15\n","\n","Best epoch: 5, Best val_f1: 0.8136\n","\n","--- Phase 2: Retrain on full training set (target epoch=5) ---\n","Epoch   0: Train Loss=0.8656, Acc=0.6338, F1=0.6497\n","Epoch   5: Train Loss=0.3482, Acc=0.8795, F1=0.8912\n","\n","Test set: Loss=1.2578, Acc=0.6293, F1=0.5149\n","✓ Model saved: /content/models/tst_fold9.pt\n","\n","============================================================\n","Fold 10\n","============================================================\n","Data: Train=33690, Test=2932\n","Shapes: C=6, L=150, Patch=25\n","\n","--- Phase 1: Validation-based early stopping ---\n","Train: 28504, Val: 5186\n","Epoch   0: Train Loss=0.9012, F1=0.6276 | Val Loss=1.0029, F1=0.6285\n","Epoch   1: Train Loss=0.6543, F1=0.7656 | Val Loss=1.0347, F1=0.6445\n","Epoch   2: Train Loss=0.5364, F1=0.8277 | Val Loss=1.0359, F1=0.7005\n","Epoch   3: Train Loss=0.4728, F1=0.8550 | Val Loss=1.0120, F1=0.7039\n","Epoch   4: Train Loss=0.4180, F1=0.8724 | Val Loss=1.0629, F1=0.7279\n","Epoch   5: Train Loss=0.3781, F1=0.8826 | Val Loss=1.1574, F1=0.7006\n","Epoch   9: Train Loss=0.2840, F1=0.9154 | Val Loss=1.0594, F1=0.7570\n","Epoch  10: Train Loss=0.2688, F1=0.9162 | Val Loss=1.1577, F1=0.6804\n","Epoch  15: Train Loss=0.2143, F1=0.9361 | Val Loss=1.3278, F1=0.6804\n","Early stopping at epoch 19\n","\n","Best epoch: 9, Best val_f1: 0.7570\n","\n","--- Phase 2: Retrain on full training set (target epoch=9) ---\n","Epoch   0: Train Loss=0.8902, Acc=0.6201, F1=0.6322\n","Epoch   9: Train Loss=0.2858, Acc=0.9027, F1=0.9137\n","\n","Test set: Loss=0.6995, Acc=0.7780, F1=0.8025\n","✓ Model saved: /content/models/tst_fold10.pt\n","\n","============================================================\n","Fold 11\n","============================================================\n","Data: Train=34091, Test=2531\n","Shapes: C=6, L=150, Patch=25\n","\n","--- Phase 1: Validation-based early stopping ---\n","Train: 26972, Val: 7119\n","Epoch   0: Train Loss=0.9707, F1=0.6006 | Val Loss=0.8108, F1=0.6606\n","Epoch   1: Train Loss=0.7070, F1=0.7572 | Val Loss=0.7725, F1=0.7523\n","Epoch   2: Train Loss=0.5708, F1=0.8233 | Val Loss=0.6559, F1=0.7949\n","Epoch   5: Train Loss=0.4030, F1=0.8755 | Val Loss=0.6733, F1=0.7822\n","Epoch   6: Train Loss=0.3713, F1=0.8865 | Val Loss=0.6125, F1=0.8156\n","Epoch  10: Train Loss=0.2847, F1=0.9143 | Val Loss=0.6429, F1=0.8144\n","Epoch  15: Train Loss=0.1726, F1=0.9511 | Val Loss=0.8066, F1=0.7881\n","Early stopping at epoch 16\n","\n","Best epoch: 6, Best val_f1: 0.8156\n","\n","--- Phase 2: Retrain on full training set (target epoch=6) ---\n","Epoch   0: Train Loss=0.9058, Acc=0.6159, F1=0.6310\n","Epoch   6: Train Loss=0.3559, Acc=0.8742, F1=0.8886\n","\n","Test set: Loss=0.4632, Acc=0.8412, F1=0.8535\n","✓ Model saved: /content/models/tst_fold11.pt\n","\n","============================================================\n","Fold 12\n","============================================================\n","Data: Train=34620, Test=2002\n","Shapes: C=6, L=150, Patch=25\n","\n","--- Phase 1: Validation-based early stopping ---\n","Train: 27486, Val: 7134\n","Epoch   0: Train Loss=0.9258, F1=0.6152 | Val Loss=0.9485, F1=0.6255\n","Epoch   2: Train Loss=0.6081, F1=0.7774 | Val Loss=0.9541, F1=0.6487\n","Epoch   3: Train Loss=0.5170, F1=0.8315 | Val Loss=0.6460, F1=0.7695\n","Epoch   5: Train Loss=0.3931, F1=0.8741 | Val Loss=0.7128, F1=0.7595\n","Epoch   6: Train Loss=0.3538, F1=0.8924 | Val Loss=0.7281, F1=0.7735\n","Epoch   7: Train Loss=0.3280, F1=0.8993 | Val Loss=0.6797, F1=0.7755\n","Epoch   9: Train Loss=0.2792, F1=0.9137 | Val Loss=0.6770, F1=0.7848\n","Epoch  10: Train Loss=0.2589, F1=0.9224 | Val Loss=0.8576, F1=0.7485\n","Epoch  15: Train Loss=0.2019, F1=0.9395 | Val Loss=0.9011, F1=0.7593\n","Early stopping at epoch 19\n","\n","Best epoch: 9, Best val_f1: 0.7848\n","\n","--- Phase 2: Retrain on full training set (target epoch=9) ---\n","Epoch   0: Train Loss=0.8894, Acc=0.6216, F1=0.6435\n","Epoch   9: Train Loss=0.2737, Acc=0.9067, F1=0.9165\n","\n","Test set: Loss=0.5919, Acc=0.8102, F1=0.6331\n","✓ Model saved: /content/models/tst_fold12.pt\n","\n","============================================================\n","Fold 13\n","============================================================\n","Data: Train=33728, Test=2894\n","Shapes: C=6, L=150, Patch=25\n","\n","--- Phase 1: Validation-based early stopping ---\n","Train: 28504, Val: 5224\n","Epoch   0: Train Loss=0.8881, F1=0.6311 | Val Loss=0.9493, F1=0.6059\n","Epoch   1: Train Loss=0.6482, F1=0.7666 | Val Loss=0.8127, F1=0.7055\n","Epoch   2: Train Loss=0.5218, F1=0.8343 | Val Loss=0.6321, F1=0.7849\n","Epoch   3: Train Loss=0.4446, F1=0.8629 | Val Loss=0.6236, F1=0.8055\n","Epoch   5: Train Loss=0.3557, F1=0.8904 | Val Loss=0.5682, F1=0.8127\n","Epoch  10: Train Loss=0.2732, F1=0.9184 | Val Loss=0.7550, F1=0.7696\n","Epoch  15: Train Loss=0.1753, F1=0.9507 | Val Loss=0.7608, F1=0.7976\n","Early stopping at epoch 15\n","\n","Best epoch: 5, Best val_f1: 0.8127\n","\n","--- Phase 2: Retrain on full training set (target epoch=5) ---\n","Epoch   0: Train Loss=0.8875, Acc=0.6208, F1=0.6364\n","Epoch   5: Train Loss=0.3591, Acc=0.8723, F1=0.8870\n","\n","Test set: Loss=1.2373, Acc=0.6759, F1=0.6878\n","✓ Model saved: /content/models/tst_fold13.pt\n","\n","============================================================\n","Fold 14\n","============================================================\n","Data: Train=33946, Test=2676\n","Shapes: C=6, L=150, Patch=25\n","\n","--- Phase 1: Validation-based early stopping ---\n","Train: 26887, Val: 7059\n","Epoch   0: Train Loss=0.9298, F1=0.6087 | Val Loss=0.8848, F1=0.6541\n","Epoch   1: Train Loss=0.6686, F1=0.7553 | Val Loss=0.8113, F1=0.6903\n","Epoch   2: Train Loss=0.5607, F1=0.8169 | Val Loss=0.6553, F1=0.7846\n","Epoch   4: Train Loss=0.4445, F1=0.8572 | Val Loss=0.6080, F1=0.8076\n","Epoch   5: Train Loss=0.4103, F1=0.8683 | Val Loss=0.6257, F1=0.7903\n","Epoch  10: Train Loss=0.2683, F1=0.9191 | Val Loss=0.7288, F1=0.7466\n","Early stopping at epoch 14\n","\n","Best epoch: 4, Best val_f1: 0.8076\n","\n","--- Phase 2: Retrain on full training set (target epoch=4) ---\n","Epoch   0: Train Loss=0.8969, Acc=0.6203, F1=0.6350\n","Epoch   4: Train Loss=0.4158, Acc=0.8523, F1=0.8680\n","\n","Test set: Loss=0.5646, Acc=0.8195, F1=0.8408\n","✓ Model saved: /content/models/tst_fold14.pt\n","\n","============================================================\n","✓ Completed training for 15 folds\n","✓ Models: models/tst_fold{k}.pt\n","✓ Curves: figures/step14_tst_fold{k}.png\n","✓ Reports: logs/step14_tst_fold{k}.json\n","✓ Summary: logs/step14_tst_summary.json\n","\n","Average test-set performance:\n","  Accuracy: 0.7847\n","  Macro F1: 0.7452\n","============================================================\n","\n","[master b92365f] train: TST with two-phase training and early stopping\n"," 76 files changed, 6393 insertions(+)\n"," create mode 100644 figures/step14_tst_fold0.png\n"," create mode 100644 figures/step14_tst_fold1.png\n"," create mode 100644 figures/step14_tst_fold10.png\n"," create mode 100644 figures/step14_tst_fold11.png\n"," create mode 100644 figures/step14_tst_fold12.png\n"," create mode 100644 figures/step14_tst_fold13.png\n"," create mode 100644 figures/step14_tst_fold14.png\n"," create mode 100644 figures/step14_tst_fold2.png\n"," create mode 100644 figures/step14_tst_fold3.png\n"," create mode 100644 figures/step14_tst_fold4.png\n"," create mode 100644 figures/step14_tst_fold5.png\n"," create mode 100644 figures/step14_tst_fold6.png\n"," create mode 100644 figures/step14_tst_fold7.png\n"," create mode 100644 figures/step14_tst_fold8.png\n"," create mode 100644 figures/step14_tst_fold9.png\n"," create mode 100644 logs/step14_tst_fold0.json\n"," create mode 100644 logs/step14_tst_fold1.json\n"," create mode 100644 logs/step14_tst_fold10.json\n"," create mode 100644 logs/step14_tst_fold11.json\n"," create mode 100644 logs/step14_tst_fold12.json\n"," create mode 100644 logs/step14_tst_fold13.json\n"," create mode 100644 logs/step14_tst_fold14.json\n"," create mode 100644 logs/step14_tst_fold2.json\n"," create mode 100644 logs/step14_tst_fold3.json\n"," create mode 100644 logs/step14_tst_fold4.json\n"," create mode 100644 logs/step14_tst_fold5.json\n"," create mode 100644 logs/step14_tst_fold6.json\n"," create mode 100644 logs/step14_tst_fold7.json\n"," create mode 100644 logs/step14_tst_fold8.json\n"," create mode 100644 logs/step14_tst_fold9.json\n"," create mode 100644 logs/step14_tst_summary.json\n"," create mode 100644 models/ridge_fold0.pkl\n"," create mode 100644 models/ridge_fold1.pkl\n"," create mode 100644 models/ridge_fold10.pkl\n"," create mode 100644 models/ridge_fold11.pkl\n"," create mode 100644 models/ridge_fold12.pkl\n"," create mode 100644 models/ridge_fold13.pkl\n"," create mode 100644 models/ridge_fold14.pkl\n"," create mode 100644 models/ridge_fold2.pkl\n"," create mode 100644 models/ridge_fold3.pkl\n"," create mode 100644 models/ridge_fold4.pkl\n"," create mode 100644 models/ridge_fold5.pkl\n"," create mode 100644 models/ridge_fold6.pkl\n"," create mode 100644 models/ridge_fold7.pkl\n"," create mode 100644 models/ridge_fold8.pkl\n"," create mode 100644 models/ridge_fold9.pkl\n"," create mode 100644 models/transformer_minirocket_fold0.pkl\n"," create mode 100644 models/transformer_minirocket_fold1.pkl\n"," create mode 100644 models/transformer_minirocket_fold10.pkl\n"," create mode 100644 models/transformer_minirocket_fold11.pkl\n"," create mode 100644 models/transformer_minirocket_fold12.pkl\n"," create mode 100644 models/transformer_minirocket_fold13.pkl\n"," create mode 100644 models/transformer_minirocket_fold14.pkl\n"," create mode 100644 models/transformer_minirocket_fold2.pkl\n"," create mode 100644 models/transformer_minirocket_fold3.pkl\n"," create mode 100644 models/transformer_minirocket_fold4.pkl\n"," create mode 100644 models/transformer_minirocket_fold5.pkl\n"," create mode 100644 models/transformer_minirocket_fold6.pkl\n"," create mode 100644 models/transformer_minirocket_fold7.pkl\n"," create mode 100644 models/transformer_minirocket_fold8.pkl\n"," create mode 100644 models/transformer_minirocket_fold9.pkl\n"," create mode 100644 models/tst_fold0.pt\n"," create mode 100644 models/tst_fold1.pt\n"," create mode 100644 models/tst_fold10.pt\n"," create mode 100644 models/tst_fold11.pt\n"," create mode 100644 models/tst_fold12.pt\n"," create mode 100644 models/tst_fold13.pt\n"," create mode 100644 models/tst_fold14.pt\n"," create mode 100644 models/tst_fold2.pt\n"," create mode 100644 models/tst_fold3.pt\n"," create mode 100644 models/tst_fold4.pt\n"," create mode 100644 models/tst_fold5.pt\n"," create mode 100644 models/tst_fold6.pt\n"," create mode 100644 models/tst_fold7.pt\n"," create mode 100644 models/tst_fold8.pt\n"," create mode 100644 models/tst_fold9.pt\n","Step 14 completed\n","============================================================\n"]}]},{"cell_type":"code","source":["# ================ Step 15: Inference & Prediction (Revised) ================\n","import os\n","os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n","os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n","os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from pathlib import Path\n","import json\n","import pickle\n","import time\n","from datetime import datetime\n","import subprocess\n","import random\n","from threadpoolctl import threadpool_limits\n","\n","print(\"\\n\\nStep 15: Inference & Prediction (Revised)\")\n","print(\"=\" * 60)\n","\n","# Fix random seeds\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)\n","torch.cuda.manual_seed_all(42)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","# Create directories\n","Path('preds').mkdir(parents=True, exist_ok=True)\n","Path('logs').mkdir(parents=True, exist_ok=True)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def get_active_folds(path=\"logs/active_folds.json\"):\n","    p = Path(path)\n","    if p.exists():\n","        return json.loads(p.read_text())[\"folds\"]\n","    return []\n","\n","with open('configs/splits.json', 'r') as f:\n","    splits_cfg = json.load(f)\n","\n","with open('configs/classes.json', 'r') as f:\n","    classes_cfg = json.load(f)\n","\n","active_folds = get_active_folds()\n","N_REPEATS = 50\n","NUM_CLASSES = classes_cfg['num_classes']\n","\n","print(f\"Inference settings: batch=1 (online scenario), repetitions={N_REPEATS}\")\n","print(f\"Latency statistics: processing the entire test set sample-by-sample counts as one run; repeat N times and report p50/p90\")\n","print(f\"Device: {device}, single-threaded: BLAS=1, fixed seed=42\\n\")\n","\n","git_hash = subprocess.getoutput(\"git rev-parse HEAD\")[:8]\n","\n","# TST model definition\n","class PatchEmbedding(nn.Module):\n","    def __init__(self, n_channels, seq_len, patch_len, d_model):\n","        super().__init__()\n","        self.patch_len = patch_len\n","        self.n_patches = seq_len // patch_len\n","        self.proj = nn.Linear(n_channels * patch_len, d_model)\n","\n","    def forward(self, x):\n","        B, C, L = x.shape\n","        x = x.unfold(2, self.patch_len, self.patch_len)\n","        x = x.permute(0, 2, 1, 3).contiguous()\n","        x = x.view(B, self.n_patches, -1)\n","        return self.proj(x)\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1)]\n","\n","class TST(nn.Module):\n","    def __init__(self, n_channels, seq_len, patch_len, num_classes, d_model, n_heads, depth, dropout):\n","        super().__init__()\n","        self.patch_embedding = PatchEmbedding(n_channels, seq_len, patch_len, d_model)\n","        self.pos_encoding = PositionalEncoding(d_model)\n","\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model,\n","            nhead=n_heads,\n","            dim_feedforward=d_model * 4,\n","            dropout=dropout,\n","            batch_first=True\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(d_model, num_classes)\n","\n","    def forward(self, x):\n","        x = self.patch_embedding(x)\n","        x = self.pos_encoding(x)\n","        x = self.transformer(x)\n","        x = x.mean(dim=1)\n","        x = self.dropout(x)\n","        return self.fc(x)\n","\n","all_summaries = []\n","\n","for fold in splits_cfg['folds']:\n","    k = fold['fold']\n","\n","    if k not in active_folds:\n","        print(f\"⏭️  Skipping Fold {k}\")\n","        continue\n","\n","    test_subj = fold['test_subject']\n","\n","    print(f\"\\n{'='*60}\")\n","    print(f\"Fold {k}: test subject={test_subj}\")\n","    print(f\"{'='*60}\")\n","\n","    # ============ MiniROCKET + Ridge Inference ============\n","    print(f\"\\n--- MiniROCKET + Ridge ---\")\n","\n","    X_test_raw = np.load(f'features/X_minirocket_test_fold{k}.npy', mmap_mode='r')\n","    meta_test = np.load(f'features/meta_minirocket_test_fold{k}.npz', allow_pickle=True)\n","    y_test = meta_test['y']\n","    subjects_test = meta_test['subjects']\n","    window_ids_test = meta_test['window_ids']\n","\n","    with open(f'models/ridge_fold{k}.pkl', 'rb') as f:\n","        model_data = pickle.load(f)\n","\n","    ridge = model_data['ridge']\n","    vt = model_data['variance_filter']\n","\n","    X_test = vt.transform(X_test_raw)\n","    n_test = len(X_test)\n","\n","    print(f\"Test set: {n_test} samples, {X_test.shape[1]} features\")\n","\n","    # Assert dimensional consistency\n","    assert X_test.shape[1] == ridge.coef_.shape[1], f\"Feature dimension mismatch: {X_test.shape[1]} vs {ridge.coef_.shape[1]}\"\n","\n","    # Latency statistics: treat processing the entire test set as one run\n","    latencies_minirocket = []\n","    with threadpool_limits(limits=1, user_api='blas'):\n","        for _ in range(N_REPEATS):\n","            start = time.perf_counter()\n","            for i in range(n_test):\n","                _ = ridge.predict(X_test[i:i+1])\n","            total_time = (time.perf_counter() - start) * 1000\n","            latencies_minirocket.append(total_time)\n","\n","    # Final prediction\n","    with threadpool_limits(limits=1, user_api='blas'):\n","        y_pred_minirocket = ridge.predict(X_test)\n","        scores_minirocket = ridge.decision_function(X_test)\n","\n","    # Handle the binary-class case\n","    if scores_minirocket.ndim == 1:\n","        scores_minirocket = np.column_stack([-scores_minirocket, scores_minirocket])\n","\n","    # Save predictions and scores\n","    np.save(f'preds/preds_fold{k}_minirocket.npy', y_pred_minirocket)\n","    np.save(f'preds/scores_fold{k}_minirocket.npy', scores_minirocket.astype(np.float32))\n","    np.savez(f'preds/meta_fold{k}_minirocket.npz',\n","             y_true=y_test,\n","             subjects=subjects_test,\n","             window_ids=window_ids_test,\n","             indices=np.arange(n_test))\n","\n","    p50_mr = np.percentile(latencies_minirocket, 50)\n","    p90_mr = np.percentile(latencies_minirocket, 90)\n","    per_sample_p50_mr = p50_mr / n_test\n","    per_sample_p90_mr = p90_mr / n_test\n","\n","    print(f\"✓ Prediction complete: {n_test} samples\")\n","    print(f\"  Total latency: p50={p50_mr:.1f}ms, p90={p90_mr:.1f}ms\")\n","    print(f\"  Per-sample: p50={per_sample_p50_mr:.3f}ms, p90={per_sample_p90_mr:.3f}ms\")\n","    print(f\"  Saved: preds/{{preds,scores,meta}}_fold{k}_minirocket.*\")\n","\n","    # ============ TST Inference ============\n","    print(f\"\\n--- TST ---\")\n","\n","    tensors = torch.load(f'interim/tensors_fold{k}.pt', weights_only=False)\n","    X_test_tst = tensors['X_test']\n","    y_test_tst = tensors['y_test']\n","    subjects_test_tst = tensors['subjects_test']\n","    n_test_tst = len(X_test_tst)\n","\n","    checkpoint = torch.load(f'models/tst_fold{k}.pt', weights_only=False, map_location=device)\n","    model_cfg = checkpoint['model_config']\n","\n","    model = TST(**model_cfg).to(device)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.eval()\n","\n","    print(f\"Test set: {n_test_tst} samples\")\n","\n","    # Assert dimensional consistency\n","    assert X_test_tst.shape[1] == model_cfg['n_channels'], f\"Channel count mismatch: {X_test_tst.shape[1]} vs {model_cfg['n_channels']}\"\n","    assert X_test_tst.shape[2] == model_cfg['seq_len'], f\"Sequence length mismatch: {X_test_tst.shape[2]} vs {model_cfg['seq_len']}\"\n","\n","    # Latency statistics: treat processing the entire test set as one run\n","    latencies_tst = []\n","    with torch.inference_mode():\n","        for _ in range(N_REPEATS):\n","            if device.type == 'cuda':\n","                torch.cuda.synchronize()\n","            start = time.perf_counter()\n","\n","            for i in range(n_test_tst):\n","                sample = X_test_tst[i:i+1].to(device)\n","                _ = model(sample)\n","\n","            if device.type == 'cuda':\n","                torch.cuda.synchronize()\n","            total_time = (time.perf_counter() - start) * 1000\n","            latencies_tst.append(total_time)\n","\n","    # Final prediction\n","    all_preds_tst = []\n","    all_logits_tst = []\n","    with torch.inference_mode():\n","        for i in range(n_test_tst):\n","            sample = X_test_tst[i:i+1].to(device)\n","            logits = model(sample)\n","            pred = logits.argmax(dim=1).cpu().numpy()[0]\n","            all_preds_tst.append(pred)\n","            all_logits_tst.append(logits.cpu().numpy()[0])\n","\n","    y_pred_tst = np.array(all_preds_tst)\n","    logits_tst = np.array(all_logits_tst, dtype=np.float32)\n","    probs_tst = torch.softmax(torch.from_numpy(logits_tst), dim=1).numpy()\n","\n","    # Save predictions and scores\n","    np.save(f'preds/preds_fold{k}_tst.npy', y_pred_tst)\n","    np.save(f'preds/logits_fold{k}_tst.npy', logits_tst)\n","    np.save(f'preds/probs_fold{k}_tst.npy', probs_tst)\n","    np.savez(f'preds/meta_fold{k}_tst.npz',\n","             y_true=y_test_tst.numpy(),\n","             subjects=subjects_test_tst,\n","             indices=np.arange(n_test_tst))\n","\n","    p50_tst = np.percentile(latencies_tst, 50)\n","    p90_tst = np.percentile(latencies_tst, 90)\n","    per_sample_p50_tst = p50_tst / n_test_tst\n","    per_sample_p90_tst = p90_tst / n_test_tst\n","\n","    print(f\"✓ Prediction complete: {n_test_tst} samples\")\n","    print(f\"  Total latency: p50={p50_tst:.1f}ms, p90={p90_tst:.1f}ms\")\n","    print(f\"  Per-sample: p50={per_sample_p50_tst:.3f}ms, p90={per_sample_p90_tst:.3f}ms\")\n","    print(f\"  Saved: preds/{{preds,logits,probs,meta}}_fold{k}_tst.*\")\n","\n","    # Summary\n","    summary = {\n","        'fold': k,\n","        'test_subject': test_subj,\n","        'timestamp': datetime.now().isoformat(),\n","        'git_hash': git_hash,\n","        'n_test_samples': int(n_test),\n","        'minirocket': {\n","            'n_predictions': int(len(y_pred_minirocket)),\n","            'total_latency_p50_ms': float(p50_mr),\n","            'total_latency_p90_ms': float(p90_mr),\n","            'per_sample_p50_ms': float(per_sample_p50_mr),\n","            'per_sample_p90_ms': float(per_sample_p90_mr),\n","            'n_repeats': N_REPEATS,\n","            'batch_size': 1\n","        },\n","        'tst': {\n","            'n_predictions': int(len(y_pred_tst)),\n","            'total_latency_p50_ms': float(p50_tst),\n","            'total_latency_p90_ms': float(p90_tst),\n","            'per_sample_p50_ms': float(per_sample_p50_tst),\n","            'per_sample_p90_ms': float(per_sample_p90_tst),\n","            'n_repeats': N_REPEATS,\n","            'batch_size': 1\n","        }\n","    }\n","\n","    all_summaries.append(summary)\n","\n","# Save aggregate summary\n","with open('logs/step15_inference_summary.json', 'w') as f:\n","    json.dump({\n","        'procedure': 'Each repetition processes the entire test set sequentially with batch=1; p50/p90 are computed over N repetitions',\n","        'n_repeats': N_REPEATS,\n","        'git_hash': git_hash,\n","        'random_seed': 42,\n","        'deterministic': True,\n","        'single_thread': 'BLAS=1',\n","        'outputs': {\n","            'predictions': 'preds/preds_fold{k}_{minirocket,tst}.npy',\n","            'scores': 'preds/scores_fold{k}_minirocket.npy (decision_function)',\n","            'logits': 'preds/logits_fold{k}_tst.npy',\n","            'probs': 'preds/probs_fold{k}_tst.npy (softmax)',\n","            'meta': 'preds/meta_fold{k}_{minirocket,tst}.npz (y_true, subjects, indices)'\n","        },\n","        'folds': all_summaries,\n","        'aggregated': {\n","            'avg_minirocket_total_p50_ms': float(np.mean([s['minirocket']['total_latency_p50_ms'] for s in all_summaries])),\n","            'avg_minirocket_total_p90_ms': float(np.mean([s['minirocket']['total_latency_p90_ms'] for s in all_summaries])),\n","            'avg_minirocket_per_sample_p50_ms': float(np.mean([s['minirocket']['per_sample_p50_ms'] for s in all_summaries])),\n","            'avg_minirocket_per_sample_p90_ms': float(np.mean([s['minirocket']['per_sample_p90_ms'] for s in all_summaries])),\n","            'avg_tst_total_p50_ms': float(np.mean([s['tst']['total_latency_p50_ms'] for s in all_summaries])),\n","            'avg_tst_total_p90_ms': float(np.mean([s['tst']['total_latency_p90_ms'] for s in all_summaries])),\n","            'avg_tst_per_sample_p50_ms': float(np.mean([s['tst']['per_sample_p50_ms'] for s in all_summaries])),\n","            'avg_tst_per_sample_p90_ms': float(np.mean([s['tst']['per_sample_p90_ms'] for s in all_summaries]))\n","        }\n","    }, f, indent=2)\n","\n","print(f\"\\n{'='*60}\")\n","print(f\"✓ Completed inference for {len(active_folds)} folds\")\n","print(f\"✓ Predictions: preds/preds_fold{{k}}_{{minirocket,tst}}.npy\")\n","print(f\"✓ Scores: preds/{{scores,logits,probs}}_fold{{k}}_*.npy\")\n","print(f\"✓ Metadata: preds/meta_fold{{k}}_{{minirocket,tst}}.npz\")\n","print(f\"✓ Summary: logs/step15_inference_summary.json\")\n","print(f\"\\nAverage inference latency (per sample):\")\n","print(f\"  MiniROCKET: p50={np.mean([s['minirocket']['per_sample_p50_ms'] for s in all_summaries]):.3f}ms, p90={np.mean([s['minirocket']['per_sample_p90_ms'] for s in all_summaries]):.3f}ms\")\n","print(f\"  TST: p50={np.mean([s['tst']['per_sample_p50_ms'] for s in all_summaries]):.3f}ms, p90={np.mean([s['tst']['per_sample_p90_ms'] for s in all_summaries]):.3f}ms\")\n","print(f\"{'='*60}\\n\")\n","\n","get_ipython().system('git add preds/ logs/step15_*.json')\n","get_ipython().system('git commit -m \\\"inference: batch=1 with corrected latency statistics and score outputs\\\"')\n","\n","print(f\"Step 15 completed\\n{'='*60}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1FbR1VqvWpQ4","executionInfo":{"status":"ok","timestamp":1762953982098,"user_tz":0,"elapsed":3241479,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"29d9dcb3-f8f3-49ea-efd8-5053091dc822"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Step 15: Inference & Prediction (Revised)\n","============================================================\n","Inference settings: batch=1 (online scenario), repetitions=50\n","Latency statistics: processing the entire test set sample-by-sample counts as one run; repeat N times and report p50/p90\n","Device: cuda, single-threaded: BLAS=1, fixed seed=42\n","\n","\n","============================================================\n","Fold 0: test subject=proband1\n","============================================================\n","\n","--- MiniROCKET + Ridge ---\n","Test set: 1895 samples, 9996 features\n","✓ Prediction complete: 1895 samples\n","  Total latency: p50=287.1ms, p90=297.7ms\n","  Per-sample: p50=0.151ms, p90=0.157ms\n","  Saved: preds/{preds,scores,meta}_fold0_minirocket.*\n","\n","--- TST ---\n","Test set: 1895 samples\n","✓ Prediction complete: 1895 samples\n","  Total latency: p50=3044.5ms, p90=3057.2ms\n","  Per-sample: p50=1.607ms, p90=1.613ms\n","  Saved: preds/{preds,logits,probs,meta}_fold0_tst.*\n","\n","============================================================\n","Fold 1: test subject=proband10\n","============================================================\n","\n","--- MiniROCKET + Ridge ---\n","Test set: 2463 samples, 9996 features\n","✓ Prediction complete: 2463 samples\n","  Total latency: p50=387.1ms, p90=394.6ms\n","  Per-sample: p50=0.157ms, p90=0.160ms\n","  Saved: preds/{preds,scores,meta}_fold1_minirocket.*\n","\n","--- TST ---\n","Test set: 2463 samples\n","✓ Prediction complete: 2463 samples\n","  Total latency: p50=3871.7ms, p90=3891.6ms\n","  Per-sample: p50=1.572ms, p90=1.580ms\n","  Saved: preds/{preds,logits,probs,meta}_fold1_tst.*\n","\n","============================================================\n","Fold 2: test subject=proband11\n","============================================================\n","\n","--- MiniROCKET + Ridge ---\n","Test set: 2580 samples, 9996 features\n","✓ Prediction complete: 2580 samples\n","  Total latency: p50=397.6ms, p90=405.3ms\n","  Per-sample: p50=0.154ms, p90=0.157ms\n","  Saved: preds/{preds,scores,meta}_fold2_minirocket.*\n","\n","--- TST ---\n","Test set: 2580 samples\n","✓ Prediction complete: 2580 samples\n","  Total latency: p50=4056.7ms, p90=4086.5ms\n","  Per-sample: p50=1.572ms, p90=1.584ms\n","  Saved: preds/{preds,logits,probs,meta}_fold2_tst.*\n","\n","============================================================\n","Fold 3: test subject=proband12\n","============================================================\n","\n","--- MiniROCKET + Ridge ---\n","Test set: 2367 samples, 9996 features\n","✓ Prediction complete: 2367 samples\n","  Total latency: p50=370.3ms, p90=379.7ms\n","  Per-sample: p50=0.156ms, p90=0.160ms\n","  Saved: preds/{preds,scores,meta}_fold3_minirocket.*\n","\n","--- TST ---\n","Test set: 2367 samples\n","✓ Prediction complete: 2367 samples\n","  Total latency: p50=3730.6ms, p90=3751.3ms\n","  Per-sample: p50=1.576ms, p90=1.585ms\n","  Saved: preds/{preds,logits,probs,meta}_fold3_tst.*\n","\n","============================================================\n","Fold 4: test subject=proband13\n","============================================================\n","\n","--- MiniROCKET + Ridge ---\n","Test set: 2589 samples, 9996 features\n","✓ Prediction complete: 2589 samples\n","  Total latency: p50=414.3ms, p90=424.5ms\n","  Per-sample: p50=0.160ms, p90=0.164ms\n","  Saved: preds/{preds,scores,meta}_fold4_minirocket.*\n","\n","--- TST ---\n","Test set: 2589 samples\n","✓ Prediction complete: 2589 samples\n","  Total latency: p50=4071.9ms, p90=4091.5ms\n","  Per-sample: p50=1.573ms, p90=1.580ms\n","  Saved: preds/{preds,logits,probs,meta}_fold4_tst.*\n","\n","============================================================\n","Fold 5: test subject=proband14\n","============================================================\n","\n","--- MiniROCKET + Ridge ---\n","Test set: 1835 samples, 9996 features\n","✓ Prediction complete: 1835 samples\n","  Total latency: p50=302.0ms, p90=309.5ms\n","  Per-sample: p50=0.165ms, p90=0.169ms\n","  Saved: preds/{preds,scores,meta}_fold5_minirocket.*\n","\n","--- TST ---\n","Test set: 1835 samples\n","✓ Prediction complete: 1835 samples\n","  Total latency: p50=2877.4ms, p90=2893.1ms\n","  Per-sample: p50=1.568ms, p90=1.577ms\n","  Saved: preds/{preds,logits,probs,meta}_fold5_tst.*\n","\n","============================================================\n","Fold 6: test subject=proband15\n","============================================================\n","\n","--- MiniROCKET + Ridge ---\n","Test set: 2614 samples, 9996 features\n","✓ Prediction complete: 2614 samples\n","  Total latency: p50=398.7ms, p90=404.5ms\n","  Per-sample: p50=0.153ms, p90=0.155ms\n","  Saved: preds/{preds,scores,meta}_fold6_minirocket.*\n","\n","--- TST ---\n","Test set: 2614 samples\n","✓ Prediction complete: 2614 samples\n","  Total latency: p50=4099.5ms, p90=4123.8ms\n","  Per-sample: p50=1.568ms, p90=1.578ms\n","  Saved: preds/{preds,logits,probs,meta}_fold6_tst.*\n","\n","============================================================\n","Fold 7: test subject=proband2\n","============================================================\n","\n","--- MiniROCKET + Ridge ---\n","Test set: 2292 samples, 9996 features\n","✓ Prediction complete: 2292 samples\n","  Total latency: p50=345.7ms, p90=354.8ms\n","  Per-sample: p50=0.151ms, p90=0.155ms\n","  Saved: preds/{preds,scores,meta}_fold7_minirocket.*\n","\n","--- TST ---\n","Test set: 2292 samples\n","✓ Prediction complete: 2292 samples\n","  Total latency: p50=3613.0ms, p90=3634.2ms\n","  Per-sample: p50=1.576ms, p90=1.586ms\n","  Saved: preds/{preds,logits,probs,meta}_fold7_tst.*\n","\n","============================================================\n","Fold 8: test subject=proband3\n","============================================================\n","\n","--- MiniROCKET + Ridge ---\n","Test set: 2705 samples, 9996 features\n","✓ Prediction complete: 2705 samples\n","  Total latency: p50=429.2ms, p90=438.3ms\n","  Per-sample: p50=0.159ms, p90=0.162ms\n","  Saved: preds/{preds,scores,meta}_fold8_minirocket.*\n","\n","--- TST ---\n","Test set: 2705 samples\n","✓ Prediction complete: 2705 samples\n","  Total latency: p50=4242.9ms, p90=4265.7ms\n","  Per-sample: p50=1.569ms, p90=1.577ms\n","  Saved: preds/{preds,logits,probs,meta}_fold8_tst.*\n","\n","============================================================\n","Fold 9: test subject=proband4\n","============================================================\n","\n","--- MiniROCKET + Ridge ---\n","Test set: 2247 samples, 9996 features\n","✓ Prediction complete: 2247 samples\n","  Total latency: p50=350.6ms, p90=356.2ms\n","  Per-sample: p50=0.156ms, p90=0.159ms\n","  Saved: preds/{preds,scores,meta}_fold9_minirocket.*\n","\n","--- TST ---\n","Test set: 2247 samples\n","✓ Prediction complete: 2247 samples\n","  Total latency: p50=3527.5ms, p90=3552.0ms\n","  Per-sample: p50=1.570ms, p90=1.581ms\n","  Saved: preds/{preds,logits,probs,meta}_fold9_tst.*\n","\n","============================================================\n","Fold 10: test subject=proband5\n","============================================================\n","\n","--- MiniROCKET + Ridge ---\n","Test set: 2932 samples, 9996 features\n","✓ Prediction complete: 2932 samples\n","  Total latency: p50=446.5ms, p90=458.1ms\n","  Per-sample: p50=0.152ms, p90=0.156ms\n","  Saved: preds/{preds,scores,meta}_fold10_minirocket.*\n","\n","--- TST ---\n","Test set: 2932 samples\n","✓ Prediction complete: 2932 samples\n","  Total latency: p50=4606.4ms, p90=4640.7ms\n","  Per-sample: p50=1.571ms, p90=1.583ms\n","  Saved: preds/{preds,logits,probs,meta}_fold10_tst.*\n","\n","============================================================\n","Fold 11: test subject=proband6\n","============================================================\n","\n","--- MiniROCKET + Ridge ---\n","Test set: 2531 samples, 9996 features\n","✓ Prediction complete: 2531 samples\n","  Total latency: p50=403.1ms, p90=409.9ms\n","  Per-sample: p50=0.159ms, p90=0.162ms\n","  Saved: preds/{preds,scores,meta}_fold11_minirocket.*\n","\n","--- TST ---\n","Test set: 2531 samples\n","✓ Prediction complete: 2531 samples\n","  Total latency: p50=3970.6ms, p90=3991.5ms\n","  Per-sample: p50=1.569ms, p90=1.577ms\n","  Saved: preds/{preds,logits,probs,meta}_fold11_tst.*\n","\n","============================================================\n","Fold 12: test subject=proband7\n","============================================================\n","\n","--- MiniROCKET + Ridge ---\n","Test set: 2002 samples, 9996 features\n","✓ Prediction complete: 2002 samples\n","  Total latency: p50=297.4ms, p90=305.4ms\n","  Per-sample: p50=0.149ms, p90=0.153ms\n","  Saved: preds/{preds,scores,meta}_fold12_minirocket.*\n","\n","--- TST ---\n","Test set: 2002 samples\n","✓ Prediction complete: 2002 samples\n","  Total latency: p50=3140.8ms, p90=3160.5ms\n","  Per-sample: p50=1.569ms, p90=1.579ms\n","  Saved: preds/{preds,logits,probs,meta}_fold12_tst.*\n","\n","============================================================\n","Fold 13: test subject=proband8\n","============================================================\n","\n","--- MiniROCKET + Ridge ---\n","Test set: 2894 samples, 9995 features\n","✓ Prediction complete: 2894 samples\n","  Total latency: p50=430.4ms, p90=438.2ms\n","  Per-sample: p50=0.149ms, p90=0.151ms\n","  Saved: preds/{preds,scores,meta}_fold13_minirocket.*\n","\n","--- TST ---\n","Test set: 2894 samples\n","✓ Prediction complete: 2894 samples\n","  Total latency: p50=4540.9ms, p90=4569.3ms\n","  Per-sample: p50=1.569ms, p90=1.579ms\n","  Saved: preds/{preds,logits,probs,meta}_fold13_tst.*\n","\n","============================================================\n","Fold 14: test subject=proband9\n","============================================================\n","\n","--- MiniROCKET + Ridge ---\n","Test set: 2676 samples, 9996 features\n","✓ Prediction complete: 2676 samples\n","  Total latency: p50=409.2ms, p90=422.1ms\n","  Per-sample: p50=0.153ms, p90=0.158ms\n","  Saved: preds/{preds,scores,meta}_fold14_minirocket.*\n","\n","--- TST ---\n","Test set: 2676 samples\n","✓ Prediction complete: 2676 samples\n","  Total latency: p50=4219.0ms, p90=4241.8ms\n","  Per-sample: p50=1.577ms, p90=1.585ms\n","  Saved: preds/{preds,logits,probs,meta}_fold14_tst.*\n","\n","============================================================\n","✓ Completed inference for 15 folds\n","✓ Predictions: preds/preds_fold{k}_{minirocket,tst}.npy\n","✓ Scores: preds/{scores,logits,probs}_fold{k}_*.npy\n","✓ Metadata: preds/meta_fold{k}_{minirocket,tst}.npz\n","✓ Summary: logs/step15_inference_summary.json\n","\n","Average inference latency (per sample):\n","  MiniROCKET: p50=0.155ms, p90=0.158ms\n","  TST: p50=1.574ms, p90=1.583ms\n","============================================================\n","\n","[master 32c495e] inference: batch=1 with corrected latency statistics and score outputs\n"," 106 files changed, 402 insertions(+)\n"," create mode 100644 logs/step15_inference_summary.json\n"," create mode 100644 preds/logits_fold0_tst.npy\n"," create mode 100644 preds/logits_fold10_tst.npy\n"," create mode 100644 preds/logits_fold11_tst.npy\n"," create mode 100644 preds/logits_fold12_tst.npy\n"," create mode 100644 preds/logits_fold13_tst.npy\n"," create mode 100644 preds/logits_fold14_tst.npy\n"," create mode 100644 preds/logits_fold1_tst.npy\n"," create mode 100644 preds/logits_fold2_tst.npy\n"," create mode 100644 preds/logits_fold3_tst.npy\n"," create mode 100644 preds/logits_fold4_tst.npy\n"," create mode 100644 preds/logits_fold5_tst.npy\n"," create mode 100644 preds/logits_fold6_tst.npy\n"," create mode 100644 preds/logits_fold7_tst.npy\n"," create mode 100644 preds/logits_fold8_tst.npy\n"," create mode 100644 preds/logits_fold9_tst.npy\n"," create mode 100644 preds/meta_fold0_minirocket.npz\n"," create mode 100644 preds/meta_fold0_tst.npz\n"," create mode 100644 preds/meta_fold10_minirocket.npz\n"," create mode 100644 preds/meta_fold10_tst.npz\n"," create mode 100644 preds/meta_fold11_minirocket.npz\n"," create mode 100644 preds/meta_fold11_tst.npz\n"," create mode 100644 preds/meta_fold12_minirocket.npz\n"," create mode 100644 preds/meta_fold12_tst.npz\n"," create mode 100644 preds/meta_fold13_minirocket.npz\n"," create mode 100644 preds/meta_fold13_tst.npz\n"," create mode 100644 preds/meta_fold14_minirocket.npz\n"," create mode 100644 preds/meta_fold14_tst.npz\n"," create mode 100644 preds/meta_fold1_minirocket.npz\n"," create mode 100644 preds/meta_fold1_tst.npz\n"," create mode 100644 preds/meta_fold2_minirocket.npz\n"," create mode 100644 preds/meta_fold2_tst.npz\n"," create mode 100644 preds/meta_fold3_minirocket.npz\n"," create mode 100644 preds/meta_fold3_tst.npz\n"," create mode 100644 preds/meta_fold4_minirocket.npz\n"," create mode 100644 preds/meta_fold4_tst.npz\n"," create mode 100644 preds/meta_fold5_minirocket.npz\n"," create mode 100644 preds/meta_fold5_tst.npz\n"," create mode 100644 preds/meta_fold6_minirocket.npz\n"," create mode 100644 preds/meta_fold6_tst.npz\n"," create mode 100644 preds/meta_fold7_minirocket.npz\n"," create mode 100644 preds/meta_fold7_tst.npz\n"," create mode 100644 preds/meta_fold8_minirocket.npz\n"," create mode 100644 preds/meta_fold8_tst.npz\n"," create mode 100644 preds/meta_fold9_minirocket.npz\n"," create mode 100644 preds/meta_fold9_tst.npz\n"," create mode 100644 preds/preds_fold0_minirocket.npy\n"," create mode 100644 preds/preds_fold0_tst.npy\n"," create mode 100644 preds/preds_fold10_minirocket.npy\n"," create mode 100644 preds/preds_fold10_tst.npy\n"," create mode 100644 preds/preds_fold11_minirocket.npy\n"," create mode 100644 preds/preds_fold11_tst.npy\n"," create mode 100644 preds/preds_fold12_minirocket.npy\n"," create mode 100644 preds/preds_fold12_tst.npy\n"," create mode 100644 preds/preds_fold13_minirocket.npy\n"," create mode 100644 preds/preds_fold13_tst.npy\n"," create mode 100644 preds/preds_fold14_minirocket.npy\n"," create mode 100644 preds/preds_fold14_tst.npy\n"," create mode 100644 preds/preds_fold1_minirocket.npy\n"," create mode 100644 preds/preds_fold1_tst.npy\n"," create mode 100644 preds/preds_fold2_minirocket.npy\n"," create mode 100644 preds/preds_fold2_tst.npy\n"," create mode 100644 preds/preds_fold3_minirocket.npy\n"," create mode 100644 preds/preds_fold3_tst.npy\n"," create mode 100644 preds/preds_fold4_minirocket.npy\n"," create mode 100644 preds/preds_fold4_tst.npy\n"," create mode 100644 preds/preds_fold5_minirocket.npy\n"," create mode 100644 preds/preds_fold5_tst.npy\n"," create mode 100644 preds/preds_fold6_minirocket.npy\n"," create mode 100644 preds/preds_fold6_tst.npy\n"," create mode 100644 preds/preds_fold7_minirocket.npy\n"," create mode 100644 preds/preds_fold7_tst.npy\n"," create mode 100644 preds/preds_fold8_minirocket.npy\n"," create mode 100644 preds/preds_fold8_tst.npy\n"," create mode 100644 preds/preds_fold9_minirocket.npy\n"," create mode 100644 preds/preds_fold9_tst.npy\n"," create mode 100644 preds/probs_fold0_tst.npy\n"," create mode 100644 preds/probs_fold10_tst.npy\n"," create mode 100644 preds/probs_fold11_tst.npy\n"," create mode 100644 preds/probs_fold12_tst.npy\n"," create mode 100644 preds/probs_fold13_tst.npy\n"," create mode 100644 preds/probs_fold14_tst.npy\n"," create mode 100644 preds/probs_fold1_tst.npy\n"," create mode 100644 preds/probs_fold2_tst.npy\n"," create mode 100644 preds/probs_fold3_tst.npy\n"," create mode 100644 preds/probs_fold4_tst.npy\n"," create mode 100644 preds/probs_fold5_tst.npy\n"," create mode 100644 preds/probs_fold6_tst.npy\n"," create mode 100644 preds/probs_fold7_tst.npy\n"," create mode 100644 preds/probs_fold8_tst.npy\n"," create mode 100644 preds/probs_fold9_tst.npy\n"," create mode 100644 preds/scores_fold0_minirocket.npy\n"," create mode 100644 preds/scores_fold10_minirocket.npy\n"," create mode 100644 preds/scores_fold11_minirocket.npy\n"," create mode 100644 preds/scores_fold12_minirocket.npy\n"," create mode 100644 preds/scores_fold13_minirocket.npy\n"," create mode 100644 preds/scores_fold14_minirocket.npy\n"," create mode 100644 preds/scores_fold1_minirocket.npy\n"," create mode 100644 preds/scores_fold2_minirocket.npy\n"," create mode 100644 preds/scores_fold3_minirocket.npy\n"," create mode 100644 preds/scores_fold4_minirocket.npy\n"," create mode 100644 preds/scores_fold5_minirocket.npy\n"," create mode 100644 preds/scores_fold6_minirocket.npy\n"," create mode 100644 preds/scores_fold7_minirocket.npy\n"," create mode 100644 preds/scores_fold8_minirocket.npy\n"," create mode 100644 preds/scores_fold9_minirocket.npy\n","Step 15 completed\n","============================================================\n"]}]},{"cell_type":"code","source":["# ================ Step 16: Metric Computation ================\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import json\n","from sklearn.metrics import f1_score, classification_report, confusion_matrix\n","\n","print(\"\\n\\nStep 16: Metric Computation\")\n","print(\"=\" * 60)\n","\n","Path('logs').mkdir(parents=True, exist_ok=True)\n","\n","def get_active_folds(path=\"logs/active_folds.json\"):\n","    p = Path(path)\n","    if p.exists():\n","        return json.loads(p.read_text())[\"folds\"]\n","    return []\n","\n","with open('configs/splits.json', 'r') as f:\n","    splits_cfg = json.load(f)\n","\n","with open('configs/classes.json', 'r') as f:\n","    classes_cfg = json.load(f)\n","\n","active_folds = get_active_folds()\n","\n","id_to_label = {int(k): v for k, v in classes_cfg['id_to_label'].items()}\n","label_order = sorted(id_to_label.keys())\n","label_names = [id_to_label[i] for i in label_order]\n","\n","print(f\"Class order: {label_names}\\n\")\n","\n","all_fold_results = []\n","\n","for fold in splits_cfg['folds']:\n","    k = fold['fold']\n","\n","    if k not in active_folds:\n","        print(f\"⏭️  Skipping Fold {k}\")\n","        continue\n","\n","    test_subj = fold['test_subject']\n","\n","    print(f\"\\n{'='*60}\")\n","    print(f\"Fold {k}: test subject={test_subj}\")\n","    print(f\"{'='*60}\")\n","\n","    for method in ['minirocket', 'tst']:\n","        print(f\"\\n--- {method.upper()} ---\")\n","\n","        y_pred = np.load(f'preds/preds_fold{k}_{method}.npy')\n","        meta = np.load(f'preds/meta_fold{k}_{method}.npz', allow_pickle=True)\n","        y_true = meta['y_true']\n","\n","        macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n","        per_class_f1 = f1_score(y_true, y_pred, labels=label_order, average=None, zero_division=0)\n","\n","        report = classification_report(y_true, y_pred, labels=label_order,\n","                                       target_names=label_names,\n","                                       output_dict=True, zero_division=0)\n","\n","        cm = confusion_matrix(y_true, y_pred, labels=label_order)\n","\n","        metrics_list = []\n","        for i, label_id in enumerate(label_order):\n","            label_name = id_to_label[label_id]\n","            metrics_list.append({\n","                'fold': k,\n","                'method': method,\n","                'class_id': label_id,\n","                'class_name': label_name,\n","                'f1': per_class_f1[i],\n","                'precision': report[label_name]['precision'],\n","                'recall': report[label_name]['recall'],\n","                'support': int(report[label_name]['support'])\n","            })\n","\n","        metrics_list.append({\n","            'fold': k,\n","            'method': method,\n","            'class_id': -1,\n","            'class_name': 'macro_avg',\n","            'f1': macro_f1,\n","            'precision': report['macro avg']['precision'],\n","            'recall': report['macro avg']['recall'],\n","            'support': int(report['macro avg']['support'])\n","        })\n","\n","        metrics_df = pd.DataFrame(metrics_list)\n","        metrics_df.to_csv(f'logs/fold{k}_metrics_{method}.csv', index=False)\n","\n","        cm_df = pd.DataFrame(cm, index=label_names, columns=label_names)\n","        cm_df.to_csv(f'logs/fold{k}_cm_{method}.csv')\n","\n","        print(f\"Macro F1: {macro_f1:.4f}\")\n","        print(f\"Per-class F1:\")\n","        for i, label_id in enumerate(label_order):\n","            support = int(report[id_to_label[label_id]]['support'])\n","            print(f\"  {id_to_label[label_id]:15s} (n={support:4d}): {per_class_f1[i]:.4f}\")\n","\n","        print(f\"✓ Saved: logs/fold{k}_metrics_{method}.csv\")\n","        print(f\"✓ Saved: logs/fold{k}_cm_{method}.csv\")\n","\n","        all_fold_results.append({\n","            'fold': k,\n","            'test_subject': test_subj,\n","            'method': method,\n","            'macro_f1': float(macro_f1),\n","            'per_class_f1': {id_to_label[label_order[i]]: float(per_class_f1[i]) for i in range(len(label_order))},\n","            'per_class_support': {id_to_label[label_id]: int(report[id_to_label[label_id]]['support']) for label_id in label_order}\n","        })\n","\n","summary_rows = []\n","for method in ['minirocket', 'tst']:\n","    method_results = [r for r in all_fold_results if r['method'] == method]\n","\n","    avg_macro_f1 = np.mean([r['macro_f1'] for r in method_results])\n","    std_macro_f1 = np.std([r['macro_f1'] for r in method_results])\n","\n","    avg_per_class = {}\n","    for label_name in label_names:\n","        f1_values = [r['per_class_f1'][label_name] for r in method_results]\n","        avg_per_class[label_name] = np.mean(f1_values)\n","\n","    summary_rows.append({\n","        'method': method,\n","        'macro_f1_mean': avg_macro_f1,\n","        'macro_f1_std': std_macro_f1,\n","        **{f'{label}_f1': avg_per_class[label] for label in label_names}\n","    })\n","\n","summary_df = pd.DataFrame(summary_rows)\n","summary_df.to_csv('logs/metrics_summary.csv', index=False)\n","\n","with open('logs/step16_metrics_summary.json', 'w') as f:\n","    json.dump({\n","        'class_order': label_names,\n","        'n_folds': len([r for r in all_fold_results if r['method'] == 'minirocket']),\n","        'per_fold_results': all_fold_results,\n","        'aggregated': {\n","            method: {\n","                'macro_f1_mean': float(summary_df[summary_df['method'] == method]['macro_f1_mean'].values[0]),\n","                'macro_f1_std': float(summary_df[summary_df['method'] == method]['macro_f1_std'].values[0]),\n","                'per_class_f1_mean': {label: float(summary_df[summary_df['method'] == method][f'{label}_f1'].values[0]) for label in label_names}\n","            }\n","            for method in ['minirocket', 'tst']\n","        }\n","    }, f, indent=2)\n","\n","print(f\"\\n{'='*60}\")\n","print(f\"✓ Completed metric computation for {len(active_folds)} folds\")\n","print(f\"✓ Per-fold metrics: logs/fold{{k}}_metrics_{{minirocket,tst}}.csv\")\n","print(f\"✓ Per-fold confusion matrices: logs/fold{{k}}_cm_{{minirocket,tst}}.csv\")\n","print(f\"✓ Summary: logs/metrics_summary.csv\")\n","print(f\"✓ JSON: logs/step16_metrics_summary.json\")\n","print(f\"\\nAggregated results:\")\n","for method in ['minirocket', 'tst']:\n","    method_data = summary_df[summary_df['method'] == method].iloc[0]\n","    print(f\"  {method.upper()}: Macro F1 = {method_data['macro_f1_mean']:.4f} ± {method_data['macro_f1_std']:.4f}\")\n","print(f\"{'='*60}\\n\")\n","\n","get_ipython().system('git add logs/fold*_metrics_*.csv logs/fold*_cm_*.csv logs/metrics_summary.csv logs/step16_*.json')\n","get_ipython().system('git commit -m \\\"metrics: compute per-fold F1 and confusion matrices\\\"')\n","\n","print(f\"Step 16 completed\\n{'='*60}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wgZKxkZ9kZPl","executionInfo":{"status":"ok","timestamp":1762953982768,"user_tz":0,"elapsed":666,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"ae6eadb6-2e4d-4134-ae0d-f3d9f5836725"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Step 16: Metric Computation\n","============================================================\n","Class order: ['walking', 'running', 'sitting', 'standing', 'lying', 'stairs_up', 'stairs_down', 'jumping']\n","\n","\n","============================================================\n","Fold 0: test subject=proband1\n","============================================================\n","\n","--- MINIROCKET ---\n","Macro F1: 0.6667\n","Per-class F1:\n","  walking         (n= 396): 0.8958\n","  running         (n= 379): 0.9400\n","  sitting         (n=   0): 0.0000\n","  standing        (n= 382): 0.7373\n","  lying           (n=   0): 0.0000\n","  stairs_up       (n= 385): 0.9233\n","  stairs_down     (n= 303): 0.8476\n","  jumping         (n=  50): 0.9899\n","✓ Saved: logs/fold0_metrics_minirocket.csv\n","✓ Saved: logs/fold0_cm_minirocket.csv\n","\n","--- TST ---\n","Macro F1: 0.6492\n","Per-class F1:\n","  walking         (n= 396): 0.8475\n","  running         (n= 379): 0.9458\n","  sitting         (n=   0): 0.0000\n","  standing        (n= 382): 0.6862\n","  lying           (n=   0): 0.0000\n","  stairs_up       (n= 385): 0.9032\n","  stairs_down     (n= 303): 0.8779\n","  jumping         (n=  50): 0.9333\n","✓ Saved: logs/fold0_metrics_tst.csv\n","✓ Saved: logs/fold0_cm_tst.csv\n","\n","============================================================\n","Fold 1: test subject=proband10\n","============================================================\n","\n","--- MINIROCKET ---\n","Macro F1: 0.8023\n","Per-class F1:\n","  walking         (n= 372): 0.7587\n","  running         (n= 367): 0.9515\n","  sitting         (n= 366): 0.7703\n","  standing        (n= 388): 0.5886\n","  lying           (n= 384): 0.8575\n","  stairs_up       (n= 264): 0.8017\n","  stairs_down     (n= 254): 0.7281\n","  jumping         (n=  68): 0.9618\n","✓ Saved: logs/fold1_metrics_minirocket.csv\n","✓ Saved: logs/fold1_cm_minirocket.csv\n","\n","--- TST ---\n","Macro F1: 0.8410\n","Per-class F1:\n","  walking         (n= 372): 0.8805\n","  running         (n= 367): 0.9584\n","  sitting         (n= 366): 0.7273\n","  standing        (n= 388): 0.5719\n","  lying           (n= 384): 0.9354\n","  stairs_up       (n= 264): 0.8559\n","  stairs_down     (n= 254): 0.8686\n","  jumping         (n=  68): 0.9302\n","✓ Saved: logs/fold1_metrics_tst.csv\n","✓ Saved: logs/fold1_cm_tst.csv\n","\n","============================================================\n","Fold 2: test subject=proband11\n","============================================================\n","\n","--- MINIROCKET ---\n","Macro F1: 0.9064\n","Per-class F1:\n","  walking         (n= 382): 0.9091\n","  running         (n= 347): 0.9742\n","  sitting         (n= 364): 0.8071\n","  standing        (n= 378): 0.9079\n","  lying           (n= 396): 0.8131\n","  stairs_up       (n= 367): 0.9071\n","  stairs_down     (n= 293): 0.9517\n","  jumping         (n=  53): 0.9808\n","✓ Saved: logs/fold2_metrics_minirocket.csv\n","✓ Saved: logs/fold2_cm_minirocket.csv\n","\n","--- TST ---\n","Macro F1: 0.9201\n","Per-class F1:\n","  walking         (n= 382): 0.8917\n","  running         (n= 347): 0.9399\n","  sitting         (n= 364): 0.8952\n","  standing        (n= 378): 0.9005\n","  lying           (n= 396): 0.9340\n","  stairs_up       (n= 367): 0.9086\n","  stairs_down     (n= 293): 0.9197\n","  jumping         (n=  53): 0.9709\n","✓ Saved: logs/fold2_metrics_tst.csv\n","✓ Saved: logs/fold2_cm_tst.csv\n","\n","============================================================\n","Fold 3: test subject=proband12\n","============================================================\n","\n","--- MINIROCKET ---\n","Macro F1: 0.9151\n","Per-class F1:\n","  walking         (n= 318): 0.8536\n","  running         (n= 349): 0.9986\n","  sitting         (n= 299): 0.8380\n","  standing        (n= 353): 0.9254\n","  lying           (n= 373): 0.9103\n","  stairs_up       (n= 345): 0.8876\n","  stairs_down     (n= 289): 0.9197\n","  jumping         (n=  41): 0.9877\n","✓ Saved: logs/fold3_metrics_minirocket.csv\n","✓ Saved: logs/fold3_cm_minirocket.csv\n","\n","--- TST ---\n","Macro F1: 0.8064\n","Per-class F1:\n","  walking         (n= 318): 0.8529\n","  running         (n= 349): 0.9444\n","  sitting         (n= 299): 0.3886\n","  standing        (n= 353): 0.7692\n","  lying           (n= 373): 0.8507\n","  stairs_up       (n= 345): 0.9041\n","  stairs_down     (n= 289): 0.8731\n","  jumping         (n=  41): 0.8684\n","✓ Saved: logs/fold3_metrics_tst.csv\n","✓ Saved: logs/fold3_cm_tst.csv\n","\n","============================================================\n","Fold 4: test subject=proband13\n","============================================================\n","\n","--- MINIROCKET ---\n","Macro F1: 0.8967\n","Per-class F1:\n","  walking         (n= 398): 0.8509\n","  running         (n= 368): 0.9545\n","  sitting         (n= 380): 0.8064\n","  standing        (n= 384): 0.8551\n","  lying           (n= 385): 0.9577\n","  stairs_up       (n= 353): 0.8478\n","  stairs_down     (n= 252): 0.9234\n","  jumping         (n=  69): 0.9778\n","✓ Saved: logs/fold4_metrics_minirocket.csv\n","✓ Saved: logs/fold4_cm_minirocket.csv\n","\n","--- TST ---\n","Macro F1: 0.8370\n","Per-class F1:\n","  walking         (n= 398): 0.8624\n","  running         (n= 368): 0.7931\n","  sitting         (n= 380): 0.6480\n","  standing        (n= 384): 0.7631\n","  lying           (n= 385): 0.9538\n","  stairs_up       (n= 353): 0.8688\n","  stairs_down     (n= 252): 0.8289\n","  jumping         (n=  69): 0.9778\n","✓ Saved: logs/fold4_metrics_tst.csv\n","✓ Saved: logs/fold4_cm_tst.csv\n","\n","============================================================\n","Fold 5: test subject=proband14\n","============================================================\n","\n","--- MINIROCKET ---\n","Macro F1: 0.6144\n","Per-class F1:\n","  walking         (n= 351): 0.9768\n","  running         (n= 346): 0.9855\n","  sitting         (n= 362): 0.3034\n","  standing        (n= 352): 0.6740\n","  lying           (n= 369): 0.9753\n","  stairs_up       (n=   0): 0.0000\n","  stairs_down     (n=   0): 0.0000\n","  jumping         (n=  55): 1.0000\n","✓ Saved: logs/fold5_metrics_minirocket.csv\n","✓ Saved: logs/fold5_cm_minirocket.csv\n","\n","--- TST ---\n","Macro F1: 0.5142\n","Per-class F1:\n","  walking         (n= 351): 0.7315\n","  running         (n= 346): 0.7522\n","  sitting         (n= 362): 0.2710\n","  standing        (n= 352): 0.6917\n","  lying           (n= 369): 0.9745\n","  stairs_up       (n=   0): 0.0000\n","  stairs_down     (n=   0): 0.0000\n","  jumping         (n=  55): 0.6923\n","✓ Saved: logs/fold5_metrics_tst.csv\n","✓ Saved: logs/fold5_cm_tst.csv\n","\n","============================================================\n","Fold 6: test subject=proband15\n","============================================================\n","\n","--- MINIROCKET ---\n","Macro F1: 0.8871\n","Per-class F1:\n","  walking         (n= 388): 0.8071\n","  running         (n= 395): 0.9302\n","  sitting         (n= 375): 0.9116\n","  standing        (n= 374): 0.8863\n","  lying           (n= 392): 0.9016\n","  stairs_up       (n= 343): 0.8997\n","  stairs_down     (n= 296): 0.7703\n","  jumping         (n=  51): 0.9901\n","✓ Saved: logs/fold6_metrics_minirocket.csv\n","✓ Saved: logs/fold6_cm_minirocket.csv\n","\n","--- TST ---\n","Macro F1: 0.8354\n","Per-class F1:\n","  walking         (n= 388): 0.8199\n","  running         (n= 395): 0.9059\n","  sitting         (n= 375): 0.8363\n","  standing        (n= 374): 0.7898\n","  lying           (n= 392): 0.8867\n","  stairs_up       (n= 343): 0.8374\n","  stairs_down     (n= 296): 0.8168\n","  jumping         (n=  51): 0.7907\n","✓ Saved: logs/fold6_metrics_tst.csv\n","✓ Saved: logs/fold6_cm_tst.csv\n","\n","============================================================\n","Fold 7: test subject=proband2\n","============================================================\n","\n","--- MINIROCKET ---\n","Macro F1: 0.9623\n","Per-class F1:\n","  walking         (n= 331): 0.9316\n","  running         (n= 322): 0.9829\n","  sitting         (n= 373): 0.9717\n","  standing        (n= 312): 0.9669\n","  lying           (n= 358): 0.9845\n","  stairs_up       (n= 259): 0.9240\n","  stairs_down     (n= 281): 0.9367\n","  jumping         (n=  56): 1.0000\n","✓ Saved: logs/fold7_metrics_minirocket.csv\n","✓ Saved: logs/fold7_cm_minirocket.csv\n","\n","--- TST ---\n","Macro F1: 0.8729\n","Per-class F1:\n","  walking         (n= 331): 0.8381\n","  running         (n= 322): 0.9712\n","  sitting         (n= 373): 0.7449\n","  standing        (n= 312): 0.7893\n","  lying           (n= 358): 0.9471\n","  stairs_up       (n= 259): 0.8125\n","  stairs_down     (n= 281): 0.9061\n","  jumping         (n=  56): 0.9739\n","✓ Saved: logs/fold7_metrics_tst.csv\n","✓ Saved: logs/fold7_cm_tst.csv\n","\n","============================================================\n","Fold 8: test subject=proband3\n","============================================================\n","\n","--- MINIROCKET ---\n","Macro F1: 0.7636\n","Per-class F1:\n","  walking         (n= 415): 0.7873\n","  running         (n= 443): 0.8796\n","  sitting         (n= 370): 0.9418\n","  standing        (n= 368): 0.8384\n","  lying           (n= 371): 0.9615\n","  stairs_up       (n= 348): 0.6394\n","  stairs_down     (n= 332): 0.0611\n","  jumping         (n=  58): 1.0000\n","✓ Saved: logs/fold8_metrics_minirocket.csv\n","✓ Saved: logs/fold8_cm_minirocket.csv\n","\n","--- TST ---\n","Macro F1: 0.5686\n","Per-class F1:\n","  walking         (n= 415): 0.0000\n","  running         (n= 443): 0.8091\n","  sitting         (n= 370): 0.8398\n","  standing        (n= 368): 0.7008\n","  lying           (n= 371): 0.8079\n","  stairs_up       (n= 348): 0.5231\n","  stairs_down     (n= 332): 0.2131\n","  jumping         (n=  58): 0.6552\n","✓ Saved: logs/fold8_metrics_tst.csv\n","✓ Saved: logs/fold8_cm_tst.csv\n","\n","============================================================\n","Fold 9: test subject=proband4\n","============================================================\n","\n","--- MINIROCKET ---\n","Macro F1: 0.5511\n","Per-class F1:\n","  walking         (n= 388): 0.9793\n","  running         (n= 615): 0.5133\n","  sitting         (n= 395): 0.6219\n","  standing        (n= 380): 0.5472\n","  lying           (n= 418): 0.7866\n","  stairs_up       (n=   0): 0.0000\n","  stairs_down     (n=   0): 0.0000\n","  jumping         (n=  51): 0.9608\n","✓ Saved: logs/fold9_metrics_minirocket.csv\n","✓ Saved: logs/fold9_cm_minirocket.csv\n","\n","--- TST ---\n","Macro F1: 0.5149\n","Per-class F1:\n","  walking         (n= 388): 0.9693\n","  running         (n= 615): 0.5036\n","  sitting         (n= 395): 0.4251\n","  standing        (n= 380): 0.5797\n","  lying           (n= 418): 0.6901\n","  stairs_up       (n=   0): 0.0000\n","  stairs_down     (n=   0): 0.0000\n","  jumping         (n=  51): 0.9515\n","✓ Saved: logs/fold9_metrics_tst.csv\n","✓ Saved: logs/fold9_cm_tst.csv\n","\n","============================================================\n","Fold 10: test subject=proband5\n","============================================================\n","\n","--- MINIROCKET ---\n","Macro F1: 0.7620\n","Per-class F1:\n","  walking         (n= 413): 0.5450\n","  running         (n= 659): 0.6938\n","  sitting         (n= 404): 0.8582\n","  standing        (n= 358): 0.6573\n","  lying           (n= 382): 0.9018\n","  stairs_up       (n= 363): 0.7760\n","  stairs_down     (n= 294): 0.6897\n","  jumping         (n=  59): 0.9739\n","✓ Saved: logs/fold10_metrics_minirocket.csv\n","✓ Saved: logs/fold10_cm_minirocket.csv\n","\n","--- TST ---\n","Macro F1: 0.8017\n","Per-class F1:\n","  walking         (n= 413): 0.7446\n","  running         (n= 659): 0.7745\n","  sitting         (n= 404): 0.8127\n","  standing        (n= 358): 0.6631\n","  lying           (n= 382): 0.9163\n","  stairs_up       (n= 363): 0.6968\n","  stairs_down     (n= 294): 0.8403\n","  jumping         (n=  59): 0.9655\n","✓ Saved: logs/fold10_metrics_tst.csv\n","✓ Saved: logs/fold10_cm_tst.csv\n","\n","============================================================\n","Fold 11: test subject=proband6\n","============================================================\n","\n","--- MINIROCKET ---\n","Macro F1: 0.9094\n","Per-class F1:\n","  walking         (n= 362): 0.8169\n","  running         (n= 370): 0.9629\n","  sitting         (n= 399): 0.9205\n","  standing        (n= 363): 0.8897\n","  lying           (n= 378): 0.9595\n","  stairs_up       (n= 309): 0.8218\n","  stairs_down     (n= 290): 0.9040\n","  jumping         (n=  60): 1.0000\n","✓ Saved: logs/fold11_metrics_minirocket.csv\n","✓ Saved: logs/fold11_cm_minirocket.csv\n","\n","--- TST ---\n","Macro F1: 0.8528\n","Per-class F1:\n","  walking         (n= 362): 0.8474\n","  running         (n= 370): 0.9361\n","  sitting         (n= 399): 0.6566\n","  standing        (n= 363): 0.8522\n","  lying           (n= 378): 0.8254\n","  stairs_up       (n= 309): 0.8548\n","  stairs_down     (n= 290): 0.8926\n","  jumping         (n=  60): 0.9573\n","✓ Saved: logs/fold11_metrics_tst.csv\n","✓ Saved: logs/fold11_cm_tst.csv\n","\n","============================================================\n","Fold 12: test subject=proband7\n","============================================================\n","\n","--- MINIROCKET ---\n","Macro F1: 0.6548\n","Per-class F1:\n","  walking         (n= 360): 0.9861\n","  running         (n= 434): 0.9455\n","  sitting         (n= 386): 0.7091\n","  standing        (n= 391): 0.8243\n","  lying           (n= 375): 0.7920\n","  stairs_up       (n=   0): 0.0000\n","  stairs_down     (n=   0): 0.0000\n","  jumping         (n=  56): 0.9818\n","✓ Saved: logs/fold12_metrics_minirocket.csv\n","✓ Saved: logs/fold12_cm_minirocket.csv\n","\n","--- TST ---\n","Macro F1: 0.6337\n","Per-class F1:\n","  walking         (n= 360): 0.9595\n","  running         (n= 434): 0.9225\n","  sitting         (n= 386): 0.6596\n","  standing        (n= 391): 0.7281\n","  lying           (n= 375): 0.8269\n","  stairs_up       (n=   0): 0.0000\n","  stairs_down     (n=   0): 0.0000\n","  jumping         (n=  56): 0.9730\n","✓ Saved: logs/fold12_metrics_tst.csv\n","✓ Saved: logs/fold12_cm_tst.csv\n","\n","============================================================\n","Fold 13: test subject=proband8\n","============================================================\n","\n","--- MINIROCKET ---\n","Macro F1: 0.6520\n","Per-class F1:\n","  walking         (n= 375): 0.0848\n","  running         (n= 368): 0.9863\n","  sitting         (n= 401): 0.5973\n","  standing        (n= 399): 0.7891\n","  lying           (n= 378): 0.5297\n","  stairs_up       (n= 674): 0.6766\n","  stairs_down     (n= 245): 0.5707\n","  jumping         (n=  54): 0.9815\n","✓ Saved: logs/fold13_metrics_minirocket.csv\n","✓ Saved: logs/fold13_cm_minirocket.csv\n","\n","--- TST ---\n","Macro F1: 0.6878\n","Per-class F1:\n","  walking         (n= 375): 0.6418\n","  running         (n= 368): 0.7775\n","  sitting         (n= 401): 0.6990\n","  standing        (n= 399): 0.7388\n","  lying           (n= 378): 0.6236\n","  stairs_up       (n= 674): 0.5641\n","  stairs_down     (n= 245): 0.6810\n","  jumping         (n=  54): 0.7767\n","✓ Saved: logs/fold13_metrics_tst.csv\n","✓ Saved: logs/fold13_cm_tst.csv\n","\n","============================================================\n","Fold 14: test subject=proband9\n","============================================================\n","\n","--- MINIROCKET ---\n","Macro F1: 0.8751\n","Per-class F1:\n","  walking         (n= 369): 0.8863\n","  running         (n= 468): 0.8197\n","  sitting         (n= 385): 0.8324\n","  standing        (n= 392): 0.7134\n","  lying           (n= 384): 0.9715\n","  stairs_up       (n= 321): 0.9127\n","  stairs_down     (n= 296): 0.8897\n","  jumping         (n=  61): 0.9752\n","✓ Saved: logs/fold14_metrics_minirocket.csv\n","✓ Saved: logs/fold14_cm_minirocket.csv\n","\n","--- TST ---\n","Macro F1: 0.8417\n","Per-class F1:\n","  walking         (n= 369): 0.8373\n","  running         (n= 468): 0.8005\n","  sitting         (n= 385): 0.7743\n","  standing        (n= 392): 0.6553\n","  lying           (n= 384): 0.9467\n","  stairs_up       (n= 321): 0.8680\n","  stairs_down     (n= 296): 0.8679\n","  jumping         (n=  61): 0.9833\n","✓ Saved: logs/fold14_metrics_tst.csv\n","✓ Saved: logs/fold14_cm_tst.csv\n","\n","============================================================\n","✓ Completed metric computation for 15 folds\n","✓ Per-fold metrics: logs/fold{k}_metrics_{minirocket,tst}.csv\n","✓ Per-fold confusion matrices: logs/fold{k}_cm_{minirocket,tst}.csv\n","✓ Summary: logs/metrics_summary.csv\n","✓ JSON: logs/step16_metrics_summary.json\n","\n","Aggregated results:\n","  MINIROCKET: Macro F1 = 0.7879 ± 0.1273\n","  TST: Macro F1 = 0.7452 ± 0.1324\n","============================================================\n","\n","[master ef783f1] metrics: compute per-fold F1 and confusion matrices\n"," 62 files changed, 1398 insertions(+)\n"," create mode 100644 logs/fold0_cm_minirocket.csv\n"," create mode 100644 logs/fold0_cm_tst.csv\n"," create mode 100644 logs/fold0_metrics_minirocket.csv\n"," create mode 100644 logs/fold0_metrics_tst.csv\n"," create mode 100644 logs/fold10_cm_minirocket.csv\n"," create mode 100644 logs/fold10_cm_tst.csv\n"," create mode 100644 logs/fold10_metrics_minirocket.csv\n"," create mode 100644 logs/fold10_metrics_tst.csv\n"," create mode 100644 logs/fold11_cm_minirocket.csv\n"," create mode 100644 logs/fold11_cm_tst.csv\n"," create mode 100644 logs/fold11_metrics_minirocket.csv\n"," create mode 100644 logs/fold11_metrics_tst.csv\n"," create mode 100644 logs/fold12_cm_minirocket.csv\n"," create mode 100644 logs/fold12_cm_tst.csv\n"," create mode 100644 logs/fold12_metrics_minirocket.csv\n"," create mode 100644 logs/fold12_metrics_tst.csv\n"," create mode 100644 logs/fold13_cm_minirocket.csv\n"," create mode 100644 logs/fold13_cm_tst.csv\n"," create mode 100644 logs/fold13_metrics_minirocket.csv\n"," create mode 100644 logs/fold13_metrics_tst.csv\n"," create mode 100644 logs/fold14_cm_minirocket.csv\n"," create mode 100644 logs/fold14_cm_tst.csv\n"," create mode 100644 logs/fold14_metrics_minirocket.csv\n"," create mode 100644 logs/fold14_metrics_tst.csv\n"," create mode 100644 logs/fold1_cm_minirocket.csv\n"," create mode 100644 logs/fold1_cm_tst.csv\n"," create mode 100644 logs/fold1_metrics_minirocket.csv\n"," create mode 100644 logs/fold1_metrics_tst.csv\n"," create mode 100644 logs/fold2_cm_minirocket.csv\n"," create mode 100644 logs/fold2_cm_tst.csv\n"," create mode 100644 logs/fold2_metrics_minirocket.csv\n"," create mode 100644 logs/fold2_metrics_tst.csv\n"," create mode 100644 logs/fold3_cm_minirocket.csv\n"," create mode 100644 logs/fold3_cm_tst.csv\n"," create mode 100644 logs/fold3_metrics_minirocket.csv\n"," create mode 100644 logs/fold3_metrics_tst.csv\n"," create mode 100644 logs/fold4_cm_minirocket.csv\n"," create mode 100644 logs/fold4_cm_tst.csv\n"," create mode 100644 logs/fold4_metrics_minirocket.csv\n"," create mode 100644 logs/fold4_metrics_tst.csv\n"," create mode 100644 logs/fold5_cm_minirocket.csv\n"," create mode 100644 logs/fold5_cm_tst.csv\n"," create mode 100644 logs/fold5_metrics_minirocket.csv\n"," create mode 100644 logs/fold5_metrics_tst.csv\n"," create mode 100644 logs/fold6_cm_minirocket.csv\n"," create mode 100644 logs/fold6_cm_tst.csv\n"," create mode 100644 logs/fold6_metrics_minirocket.csv\n"," create mode 100644 logs/fold6_metrics_tst.csv\n"," create mode 100644 logs/fold7_cm_minirocket.csv\n"," create mode 100644 logs/fold7_cm_tst.csv\n"," create mode 100644 logs/fold7_metrics_minirocket.csv\n"," create mode 100644 logs/fold7_metrics_tst.csv\n"," create mode 100644 logs/fold8_cm_minirocket.csv\n"," create mode 100644 logs/fold8_cm_tst.csv\n"," create mode 100644 logs/fold8_metrics_minirocket.csv\n"," create mode 100644 logs/fold8_metrics_tst.csv\n"," create mode 100644 logs/fold9_cm_minirocket.csv\n"," create mode 100644 logs/fold9_cm_tst.csv\n"," create mode 100644 logs/fold9_metrics_minirocket.csv\n"," create mode 100644 logs/fold9_metrics_tst.csv\n"," create mode 100644 logs/metrics_summary.csv\n"," create mode 100644 logs/step16_metrics_summary.json\n","Step 16 completed\n","============================================================\n"]}]},{"cell_type":"code","source":["# ================ Step 17: Aggregation & Confidence ================\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import json\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"\\n\\nStep 17: Aggregation & Confidence\")\n","print(\"=\" * 60)\n","\n","Path('figures').mkdir(parents=True, exist_ok=True)\n","\n","with open('logs/step16_metrics_summary.json', 'r') as f:\n","    metrics_data = json.load(f)\n","\n","with open('configs/classes.json', 'r') as f:\n","    classes_cfg = json.load(f)\n","\n","label_names = metrics_data['class_order']\n","per_fold_results = metrics_data['per_fold_results']\n","METHODS = sorted({r['method'] for r in per_fold_results})\n","\n","def bootstrap_ci(values, subjects, n_bootstrap=10000, ci=0.95):\n","    unique_subj = list(dict.fromkeys(subjects))\n","    if len(unique_subj) < 2:\n","        m = float(np.mean(values))\n","        return m, m\n","\n","    rng = np.random.default_rng(42)\n","    n_subjects = len(unique_subj)\n","\n","    bootstrap_means = []\n","    for _ in range(n_bootstrap):\n","        sampled_subjects = rng.choice(unique_subj, size=n_subjects, replace=True)\n","        sampled_values = []\n","        for subj in sampled_subjects:\n","            subj_values = [v for v, s in zip(values, subjects) if s == subj]\n","            if subj_values:\n","                sampled_values.append(np.mean(subj_values))\n","        bootstrap_means.append(np.mean(sampled_values))\n","\n","    lower = np.percentile(bootstrap_means, (1 - ci) / 2 * 100)\n","    upper = np.percentile(bootstrap_means, (1 + ci) / 2 * 100)\n","    return lower, upper\n","\n","def macro_f1_present_of(record):\n","    present = [lab for lab in label_names if record['per_class_support'].get(lab, 0) > 0]\n","    if not present:\n","        return np.nan\n","    return float(np.mean([record['per_class_f1'][lab] for lab in present]))\n","\n","summary_data = []\n","\n","for method in METHODS:\n","    method_results = [r for r in per_fold_results if r['method'] == method]\n","    subjects = [r['test_subject'] for r in method_results]\n","\n","    macro_f1_values = [r['macro_f1'] for r in method_results]\n","    macro_f1_mean = np.mean(macro_f1_values)\n","    macro_f1_std = np.std(macro_f1_values)\n","    macro_f1_ci_lower, macro_f1_ci_upper = bootstrap_ci(macro_f1_values, subjects)\n","\n","    macro_f1_present_values = [macro_f1_present_of(r) for r in method_results]\n","    vals, subs = zip(*[(v, s) for v, s in zip(macro_f1_present_values, subjects) if not np.isnan(v)])\n","    macro_f1_present_mean = np.nanmean(macro_f1_present_values)\n","    macro_f1_present_std = np.nanstd(macro_f1_present_values)\n","    macro_f1_present_ci_lower, macro_f1_present_ci_upper = bootstrap_ci(list(vals), list(subs))\n","\n","    per_class_f1_mean = {}\n","    per_class_f1_std = {}\n","    per_class_ci = {}\n","\n","    for label in label_names:\n","        f1_values = [r['per_class_f1'][label] for r in method_results]\n","        per_class_f1_mean[label] = np.mean(f1_values)\n","        per_class_f1_std[label] = np.std(f1_values)\n","        ci_lower, ci_upper = bootstrap_ci(f1_values, subjects)\n","        per_class_ci[label] = (ci_lower, ci_upper)\n","\n","    summary_data.append({\n","        'method': method,\n","        'n_folds': len(method_results),\n","        'n_subjects': len(set(subjects)),\n","        'macro_f1_mean': macro_f1_mean,\n","        'macro_f1_std': macro_f1_std,\n","        'macro_f1_ci_lower': macro_f1_ci_lower,\n","        'macro_f1_ci_upper': macro_f1_ci_upper,\n","        'macro_f1_present_mean': macro_f1_present_mean,\n","        'macro_f1_present_std': macro_f1_present_std,\n","        'macro_f1_present_ci_lower': macro_f1_present_ci_lower,\n","        'macro_f1_present_ci_upper': macro_f1_present_ci_upper,\n","        **{f'{label}_f1_mean': per_class_f1_mean[label] for label in label_names},\n","        **{f'{label}_f1_std': per_class_f1_std[label] for label in label_names}\n","    })\n","\n","    print(f\"\\n{method.upper()}:\")\n","    print(f\"  Macro F1: {macro_f1_mean:.4f} ± {macro_f1_std:.4f}\")\n","    print(f\"  Bootstrap 95% CI: [{macro_f1_ci_lower:.4f}, {macro_f1_ci_upper:.4f}]\")\n","    print(f\"  Macro F1 (present): {macro_f1_present_mean:.4f} ± {macro_f1_present_std:.4f}\")\n","    print(f\"  Bootstrap 95% CI: [{macro_f1_present_ci_lower:.4f}, {macro_f1_present_ci_upper:.4f}]\")\n","    print(f\"  Per-class F1 (mean ± std):\")\n","    for label in label_names:\n","        ci_l, ci_u = per_class_ci[label]\n","        print(f\"    {label:15s}: {per_class_f1_mean[label]:.4f} ± {per_class_f1_std[label]:.4f}  CI:[{ci_l:.4f}, {ci_u:.4f}]\")\n","\n","summary_df = pd.DataFrame(summary_data)\n","summary_df.to_csv('logs/summary_metrics.csv', index=False)\n","print(f\"\\n✓ Saved: logs/summary_metrics.csv\")\n","\n","fig, axes = plt.subplots(1, 2, figsize=(14, 6), subplot_kw=dict(polar=True))\n","\n","for idx, method in enumerate(METHODS):\n","    method_results = [r for r in per_fold_results if r['method'] == method]\n","    per_class_f1 = [np.mean([r['per_class_f1'][lbl] for r in method_results]) for lbl in label_names]\n","\n","    angles = np.linspace(0, 2*np.pi, len(label_names), endpoint=False)\n","    values = np.array(per_class_f1)\n","    angles_plot = np.concatenate([angles, angles[:1]])\n","    values_plot = np.concatenate([values, values[:1]])\n","\n","    ax = axes[idx]\n","    ax.plot(angles_plot, values_plot, linewidth=2, marker='o', label=method.upper())\n","    ax.fill(angles_plot, values_plot, alpha=0.25)\n","    ax.set_xticks(angles)\n","    ax.set_xticklabels(label_names, fontsize=9)\n","    ax.set_ylim(0, 1)\n","    ax.set_title(f'{method.upper()} - Per-Class F1', fontsize=12, fontweight='bold')\n","    ax.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig('figures/step17_radar.svg', format='svg', dpi=150)\n","plt.close()\n","print(f\"✓ Saved: figures/step17_radar.svg\")\n","\n","fig, ax = plt.subplots(figsize=(12, 6))\n","\n","x = np.arange(len(label_names))\n","width = 0.35\n","\n","for idx, method in enumerate(METHODS):\n","    method_results = [r for r in per_fold_results if r['method'] == method]\n","    per_class_f1 = [np.mean([r['per_class_f1'][label] for r in method_results]) for label in label_names]\n","    per_class_std = [np.std([r['per_class_f1'][label] for r in method_results]) for label in label_names]\n","\n","    offset = width * (idx - 0.5)\n","    ax.bar(x + offset, per_class_f1, width, yerr=per_class_std,\n","           label=method.upper(), alpha=0.8, capsize=5)\n","\n","ax.set_xlabel('Class', fontsize=12)\n","ax.set_ylabel('F1 Score', fontsize=12)\n","ax.set_title('Per-Class F1 Score Comparison', fontsize=14, weight='bold')\n","ax.set_xticks(x)\n","ax.set_xticklabels(label_names, rotation=45, ha='right')\n","ax.set_ylim(0, 1.0)\n","ax.legend()\n","ax.grid(axis='y', alpha=0.3)\n","plt.tight_layout()\n","plt.savefig('figures/step17_bar.svg', format='svg', dpi=150)\n","plt.close()\n","print(f\"✓ Saved: figures/step17_bar.svg\")\n","\n","with open('logs/step17_summary.json', 'w') as f:\n","    json.dump({\n","        'bootstrap_config': {\n","            'n_bootstrap': 10000,\n","            'confidence_interval': 0.95,\n","            'level': 'subject',\n","            'random_seed': 42\n","        },\n","        'methods': {\n","            method: {\n","                'n_folds': int(summary_df[summary_df['method'] == method]['n_folds'].values[0]),\n","                'n_subjects': int(summary_df[summary_df['method'] == method]['n_subjects'].values[0]),\n","                'macro_f1': {\n","                    'mean': float(summary_df[summary_df['method'] == method]['macro_f1_mean'].values[0]),\n","                    'std': float(summary_df[summary_df['method'] == method]['macro_f1_std'].values[0]),\n","                    'ci_lower': float(summary_df[summary_df['method'] == method]['macro_f1_ci_lower'].values[0]),\n","                    'ci_upper': float(summary_df[summary_df['method'] == method]['macro_f1_ci_upper'].values[0])\n","                },\n","                'macro_f1_present': {\n","                    'mean': float(summary_df[summary_df['method'] == method]['macro_f1_present_mean'].values[0]),\n","                    'std': float(summary_df[summary_df['method'] == method]['macro_f1_present_std'].values[0]),\n","                    'ci_lower': float(summary_df[summary_df['method'] == method]['macro_f1_present_ci_lower'].values[0]),\n","                    'ci_upper': float(summary_df[summary_df['method'] == method]['macro_f1_present_ci_upper'].values[0])\n","                },\n","                'per_class_f1': {\n","                    label: {\n","                        'mean': float(summary_df[summary_df['method'] == method][f'{label}_f1_mean'].values[0]),\n","                        'std': float(summary_df[summary_df['method'] == method][f'{label}_f1_std'].values[0])\n","                    }\n","                    for label in label_names\n","                }\n","            }\n","            for method in METHODS\n","        }\n","    }, f, indent=2)\n","\n","print(f\"\\n{'='*60}\")\n","print(f\"✓ Aggregation completed\")\n","print(f\"✓ CSV: logs/summary_metrics.csv\")\n","print(f\"✓ JSON: logs/step17_summary.json\")\n","print(f\"✓ Radar plot: figures/step17_radar.svg\")\n","print(f\"✓ Bar chart: figures/step17_bar.svg\")\n","print(f\"{'='*60}\\n\")\n","\n","get_ipython().system('git add logs/summary_metrics.csv logs/step17_*.json figures/step17_*.svg')\n","get_ipython().system('git commit -m \"aggregate: compute mean±std and bootstrap CI for metrics\"')\n","\n","print(f\"Step 17 completed\\n{'='*60}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yLOs0rjFlD1s","executionInfo":{"status":"ok","timestamp":1762954014753,"user_tz":0,"elapsed":31973,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"14a57a08-abfb-45e2-e01a-752f15811dee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Step 17: Aggregation & Confidence\n","============================================================\n","\n","MINIROCKET:\n","  Macro F1: 0.7879 ± 0.1273\n","  Bootstrap 95% CI: [0.7223, 0.8513]\n","  Macro F1 (present): 0.8432 ± 0.0817\n","  Bootstrap 95% CI: [0.7998, 0.8832]\n","  Per-class F1 (mean ± std):\n","    walking        : 0.8046 ± 0.2203  CI:[0.6773, 0.8961]\n","    running        : 0.9012 ± 0.1291  CI:[0.8259, 0.9568]\n","    sitting        : 0.7260 ± 0.2542  CI:[0.5871, 0.8387]\n","    standing       : 0.7867 ± 0.1237  CI:[0.7218, 0.8465]\n","    lying          : 0.8202 ± 0.2469  CI:[0.6786, 0.9210]\n","    stairs_up      : 0.6678 ± 0.3440  CI:[0.4849, 0.8254]\n","    stairs_down    : 0.6129 ± 0.3745  CI:[0.4170, 0.7920]\n","    jumping        : 0.9841 ± 0.0126  CI:[0.9774, 0.9903]\n","\n","TST:\n","  Macro F1: 0.7452 ± 0.1324\n","  Bootstrap 95% CI: [0.6778, 0.8103]\n","  Macro F1 (present): 0.7965 ± 0.0920\n","  Bootstrap 95% CI: [0.7481, 0.8395]\n","  Per-class F1 (mean ± std):\n","    walking        : 0.7816 ± 0.2238  CI:[0.6538, 0.8679]\n","    running        : 0.8490 ± 0.1201  CI:[0.7834, 0.9035]\n","    sitting        : 0.6252 ± 0.2403  CI:[0.4993, 0.7357]\n","    standing       : 0.7253 ± 0.0877  CI:[0.6807, 0.7694]\n","    lying          : 0.8079 ± 0.2368  CI:[0.6725, 0.9015]\n","    stairs_up      : 0.6398 ± 0.3395  CI:[0.4565, 0.7976]\n","    stairs_down    : 0.6391 ± 0.3603  CI:[0.4496, 0.8087]\n","    jumping        : 0.8933 ± 0.1070  CI:[0.8367, 0.9432]\n","\n","✓ Saved: logs/summary_metrics.csv\n","✓ Saved: figures/step17_radar.svg\n","✓ Saved: figures/step17_bar.svg\n","\n","============================================================\n","✓ Aggregation completed\n","✓ CSV: logs/summary_metrics.csv\n","✓ JSON: logs/step17_summary.json\n","✓ Radar plot: figures/step17_radar.svg\n","✓ Bar chart: figures/step17_bar.svg\n","============================================================\n","\n","[master f010659] aggregate: compute mean±std and bootstrap CI for metrics\n"," 4 files changed, 3862 insertions(+)\n"," create mode 100644 figures/step17_bar.svg\n"," create mode 100644 figures/step17_radar.svg\n"," create mode 100644 logs/step17_summary.json\n"," create mode 100644 logs/summary_metrics.csv\n","Step 17 completed\n","============================================================\n"]}]},{"cell_type":"code","source":["# ================ Step 18: Significance Testing ================\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import json\n","import matplotlib.pyplot as plt\n","from scipy.stats import wilcoxon\n","from scipy.stats import rankdata\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"\\n\\nStep 18: Significance Testing\")\n","print(\"=\" * 60)\n","\n","Path('figures').mkdir(parents=True, exist_ok=True)\n","\n","with open('logs/step16_metrics_summary.json', 'r') as f:\n","    metrics_data = json.load(f)\n","\n","per_fold_results = metrics_data['per_fold_results']\n","\n","# Extract per-subject Macro-F1 vectors\n","subjects = sorted(list(set([r['test_subject'] for r in per_fold_results if r['method'] == 'minirocket'])))\n","models = ['minirocket', 'tst']\n","\n","print(f\"Number of subjects: {len(subjects)}\")\n","print(f\"Models: {models}\\n\")\n","\n","# Construct subject × model matrix\n","f1_matrix = np.zeros((len(subjects), len(models)))\n","for i, subj in enumerate(subjects):\n","    for j, model in enumerate(models):\n","        result = [r for r in per_fold_results if r['test_subject'] == subj and r['method'] == model]\n","        if result:\n","            f1_matrix[i, j] = result[0]['macro_f1']\n","\n","print(\"Subject × Model Macro-F1 matrix:\")\n","df_matrix = pd.DataFrame(f1_matrix, index=subjects, columns=models)\n","print(df_matrix)\n","print()\n","\n","# Paired Wilcoxon signed-rank test (requires at least 6 samples)\n","if len(subjects) >= 6:\n","    stat_wilcoxon, p_wilcoxon = wilcoxon(f1_matrix[:, 0], f1_matrix[:, 1])\n","    print(f\"Wilcoxon signed-rank test (minirocket vs tst):\")\n","    print(f\"  Statistic = {stat_wilcoxon:.4f}\")\n","    print(f\"  p-value = {p_wilcoxon:.4f}\")\n","    print(f\"  Significant: {'Yes' if p_wilcoxon < 0.05 else 'No'} (α=0.05)\\n\")\n","else:\n","    stat_wilcoxon, p_wilcoxon = None, None\n","    print(f\"⚠️  Insufficient sample size (n={len(subjects)} < 6), skipping Wilcoxon test\\n\")\n","\n","# Cliff's δ effect size\n","def cliffs_delta(x, y):\n","    n1, n2 = len(x), len(y)\n","    delta = 0\n","    for i in x:\n","        for j in y:\n","            if i > j:\n","                delta += 1\n","            elif i < j:\n","                delta -= 1\n","    return delta / (n1 * n2)\n","\n","delta = cliffs_delta(f1_matrix[:, 0], f1_matrix[:, 1])\n","print(f\"Cliff's δ (minirocket vs tst):\")\n","print(f\"  δ = {delta:.4f}\")\n","\n","if abs(delta) < 0.147:\n","    magnitude = \"negligible\"\n","elif abs(delta) < 0.33:\n","    magnitude = \"small\"\n","elif abs(delta) < 0.474:\n","    magnitude = \"medium\"\n","else:\n","    magnitude = \"large\"\n","print(f\"  Effect size: {magnitude}\\n\")\n","\n","# Average ranks\n","avg_ranks = np.mean(rankdata(-f1_matrix, axis=1), axis=0)\n","print(f\"Average ranks (lower is better):\")\n","for model, rank in zip(models, avg_ranks):\n","    print(f\"  {model:15s}: {rank:.2f}\")\n","print()\n","\n","# Critical Difference plot\n","fig, ax = plt.subplots(figsize=(8, 2))\n","\n","lowv = min(avg_ranks) - 0.5\n","highv = max(avg_ranks) + 0.5\n","cline = 0.5\n","\n","for i, (name, rank) in enumerate(zip(models, avg_ranks)):\n","    ax.plot([rank, rank], [cline - 0.05, cline + 0.05], 'k-', linewidth=2)\n","    ax.text(rank, cline - 0.25, f'{rank:.2f}', ha='center', va='top', fontsize=10)\n","    ax.text(rank, cline + 0.25, name, ha='center', va='bottom', fontsize=11, weight='bold')\n","\n","ax.set_xlim(lowv, highv)\n","ax.set_ylim(0, 1)\n","ax.axis('off')\n","plt.tight_layout()\n","plt.savefig('figures/step18_cd.svg', format='svg', dpi=150, bbox_inches='tight')\n","plt.close()\n","print(\"✓ Saved: figures/step18_cd.svg\")\n","\n","# Save results\n","results = {\n","    'subjects': subjects,\n","    'models': models,\n","    'n_subjects': len(subjects),\n","    'f1_matrix': f1_matrix.tolist(),\n","    'wilcoxon_test': {\n","        'comparison': 'minirocket vs tst',\n","        'statistic': float(stat_wilcoxon) if stat_wilcoxon is not None else None,\n","        'p_value': float(p_wilcoxon) if p_wilcoxon is not None else None,\n","        'significant': bool(p_wilcoxon < 0.05) if p_wilcoxon is not None else None,\n","        'note': 'Skipped due to insufficient samples' if len(subjects) < 6 else None\n","    },\n","    'cliffs_delta': {\n","        'comparison': 'minirocket vs tst',\n","        'delta': float(delta),\n","        'magnitude': magnitude\n","    },\n","    'average_ranks': {model: float(rank) for model, rank in zip(models, avg_ranks)}\n","}\n","\n","with open('logs/step18_stats.json', 'w') as f:\n","    json.dump(results, f, indent=2)\n","\n","df_results = pd.DataFrame({\n","    'metric': ['Wilcoxon', 'Cliff\\'s δ', 'Avg Rank (MiniROCKET)', 'Avg Rank (TST)'],\n","    'value': [\n","        stat_wilcoxon if stat_wilcoxon is not None else np.nan,\n","        delta,\n","        avg_ranks[0],\n","        avg_ranks[1]\n","    ],\n","    'p_value': [\n","        p_wilcoxon if p_wilcoxon is not None else np.nan,\n","        np.nan,\n","        np.nan,\n","        np.nan\n","    ]\n","})\n","df_results.to_csv('logs/step18_stats.csv', index=False)\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"✓ Significance testing completed\")\n","print(\"✓ Results: logs/step18_stats.json\")\n","print(\"✓ CSV: logs/step18_stats.csv\")\n","print(\"✓ CD plot: figures/step18_cd.svg\")\n","print(\"=\" * 60 + \"\\n\")\n","\n","print(\"Step 18 completed\\n\" + \"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZjwAa5rNltBo","executionInfo":{"status":"ok","timestamp":1762954014825,"user_tz":0,"elapsed":54,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"f7af851b-b82c-4a9e-9ea9-21063274d2f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Step 18: Significance Testing\n","============================================================\n","Number of subjects: 15\n","Models: ['minirocket', 'tst']\n","\n","Subject × Model Macro-F1 matrix:\n","           minirocket       tst\n","proband1     0.666741  0.649242\n","proband10    0.802280  0.841033\n","proband11    0.906379  0.920052\n","proband12    0.915105  0.806431\n","proband13    0.896687  0.836988\n","proband14    0.614378  0.514166\n","proband15    0.887102  0.835436\n","proband2     0.962299  0.872871\n","proband3     0.763632  0.568620\n","proband4     0.551127  0.514923\n","proband5     0.761963  0.801733\n","proband6     0.909414  0.852789\n","proband7     0.654848  0.633691\n","proband8     0.651998  0.687805\n","proband9     0.875110  0.841656\n","\n","Wilcoxon signed-rank test (minirocket vs tst):\n","  Statistic = 21.0000\n","  p-value = 0.0256\n","  Significant: Yes (α=0.05)\n","\n","Cliff's δ (minirocket vs tst):\n","  δ = 0.2267\n","  Effect size: small\n","\n","Average ranks (lower is better):\n","  minirocket     : 1.27\n","  tst            : 1.73\n","\n","✓ Saved: figures/step18_cd.svg\n","\n","============================================================\n","✓ Significance testing completed\n","✓ Results: logs/step18_stats.json\n","✓ CSV: logs/step18_stats.csv\n","✓ CD plot: figures/step18_cd.svg\n","============================================================\n","\n","Step 18 completed\n","============================================================\n"]}]},{"cell_type":"code","source":["# ================ Step 19: Latency/Resource Evaluation ================\n","import os\n","os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n","os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n","os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from pathlib import Path\n","import json\n","import pickle\n","import time\n","import psutil\n","import platform\n","import subprocess\n","from threadpoolctl import threadpool_limits\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"\\n\\nStep 19: Latency/Resource Evaluation\")\n","print(\"=\" * 60)\n","\n","Path('figures').mkdir(parents=True, exist_ok=True)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def get_active_folds(path=\"logs/active_folds.json\"):\n","    p = Path(path)\n","    if p.exists():\n","        return json.loads(p.read_text())[\"folds\"]\n","    return []\n","\n","with open('configs/splits.json', 'r') as f:\n","    splits_cfg = json.load(f)\n","\n","with open('logs/step16_metrics_summary.json', 'r') as f:\n","    metrics_data = json.load(f)\n","\n","active_folds = get_active_folds()\n","N_REPEATS = 50\n","\n","# Hardware information\n","hw_info = {\n","    'cpu': platform.processor() or subprocess.getoutput(\"cat /proc/cpuinfo | grep 'model name' | head -1\").split(':')[1].strip(),\n","    'gpu': subprocess.getoutput(\"nvidia-smi --query-gpu=name --format=csv,noheader\") if torch.cuda.is_available() else 'N/A',\n","    'ram_gb': round(psutil.virtual_memory().total / (1024**3), 1),\n","    'python': platform.python_version(),\n","    'torch': torch.__version__,\n","    'numpy': np.__version__,\n","    'sklearn': __import__('sklearn').__version__,\n","    'sktime': __import__('sktime').__version__\n","}\n","\n","print(f\"Hardware information:\")\n","for k, v in hw_info.items():\n","    print(f\"  {k}: {v}\")\n","print()\n","\n","# TST model definition\n","class PatchEmbedding(nn.Module):\n","    def __init__(self, n_channels, seq_len, patch_len, d_model):\n","        super().__init__()\n","        self.patch_len = patch_len\n","        self.n_patches = seq_len // patch_len\n","        self.proj = nn.Linear(n_channels * patch_len, d_model)\n","\n","    def forward(self, x):\n","        B, C, L = x.shape\n","        x = x.unfold(2, self.patch_len, self.patch_len)\n","        x = x.permute(0, 2, 1, 3).contiguous()\n","        x = x.view(B, self.n_patches, -1)\n","        return self.proj(x)\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1)]\n","\n","class TST(nn.Module):\n","    def __init__(self, n_channels, seq_len, patch_len, num_classes, d_model, n_heads, depth, dropout):\n","        super().__init__()\n","        self.patch_embedding = PatchEmbedding(n_channels, seq_len, patch_len, d_model)\n","        self.pos_encoding = PositionalEncoding(d_model)\n","\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model,\n","            nhead=n_heads,\n","            dim_feedforward=d_model * 4,\n","            dropout=dropout,\n","            batch_first=True\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(d_model, num_classes)\n","\n","    def forward(self, x):\n","        x = self.patch_embedding(x)\n","        x = self.pos_encoding(x)\n","        x = self.transformer(x)\n","        x = x.mean(dim=1)\n","        x = self.dropout(x)\n","        return self.fc(x)\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters())\n","\n","def estimate_flops(model, input_shape):\n","    from thop import profile, clever_format\n","    dummy_input = torch.randn(1, *input_shape).to(next(model.parameters()).device)\n","    flops, params = profile(model, inputs=(dummy_input,), verbose=False)\n","    flops, params = clever_format([flops, params], \"%.3f\")\n","    return flops, params\n","\n","all_results = []\n","\n","for fold in splits_cfg['folds']:\n","    k = fold['fold']\n","\n","    if k not in active_folds:\n","        continue\n","\n","    test_subj = fold['test_subject']\n","\n","    print(f\"\\n{'='*60}\")\n","    print(f\"Fold {k}: test subject={test_subj}\")\n","    print(f\"{'='*60}\")\n","\n","    # Retrieve F1 scores\n","    mr_result = [r for r in metrics_data['per_fold_results'] if r['fold'] == k and r['method'] == 'minirocket'][0]\n","    tst_result = [r for r in metrics_data['per_fold_results'] if r['fold'] == k and r['method'] == 'tst'][0]\n","\n","    mr_f1 = mr_result['macro_f1']\n","    tst_f1 = tst_result['macro_f1']\n","\n","    # ============ MiniROCKET Resource Evaluation ============\n","    print(f\"\\n--- MiniROCKET ---\")\n","\n","    # Load test data (raw windows)\n","    norm_data = np.load(f'features/windows_normalized_fold{k}.npz')\n","    test_mask = norm_data['splits'] == 'test'\n","    CHANNELS = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']\n","    X_raw = np.stack([norm_data[ch] for ch in CHANNELS], axis=1)[test_mask]\n","    n_test = len(X_raw)\n","\n","    # Load transformer and Ridge\n","    with open(f'models/transformer_minirocket_fold{k}.pkl', 'rb') as f:\n","        transformer = pickle.load(f)\n","\n","    with open(f'models/ridge_fold{k}.pkl', 'rb') as f:\n","        model_data = pickle.load(f)\n","    ridge = model_data['ridge']\n","    vt = model_data['variance_filter']\n","\n","    # Single-sample test: feature generation time\n","    transform_times = []\n","    with threadpool_limits(limits=1, user_api='blas'):\n","        for _ in range(N_REPEATS):\n","            sample = X_raw[:1]\n","            start = time.perf_counter()\n","            features = transformer.transform(sample)\n","            if hasattr(features, 'values'):\n","                features = features.values\n","            transform_times.append((time.perf_counter() - start) * 1000)\n","\n","    # Single-sample test: classification time\n","    sample_features = transformer.transform(X_raw[:1])\n","    if hasattr(sample_features, 'values'):\n","        sample_features = sample_features.values\n","    sample_features = vt.transform(sample_features)\n","\n","    clf_times = []\n","    with threadpool_limits(limits=1, user_api='blas'):\n","        for _ in range(N_REPEATS):\n","            start = time.perf_counter()\n","            _ = ridge.predict(sample_features)\n","            clf_times.append((time.perf_counter() - start) * 1000)\n","\n","    transform_p50 = np.percentile(transform_times, 50)\n","    transform_p90 = np.percentile(transform_times, 90)\n","    clf_p50 = np.percentile(clf_times, 50)\n","    clf_p90 = np.percentile(clf_times, 90)\n","    total_p50 = transform_p50 + clf_p50\n","    total_p90 = transform_p90 + clf_p90\n","\n","    # Model size\n","    transformer_size = Path(f'models/transformer_minirocket_fold{k}.pkl').stat().st_size / (1024**2)\n","    ridge_size = Path(f'models/ridge_fold{k}.pkl').stat().st_size / (1024**2)\n","    total_size = transformer_size + ridge_size\n","\n","    print(f\"  Transform time: p50={transform_p50:.3f}ms, p90={transform_p90:.3f}ms\")\n","    print(f\"  Classification time: p50={clf_p50:.3f}ms, p90={clf_p90:.3f}ms\")\n","    print(f\"  Total latency: p50={total_p50:.3f}ms, p90={total_p90:.3f}ms\")\n","    print(f\"  Model size: {total_size:.2f}MB (transformer={transformer_size:.2f}MB, ridge={ridge_size:.2f}MB)\")\n","    print(f\"  Macro F1: {mr_f1:.4f}\")\n","\n","    # ============ TST Resource Evaluation ============\n","    print(f\"\\n--- TST ---\")\n","\n","    # Load model\n","    checkpoint = torch.load(f'models/tst_fold{k}.pt', weights_only=False, map_location=device)\n","    model_cfg = checkpoint['model_config']\n","\n","    model = TST(**model_cfg).to(device)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.eval()\n","\n","    # Parameter count\n","    n_params = count_parameters(model)\n","\n","    # FLOPs\n","    try:\n","        flops, _ = estimate_flops(model, (model_cfg['n_channels'], model_cfg['seq_len']))\n","    except:\n","        flops = 'N/A'\n","\n","    # Latency test\n","    tensors = torch.load(f'interim/tensors_fold{k}.pt', weights_only=False)\n","    X_test = tensors['X_test']\n","\n","    latencies = []\n","    with torch.inference_mode():\n","        for _ in range(N_REPEATS):\n","            sample = X_test[:1].to(device)\n","            if device.type == 'cuda':\n","                torch.cuda.synchronize()\n","            start = time.perf_counter()\n","            _ = model(sample)\n","            if device.type == 'cuda':\n","                torch.cuda.synchronize()\n","            latencies.append((time.perf_counter() - start) * 1000)\n","\n","    latency_p50 = np.percentile(latencies, 50)\n","    latency_p90 = np.percentile(latencies, 90)\n","\n","    # Peak memory (approximate)\n","    if device.type == 'cuda':\n","        torch.cuda.reset_peak_memory_stats()\n","        with torch.inference_mode():\n","            sample = X_test[:1].to(device)\n","            _ = model(sample)\n","        peak_mem_mb = torch.cuda.max_memory_allocated() / (1024**2)\n","    else:\n","        peak_mem_mb = None\n","\n","    # Model size\n","    tst_size = Path(f'models/tst_fold{k}.pt').stat().st_size / (1024**2)\n","\n","    print(f\"  #Parameters: {n_params:,}\")\n","    print(f\"  FLOPs: {flops}\")\n","    print(f\"  Latency: p50={latency_p50:.3f}ms, p90={latency_p90:.3f}ms\")\n","    if peak_mem_mb:\n","        print(f\"  Peak memory: {peak_mem_mb:.2f}MB\")\n","    print(f\"  Model size: {tst_size:.2f}MB\")\n","    print(f\"  Macro F1: {tst_f1:.4f}\")\n","\n","    all_results.append({\n","        'fold': k,\n","        'test_subject': test_subj,\n","        'method': 'minirocket',\n","        'macro_f1': float(mr_f1),\n","        'transform_time_p50_ms': float(transform_p50),\n","        'transform_time_p90_ms': float(transform_p90),\n","        'clf_time_p50_ms': float(clf_p50),\n","        'clf_time_p90_ms': float(clf_p90),\n","        'total_latency_p50_ms': float(total_p50),\n","        'total_latency_p90_ms': float(total_p90),\n","        'model_size_mb': float(total_size),\n","        'params': None,\n","        'flops': None\n","    })\n","\n","    all_results.append({\n","        'fold': k,\n","        'test_subject': test_subj,\n","        'method': 'tst',\n","        'macro_f1': float(tst_f1),\n","        'transform_time_p50_ms': None,\n","        'transform_time_p90_ms': None,\n","        'clf_time_p50_ms': None,\n","        'clf_time_p90_ms': None,\n","        'total_latency_p50_ms': float(latency_p50),\n","        'total_latency_p90_ms': float(latency_p90),\n","        'model_size_mb': float(tst_size),\n","        'params': int(n_params),\n","        'flops': str(flops)\n","    })\n","\n","# Save results\n","df_results = pd.DataFrame(all_results)\n","df_results.to_csv('logs/step19_resources.csv', index=False)\n","\n","# Aggregated statistics\n","agg_results = []\n","for method in ['minirocket', 'tst']:\n","    method_data = df_results[df_results['method'] == method]\n","\n","    agg = {\n","        'method': method,\n","        'macro_f1_mean': method_data['macro_f1'].mean(),\n","        'total_latency_p50_mean_ms': method_data['total_latency_p50_ms'].mean(),\n","        'total_latency_p90_mean_ms': method_data['total_latency_p90_ms'].mean(),\n","        'model_size_mean_mb': method_data['model_size_mb'].mean()\n","    }\n","\n","    if method == 'minirocket':\n","        agg['transform_time_p50_mean_ms'] = method_data['transform_time_p50_ms'].mean()\n","        agg['transform_time_p90_mean_ms'] = method_data['transform_time_p90_ms'].mean()\n","        agg['clf_time_p50_mean_ms'] = method_data['clf_time_p50_ms'].mean()\n","        agg['clf_time_p90_mean_ms'] = method_data['clf_time_p90_ms'].mean()\n","    else:\n","        agg['params'] = int(method_data['params'].iloc[0])\n","        agg['flops'] = method_data['flops'].iloc[0]\n","\n","    agg_results.append(agg)\n","\n","df_agg = pd.DataFrame(agg_results)\n","df_agg.to_csv('logs/step19_resources_agg.csv', index=False)\n","\n","# Pareto plots\n","fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n","\n","# F1 vs Latency\n","ax = axes[0]\n","for method in ['minirocket', 'tst']:\n","    data = df_results[df_results['method'] == method]\n","    ax.scatter(data['total_latency_p50_ms'], data['macro_f1'],\n","              label=method.upper(), s=100, alpha=0.7)\n","\n","avg_mr = df_results[df_results['method'] == 'minirocket']\n","avg_tst = df_results[df_results['method'] == 'tst']\n","ax.scatter(avg_mr['total_latency_p50_ms'].mean(), avg_mr['macro_f1'].mean(),\n","          marker='*', s=300, c='C0', edgecolors='black', linewidths=1.5, label='MiniROCKET (avg)')\n","ax.scatter(avg_tst['total_latency_p50_ms'].mean(), avg_tst['macro_f1'].mean(),\n","          marker='*', s=300, c='C1', edgecolors='black', linewidths=1.5, label='TST (avg)')\n","\n","ax.set_xlabel('Latency p50 (ms)', fontsize=11)\n","ax.set_ylabel('Macro F1', fontsize=11)\n","ax.set_title('F1 vs Latency', fontsize=12, weight='bold')\n","ax.legend()\n","ax.grid(alpha=0.3)\n","\n","# F1 vs Model Size\n","ax = axes[1]\n","for method in ['minirocket', 'tst']:\n","    data = df_results[df_results['method'] == method]\n","    ax.scatter(data['model_size_mb'], data['macro_f1'],\n","              label=method.upper(), s=100, alpha=0.7)\n","\n","ax.scatter(avg_mr['model_size_mb'].mean(), avg_mr['macro_f1'].mean(),\n","          marker='*', s=300, c='C0', edgecolors='black', linewidths=1.5, label='MiniROCKET (avg)')\n","ax.scatter(avg_tst['model_size_mb'].mean(), avg_tst['macro_f1'].mean(),\n","          marker='*', s=300, c='C1', edgecolors='black', linewidths=1.5, label='TST (avg)')\n","\n","ax.set_xlabel('Model Size (MB)', fontsize=11)\n","ax.set_ylabel('Macro F1', fontsize=11)\n","ax.set_title('F1 vs Model Size', fontsize=12, weight='bold')\n","ax.legend()\n","ax.grid(alpha=0.3)\n","\n","# Latency vs Model Size\n","ax = axes[2]\n","for method in ['minirocket', 'tst']:\n","    data = df_results[df_results['method'] == method]\n","    ax.scatter(data['model_size_mb'], data['total_latency_p50_ms'],\n","              label=method.upper(), s=100, alpha=0.7)\n","\n","ax.scatter(avg_mr['model_size_mb'].mean(), avg_mr['total_latency_p50_ms'].mean(),\n","          marker='*', s=300, c='C0', edgecolors='black', linewidths=1.5, label='MiniROCKET (avg)')\n","ax.scatter(avg_tst['model_size_mb'].mean(), avg_tst['total_latency_p50_ms'].mean(),\n","          marker='*', s=300, c='C1', edgecolors='black', linewidths=1.5, label='TST (avg)')\n","\n","ax.set_xlabel('Model Size (MB)', fontsize=11)\n","ax.set_ylabel('Latency p50 (ms)', fontsize=11)\n","ax.set_title('Latency vs Model Size', fontsize=12, weight='bold')\n","ax.legend()\n","ax.grid(alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig('figures/step19_pareto.svg', format='svg', dpi=150)\n","plt.close()\n","\n","# Save full report\n","report = {\n","    'hardware': hw_info,\n","    'config': {\n","        'cpu_threads': 1,\n","        'batch_size': 1,\n","        'n_repeats': N_REPEATS,\n","        'platform': device.type\n","    },\n","    'per_fold_results': all_results,\n","    'aggregated_results': agg_results,\n","    'notes': {\n","        'minirocket': 'Transform time + Classification time (both CPU single-thread)',\n","        'tst': 'End-to-end inference time',\n","        'fairness': 'MiniROCKET includes preprocessing, TST is raw end-to-end'\n","    }\n","}\n","\n","with open('logs/step19_resources.json', 'w') as f:\n","    json.dump(report, f, indent=2)\n","\n","print(f\"\\n{'='*60}\")\n","print(f\"✓ Completed resource evaluation for {len(active_folds)} folds\")\n","print(f\"✓ Per-fold results: logs/step19_resources.csv\")\n","print(f\"✓ Aggregated results: logs/step19_resources_agg.csv\")\n","print(f\"✓ JSON report: logs/step19_resources.json\")\n","print(f\"✓ Pareto plot: figures/step19_pareto.svg\")\n","print(f\"\\nSummary:\")\n","print(df_agg.to_string(index=False))\n","print(f\"{'='*60}\\n\")\n","\n","print(f\"Step 19 completed\\n{'='*60}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MWSFzJW7lyee","executionInfo":{"status":"ok","timestamp":1762954032125,"user_tz":0,"elapsed":17297,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"51d03a35-59fd-4eab-b2e2-76b6f0184040"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Step 19: Latency/Resource Evaluation\n","============================================================\n","Hardware information:\n","  cpu: x86_64\n","  gpu: NVIDIA L4\n","  ram_gb: 53.0\n","  python: 3.12.12\n","  torch: 2.8.0+cu126\n","  numpy: 1.26.4\n","  sklearn: 1.4.2\n","  sktime: 0.30.0\n","\n","\n","============================================================\n","Fold 0: test subject=proband1\n","============================================================\n","\n","--- MiniROCKET ---\n","  Transform time: p50=4.423ms, p90=4.542ms\n","  Classification time: p50=0.086ms, p90=0.108ms\n","  Total latency: p50=4.509ms, p90=4.650ms\n","  Model size: 0.44MB (transformer=0.06MB, ridge=0.38MB)\n","  Macro F1: 0.6667\n","\n","--- TST ---\n","  #Parameters: 210,120\n","  FLOPs: N/A\n","  Latency: p50=1.615ms, p90=1.644ms\n","  Peak memory: 21.33MB\n","  Model size: 2.04MB\n","  Macro F1: 0.6492\n","\n","============================================================\n","Fold 1: test subject=proband10\n","============================================================\n","\n","--- MiniROCKET ---\n","  Transform time: p50=4.428ms, p90=4.538ms\n","  Classification time: p50=0.082ms, p90=0.102ms\n","  Total latency: p50=4.510ms, p90=4.640ms\n","  Model size: 0.44MB (transformer=0.06MB, ridge=0.38MB)\n","  Macro F1: 0.8023\n","\n","--- TST ---\n","  #Parameters: 210,120\n","  FLOPs: N/A\n","  Latency: p50=1.585ms, p90=1.652ms\n","  Peak memory: 21.41MB\n","  Model size: 2.04MB\n","  Macro F1: 0.8410\n","\n","============================================================\n","Fold 2: test subject=proband11\n","============================================================\n","\n","--- MiniROCKET ---\n","  Transform time: p50=4.415ms, p90=4.589ms\n","  Classification time: p50=0.082ms, p90=0.102ms\n","  Total latency: p50=4.496ms, p90=4.691ms\n","  Model size: 0.44MB (transformer=0.06MB, ridge=0.38MB)\n","  Macro F1: 0.9064\n","\n","--- TST ---\n","  #Parameters: 210,120\n","  FLOPs: N/A\n","  Latency: p50=1.606ms, p90=1.640ms\n","  Peak memory: 21.41MB\n","  Model size: 2.04MB\n","  Macro F1: 0.9201\n","\n","============================================================\n","Fold 3: test subject=proband12\n","============================================================\n","\n","--- MiniROCKET ---\n","  Transform time: p50=4.395ms, p90=4.458ms\n","  Classification time: p50=0.084ms, p90=0.116ms\n","  Total latency: p50=4.479ms, p90=4.574ms\n","  Model size: 0.44MB (transformer=0.06MB, ridge=0.38MB)\n","  Macro F1: 0.9151\n","\n","--- TST ---\n","  #Parameters: 210,120\n","  FLOPs: N/A\n","  Latency: p50=1.567ms, p90=1.640ms\n","  Peak memory: 21.33MB\n","  Model size: 2.04MB\n","  Macro F1: 0.8064\n","\n","============================================================\n","Fold 4: test subject=proband13\n","============================================================\n","\n","--- MiniROCKET ---\n","  Transform time: p50=4.407ms, p90=4.492ms\n","  Classification time: p50=0.084ms, p90=0.104ms\n","  Total latency: p50=4.491ms, p90=4.596ms\n","  Model size: 0.44MB (transformer=0.06MB, ridge=0.38MB)\n","  Macro F1: 0.8967\n","\n","--- TST ---\n","  #Parameters: 210,120\n","  FLOPs: N/A\n","  Latency: p50=1.591ms, p90=1.649ms\n","  Peak memory: 21.41MB\n","  Model size: 2.04MB\n","  Macro F1: 0.8370\n","\n","============================================================\n","Fold 5: test subject=proband14\n","============================================================\n","\n","--- MiniROCKET ---\n","  Transform time: p50=4.407ms, p90=4.475ms\n","  Classification time: p50=0.079ms, p90=0.102ms\n","  Total latency: p50=4.486ms, p90=4.577ms\n","  Model size: 0.44MB (transformer=0.06MB, ridge=0.38MB)\n","  Macro F1: 0.6144\n","\n","--- TST ---\n","  #Parameters: 210,120\n","  FLOPs: N/A\n","  Latency: p50=1.610ms, p90=1.757ms\n","  Peak memory: 21.41MB\n","  Model size: 2.04MB\n","  Macro F1: 0.5142\n","\n","============================================================\n","Fold 6: test subject=proband15\n","============================================================\n","\n","--- MiniROCKET ---\n","  Transform time: p50=4.395ms, p90=4.461ms\n","  Classification time: p50=0.081ms, p90=0.102ms\n","  Total latency: p50=4.476ms, p90=4.563ms\n","  Model size: 0.44MB (transformer=0.06MB, ridge=0.38MB)\n","  Macro F1: 0.8871\n","\n","--- TST ---\n","  #Parameters: 210,120\n","  FLOPs: N/A\n","  Latency: p50=1.601ms, p90=1.638ms\n","  Peak memory: 21.33MB\n","  Model size: 2.04MB\n","  Macro F1: 0.8354\n","\n","============================================================\n","Fold 7: test subject=proband2\n","============================================================\n","\n","--- MiniROCKET ---\n","  Transform time: p50=4.420ms, p90=4.477ms\n","  Classification time: p50=0.083ms, p90=0.105ms\n","  Total latency: p50=4.502ms, p90=4.582ms\n","  Model size: 0.44MB (transformer=0.06MB, ridge=0.38MB)\n","  Macro F1: 0.9623\n","\n","--- TST ---\n","  #Parameters: 210,120\n","  FLOPs: N/A\n","  Latency: p50=1.591ms, p90=1.634ms\n","  Peak memory: 21.41MB\n","  Model size: 2.04MB\n","  Macro F1: 0.8729\n","\n","============================================================\n","Fold 8: test subject=proband3\n","============================================================\n","\n","--- MiniROCKET ---\n","  Transform time: p50=4.407ms, p90=4.470ms\n","  Classification time: p50=0.085ms, p90=0.104ms\n","  Total latency: p50=4.492ms, p90=4.574ms\n","  Model size: 0.44MB (transformer=0.06MB, ridge=0.38MB)\n","  Macro F1: 0.7636\n","\n","--- TST ---\n","  #Parameters: 210,120\n","  FLOPs: N/A\n","  Latency: p50=1.623ms, p90=1.681ms\n","  Peak memory: 21.41MB\n","  Model size: 2.04MB\n","  Macro F1: 0.5686\n","\n","============================================================\n","Fold 9: test subject=proband4\n","============================================================\n","\n","--- MiniROCKET ---\n","  Transform time: p50=4.403ms, p90=4.456ms\n","  Classification time: p50=0.080ms, p90=0.102ms\n","  Total latency: p50=4.484ms, p90=4.558ms\n","  Model size: 0.44MB (transformer=0.06MB, ridge=0.38MB)\n","  Macro F1: 0.5511\n","\n","--- TST ---\n","  #Parameters: 210,120\n","  FLOPs: N/A\n","  Latency: p50=1.612ms, p90=1.837ms\n","  Peak memory: 21.33MB\n","  Model size: 2.04MB\n","  Macro F1: 0.5149\n","\n","============================================================\n","Fold 10: test subject=proband5\n","============================================================\n","\n","--- MiniROCKET ---\n","  Transform time: p50=4.387ms, p90=4.456ms\n","  Classification time: p50=0.080ms, p90=0.106ms\n","  Total latency: p50=4.467ms, p90=4.562ms\n","  Model size: 0.44MB (transformer=0.06MB, ridge=0.38MB)\n","  Macro F1: 0.7620\n","\n","--- TST ---\n","  #Parameters: 210,120\n","  FLOPs: N/A\n","  Latency: p50=1.599ms, p90=1.657ms\n","  Peak memory: 21.41MB\n","  Model size: 2.04MB\n","  Macro F1: 0.8017\n","\n","============================================================\n","Fold 11: test subject=proband6\n","============================================================\n","\n","--- MiniROCKET ---\n","  Transform time: p50=4.383ms, p90=4.480ms\n","  Classification time: p50=0.083ms, p90=0.106ms\n","  Total latency: p50=4.465ms, p90=4.586ms\n","  Model size: 0.44MB (transformer=0.06MB, ridge=0.38MB)\n","  Macro F1: 0.9094\n","\n","--- TST ---\n","  #Parameters: 210,120\n","  FLOPs: N/A\n","  Latency: p50=1.576ms, p90=1.621ms\n","  Peak memory: 21.41MB\n","  Model size: 2.04MB\n","  Macro F1: 0.8528\n","\n","============================================================\n","Fold 12: test subject=proband7\n","============================================================\n","\n","--- MiniROCKET ---\n","  Transform time: p50=4.384ms, p90=4.460ms\n","  Classification time: p50=0.081ms, p90=0.100ms\n","  Total latency: p50=4.465ms, p90=4.561ms\n","  Model size: 0.44MB (transformer=0.06MB, ridge=0.38MB)\n","  Macro F1: 0.6548\n","\n","--- TST ---\n","  #Parameters: 210,120\n","  FLOPs: N/A\n","  Latency: p50=1.584ms, p90=1.633ms\n","  Peak memory: 21.33MB\n","  Model size: 2.04MB\n","  Macro F1: 0.6337\n","\n","============================================================\n","Fold 13: test subject=proband8\n","============================================================\n","\n","--- MiniROCKET ---\n","  Transform time: p50=4.396ms, p90=4.538ms\n","  Classification time: p50=0.081ms, p90=0.100ms\n","  Total latency: p50=4.478ms, p90=4.638ms\n","  Model size: 0.44MB (transformer=0.06MB, ridge=0.38MB)\n","  Macro F1: 0.6520\n","\n","--- TST ---\n","  #Parameters: 210,120\n","  FLOPs: N/A\n","  Latency: p50=1.593ms, p90=1.727ms\n","  Peak memory: 21.41MB\n","  Model size: 2.04MB\n","  Macro F1: 0.6878\n","\n","============================================================\n","Fold 14: test subject=proband9\n","============================================================\n","\n","--- MiniROCKET ---\n","  Transform time: p50=4.422ms, p90=4.481ms\n","  Classification time: p50=0.080ms, p90=0.100ms\n","  Total latency: p50=4.502ms, p90=4.581ms\n","  Model size: 0.44MB (transformer=0.06MB, ridge=0.38MB)\n","  Macro F1: 0.8751\n","\n","--- TST ---\n","  #Parameters: 210,120\n","  FLOPs: N/A\n","  Latency: p50=1.601ms, p90=1.643ms\n","  Peak memory: 21.41MB\n","  Model size: 2.04MB\n","  Macro F1: 0.8417\n","\n","============================================================\n","✓ Completed resource evaluation for 15 folds\n","✓ Per-fold results: logs/step19_resources.csv\n","✓ Aggregated results: logs/step19_resources_agg.csv\n","✓ JSON report: logs/step19_resources.json\n","✓ Pareto plot: figures/step19_pareto.svg\n","\n","Summary:\n","    method  macro_f1_mean  total_latency_p50_mean_ms  total_latency_p90_mean_ms  model_size_mean_mb  transform_time_p50_mean_ms  transform_time_p90_mean_ms  clf_time_p50_mean_ms  clf_time_p90_mean_ms   params flops\n","minirocket       0.787937                   4.486892                   4.595466            0.440078                    4.404745                    4.491608              0.082147              0.103858      NaN   NaN\n","       tst       0.745162                   1.596821                   1.670143            2.042066                         NaN                         NaN                   NaN                   NaN 210120.0   N/A\n","============================================================\n","\n","Step 19 completed\n","============================================================\n"]}]}]}