{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyOBmRHsWEZtjLdJ1UlzwfLq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"py8U4wWia3Mv","executionInfo":{"status":"ok","timestamp":1763394093486,"user_tz":0,"elapsed":3616,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"7727bcd5-e0b7-48e0-9902-b870ca6eb8b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Nov 17 15:41:30 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["# Inspect available GPU devices\n","!nvidia-smi\n","\n","# Install RAPIDS (cuML) + CuPy + NVML\n","# Note: Official RAPIDS wheels are hosted on NVIDIA's PyPI, so --extra-index-url is required\n","!pip -q install --extra-index-url=https://pypi.nvidia.com \\\n","    rmm-cu12 cudf-cu12 cuml-cu12 \\\n","    cupy-cuda12x pynvml scikit-learn"]},{"cell_type":"code","source":["# ================ Step 19: GPU Energy Consumption (NVML Integration Method) ================\n","import os, json, math, time, multiprocessing as mp, pathlib, sys, subprocess\n","import numpy as np\n","import pandas as pd\n","\n","# Multi-backend GPU synchronization (prioritize CuPy, then PyTorch)\n","def gpu_sync():\n","    try:\n","        import cupy as cp\n","        cp.cuda.runtime.deviceSynchronize()\n","    except Exception:\n","        pass\n","    try:\n","        import torch\n","        if torch.cuda.is_available():\n","            torch.cuda.synchronize()\n","    except Exception:\n","        pass\n","\n","# --------- NVML sampling subprocess ---------\n","def nvml_sampler(stop_event, q, dev_index=0, interval=0.02):\n","    \"\"\"\n","    Periodically read NVML power (mW) and push (t_absolute, power_mW) into the queue.\n","    t uses the absolute monotonic clock time.perf_counter(), so it can be aligned with the main process.\n","    \"\"\"\n","    import pynvml, time\n","    pynvml.nvmlInit()\n","    h = pynvml.nvmlDeviceGetHandleByIndex(dev_index)\n","    try:\n","        while not stop_event.is_set():\n","            t = time.perf_counter()\n","            p_mw = pynvml.nvmlDeviceGetPowerUsage(h)  # milliwatt\n","            q.put((t, p_mw))\n","            time.sleep(interval)\n","    finally:\n","        pynvml.nvmlShutdown()\n","\n","def integrate_energy_mJ_between(samples, t0, t1):\n","    \"\"\"\n","    Perform trapezoidal integration of the power–time curve over [t0, t1].\n","    samples: List[(t, mW)] (t is the absolute time from perf_counter)\n","    Return energy in mJ.\n","    \"\"\"\n","    if not samples:\n","        return 0.0\n","    samples = sorted(samples, key=lambda x: x[0])\n","    ts = np.array([t for t,_ in samples], dtype=np.float64)\n","    ps = np.array([p for _,p in samples], dtype=np.float64)  # mW\n","    # Select the interval and interpolate the endpoints\n","    mask = (ts >= t0) & (ts <= t1)\n","    ts_win = ts[mask]\n","    ps_win = ps[mask]\n","    if ts_win.size == 0 or ts_win[0] > t0:\n","        p0 = np.interp(t0, ts, ps)\n","        ts_win = np.insert(ts_win, 0, t0)\n","        ps_win = np.insert(ps_win, 0, p0)\n","    if ts_win[-1] < t1:\n","        p1 = np.interp(t1, ts, ps)\n","        ts_win = np.append(ts_win, t1)\n","        ps_win = np.append(ps_win, p1)\n","    # Integration: mW * s = mJ\n","    E_mJ = float(np.trapz(ps_win, ts_win))\n","    return E_mJ\n","\n","def sample_idle_power_mW(duration_s=20.0, dev_index=0, interval=0.02, save_csv=None):\n","    \"\"\"\n","    Sample the idle power and return the mean power (mW) and the power trace.\n","    \"\"\"\n","    q = mp.Queue()\n","    stop = mp.Event()\n","    proc = mp.Process(target=nvml_sampler, args=(stop, q, dev_index, interval))\n","    proc.start()\n","    t_begin = time.perf_counter()\n","    time.sleep(duration_s)\n","    stop.set(); proc.join()\n","    samples = []\n","    while not q.empty():\n","        samples.append(q.get())\n","    if not samples:\n","        raise RuntimeError(\"NVML did not return any power samples.\")\n","    samples = sorted(samples, key=lambda x: x[0])\n","    t0, t1 = samples[0][0], samples[-1][0]\n","    E_idle_mJ = integrate_energy_mJ_between(samples, t0, t1)\n","    T_idle_s  = max(1e-9, (t1 - t0))\n","    P_idle_mW = E_idle_mJ / T_idle_s\n","    if save_csv:\n","        df = pd.DataFrame(samples, columns=[\"t_abs_s\",\"power_mW\"])\n","        df.to_csv(save_csv, index=False)\n","    return P_idle_mW, samples\n","\n","def calibrate_repeats(run_once, target_s=8.0, min_rep=3, max_rep=2000):\n","    \"\"\"\n","    Adaptively estimate how many repetitions are required to obtain an effective\n","    measurement window of length target_s.\n","    \"\"\"\n","    gpu_sync()\n","    t0 = time.perf_counter(); run_once(); gpu_sync(); t1 = time.perf_counter()\n","    dt = max(1e-4, t1 - t0)\n","    reps = int(math.ceil(target_s / dt))\n","    return int(np.clip(reps, min_rep, max_rep))\n","\n","def measure_mJ_per_inference(run_once, n_items_per_call:int, repeats:int,\n","                             P_idle_mW:float, dev_index=0, interval=0.02,\n","                             save_csv=None):\n","    \"\"\"\n","    Sample power concurrently while run_once() is executed repeats times,\n","    perform integration with idle power subtraction, and return mJ per inference\n","    along with detailed statistics.\n","    \"\"\"\n","    q = mp.Queue()\n","    stop = mp.Event()\n","    proc = mp.Process(target=nvml_sampler, args=(stop, q, dev_index, interval))\n","    proc.start()\n","\n","    gpu_sync()\n","    t0 = time.perf_counter()\n","    for _ in range(repeats):\n","        run_once()\n","    gpu_sync()\n","    t1 = time.perf_counter()\n","\n","    stop.set(); proc.join()\n","    samples = []\n","    while not q.empty():\n","        samples.append(q.get())\n","    if not samples:\n","        raise RuntimeError(\"NVML did not return any power samples (active phase).\")\n","\n","    E_total_mJ = integrate_energy_mJ_between(samples, t0, t1)\n","    T_total_s  = max(1e-9, (t1 - t0))\n","    E_idle_mJ  = P_idle_mW * T_total_s\n","    n_inf      = max(1, repeats * n_items_per_call)\n","    mJ_per_inf = max(0.0, (E_total_mJ - E_idle_mJ) / n_inf)\n","    ms_per_inf = (T_total_s / n_inf) * 1e3\n","    throughput = n_inf / T_total_s\n","\n","    if save_csv:\n","        df = pd.DataFrame(samples, columns=[\"t_abs_s\",\"power_mW\"])\n","        df.to_csv(save_csv, index=False)\n","\n","    return {\n","        \"mJ_per_inf\": mJ_per_inf,\n","        \"ms_per_inf\": ms_per_inf,\n","        \"throughput_inf_per_s\": throughput,\n","        \"n_inferences\": n_inf,\n","        \"repeats\": repeats,\n","        \"T_total_s\": T_total_s,\n","        \"E_total_mJ\": E_total_mJ,\n","        \"E_idle_mJ\": E_idle_mJ,\n","        \"P_idle_mW\": P_idle_mW,\n","        \"t0_abs\": t0, \"t1_abs\": t1,\n","    }\n","\n","# ========== Construct synthetic data and train cuML models ==========\n","print(\"\\n[Info] Preparing data and models (cuML KNN / RandomForest on GPU)\")\n","import cupy as cp\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from cuml.neighbors import KNeighborsClassifier as cuKNN\n","from cuml.ensemble import RandomForestClassifier as cuRF\n","\n","# Generate a medium-scale dataset (can be increased if needed)\n","X, y = make_classification(n_samples=30000, n_features=64, n_informative=48,\n","                           n_redundant=0, n_classes=8, random_state=7)\n","X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=5000, random_state=42)\n","\n","# Move data to GPU (float32)\n","Xtr = cp.asarray(X_tr, dtype=cp.float32); ytr = cp.asarray(y_tr.astype(np.int32))\n","Xte = cp.asarray(X_te, dtype=cp.float32); yte = cp.asarray(y_te.astype(np.int32))\n","\n","print(f\"Train: {Xtr.shape}, Test: {Xte.shape}\")\n","\n","# KNN (brute-force, the most stable GPU path)\n","knn = cuKNN(n_neighbors=5, algorithm=\"brute\", metric=\"euclidean\")\n","knn.fit(Xtr, ytr)\n","gpu_sync()\n","\n","# Random Forest (GPU)\n","rf = cuRF(n_estimators=100, max_depth=16, n_bins=128, bootstrap=True, n_streams=8)\n","rf.fit(Xtr, ytr)\n","gpu_sync()\n","\n","# ========== Warm-up phase ==========\n","def _run_knn_once():\n","    _ = knn.predict(Xte)\n","    gpu_sync()\n","\n","def _run_rf_once():\n","    _ = rf.predict(Xte)\n","    gpu_sync()\n","\n","print(\"\\n[Info] Warming up ...\")\n","for _ in range(30):\n","    _run_knn_once()\n","    _run_rf_once()\n","gpu_sync()\n","\n","# ========== Create log directories ==========\n","pathlib.Path(\"logs\").mkdir(exist_ok=True)\n","pathlib.Path(\"figures\").mkdir(exist_ok=True)\n","\n","# ========== Idle-power baseline ==========\n","print(\"\\n[Info] Sampling idle power (20 s) ...\")\n","P_idle_mW, idle_trace = sample_idle_power_mW(\n","    duration_s=20.0, dev_index=0, interval=0.02, save_csv=\"logs/power_idle_trace.csv\"\n",")\n","print(f\"[Info] Mean idle power ~ {P_idle_mW:.1f} mW\")\n","\n","# ========== Adaptively determine repeats (ensure effective window ≥ 8 s) ==========\n","rep_knn = calibrate_repeats(_run_knn_once, target_s=8.0, min_rep=5, max_rep=2000)\n","rep_rf  = calibrate_repeats(_run_rf_once,  target_s=8.0, min_rep=5, max_rep=2000)\n","print(f\"[Info] KNN repeats = {rep_knn}, RF repeats = {rep_rf}\")\n","\n","# ========== Multiple repeated measurements with bootstrap confidence intervals ==========\n","def measure_with_bootstrap(name, run_once, n_items, repeats, n_runs=5, n_boot=1000):\n","    results = []\n","    for i in range(n_runs):\n","        print(f\"[Measure] {name} - run {i+1}/{n_runs} ...\")\n","        res = measure_mJ_per_inference(\n","            run_once, n_items_per_call=n_items, repeats=repeats,\n","            P_idle_mW=P_idle_mW, dev_index=0, interval=0.02,\n","            save_csv=f\"logs/power_trace_{name}_run{i+1}.csv\"\n","        )\n","        results.append(res)\n","\n","    mJs = np.array([r[\"mJ_per_inf\"] for r in results], dtype=np.float64)\n","    # Percentile bootstrap confidence interval (simple and robust)\n","    rng = np.random.default_rng(123)\n","    boots = []\n","    for _ in range(n_boot):\n","        idx = rng.integers(0, len(mJs), size=len(mJs))\n","        boots.append(float(np.mean(mJs[idx])))\n","    ci_low, ci_high = np.percentile(boots, [2.5, 97.5])\n","    summary = {\n","        \"model\": name,\n","        \"mean_mJ_per_inf\": float(np.mean(mJs)),\n","        \"ci95_low\": float(ci_low),\n","        \"ci95_high\": float(ci_high),\n","        \"runs\": results,\n","    }\n","    with open(f\"logs/energy_{name}.json\", \"w\") as f:\n","        json.dump(summary, f, indent=2)\n","    print(f\"[Result] {name}: {summary['mean_mJ_per_inf']:.3f} mJ/inf \"\n","          f\"(95% CI [{summary['ci95_low']:.3f}, {summary['ci95_high']:.3f}])\")\n","    return summary\n","\n","print(\"\\n[Info] Starting KNN measurement ...\")\n","sum_knn = measure_with_bootstrap(\n","    name=\"knn_cuml\",\n","    run_once=_run_knn_once,\n","    n_items=Xte.shape[0],\n","    repeats=rep_knn,\n","    n_runs=5, n_boot=1000\n",")\n","\n","print(\"\\n[Info] Starting RandomForest measurement ...\")\n","sum_rf = measure_with_bootstrap(\n","    name=\"rf_cuml\",\n","    run_once=_run_rf_once,\n","    n_items=Xte.shape[0],\n","    repeats=rep_rf,\n","    n_runs=5, n_boot=1000\n",")\n","\n","# Summary table\n","df_sum = pd.DataFrame([\n","    {\"model\":\"KNN (cuML)\", \"mJ/inf_mean\":sum_knn[\"mean_mJ_per_inf\"],\n","     \"CI95_low\":sum_knn[\"ci95_low\"], \"CI95_high\":sum_knn[\"ci95_high\"]},\n","    {\"model\":\"RandomForest (cuML)\", \"mJ/inf_mean\":sum_rf[\"mean_mJ_per_inf\"],\n","     \"CI95_low\":sum_rf[\"ci95_low\"], \"CI95_high\":sum_rf[\"ci95_high\"]},\n","])\n","df_sum.to_csv(\"logs/energy_summary.csv\", index=False)\n","print(\"\\n=== Energy measurement completed ===\")\n","print(df_sum)\n","print(\"\\nLog files:\")\n","print(\"- logs/power_idle_trace.csv\")\n","print(\"- logs/power_trace_knn_cuml_run*.csv\")\n","print(\"- logs/power_trace_rf_cuml_run*.csv\")\n","print(\"- logs/energy_knn_cuml.json\")\n","print(\"- logs/energy_rf_cuml.json\")\n","print(\"- logs/energy_summary.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zyL2HQtLdAg4","executionInfo":{"status":"ok","timestamp":1763394158944,"user_tz":0,"elapsed":65428,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"6df4f9e7-dc20-4fb4-8ed3-b747cb7fb866"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[Info] Preparing data and models (cuML KNN / RandomForest on GPU)\n","Train: (25000, 64), Test: (5000, 64)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n","  import pynvml  # type: ignore[import]\n"]},{"output_type":"stream","name":"stdout","text":["\n","[Info] Warming up ...\n","\n","[Info] Sampling idle power (20 s) ...\n","[Info] Mean idle power ~ 27726.4 mW\n","[Info] KNN repeats = 251, RF repeats = 2000\n","\n","[Info] Starting KNN measurement ...\n","[Measure] knn_cuml - run 1/5 ...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-113977451.py:62: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n","  E_mJ = float(np.trapz(ps_win, ts_win))\n"]},{"output_type":"stream","name":"stdout","text":["[Measure] knn_cuml - run 2/5 ...\n","[Measure] knn_cuml - run 3/5 ...\n","[Measure] knn_cuml - run 4/5 ...\n","[Measure] knn_cuml - run 5/5 ...\n","[Result] knn_cuml: 0.117 mJ/inf (95% CI [0.115, 0.118])\n","\n","[Info] Starting RandomForest measurement ...\n","[Measure] rf_cuml - run 1/5 ...\n","[Measure] rf_cuml - run 2/5 ...\n","[Measure] rf_cuml - run 3/5 ...\n","[Measure] rf_cuml - run 4/5 ...\n","[Measure] rf_cuml - run 5/5 ...\n","[Result] rf_cuml: 0.009 mJ/inf (95% CI [0.008, 0.009])\n","\n","=== Energy measurement completed ===\n","                 model  mJ/inf_mean  CI95_low  CI95_high\n","0           KNN (cuML)     0.116521  0.115064   0.117602\n","1  RandomForest (cuML)     0.008627  0.008096   0.009497\n","\n","Log files:\n","- logs/power_idle_trace.csv\n","- logs/power_trace_knn_cuml_run*.csv\n","- logs/power_trace_rf_cuml_run*.csv\n","- logs/energy_knn_cuml.json\n","- logs/energy_rf_cuml.json\n","- logs/energy_summary.csv\n"]}]},{"cell_type":"code","source":["!pip -q install sktime"],"metadata":{"id":"kEbqCAsOb_rW","executionInfo":{"status":"ok","timestamp":1763394161853,"user_tz":0,"elapsed":2904,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# ===== Step 19b (FIXED): MiniROCKET / MultiROCKET + GPU Linear Head Energy Consumption =====\n","import numpy as np, cupy as cp, pandas as pd, pathlib, json, time\n","from sklearn.linear_model import RidgeClassifier\n","from sklearn.model_selection import train_test_split\n","from sktime.transformations.panel.rocket import MiniRocketMultivariate, MultiRocketMultivariate\n","\n","# Reuse the NVML utility functions defined in the previous cell:\n","# gpu_sync, sample_idle_power_mW, calibrate_repeats,\n","# measure_mJ_per_inference, measure_with_bootstrap\n","assert 'gpu_sync' in globals(), \"Please run the previous NVML utility cell first (the KNN/RF one).\"\n","\n","pathlib.Path(\"logs\").mkdir(exist_ok=True)\n","\n","# ---------- 1) Synthetic multivariate time-series dataset (independent from previous X_tr) ----------\n","def make_synth_ts(n_samples=2800, n_channels=6, length=150, n_classes=8, seed=2025):\n","    \"\"\"\n","    Generate a simple synthetic multivariate time-series dataset:\n","    X: (N, C, L), y: (N,)\n","    \"\"\"\n","    rng = np.random.default_rng(seed)\n","    X = rng.normal(0, 1, size=(n_samples, n_channels, length)).astype(np.float32)\n","    y = rng.integers(0, n_classes, size=n_samples).astype(np.int32)\n","\n","    # add simple class-dependent sinusoidal patterns on channel 0/1\n","    t = np.linspace(0, 2*np.pi, length, dtype=np.float32)\n","    for c in range(n_classes):\n","        idx = (y == c)\n","        if idx.any():\n","            freq = 1.0 + 0.2 * c\n","            X[idx, 0, :] += 0.6 * np.sin(freq * t)\n","            X[idx, 1, :] += 0.4 * np.cos(0.5 * freq * t)\n","    return X, y\n","\n","# ALWAYS create a fresh TS dataset here; do NOT reuse global X_tr from the KNN/RF step\n","X_ts, y_ts = make_synth_ts()\n","X_tr_ts, X_te_ts, y_tr_ts, y_te_ts = train_test_split(\n","    X_ts, y_ts, test_size=800, random_state=7, stratify=y_ts\n",")\n","print(f\"[Info] Time-series windows: Train={X_tr_ts.shape}, Test={X_te_ts.shape}, \"\n","      f\"Classes={len(np.unique(y_tr_ts))}\")\n","\n","# ---------- 2) MiniROCKET: CPU feature extraction (excluded from GPU energy accounting) ----------\n","print(\"\\n[MiniROCKET] feature transform (CPU) ... (not part of GPU energy)\")\n","mini = MiniRocketMultivariate(random_state=42)\n","\n","# sktime MiniROCKET/MultiROCKET for float64 + C-contiguous\n","Xtr_mini = mini.fit_transform(\n","    np.ascontiguousarray(X_tr_ts, dtype=np.float64)\n",").astype(np.float32, copy=False)\n","Xte_mini = mini.transform(\n","    np.ascontiguousarray(X_te_ts, dtype=np.float64)\n",").astype(np.float32, copy=False)\n","\n","print(f\"[MiniROCKET] features: train {Xtr_mini.shape}, test {Xte_mini.shape}\")\n","\n","print(\"[MiniROCKET] train Ridge (CPU) ...\")\n","rc_mini = RidgeClassifier(alpha=1.0)\n","rc_mini.fit(Xtr_mini, y_tr_ts)\n","acc_mini = (rc_mini.predict(Xte_mini) == y_te_ts).mean()\n","print(f\"[Check] MiniROCKET Ridge Acc: {acc_mini:.3f}\")\n","\n","# ---------- 3) MultiROCKET: CPU feature extraction (excluded from GPU energy accounting) ----------\n","print(\"\\n[MultiROCKET] feature transform (CPU, float64+C-contiguous) ... (not part of GPU energy)\")\n","multi = MultiRocketMultivariate(random_state=123)\n","\n","Xtr_multi = multi.fit_transform(\n","    np.ascontiguousarray(X_tr_ts, dtype=np.float64)\n",").astype(np.float32, copy=False)\n","Xte_multi = multi.transform(\n","    np.ascontiguousarray(X_te_ts, dtype=np.float64)\n",").astype(np.float32, copy=False)\n","\n","print(f\"[MultiROCKET] features: train {Xtr_multi.shape}, test {Xte_multi.shape}\")\n","\n","print(\"[MultiROCKET] train Ridge (CPU) ...\")\n","rc_multi = RidgeClassifier(alpha=1.0)\n","rc_multi.fit(Xtr_multi, y_tr_ts)\n","acc_multi = (rc_multi.predict(Xte_multi) == y_te_ts).mean()\n","print(f\"[Check] MultiROCKET Ridge Acc: {acc_multi:.3f}\")\n","\n","# ---------- 4) Move the linear head to GPU and define run_once() ----------\n","def make_gpu_linear_runner(X_cpu: np.ndarray, clf: RidgeClassifier, batch: int = 512):\n","    \"\"\"\n","    X_cpu: (N, D) float32 features\n","    clf.coef_: (C, D), intercept_: (C,)\n","    run_once(): performs one full-batch prediction on GPU (processed in mini-batches)\n","    \"\"\"\n","    # move features & classifier weights to GPU once\n","    X_gpu = cp.asarray(X_cpu, dtype=cp.float32)                  # (N, D)\n","    W_gpu = cp.asarray(clf.coef_.T.astype(np.float32))           # (D, C)\n","    b_gpu = cp.asarray(clf.intercept_.astype(np.float32))        # (C,)\n","    N = X_cpu.shape[0]\n","\n","    def run_once():\n","        # iterate over the test set in mini-batches\n","        for s in range(0, N, batch):\n","            e = min(N, s + batch)\n","            logits = X_gpu[s:e].dot(W_gpu) + b_gpu  # (B, C)\n","            _ = cp.argmax(logits, axis=1)\n","        gpu_sync()  # ensure all kernels are finished before timing/energy accounting\n","    return run_once, N\n","\n","run_mini,  N_mini  = make_gpu_linear_runner(Xte_mini,  rc_mini,  batch=1024)\n","run_multi, N_multi = make_gpu_linear_runner(Xte_multi, rc_multi, batch=512)\n","\n","# ---------- 5) Idle power: reuse existing value if available; otherwise, measure once ----------\n","if 'P_idle_mW' not in globals():\n","    print(\"\\n[Info] Sampling idle power for 20 s ...\")\n","    P_idle_mW, _idle = sample_idle_power_mW(\n","        duration_s=20.0, dev_index=0, interval=0.02,\n","        save_csv=\"logs/power_idle_trace_rocket.csv\"\n","    )\n","    print(f\"[Info] Mean idle power ~ {P_idle_mW:.1f} mW\")\n","else:\n","    print(f\"\\n[Info] Reusing previously measured idle power P_idle_mW = {P_idle_mW:.1f} mW\")\n","\n","# ---------- 6) Warm-up ----------\n","print(\"\\n[Warmup] GPU linear heads warmup ...\")\n","for _ in range(30):\n","    run_mini()\n","    run_multi()\n","gpu_sync()\n","\n","# ---------- 7) Adaptive determination of the measurement window (target ≥ 8 s) ----------\n","rep_mini  = calibrate_repeats(run_mini,  target_s=8.0, min_rep=3, max_rep=5000)\n","rep_multi = calibrate_repeats(run_multi, target_s=8.0, min_rep=3, max_rep=5000)\n","print(f\"[Info] repeats: MiniROCKET={rep_mini}, MultiROCKET={rep_multi}\")\n","\n","# ---------- 8) NVML measurement + bootstrap confidence intervals ----------\n","sum_mini = measure_with_bootstrap(\n","    name=\"minirocket_gpu_linear\",\n","    run_once=run_mini,\n","    n_items=N_mini,\n","    repeats=rep_mini,\n","    n_runs=5,\n","    n_boot=1000\n",")\n","sum_multi = measure_with_bootstrap(\n","    name=\"multirocket_gpu_linear\",\n","    run_once=run_multi,\n","    n_items=N_multi,\n","    repeats=rep_multi,\n","    n_runs=5,\n","    n_boot=1000\n",")\n","\n","# ---------- 9) Summary ----------\n","df_sum = pd.DataFrame([\n","    {\n","        \"model\": \"MiniROCKET (GPU linear head)\",\n","        \"mJ/inf_mean\": sum_mini[\"mean_mJ_per_inf\"],\n","        \"CI95_low\":    sum_mini[\"ci95_low\"],\n","        \"CI95_high\":   sum_mini[\"ci95_high\"],\n","        \"acc\":         float(acc_mini),\n","    },\n","    {\n","        \"model\": \"MultiROCKET (GPU linear head)\",\n","        \"mJ/inf_mean\": sum_multi[\"mean_mJ_per_inf\"],\n","        \"CI95_low\":    sum_multi[\"ci95_low\"],\n","        \"CI95_high\":   sum_multi[\"ci95_high\"],\n","        \"acc\":         float(acc_multi),\n","    },\n","])\n","df_sum.to_csv(\"logs/energy_summary_rocket_gpuhead.csv\", index=False)\n","\n","print(\"\\n=== Completed (ROCKET GPU linear head energy; CPU feature extraction excluded from energy accounting) ===\")\n","print(df_sum)\n","print(\"\\nLog files:\")\n","print(\"- logs/power_trace_minirocket_gpu_linear_run*.csv\")\n","print(\"- logs/power_trace_multirocket_gpu_linear_run*.csv\")\n","print(\"- logs/energy_minirocket_gpu_linear.json\")\n","print(\"- logs/energy_multirocket_gpu_linear.json\")\n","print(\"- logs/energy_summary_rocket_gpuhead.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0cihAvHgf4YH","executionInfo":{"status":"ok","timestamp":1763394576342,"user_tz":0,"elapsed":143467,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"11f41c8a-9ade-4b53-d72d-3d0655ce7062"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[Info] Time-series windows: Train=(2000, 6, 150), Test=(800, 6, 150), Classes=8\n","\n","[MiniROCKET] feature transform (CPU) ... (not part of GPU energy)\n","[MiniROCKET] features: train (2000, 9996), test (800, 9996)\n","[MiniROCKET] train Ridge (CPU) ...\n","[Check] MiniROCKET Ridge Acc: 0.616\n","\n","[MultiROCKET] feature transform (CPU, float64+C-contiguous) ... (not part of GPU energy)\n","[MultiROCKET] features: train (2000, 49728), test (800, 49728)\n","[MultiROCKET] train Ridge (CPU) ...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[Check] MultiROCKET Ridge Acc: 0.504\n","\n","[Info] Reusing previously measured idle power P_idle_mW = 27726.4 mW\n","\n","[Warmup] GPU linear heads warmup ...\n","[Info] repeats: MiniROCKET=5000, MultiROCKET=3695\n","[Measure] minirocket_gpu_linear - run 1/5 ...\n","[Measure] minirocket_gpu_linear - run 2/5 ...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-113977451.py:62: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n","  E_mJ = float(np.trapz(ps_win, ts_win))\n"]},{"output_type":"stream","name":"stdout","text":["[Measure] minirocket_gpu_linear - run 3/5 ...\n","[Measure] minirocket_gpu_linear - run 4/5 ...\n","[Measure] minirocket_gpu_linear - run 5/5 ...\n","[Result] minirocket_gpu_linear: 0.022 mJ/inf (95% CI [0.021, 0.023])\n","[Measure] multirocket_gpu_linear - run 1/5 ...\n","[Measure] multirocket_gpu_linear - run 2/5 ...\n","[Measure] multirocket_gpu_linear - run 3/5 ...\n","[Measure] multirocket_gpu_linear - run 4/5 ...\n","[Measure] multirocket_gpu_linear - run 5/5 ...\n","[Result] multirocket_gpu_linear: 0.092 mJ/inf (95% CI [0.091, 0.092])\n","\n","=== Completed (ROCKET GPU linear head energy; CPU feature extraction excluded from energy accounting) ===\n","                           model  mJ/inf_mean  CI95_low  CI95_high      acc\n","0   MiniROCKET (GPU linear head)     0.021889  0.020916   0.022525  0.61625\n","1  MultiROCKET (GPU linear head)     0.091602  0.090517   0.092305  0.50375\n","\n","Log files:\n","- logs/power_trace_minirocket_gpu_linear_run*.csv\n","- logs/power_trace_multirocket_gpu_linear_run*.csv\n","- logs/energy_minirocket_gpu_linear.json\n","- logs/energy_multirocket_gpu_linear.json\n","- logs/energy_summary_rocket_gpuhead.csv\n"]}]},{"cell_type":"code","source":["!pip -q install tsai fastai torch --upgrade"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TD-6YiqzgylZ","executionInfo":{"status":"ok","timestamp":1763394804450,"user_tz":0,"elapsed":142719,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"59c2bd34-c06e-457e-b1d3-0010226aa60d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.1/324.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.0/821.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.3/263.3 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["# ============ Step 19c (No-Train Hotfix · Self-contained):\n","# InceptionTime & TST Inference Energy Consumption (GPU, NVML) ============\n","\n","import os, math, time, json, pathlib, multiprocessing as mp\n","os.environ.setdefault(\"TORCH_COMPILE_DISABLE\", \"1\")\n","os.environ.setdefault(\"TORCHDYNAMO_DISABLE\", \"1\")\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","\n","# ============================================\n","# 0. NVML-based GPU energy measurement helpers\n","# ============================================\n","\n","def gpu_sync():\n","    \"\"\"\n","    Attempt to synchronize all GPU backends as much as possible:\n","    - Prefer synchronizing CuPy\n","    - If PyTorch CUDA is available, also synchronize torch.cuda\n","    \"\"\"\n","    try:\n","        import cupy as cp\n","        cp.cuda.runtime.deviceSynchronize()\n","    except Exception:\n","        pass\n","    try:\n","        if torch.cuda.is_available():\n","            torch.cuda.synchronize()\n","    except Exception:\n","        pass\n","\n","\n","def nvml_sampler(stop_event, q, dev_index=0, interval=0.02):\n","    \"\"\"\n","    Subprocess: periodically read NVML power (mW) and push (t_absolute, power_mW) into the queue.\n","    t uses the absolute monotonic clock time.perf_counter(), which facilitates alignment\n","    with the main process.\n","    \"\"\"\n","    import pynvml, time as _time\n","    pynvml.nvmlInit()\n","    h = pynvml.nvmlDeviceGetHandleByIndex(dev_index)\n","    try:\n","        while not stop_event.is_set():\n","            t = _time.perf_counter()\n","            p_mw = pynvml.nvmlDeviceGetPowerUsage(h)  # milliwatt\n","            q.put((t, p_mw))\n","            _time.sleep(interval)\n","    finally:\n","        pynvml.nvmlShutdown()\n","\n","\n","def integrate_energy_mJ_between(samples, t0, t1):\n","    \"\"\"\n","    Perform trapezoidal integration of the power–time curve over [t0, t1].\n","    samples: List[(t, mW)] (t is the absolute time returned by perf_counter)\n","    Returns energy in mJ.\n","    \"\"\"\n","    if not samples:\n","        return 0.0\n","    samples = sorted(samples, key=lambda x: x[0])\n","    ts = np.array([t for t, _ in samples], dtype=np.float64)\n","    ps = np.array([p for _, p in samples], dtype=np.float64)  # mW\n","\n","    # Select the interval and interpolate endpoints\n","    mask = (ts >= t0) & (ts <= t1)\n","    ts_win = ts[mask]\n","    ps_win = ps[mask]\n","    if ts_win.size == 0 or ts_win[0] > t0:\n","        p0 = np.interp(t0, ts, ps)\n","        ts_win = np.insert(ts_win, 0, t0)\n","        ps_win = np.insert(ps_win, 0, p0)\n","    if ts_win[-1] < t1:\n","        p1 = np.interp(t1, ts, ps)\n","        ts_win = np.append(ts_win, t1)\n","        ps_win = np.append(ps_win, p1)\n","\n","    # Integration: mW * s = mJ\n","    E_mJ = float(np.trapz(ps_win, ts_win))\n","    return E_mJ\n","\n","\n","def sample_idle_power_mW(duration_s=20.0, dev_index=0, interval=0.02, save_csv=None):\n","    \"\"\"\n","    Sample idle-state power and return the mean power (mW) and the full power trace.\n","    \"\"\"\n","    q = mp.Queue()\n","    stop = mp.Event()\n","    proc = mp.Process(target=nvml_sampler, args=(stop, q, dev_index, interval))\n","    proc.start()\n","\n","    time.sleep(duration_s)\n","    stop.set()\n","    proc.join()\n","\n","    samples = []\n","    while not q.empty():\n","        samples.append(q.get())\n","    if not samples:\n","        raise RuntimeError(\"NVML did not capture any power samples (idle).\")\n","\n","    samples = sorted(samples, key=lambda x: x[0])\n","    t0, t1 = samples[0][0], samples[-1][0]\n","    E_idle_mJ = integrate_energy_mJ_between(samples, t0, t1)\n","    T_idle_s = max(1e-9, (t1 - t0))\n","    P_idle_mW = E_idle_mJ / T_idle_s\n","\n","    if save_csv:\n","        df = pd.DataFrame(samples, columns=[\"t_abs_s\", \"power_mW\"])\n","        df.to_csv(save_csv, index=False)\n","\n","    return P_idle_mW, samples\n","\n","\n","def calibrate_repeats(run_once, target_s=8.0, min_rep=3, max_rep=2000):\n","    \"\"\"\n","    Adaptively estimate how many repetitions are required to achieve an effective\n","    measurement window of length target_s.\n","    \"\"\"\n","    gpu_sync()\n","    t0 = time.perf_counter()\n","    run_once()\n","    gpu_sync()\n","    t1 = time.perf_counter()\n","    dt = max(1e-4, (t1 - t0))\n","    reps = int(math.ceil(target_s / dt))\n","    return int(np.clip(reps, min_rep, max_rep))\n","\n","\n","def measure_mJ_per_inference(run_once, n_items_per_call: int, repeats: int,\n","                             P_idle_mW: float, dev_index=0, interval=0.02,\n","                             save_csv=None):\n","    \"\"\"\n","    Concurrently sample power while run_once() is executed 'repeats' times,\n","    perform integration with idle power subtraction, and return mJ/inf together\n","    with detailed statistics.\n","    \"\"\"\n","    q = mp.Queue()\n","    stop = mp.Event()\n","    proc = mp.Process(target=nvml_sampler, args=(stop, q, dev_index, interval))\n","    proc.start()\n","\n","    gpu_sync()\n","    t0 = time.perf_counter()\n","    for _ in range(repeats):\n","        run_once()\n","    gpu_sync()\n","    t1 = time.perf_counter()\n","\n","    stop.set()\n","    proc.join()\n","\n","    samples = []\n","    while not q.empty():\n","        samples.append(q.get())\n","    if not samples:\n","        raise RuntimeError(\"NVML did not capture any power samples (active).\")\n","\n","    E_total_mJ = integrate_energy_mJ_between(samples, t0, t1)\n","    T_total_s = max(1e-9, (t1 - t0))\n","    E_idle_mJ = P_idle_mW * T_total_s\n","    n_inf = max(1, repeats * n_items_per_call)\n","\n","    mJ_per_inf = max(0.0, (E_total_mJ - E_idle_mJ) / n_inf)\n","    ms_per_inf = (T_total_s / n_inf) * 1e3\n","    throughput = n_inf / T_total_s\n","\n","    if save_csv:\n","        df = pd.DataFrame(samples, columns=[\"t_abs_s\", \"power_mW\"])\n","        df.to_csv(save_csv, index=False)\n","\n","    return {\n","        \"mJ_per_inf\": mJ_per_inf,\n","        \"ms_per_inf\": ms_per_inf,\n","        \"throughput_inf_per_s\": throughput,\n","        \"n_inferences\": n_inf,\n","        \"repeats\": repeats,\n","        \"T_total_s\": T_total_s,\n","        \"E_total_mJ\": E_total_mJ,\n","        \"E_idle_mJ\": E_idle_mJ,\n","        \"P_idle_mW\": P_idle_mW,\n","        \"t0_abs\": t0,\n","        \"t1_abs\": t1,\n","    }\n","\n","\n","def measure_with_bootstrap(name, run_once, n_items, repeats, n_runs=5, n_boot=1000):\n","    \"\"\"\n","    Perform repeated measurements of mJ/inf and compute a bootstrap confidence\n","    interval for the mean.\n","    \"\"\"\n","    pathlib.Path(\"logs\").mkdir(exist_ok=True)\n","    results = []\n","    for i in range(n_runs):\n","        print(f\"[Measure] {name} - run {i+1}/{n_runs} ...\")\n","        res = measure_mJ_per_inference(\n","            run_once,\n","            n_items_per_call=n_items,\n","            repeats=repeats,\n","            P_idle_mW=P_idle_mW,\n","            dev_index=0,\n","            interval=0.02,\n","            save_csv=f\"logs/power_trace_{name}_run{i+1}.csv\"\n","        )\n","        results.append(res)\n","\n","    mJs = np.array([r[\"mJ_per_inf\"] for r in results], dtype=np.float64)\n","    rng = np.random.default_rng(123)\n","    boots = []\n","    for _ in range(n_boot):\n","        idx = rng.integers(0, len(mJs), size=len(mJs))\n","        boots.append(float(np.mean(mJs[idx])))\n","    ci_low, ci_high = np.percentile(boots, [2.5, 97.5])\n","\n","    summary = {\n","        \"model\": name,\n","        \"mean_mJ_per_inf\": float(np.mean(mJs)),\n","        \"ci95_low\": float(ci_low),\n","        \"ci95_high\": float(ci_high),\n","        \"runs\": results,\n","    }\n","    with open(f\"logs/energy_{name}.json\", \"w\") as f:\n","        json.dump(summary, f, indent=2)\n","\n","    print(f\"[Result] {name}: {summary['mean_mJ_per_inf']:.3f} mJ/inf \"\n","          f\"(95% CI [{summary['ci95_low']:.3f}, {summary['ci95_high']:.3f}])\")\n","    return summary\n","\n","\n","# ==============================\n","# 1. Data: synthetic 3D IMU-like\n","# ==============================\n","\n","def make_synth_ts(n_samples=2800, n_channels=6, length=150, n_classes=8, seed=2025):\n","    \"\"\"\n","    Generate a simple multichannel time series of shape (N, C, L),\n","    with class-dependent sinusoidal patterns.\n","    \"\"\"\n","    rng = np.random.default_rng(seed)\n","    X = rng.normal(0, 1, size=(n_samples, n_channels, length)).astype(np.float32)\n","    y = rng.integers(0, n_classes, size=n_samples).astype(np.int64)\n","\n","    t = np.linspace(0, 2 * np.pi, length, dtype=np.float32)\n","    for c in range(n_classes):\n","        idx = (y == c)\n","        if idx.any():\n","            freq = 1.0 + 0.2 * c\n","            X[idx, 0, :] += 0.6 * np.sin(freq * t)\n","            X[idx, 1, :] += 0.4 * np.cos(0.5 * freq * t)\n","    return X, y\n","\n","\n","# Here, we directly use synthetic data as an energy proxy for InceptionTime / TST.\n","X_ts, y_ts = make_synth_ts()\n","X_tr_ts, X_te_ts, y_tr_ts, y_te_ts = train_test_split(\n","    X_ts, y_ts, test_size=800, random_state=7, stratify=y_ts\n",")\n","print(f\"[Info] Train={X_tr_ts.shape}, Test={X_te_ts.shape}, Classes={len(np.unique(y_tr_ts))}\")\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","torch.backends.cudnn.benchmark = True\n","pathlib.Path(\"logs\").mkdir(exist_ok=True)\n","\n","# ===========================\n","# 2. Lightweight InceptionTime\n","# ===========================\n","\n","class InceptionBlock1d(nn.Module):\n","    def __init__(self, in_ch, out_ch, bottleneck=32, ks=(9, 19, 39)):\n","        super().__init__()\n","        use_bn = in_ch > 1\n","        bott = bottleneck if use_bn else in_ch\n","        self.bottleneck = nn.Conv1d(in_ch, bott, 1, bias=False) if use_bn else nn.Identity()\n","        self.conv1 = nn.Conv1d(bott, out_ch // 4, ks[0], padding=ks[0] // 2, bias=False)\n","        self.conv2 = nn.Conv1d(bott, out_ch // 4, ks[1], padding=ks[1] // 2, bias=False)\n","        self.conv3 = nn.Conv1d(bott, out_ch // 4, ks[2], padding=ks[2] // 2, bias=False)\n","        self.pool = nn.MaxPool1d(3, stride=1, padding=1)\n","        self.conv_pool = nn.Conv1d(in_ch, out_ch // 4, 1, bias=False)\n","        self.bn = nn.BatchNorm1d(out_ch)\n","        self.act = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        z = self.bottleneck(x)\n","        y = torch.cat(\n","            [self.conv1(z), self.conv2(z), self.conv3(z), self.conv_pool(self.pool(x))],\n","            dim=1\n","        )\n","        return self.act(self.bn(y))\n","\n","\n","class InceptionResNetModule(nn.Module):\n","    def __init__(self, in_ch, out_ch, **kw):\n","        super().__init__()\n","        self.b1 = InceptionBlock1d(in_ch, out_ch, **kw)\n","        self.b2 = InceptionBlock1d(out_ch, out_ch, **kw)\n","        self.b3 = InceptionBlock1d(out_ch, out_ch, **kw)\n","        self.short = nn.Identity() if in_ch == out_ch else nn.Sequential(\n","            nn.Conv1d(in_ch, out_ch, 1, bias=False),\n","            nn.BatchNorm1d(out_ch)\n","        )\n","        self.act = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        res = self.short(x)\n","        y = self.b1(x)\n","        y = self.b2(y)\n","        y = self.b3(y)\n","        return self.act(y + res)\n","\n","\n","class InceptionTimeSmall(nn.Module):\n","    def __init__(self, c_in, n_classes, nb_filters=64, n_modules=2, bottleneck=32):\n","        super().__init__()\n","        layers, in_ch = [], c_in\n","        for _ in range(n_modules):\n","            layers.append(InceptionResNetModule(in_ch, nb_filters, bottleneck=bottleneck))\n","            in_ch = nb_filters\n","        self.features = nn.Sequential(*layers)\n","        self.gap = nn.AdaptiveAvgPool1d(1)\n","        self.fc = nn.Linear(nb_filters, n_classes)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.gap(x).squeeze(-1)\n","        return self.fc(x)\n","\n","\n","# ==================\n","# 3. Lightweight TST\n","# ==================\n","\n","class PatchEmbed1D(nn.Module):\n","    def __init__(self, c_in, d_model=128, patch_len=10, stride=None):\n","        super().__init__()\n","        self.proj = nn.Conv1d(\n","            c_in, d_model, kernel_size=patch_len, stride=stride or patch_len, bias=False\n","        )\n","\n","    def forward(self, x):  # (B,C,L) -> (B,N,D)\n","        x = self.proj(x).transpose(1, 2)\n","        return x\n","\n","\n","class SinPosEncoding(nn.Module):\n","    def __init__(self, d_model):\n","        super().__init__()\n","        self.d = d_model\n","\n","    def forward(self, x):\n","        B, N, D = x.shape\n","        device = x.device\n","        pos = torch.arange(N, device=device).unsqueeze(1)\n","        div = torch.exp(torch.arange(0, D, 2, device=device) * (-math.log(10000.0) / D))\n","        pe = torch.zeros(N, D, device=device)\n","        pe[:, 0::2] = torch.sin(pos * div)\n","        pe[:, 1::2] = torch.cos(pos * div)\n","        return x + pe.unsqueeze(0)\n","\n","\n","class TSTSmall(nn.Module):\n","    def __init__(self, c_in, n_classes, d_model=128, n_heads=4,\n","                 depth=2, dim_ff=256, patch_len=10, dropout=0.1):\n","        super().__init__()\n","        self.embed = PatchEmbed1D(c_in, d_model, patch_len)\n","        enc_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model, nhead=n_heads, dim_feedforward=dim_ff,\n","            dropout=dropout, batch_first=True, norm_first=True, activation='gelu'\n","        )\n","        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=depth)\n","        self.pos = SinPosEncoding(d_model)\n","        self.head = nn.Linear(d_model, n_classes)\n","\n","    def forward(self, x):\n","        x = self.embed(x)\n","        x = self.pos(x)\n","        x = self.encoder(x)\n","        return self.head(x.mean(dim=1))\n","\n","\n","# =========================\n","# 4. Build models (random)\n","# =========================\n","\n","n_classes = int(len(np.unique(y_tr_ts)))\n","c_in = X_tr_ts.shape[1]\n","\n","it_model = InceptionTimeSmall(\n","    c_in=c_in, n_classes=n_classes, nb_filters=64, n_modules=2, bottleneck=32\n",")\n","tst_model = TSTSmall(\n","    c_in=c_in, n_classes=n_classes, d_model=128, n_heads=4,\n","    depth=2, dim_ff=256, patch_len=10\n",")\n","\n","\n","# ==========================\n","# 5. Inference runner helpers\n","# ==========================\n","\n","def make_runner(model, X_np, bs=512):\n","    model = model.to(device).eval()\n","    X_gpu = torch.as_tensor(X_np, dtype=torch.float32, device=device)\n","    N = X_np.shape[0]\n","\n","    @torch.no_grad()\n","    def run_once():\n","        for s in range(0, N, bs):\n","            e = min(N, s + bs)\n","            _ = model(X_gpu[s:e])\n","        if torch.cuda.is_available():\n","            torch.cuda.synchronize()\n","        gpu_sync()\n","\n","    return run_once, N\n","\n","\n","run_it, N_it = make_runner(it_model, X_te_ts, bs=512)\n","run_tst, N_tst = make_runner(tst_model, X_te_ts, bs=512)\n","\n","# ===============\n","# 6. Idle power\n","# ===============\n","\n","if 'P_idle_mW' not in globals():\n","    print(\"\\n[Info] Sampling idle power for 20 s ...\")\n","    P_idle_mW, _idle = sample_idle_power_mW(\n","        duration_s=20.0, dev_index=0, interval=0.02,\n","        save_csv=\"logs/power_idle_trace_deepts_hotfix.csv\"\n","    )\n","    print(f\"[Info] Mean idle power ~ {P_idle_mW:.1f} mW\")\n","else:\n","    print(f\"\\n[Info] Reusing previously measured idle power P_idle_mW = {P_idle_mW:.1f} mW\")\n","\n","# ===============\n","# 7. Warm-up\n","# ===============\n","\n","print(\"\\n[Warmup] warmup ...\")\n","for _ in range(20):\n","    run_it()\n","    run_tst()\n","gpu_sync()\n","\n","# =====================================\n","# 8. Adaptive window + NVML measurement\n","# =====================================\n","\n","rep_it = calibrate_repeats(run_it, target_s=8.0, min_rep=3, max_rep=5000)\n","rep_tst = calibrate_repeats(run_tst, target_s=8.0, min_rep=3, max_rep=5000)\n","print(f\"[Info] repeats: InceptionTime={rep_it}, TST={rep_tst}\")\n","\n","sum_it = measure_with_bootstrap(\n","    \"inceptiontime_torch_eager\", run_it, N_it, rep_it, n_runs=5, n_boot=1000\n",")\n","sum_ts = measure_with_bootstrap(\n","    \"tst_torch_eager\", run_tst, N_tst, rep_tst, n_runs=5, n_boot=1000\n",")\n","\n","# =========\n","# 9. Summary\n","# =========\n","\n","df = pd.DataFrame([\n","    {\n","        \"model\": \"InceptionTime (eager, no-train)\",\n","        \"mJ/inf_mean\": sum_it[\"mean_mJ_per_inf\"],\n","        \"CI95_low\": sum_it[\"ci95_low\"],\n","        \"CI95_high\": sum_it[\"ci95_high\"],\n","    },\n","    {\n","        \"model\": \"TST (eager, no-train)\",\n","        \"mJ/inf_mean\": sum_ts[\"mean_mJ_per_inf\"],\n","        \"CI95_low\": sum_ts[\"ci95_low\"],\n","        \"CI95_high\": sum_ts[\"ci95_high\"],\n","    },\n","])\n","df.to_csv(\"logs/energy_summary_deepts_eager.csv\", index=False)\n","\n","print(\"\\n=== Completed (InceptionTime & TST inference energy, hotfix self-contained version) ===\")\n","print(df)\n","print(\"\\nLog files:\")\n","print(\"- logs/power_idle_trace_deepts_hotfix.csv\")\n","print(\"- logs/power_trace_inceptiontime_torch_eager_run*.csv\")\n","print(\"- logs/power_trace_tst_torch_eager_run*.csv\")\n","print(\"- logs/energy_inceptiontime_torch_eager.json\")\n","print(\"- logs/energy_tst_torch_eager.json\")\n","print(\"- logs/energy_summary_deepts_eager.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z0JTuAQfk3Kf","executionInfo":{"status":"ok","timestamp":1763395886254,"user_tz":0,"elapsed":79768,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"fed9c7db-aaf5-410c-858f-a0384911ba32"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[Info] Train=(2000, 6, 150), Test=(800, 6, 150), Classes=8\n","\n","[Info] Reusing previously measured idle power P_idle_mW = 26257.8 mW\n","\n","[Warmup] warmup ...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[Info] repeats: InceptionTime=298, TST=1449\n","[Measure] inceptiontime_torch_eager - run 1/5 ...\n","[Measure] inceptiontime_torch_eager - run 2/5 ...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1635911225.py:81: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n","  E_mJ = float(np.trapz(ps_win, ts_win))\n"]},{"output_type":"stream","name":"stdout","text":["[Measure] inceptiontime_torch_eager - run 3/5 ...\n","[Measure] inceptiontime_torch_eager - run 4/5 ...\n","[Measure] inceptiontime_torch_eager - run 5/5 ...\n","[Result] inceptiontime_torch_eager: 1.508 mJ/inf (95% CI [1.499, 1.517])\n","[Measure] tst_torch_eager - run 1/5 ...\n","[Measure] tst_torch_eager - run 2/5 ...\n","[Measure] tst_torch_eager - run 3/5 ...\n","[Measure] tst_torch_eager - run 4/5 ...\n","[Measure] tst_torch_eager - run 5/5 ...\n","[Result] tst_torch_eager: 0.275 mJ/inf (95% CI [0.272, 0.277])\n","\n","=== Completed (InceptionTime & TST inference energy, hotfix self-contained version) ===\n","                             model  mJ/inf_mean  CI95_low  CI95_high\n","0  InceptionTime (eager, no-train)     1.508429  1.499234   1.516858\n","1            TST (eager, no-train)     0.274844  0.272484   0.276661\n","\n","Log files:\n","- logs/power_idle_trace_deepts_hotfix.csv\n","- logs/power_trace_inceptiontime_torch_eager_run*.csv\n","- logs/power_trace_tst_torch_eager_run*.csv\n","- logs/energy_inceptiontime_torch_eager.json\n","- logs/energy_tst_torch_eager.json\n","- logs/energy_summary_deepts_eager.csv\n"]}]}]}