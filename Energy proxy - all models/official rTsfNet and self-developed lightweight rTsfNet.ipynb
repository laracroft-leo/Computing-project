{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyPDnptBeVSFKtb4uHhv/LKX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# ================================================================\n","# rTsfNet (lightweight in-house version) × Option 1:\n","# NVML-based GPU inference energy (Colab one-click version)\n","# ================================================================\n","# 0) Basic environment and dependencies\n","!nvidia-smi\n","!pip -q install pynvml\n","\n","import os, math, json, time, pathlib, gc, warnings, multiprocessing as mp\n","from pathlib import Path\n","warnings.filterwarnings(\"ignore\")\n","\n","import numpy as np\n","import pandas as pd\n","\n","# ---------------- NVML sampling & energy integration utilities (Option 1) ----------------\n","import pynvml\n","\n","def _nvml_sampler(stop_event, q, dev_index=0, interval=0.02):\n","    \"\"\"Subprocess: sample power (mW) every `interval` and send back (t_abs, mW).\"\"\"\n","    import time, pynvml\n","    pynvml.nvmlInit()\n","    h = pynvml.nvmlDeviceGetHandleByIndex(dev_index)\n","    try:\n","        while not stop_event.is_set():\n","            q.put((time.perf_counter(), pynvml.nvmlDeviceGetPowerUsage(h)))\n","            time.sleep(interval)\n","    finally:\n","        pynvml.nvmlShutdown()\n","\n","def _integrate_mJ_between(samples, t0, t1):\n","    \"\"\"Trapezoidal integration of power (mW) over [t0, t1], returning mJ.\"\"\"\n","    if not samples:\n","        return 0.0\n","    samples = sorted(samples, key=lambda x: x[0])\n","    ts = np.array([t for t, _ in samples], dtype=np.float64)\n","    ps = np.array([p for _, p in samples], dtype=np.float64)\n","    # Select interval and interpolate endpoints\n","    m = (ts >= t0) & (ts <= t1)\n","    ts_w = ts[m]\n","    ps_w = ps[m]\n","    if ts_w.size == 0 or ts_w[0] > t0:\n","        p0 = np.interp(t0, ts, ps)\n","        ts_w = np.insert(ts_w, 0, t0)\n","        ps_w = np.insert(ps_w, 0, p0)\n","    if ts_w[-1] < t1:\n","        p1 = np.interp(t1, ts, ps)\n","        ts_w = np.append(ts_w, t1)\n","        ps_w = np.append(ps_w, p1)\n","    return float(np.trapz(ps_w, ts_w))  # mW*s = mJ\n","\n","def sample_idle_power_mW(duration_s=20.0, dev_index=0, interval=0.02, save_csv=None):\n","    \"\"\"Measure mean idle power (mW), optionally saving the power trace.\"\"\"\n","    import time\n","    q = mp.Queue()\n","    stop = mp.Event()\n","    p = mp.Process(target=_nvml_sampler, args=(stop, q, dev_index, interval))\n","    p.start()\n","    t_begin = time.perf_counter()\n","    time.sleep(duration_s)\n","    stop.set()\n","    p.join()\n","    samples = []\n","    while not q.empty():\n","        samples.append(q.get())\n","    if not samples:\n","        raise RuntimeError(\"NVML did not capture any power samples (idle).\")\n","    samples.sort(key=lambda x: x[0])\n","    t0, t1 = samples[0][0], samples[-1][0]\n","    E_idle_mJ = _integrate_mJ_between(samples, t0, t1)\n","    T_idle_s = max(1e-9, t1 - t0)\n","    P_idle_mW = E_idle_mJ / T_idle_s\n","    if save_csv:\n","        pd.DataFrame(samples, columns=[\"t_abs_s\", \"power_mW\"]).to_csv(save_csv, index=False)\n","    return P_idle_mW, samples\n","\n","def measure_mJ_per_inference(run_once, n_items_per_call, repeats, P_idle_mW,\n","                             dev_index=0, interval=0.02, save_csv=None):\n","    \"\"\"Concurrent NVML sampling + integration + idle subtraction; return mJ/inf and details.\"\"\"\n","    import time\n","    q = mp.Queue()\n","    stop = mp.Event()\n","    p = mp.Process(target=_nvml_sampler, args=(stop, q, dev_index, interval))\n","    p.start()\n","\n","    # Measurement window\n","    t0 = time.perf_counter()\n","    for _ in range(repeats):\n","        run_once()\n","    t1 = time.perf_counter()\n","\n","    stop.set()\n","    p.join()\n","    samples = []\n","    while not q.empty():\n","        samples.append(q.get())\n","    if not samples:\n","        raise RuntimeError(\"NVML did not capture any power samples (measurement).\")\n","\n","    E_total_mJ = _integrate_mJ_between(samples, t0, t1)\n","    T_total_s = max(1e-9, t1 - t0)\n","    E_idle_mJ = P_idle_mW * T_total_s\n","    n_inf = max(1, repeats * n_items_per_call)\n","\n","    if save_csv:\n","        pd.DataFrame(samples, columns=[\"t_abs_s\", \"power_mW\"]).to_csv(save_csv, index=False)\n","\n","    return {\n","        \"mJ_per_inf\": max(0.0, (E_total_mJ - E_idle_mJ) / n_inf),\n","        \"ms_per_inf\": (T_total_s / n_inf) * 1e3,\n","        \"throughput_inf_per_s\": n_inf / T_total_s,\n","        \"n_inferences\": n_inf,\n","        \"repeats\": repeats,\n","        \"T_total_s\": T_total_s,\n","        \"E_total_mJ\": E_total_mJ,\n","        \"E_idle_mJ\": E_idle_mJ,\n","        \"P_idle_mW\": P_idle_mW,\n","        \"t0_abs\": t0,\n","        \"t1_abs\": t1,\n","    }\n","\n","def calibrate_repeats(run_once, target_s=8.0, min_rep=3, max_rep=5000):\n","    \"\"\"Estimate how many repeats are needed so that one measurement window ≈ target_s, reducing sampling noise.\"\"\"\n","    import time\n","    # Warm up once to avoid first-time JIT/graph construction\n","    run_once()\n","    t0 = time.perf_counter()\n","    run_once()\n","    t1 = time.perf_counter()\n","    dt = max(1e-4, t1 - t0)\n","    reps = int(np.ceil(target_s / dt))\n","    return int(np.clip(reps, min_rep, max_rep))\n","\n","def measure_with_bootstrap(name, run_once, n_items, repeats, n_runs=5, n_boot=1000, logdir=Path(\"logs\")):\n","    \"\"\"Repeat n_runs measurements, compute bootstrap 95% CI; save traces and summary JSON.\"\"\"\n","    logdir.mkdir(exist_ok=True)\n","    res_list = []\n","    for i in range(n_runs):\n","        print(f\"[Measure] {name} run {i+1}/{n_runs} ...\")\n","        r = measure_mJ_per_inference(\n","            run_once, n_items, repeats,\n","            P_idle_mW=P_idle_mW, dev_index=0, interval=0.02,\n","            save_csv=str(logdir / f\"power_trace_{name}_run{i+1}.csv\")\n","        )\n","        res_list.append(r)\n","\n","    mJs = np.array([r[\"mJ_per_inf\"] for r in res_list], dtype=np.float64)\n","    rng = np.random.default_rng(123)\n","    boots = [float(np.mean(mJs[rng.integers(0, len(mJs), size=len(mJs))])) for _ in range(n_boot)]\n","    ci_low, ci_high = np.percentile(boots, [2.5, 97.5])\n","\n","    summary = {\n","        \"model\": name,\n","        \"mean_mJ_per_inf\": float(mJs.mean()),\n","        \"ci95_low\": float(ci_low),\n","        \"ci95_high\": float(ci_high),\n","        \"runs\": res_list,\n","    }\n","    with open(logdir / f\"energy_{name}.json\", \"w\") as f:\n","        json.dump(summary, f, indent=2)\n","    print(\n","        f\"[Result] {name}: {summary['mean_mJ_per_inf']:.3f} mJ/inf  \"\n","        f\"(95% CI [{summary['ci95_low']:.3f}, {summary['ci95_high']:.3f}])\"\n","    )\n","    return summary\n","\n","# ---------------- rTsfNet (Step 10 original architecture and hyperparameters, unchanged) ----------------\n","import tensorflow as tf\n","from tensorflow.keras import Input\n","from tensorflow.keras.layers import (\n","    Dense,\n","    Dropout,\n","    LayerNormalization,\n","    LeakyReLU,\n","    Layer,\n","    Lambda,\n","    Flatten,\n","    GlobalAveragePooling1D,\n","    Activation,\n",")\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras import backend as K\n","\n","# -- Step 10: original random seed & hyperparameters (unchanged) --\n","SEED = 42\n","tf.random.set_seed(SEED)\n","np.random.seed(SEED)\n","FS = 50.0\n","IMU_ROT_HEADS = 2\n","MLP_BASE = 128\n","MLP_DEPTH = 3\n","DROPOUT = 0.5\n","LR = 1e-3\n","WEIGHT_DECAY = 1e-6\n","USE_ORIG_INPUT = True\n","\n","BASE = Path('/content')\n","features_dir = BASE / 'features'\n","models_dir = BASE / 'models'\n","logs_dir = BASE / 'logs'\n","models_dir.mkdir(parents=True, exist_ok=True)\n","logs_dir.mkdir(parents=True, exist_ok=True)\n","\n","# -- Step 10: data loading function (original logic) --\n","def load_fold_data(fold_k, features_dir: Path):\n","    npz_file = features_dir / f'windows_normalized_fold{fold_k}.npz'\n","    data = np.load(npz_file, allow_pickle=True)\n","    X = np.stack(\n","        [\n","            data['acc_x'],\n","            data['acc_y'],\n","            data['acc_z'],\n","            data['gyro_x'],\n","            data['gyro_y'],\n","            data['gyro_z'],\n","        ],\n","        axis=-1,\n","    )  # [N,T,6]\n","    y = data['labels']\n","    splits = data['splits']\n","    train_mask = splits == 'train'\n","    test_mask = splits == 'test'\n","    return X[train_mask], y[train_mask], X[test_mask], y[test_mask]\n","\n","# -- Step 10: TSF layer (original architecture) --\n","class TSFFeatureLayer(Layer):\n","    def __init__(self, fs=50.0, **kwargs):\n","        super().__init__(**kwargs)\n","        self.fs = float(fs)\n","        self.eps = 1e-8\n","\n","    def get_config(self):\n","        cfg = super().get_config()\n","        cfg.update({'fs': self.fs})\n","        return cfg\n","\n","    def call(self, x):  # x: [B,T,C]\n","        mean = tf.reduce_mean(x, axis=1, keepdims=True)\n","        std = tf.math.reduce_std(x, axis=1, keepdims=True) + self.eps\n","        maxv = tf.reduce_max(x, axis=1, keepdims=True)\n","        minv = tf.reduce_min(x, axis=1, keepdims=True)\n","        ptp = maxv - minv\n","        rms = tf.sqrt(tf.reduce_mean(tf.square(x), axis=1, keepdims=True))\n","        energy = tf.reduce_sum(tf.square(x), axis=1, keepdims=True)\n","        skew = tf.reduce_mean(tf.pow((x - mean) / std, 3), axis=1, keepdims=True)\n","        kurt = tf.reduce_mean(tf.pow((x - mean) / std, 4), axis=1, keepdims=True)\n","\n","        signs = tf.sign(x)\n","        sign_changes = tf.abs(signs[:, 1:, :] - signs[:, :-1, :])\n","        zcr = tf.reduce_mean(sign_changes, axis=1, keepdims=True) / 2.0\n","\n","        x_t1 = x[:, :-1, :]\n","        x_tn1 = x[:, 1:, :]\n","        ar1 = tf.reduce_sum(x_t1 * x_tn1, axis=1, keepdims=True) / (\n","            tf.reduce_sum(tf.square(x_t1), axis=1, keepdims=True) + self.eps\n","        )\n","        x_t2 = x[:, :-2, :]\n","        x_tn2 = x[:, 2:, :]\n","        ar2 = tf.reduce_sum(x_t2 * x_tn2, axis=1, keepdims=True) / (\n","            tf.reduce_sum(tf.square(x_t2), axis=1, keepdims=True) + self.eps\n","        )\n","\n","        # Frequency-domain features\n","        xc = x - mean\n","        x_bc_t = tf.transpose(xc, [0, 2, 1])  # [B,C,T]\n","        fft = tf.signal.rfft(x_bc_t)  # [B,C,F]\n","        power = tf.square(tf.abs(fft)) + self.eps  # [B,C,F]\n","        power = tf.transpose(power, [0, 2, 1])  # [B,F,C]\n","        F = tf.shape(power)[1]\n","        freqs = tf.linspace(0.0, tf.cast(self.fs, tf.float32) / 2.0, F)\n","        freqs = tf.reshape(freqs, [1, F, 1])\n","\n","        p = power / (tf.reduce_sum(power, axis=1, keepdims=True) + self.eps)\n","        centroid = tf.reduce_sum(p * freqs, axis=1, keepdims=True)\n","        entropy = -tf.reduce_sum(p * tf.math.log(p + self.eps), axis=1, keepdims=True) / (\n","            tf.math.log(tf.cast(F, tf.float32) + self.eps)\n","        )\n","        geo = tf.exp(tf.reduce_mean(tf.math.log(power), axis=1, keepdims=True))\n","        ari = tf.reduce_mean(power, axis=1, keepdims=True)\n","        flatness = geo / (ari + self.eps)\n","\n","        temp = 10.0\n","        w = tf.nn.softmax(power * temp, axis=1)\n","        soft_peak = tf.reduce_sum(w * freqs, axis=1, keepdims=True)\n","\n","        def band(low, high):\n","            mask = tf.cast((freqs >= low) & (freqs < high), tf.float32)\n","            bp = tf.reduce_sum(power * mask, axis=1, keepdims=True) / (\n","                tf.reduce_sum(power, axis=1, keepdims=True) + self.eps\n","            )\n","            return bp\n","\n","        bp1 = band(0.5, 3.0)\n","        bp2 = band(3.0, 8.0)\n","        bp3 = band(8.0, 15.0)\n","\n","        feats = [\n","            mean,\n","            std,\n","            maxv,\n","            minv,\n","            ptp,\n","            rms,\n","            energy,\n","            skew,\n","            kurt,\n","            zcr,\n","            ar1,\n","            ar2,\n","            centroid,\n","            entropy,\n","            flatness,\n","            soft_peak,\n","            bp1,\n","            bp2,\n","            bp3,\n","        ]\n","        res = tf.concat(feats, axis=1)  # [B,Fnum,C]\n","        return tf.transpose(res, [0, 2, 1])  # [B,C,Fnum]\n","\n","# -- Step 10: multi-head 3D rotation layer (original architecture) --\n","class Multihead3DRotation(Layer):\n","    def __init__(self, head_nums=2, base_kn=64, param_depth=2, **kwargs):\n","        super().__init__(**kwargs)\n","        self.head_nums = head_nums\n","        self.base_kn = base_kn\n","        self.param_depth = param_depth\n","        self.eps = 1e-8\n","        self.gap = GlobalAveragePooling1D()\n","        self.mlp = [Dense(self.base_kn, activation='relu') for _ in range(self.param_depth)]\n","        self.out_heads = [Dense(4, activation='tanh') for _ in range(self.head_nums)]\n","\n","    def get_config(self):\n","        cfg = super().get_config()\n","        cfg.update(\n","            {\n","                'head_nums': self.head_nums,\n","                'base_kn': self.base_kn,\n","                'param_depth': self.param_depth,\n","            }\n","        )\n","        return cfg\n","\n","    def compute_output_shape(self, input_shape):\n","        return [tf.TensorShape(input_shape) for _ in range(self.head_nums)]\n","\n","    def _axis_angle_to_R(self, axis_raw, angle_raw):\n","        axis = axis_raw / (tf.norm(axis_raw, axis=-1, keepdims=True) + self.eps)\n","        theta = angle_raw * math.pi\n","\n","        B = tf.shape(axis)[0]\n","        ux, uy, uz = axis[:, 0], axis[:, 1], axis[:, 2]\n","        z = tf.zeros_like(ux)\n","        Kmat = tf.stack(\n","            [z, -uz, uy, uz, z, -ux, -uy, ux, z],\n","            axis=-1,\n","        )\n","        Kmat = tf.reshape(Kmat, [B, 3, 3])\n","        I3 = tf.eye(3, dtype=axis.dtype)\n","        I = tf.tile(I3[None, ...], [B, 1, 1])\n","        u = tf.expand_dims(axis, -1)\n","        uuT = tf.matmul(u, u, transpose_b=True)\n","        cos = tf.reshape(tf.cos(theta), [-1, 1, 1])\n","        sin = tf.reshape(tf.sin(theta), [-1, 1, 1])\n","        R = cos * I + (1.0 - cos) * uuT + sin * Kmat\n","        return R\n","\n","    def call(self, x):  # x: [B,T,6]\n","        acc, gyr = x[:, :, :3], x[:, :, 3:6]\n","        pooled = self.gap(x)\n","        h = pooled\n","        for layer in self.mlp:\n","            h = layer(h)\n","        out_list = []\n","        for oh in self.out_heads:\n","            p = oh(h)\n","            axis = p[:, :3]\n","            angle = tf.expand_dims(p[:, 3], -1)\n","            R = self._axis_angle_to_R(axis, angle)\n","            acc_t = tf.transpose(acc, [0, 2, 1])\n","            acc_rot_t = tf.matmul(R, acc_t)\n","            acc_rot = tf.transpose(acc_rot_t, [0, 2, 1])\n","            gyr_t = tf.transpose(gyr, [0, 2, 1])\n","            gyr_rot_t = tf.matmul(R, gyr_t)\n","            gyr_rot = tf.transpose(gyr_rot_t, [0, 2, 1])\n","            out_list.append(tf.concat([acc_rot, gyr_rot], axis=-1))\n","        return out_list\n","\n","# -- Step 10: append L2-norm channels (original function) --\n","def add_l2_channels(x):\n","    acc = x[:, :, :3]\n","    gyr = x[:, :, 3:6]\n","    l2_acc = tf.sqrt(tf.reduce_sum(tf.square(acc), axis=-1, keepdims=True))\n","    l2_gyr = tf.sqrt(tf.reduce_sum(tf.square(gyr), axis=-1, keepdims=True))\n","    return tf.concat([x, l2_acc, l2_gyr], axis=-1)  # [B,T,8]\n","\n","# -- Step 10: rTsfNet main body (original architecture and hyperparameters) --\n","def r_tsf_net(\n","    x_shape,\n","    n_classes,\n","    learning_rate=LR,\n","    base_kn=MLP_BASE,\n","    depth=MLP_DEPTH,\n","    dropout_rate=DROPOUT,\n","    imu_rot_heads=IMU_ROT_HEADS,\n","    fs=FS,\n","    use_orig_input=USE_ORIG_INPUT,\n","):\n","    inputs = Input(shape=x_shape[1:])\n","    x = inputs\n","\n","    rot_layer = Multihead3DRotation(\n","        head_nums=imu_rot_heads, base_kn=64, param_depth=2, name='multihead_rot'\n","    )\n","    rotated_list = rot_layer(x)\n","\n","    streams = []\n","    if use_orig_input:\n","        streams.append(Lambda(add_l2_channels, name='orig_plus_l2')(x))\n","    for i, xr in enumerate(rotated_list):\n","        streams.append(Lambda(add_l2_channels, name=f'rot{i}_plus_l2')(xr))\n","\n","    concat_streams = Lambda(lambda lst: tf.concat(lst, axis=-1), name='concat_streams')(streams)\n","    tsf = TSFFeatureLayer(fs=fs, name='tsf')(concat_streams)\n","    z = Flatten(name='flatten')(tsf)\n","\n","    for k in range(depth - 1, -1, -1):\n","        z = Dense(base_kn * (2**k), kernel_regularizer=l2(WEIGHT_DECAY), name=f'fc_{k}')(z)\n","        z = LayerNormalization(epsilon=1e-7, name=f'ln_{k}')(z)\n","        z = LeakyReLU(name=f'lrelu_{k}')(z)\n","        z = Dropout(dropout_rate, name=f'drop_{k}')(z)\n","\n","    logits = Dense(n_classes, kernel_regularizer=l2(WEIGHT_DECAY), name='logits')(z)\n","    probs = Activation('softmax', dtype='float32', name='softmax')(logits)\n","\n","    model = Model(inputs, probs, name='rTsfNet_officially_aligned_fixed')\n","    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate, amsgrad=True)\n","    model.compile(\n","        loss='sparse_categorical_crossentropy',\n","        optimizer=opt,\n","        metrics=['accuracy'],\n","    )\n","    return model\n","\n","# ---------------- Data: prefer real features; otherwise generate demonstration data ----------------\n","def scan_available_folds(features_dir: Path):\n","    ks = []\n","    for f in features_dir.glob(\"windows_normalized_fold*.npz\"):\n","        try:\n","            ks.append(int(f.stem.replace(\"windows_normalized_fold\", \"\")))\n","        except Exception:\n","            pass\n","    return sorted(set(ks))\n","\n","available_folds = scan_available_folds(features_dir)\n","ACTIVE_FOLDS = available_folds[:1] if available_folds else [0]  # By default, only demonstrate fold 0; for more folds, use list(range(15))\n","print(f\"[Info] Available folds: {available_folds}  |  Planned measurement: {ACTIVE_FOLDS}\")\n","\n","def make_synth_fold(\n","    n_train=4000,\n","    n_test=800,\n","    T=150,\n","    C=6,\n","    n_classes=8,\n","    seed=2025,\n","):\n","    rng = np.random.default_rng(seed)\n","    Xtr = rng.normal(0, 1, size=(n_train, T, C)).astype(np.float32)\n","    Xte = rng.normal(0, 1, size=(n_test, T, C)).astype(np.float32)\n","    ytr = rng.integers(0, n_classes, size=n_train).astype(np.int64)\n","    yte = rng.integers(0, n_classes, size=n_test).astype(np.int64)\n","    return Xtr, ytr, Xte, yte\n","\n","# ---------------- TensorFlow inference wrapper: one \"call\" over the entire test set ----------------\n","def make_tf_runner(model: tf.keras.Model, X_test_np: np.ndarray, bs: int = 256):\n","    device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n","    with tf.device(device):\n","        # Keep resident in device memory to avoid transfer noise\n","        X_gpu = tf.convert_to_tensor(X_test_np.astype(np.float32))\n","    N = X_test_np.shape[0]\n","\n","    @tf.function(jit_compile=False)\n","    def fwd(x):\n","        return model(x, training=False)\n","\n","    def run_once():\n","        last = None\n","        for s in range(0, N, bs):\n","            e = min(N, s + bs)\n","            last = fwd(X_gpu[s:e])\n","        # Force device synchronization\n","        _ = tf.reduce_sum(last).numpy()\n","\n","    return run_once, N\n","\n","# ---------------- Idle power ----------------\n","if 'P_idle_mW' not in globals():\n","    print(\"\\n[Info] Measuring idle power for 20 s ...\")\n","    P_idle_mW, _idle = sample_idle_power_mW(\n","        duration_s=20.0,\n","        dev_index=0,\n","        interval=0.02,\n","        save_csv=str(logs_dir / 'power_idle_trace_rtsfnet.csv'),\n","    )\n","    print(f\"[Info] Mean idle power ~ {P_idle_mW:.1f} mW\")\n","else:\n","    print(f\"\\n[Info] Reusing previously measured idle power: {P_idle_mW:.1f} mW\")\n","\n","# ---------------- Main loop: per-fold GPU inference energy measurement (Option 1) ----------------\n","summary_rows = []\n","for k in ACTIVE_FOLDS:\n","    print(\"\\n\" + \"=\" * 72)\n","    print(f\"[rTsfNet] Fold {k} — preparing data and model (original architecture/hyperparameters)\")\n","\n","    if k in available_folds:\n","        X_train, y_train, X_test, y_test = load_fold_data(k, features_dir)\n","        n_classes = int(np.max(np.maximum(y_train.max(), y_test.max())) + 1)\n","    else:\n","        print(\"[Warn] Real features not found; using synthetic data to demonstrate the measurement pipeline (architecture/hyperparameters unchanged).\")\n","        X_train, y_train, X_test, y_test = make_synth_fold()\n","        n_classes = int(np.max(np.maximum(y_train.max(), y_test.max())) + 1)\n","\n","    model = r_tsf_net(\n","        x_shape=X_train.shape,\n","        n_classes=n_classes,\n","        learning_rate=LR,\n","        base_kn=MLP_BASE,\n","        depth=MLP_DEPTH,\n","        dropout_rate=DROPOUT,\n","        imu_rot_heads=IMU_ROT_HEADS,\n","        fs=FS,\n","        use_orig_input=USE_ORIG_INPUT,\n","    )\n","\n","    # If trained weights exist, load them (affects only accuracy checking, not energy)\n","    wpath = models_dir / f\"model_fold{k}.weights.h5\"\n","    if wpath.exists():\n","        try:\n","            model.load_weights(wpath)\n","            print(f\"[Info] Loaded weights: {wpath.name}\")\n","        except Exception as e:\n","            print(f\"[Warn] Failed to load weights: {e}\")\n","\n","    # Quick accuracy sanity check (not included in energy measurement)\n","    try:\n","        y_prob = model.predict(X_test, batch_size=256, verbose=0)\n","        y_hat = y_prob.argmax(1)\n","        acc = (y_hat == y_test).mean()\n","        print(f\"[Check] Fold {k} quick accuracy: {acc:.3f}\")\n","    except Exception as e:\n","        print(f\"[Warn] Skipping accuracy check: {e}\")\n","\n","    # Build run_once (one full forward pass over the test set)\n","    run_once, N = make_tf_runner(model, X_test, bs=256)\n","\n","    # Warm up a few times to avoid the initial graph construction overhead\n","    for _ in range(3):\n","        run_once()\n","\n","    # Adaptive window (≥ 8 s)\n","    rep = calibrate_repeats(run_once, target_s=8.0, min_rep=3, max_rep=5000)\n","    print(f\"[Info] repeats = {rep}  (each run: {N} samples)\")\n","\n","    # Measurement + bootstrap CI\n","    tag = f\"rtsfnet_fold{k}\"\n","    summ = measure_with_bootstrap(\n","        name=tag,\n","        run_once=run_once,\n","        n_items=N,\n","        repeats=rep,\n","        n_runs=5,\n","        n_boot=1000,\n","        logdir=logs_dir,\n","    )\n","\n","    summary_rows.append(\n","        {\n","            \"fold\": k,\n","            \"model\": f\"rTsfNet (fold {k})\",\n","            \"mJ/inf_mean\": summ[\"mean_mJ_per_inf\"],\n","            \"CI95_low\": summ[\"ci95_low\"],\n","            \"CI95_high\": summ[\"ci95_high\"],\n","        }\n","    )\n","\n","    # Clear graph and caches (architecture/parameters unchanged)\n","    K.clear_session()\n","    gc.collect()\n","\n","# ---------------- Summary output ----------------\n","df_sum = pd.DataFrame(summary_rows).sort_values(\"fold\").reset_index(drop=True)\n","df_sum.to_csv(logs_dir / \"energy_summary_rtsfnet.csv\", index=False)\n","print(\"\\n=== Completed (rTsfNet GPU inference energy · Option 1 · original architecture and hyperparameters) ===\")\n","print(df_sum)\n","print(\"\\nLog files:\")\n","print(\"- logs/power_idle_trace_rtsfnet.csv\")\n","print(\"- logs/power_trace_rtsfnet_fold*_run*.csv\")\n","print(\"- logs/energy_rtsfnet_fold*.json\")\n","print(\"- logs/energy_summary_rtsfnet.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bR2lmK_ZnlPa","executionInfo":{"status":"ok","timestamp":1763396496845,"user_tz":0,"elapsed":53843,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"f4fe00b0-51bc-437e-c93e-5b77064ec954"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Nov 17 16:20:43 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   50C    P0             27W /   70W |     316MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","+-----------------------------------------------------------------------------------------+\n","[Info] Available folds: []  |  Planned measurement: [0]\n","\n","[Info] Reusing previously measured idle power: 26257.8 mW\n","\n","========================================================================\n","[rTsfNet] Fold 0 — preparing data and model (original architecture/hyperparameters)\n","[Warn] Real features not found; using synthetic data to demonstrate the measurement pipeline (architecture/hyperparameters unchanged).\n","[Check] Fold 0 quick accuracy: 0.145\n","[Info] repeats = 527  (each run: 800 samples)\n","[Measure] rtsfnet_fold0 run 1/5 ...\n","[Measure] rtsfnet_fold0 run 2/5 ...\n","[Measure] rtsfnet_fold0 run 3/5 ...\n","[Measure] rtsfnet_fold0 run 4/5 ...\n","[Measure] rtsfnet_fold0 run 5/5 ...\n","[Result] rtsfnet_fold0: 0.528 mJ/inf  (95% CI [0.506, 0.543])\n","\n","=== Completed (rTsfNet GPU inference energy · Option 1 · original architecture and hyperparameters) ===\n","   fold             model  mJ/inf_mean  CI95_low  CI95_high\n","0     0  rTsfNet (fold 0)     0.528198  0.506465   0.543197\n","\n","Log files:\n","- logs/power_idle_trace_rtsfnet.csv\n","- logs/power_trace_rtsfnet_fold*_run*.csv\n","- logs/energy_rtsfnet_fold*.json\n","- logs/energy_summary_rtsfnet.csv\n"]}]},{"cell_type":"code","source":["# ================================================================\n","# rTsfNet (Step 10 official architecture aligned · TSF-Mixer variant)\n","# × Option 1: NVML-based GPU inference energy (Colab one-click)\n","# ================================================================\n","# 0) Environment check & dependencies\n","!nvidia-smi\n","!pip -q install pynvml\n","\n","import os, math, json, time, pathlib, gc, warnings, multiprocessing as mp\n","from pathlib import Path\n","warnings.filterwarnings(\"ignore\")\n","\n","import numpy as np\n","import pandas as pd\n","\n","# ---------------- Option 1: NVML sampling + numerical integration + idle subtraction ----------------\n","import pynvml\n","\n","def _nvml_sampler(stop_event, q, dev_index=0, interval=0.02):\n","    \"\"\"Subprocess: sample power (mW) every `interval` and send back (t_abs, mW).\"\"\"\n","    import time, pynvml\n","    pynvml.nvmlInit()\n","    h = pynvml.nvmlDeviceGetHandleByIndex(dev_index)\n","    try:\n","        while not stop_event.is_set():\n","            q.put((time.perf_counter(), pynvml.nvmlDeviceGetPowerUsage(h)))\n","            time.sleep(interval)\n","    finally:\n","        pynvml.nvmlShutdown()\n","\n","def _integrate_mJ_between(samples, t0, t1):\n","    \"\"\"Perform trapezoidal integration of power (mW) over [t0, t1], returning energy in mJ.\"\"\"\n","    if not samples:\n","        return 0.0\n","    samples = sorted(samples, key=lambda x: x[0])\n","    ts = np.array([t for t, _ in samples], dtype=np.float64)\n","    ps = np.array([p for _, p in samples], dtype=np.float64)\n","    m = (ts >= t0) & (ts <= t1)\n","    ts_w, ps_w = ts[m], ps[m]\n","    if ts_w.size == 0 or ts_w[0] > t0:\n","        p0 = np.interp(t0, ts, ps)\n","        ts_w = np.insert(ts_w, 0, t0)\n","        ps_w = np.insert(ps_w, 0, p0)\n","    if ts_w[-1] < t1:\n","        p1 = np.interp(t1, ts, ps)\n","        ts_w = np.append(ts_w, t1)\n","        ps_w = np.append(ps_w, p1)\n","    return float(np.trapz(ps_w, ts_w))  # mW*s = mJ\n","\n","def sample_idle_power_mW(duration_s=20.0, dev_index=0, interval=0.02, save_csv=None):\n","    \"\"\"Measure mean idle power (mW), optionally saving the power trace.\"\"\"\n","    import time\n","    q = mp.Queue()\n","    stop = mp.Event()\n","    p = mp.Process(target=_nvml_sampler, args=(stop, q, dev_index, interval))\n","    p.start()\n","    time.sleep(duration_s)\n","    stop.set()\n","    p.join()\n","    samples = []\n","    while not q.empty():\n","        samples.append(q.get())\n","    if not samples:\n","        raise RuntimeError(\"NVML captured no idle power samples.\")\n","    samples.sort(key=lambda x: x[0])\n","    t0, t1 = samples[0][0], samples[-1][0]\n","    E_idle_mJ = _integrate_mJ_between(samples, t0, t1)\n","    T_idle_s = max(1e-9, t1 - t0)\n","    P_idle_mW = E_idle_mJ / T_idle_s\n","    if save_csv:\n","        pd.DataFrame(samples, columns=[\"t_abs_s\", \"power_mW\"]).to_csv(save_csv, index=False)\n","    return P_idle_mW, samples\n","\n","def measure_mJ_per_inference(run_once, n_items_per_call, repeats, P_idle_mW,\n","                             dev_index=0, interval=0.02, save_csv=None):\n","    \"\"\"Concurrent NVML sampling + integration + idle subtraction; return mJ/inf and related metrics.\"\"\"\n","    import time\n","    q = mp.Queue()\n","    stop = mp.Event()\n","    p = mp.Process(target=_nvml_sampler, args=(stop, q, dev_index, interval))\n","    p.start()\n","    t0 = time.perf_counter()\n","    for _ in range(repeats):\n","        run_once()\n","    t1 = time.perf_counter()\n","    stop.set()\n","    p.join()\n","    samples = []\n","    while not q.empty():\n","        samples.append(q.get())\n","    if not samples:\n","        raise RuntimeError(\"NVML captured no power samples (active phase).\")\n","    E_total_mJ = _integrate_mJ_between(samples, t0, t1)\n","    T_total_s = max(1e-9, t1 - t0)\n","    E_idle_mJ = P_idle_mW * T_total_s\n","    n_inf = max(1, repeats * n_items_per_call)\n","    if save_csv:\n","        pd.DataFrame(samples, columns=[\"t_abs_s\", \"power_mW\"]).to_csv(save_csv, index=False)\n","    return {\n","        \"mJ_per_inf\": max(0.0, (E_total_mJ - E_idle_mJ) / n_inf),\n","        \"ms_per_inf\": (T_total_s / n_inf) * 1e3,\n","        \"throughput_inf_per_s\": n_inf / T_total_s,\n","        \"n_inferences\": n_inf,\n","        \"repeats\": repeats,\n","        \"T_total_s\": T_total_s,\n","        \"E_total_mJ\": E_total_mJ,\n","        \"E_idle_mJ\": E_idle_mJ,\n","        \"P_idle_mW\": P_idle_mW,\n","        \"t0_abs\": t0,\n","        \"t1_abs\": t1\n","    }\n","\n","def calibrate_repeats(run_once, target_s=8.0, min_rep=3, max_rep=5000):\n","    \"\"\"Estimate repeats such that one measurement window ≈ target_s (warm up to avoid first-call overhead).\"\"\"\n","    import time\n","    run_once()  # warm-up\n","    t0 = time.perf_counter()\n","    run_once()\n","    t1 = time.perf_counter()\n","    dt = max(1e-4, t1 - t0)\n","    reps = int(np.ceil(target_s / dt))\n","    return int(np.clip(reps, min_rep, max_rep))\n","\n","def measure_with_bootstrap(name, run_once, n_items, repeats, n_runs=5, n_boot=1000, logdir=Path(\"logs\")):\n","    \"\"\"Repeat n_runs times, compute bootstrap 95% CI, and save traces/summary.\"\"\"\n","    logdir.mkdir(exist_ok=True)\n","    res_list = []\n","    for i in range(n_runs):\n","        print(f\"[Measure] {name} run {i+1}/{n_runs} ...\")\n","        r = measure_mJ_per_inference(\n","            run_once, n_items, repeats, P_idle_mW,\n","            dev_index=0, interval=0.02,\n","            save_csv=str(logdir / f\"power_trace_{name}_run{i+1}.csv\")\n","        )\n","        res_list.append(r)\n","    mJs = np.array([r[\"mJ_per_inf\"] for r in res_list], dtype=np.float64)\n","    rng = np.random.default_rng(123)\n","    boots = [float(np.mean(mJs[rng.integers(0, len(mJs), size=len(mJs))])) for _ in range(n_boot)]\n","    ci_low, ci_high = np.percentile(boots, [2.5, 97.5])\n","    summary = {\n","        \"model\": name,\n","        \"mean_mJ_per_inf\": float(mJs.mean()),\n","        \"ci95_low\": float(ci_low),\n","        \"ci95_high\": float(ci_high),\n","        \"runs\": res_list\n","    }\n","    with open(logdir / f\"energy_{name}.json\", \"w\") as f:\n","        json.dump(summary, f, indent=2)\n","    print(f\"[Result] {name}: {summary['mean_mJ_per_inf']:.3f} mJ/inf (95% CI [{summary['ci95_low']:.3f}, {summary['ci95_high']:.3f}])\")\n","    return summary\n","\n","# ---------------- rTsfNet (Step 10: official architecture aligned · TSF-Mixer etc.) [architecture/hyperparameters unchanged] ----------------\n","import tensorflow as tf\n","from tensorflow.keras import Input\n","from tensorflow.keras.layers import (\n","    Dense, Dropout, LayerNormalization, LeakyReLU,\n","    Layer, Activation, TimeDistributed, Flatten, Concatenate, GlobalAveragePooling1D\n",")\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras import backend as K\n","\n","# Random seed & fixed hyperparameters (consistent with Step 10)\n","SEED = 42\n","tf.random.set_seed(SEED)\n","np.random.seed(SEED)\n","\n","FS = 50.0\n","IMU_ROT_HEADS = 2\n","MLP_BASE = 128\n","MLP_DEPTH = 3\n","DROPOUT = 0.5\n","LR = 1e-3\n","WEIGHT_DECAY = 1e-6\n","USE_ORIG_INPUT = True\n","USE_BINARY_SELECTION = True\n","LN_EPS = 1e-7\n","PAD_MODE = 'SYMMETRIC'\n","\n","# Keras 3 / Graph-safe MLP stack\n","class MLPStack(Layer):\n","    def __init__(self, base_kn=128, depth=3, drop=0.5, wd=0.0, ln_eps=1e-7, name=None):\n","        super().__init__(name=name)\n","        self.base_kn = int(base_kn)\n","        self.depth = int(depth)\n","        self.drop = float(drop)\n","        self.wd = float(wd)\n","        self.ln_eps = float(ln_eps)\n","        self.seq = []\n","        for k in range(self.depth - 1, -1, -1):\n","            self.seq.append(Dense(self.base_kn * (2**k), kernel_regularizer=l2(self.wd)))\n","            self.seq.append(LayerNormalization(epsilon=self.ln_eps))\n","            self.seq.append(LeakyReLU())\n","            self.seq.append(Dropout(self.drop))\n","\n","    @property\n","    def out_dim(self):\n","        return self.base_kn\n","\n","    def call(self, x, training=None):\n","        z = x\n","        for lyr in self.seq:\n","            z = lyr(z, training=training) if isinstance(lyr, Dropout) else lyr(z)\n","        return z\n","\n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape([input_shape[0], self.out_dim])\n","\n","# TSF (per-axis feature extraction)\n","TIME_FEATS = 12\n","FREQ_FEATS = 7\n","\n","class TSFFeatureLayer(Layer):\n","    def __init__(self, fs=50.0, use_time=True, use_freq=True, **kwargs):\n","        super().__init__(**kwargs)\n","        self.fs = float(fs)\n","        self.use_time = bool(use_time)\n","        self.use_freq = bool(use_freq)\n","        self.eps = 1e-8\n","        self._feat_dim = (TIME_FEATS if self.use_time else 0) + (FREQ_FEATS if self.use_freq else 0)\n","\n","    def get_config(self):\n","        c = super().get_config()\n","        c.update({'fs': self.fs, 'use_time': self.use_time, 'use_freq': self.use_freq})\n","        return c\n","\n","    def call(self, x):  # [B,L,C]\n","        feats = []\n","        if self.use_time:\n","            mean = tf.reduce_mean(x, axis=1, keepdims=True)\n","            std = tf.math.reduce_std(x, axis=1, keepdims=True) + self.eps\n","            maxv = tf.reduce_max(x, axis=1, keepdims=True)\n","            minv = tf.reduce_min(x, axis=1, keepdims=True)\n","            ptp = maxv - minv\n","            rms = tf.sqrt(tf.reduce_mean(tf.square(x), axis=1, keepdims=True))\n","            energy = tf.reduce_sum(tf.square(x), axis=1, keepdims=True)\n","            skew = tf.reduce_mean(tf.pow((x - mean) / std, 3), axis=1, keepdims=True)\n","            kurt = tf.reduce_mean(tf.pow((x - mean) / std, 4), axis=1, keepdims=True)\n","            signs = tf.sign(x)\n","            sign_changes = tf.abs(signs[:, 1:, :] - signs[:, :-1, :])\n","            zcr = tf.reduce_mean(sign_changes, axis=1, keepdims=True) / 2.0\n","            x_t1 = x[:, :-1, :]\n","            x_tn1 = x[:, 1:, :]\n","            ar1 = tf.reduce_sum(x_t1 * x_tn1, axis=1, keepdims=True) / (\n","                tf.reduce_sum(tf.square(x_t1), axis=1, keepdims=True) + self.eps\n","            )\n","            x_t2 = x[:, :-2, :]\n","            x_tn2 = x[:, 2:, :]\n","            ar2 = tf.reduce_sum(x_t2 * x_tn2, axis=1, keepdims=True) / (\n","                tf.reduce_sum(tf.square(x_t2), axis=1, keepdims=True) + self.eps\n","            )\n","            feats += [mean, std, maxv, minv, ptp, rms, energy, skew, kurt, zcr, ar1, ar2]\n","\n","        if self.use_freq:\n","            mean = tf.reduce_mean(x, axis=1, keepdims=True)\n","            xc = x - mean\n","            x_bc_t = tf.transpose(xc, [0, 2, 1])         # [B,C,L]\n","            fft = tf.signal.rfft(x_bc_t)                 # [B,C,F]\n","            power = tf.square(tf.abs(fft)) + self.eps    # [B,C,F]\n","            power = tf.transpose(power, [0, 2, 1])       # [B,F,C]\n","            F = tf.shape(power)[1]\n","            freqs = tf.linspace(0.0, tf.cast(self.fs, tf.float32) / 2.0, F)\n","            freqs = tf.reshape(freqs, [1, F, 1])\n","            p = power / (tf.reduce_sum(power, axis=1, keepdims=True) + self.eps)\n","            centroid = tf.reduce_sum(p * freqs, axis=1, keepdims=True)\n","            entropy = -tf.reduce_sum(p * tf.math.log(p + self.eps), axis=1, keepdims=True) / (\n","                tf.math.log(tf.cast(F, tf.float32) + self.eps)\n","            )\n","            geo = tf.exp(tf.reduce_mean(tf.math.log(power), axis=1, keepdims=True))\n","            ari = tf.reduce_mean(power, axis=1, keepdims=True)\n","            flatness = geo / (ari + self.eps)\n","            w = tf.nn.softmax(power * 10.0, axis=1)\n","            soft_peak = tf.reduce_sum(w * freqs, axis=1, keepdims=True)\n","\n","            def band(low, high):\n","                mask = tf.cast((freqs >= low) & (freqs < high), tf.float32)\n","                return tf.reduce_sum(power * mask, axis=1, keepdims=True) / (\n","                    tf.reduce_sum(power, axis=1, keepdims=True) + self.eps\n","                )\n","\n","            bp1 = band(0.5, 3.0)\n","            bp2 = band(3.0, 8.0)\n","            bp3 = band(8.0, 15.0)\n","            feats += [centroid, entropy, flatness, soft_peak, bp1, bp2, bp3]\n","\n","        res = tf.concat(feats, axis=1)                 # [B,Fnum,C]\n","        return tf.transpose(res, [0, 2, 1])            # [B,C,Fnum]\n","\n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape(\n","            [input_shape[0], input_shape[2],\n","             (TIME_FEATS if self.use_time else 0) + (FREQ_FEATS if self.use_freq else 0)]\n","        )\n","\n","# L2-norm augmentation layer\n","class AddL2Channels(Layer):\n","    def call(self, x, training=None):\n","        acc = x[:, :, :3]\n","        gyr = x[:, :, 3:6]\n","        l2_acc = tf.sqrt(tf.reduce_sum(tf.square(acc), axis=-1, keepdims=True))\n","        l2_gyr = tf.sqrt(tf.reduce_sum(tf.square(gyr), axis=-1, keepdims=True))\n","        return tf.concat([x, l2_acc, l2_gyr], axis=-1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape([input_shape[0], input_shape[1], 8])\n","\n","# Block framing utilities (Keras 3 / Graph-safe)\n","def _int_ceil_div(a, b):\n","    a = tf.cast(a, tf.int32)\n","    b = tf.cast(b, tf.int32)\n","    return tf.math.floordiv(a + b - 1, b)\n","\n","def frame_signal_with_padding(x, num_blocks, pad_mode='SYMMETRIC'):\n","    B = tf.shape(x)[0]\n","    T = tf.shape(x)[1]\n","    C = tf.shape(x)[2]\n","    nb = tf.cast(num_blocks, tf.int32)\n","    L = _int_ceil_div(T, nb)\n","    total = L * nb\n","    pad_len = total - T\n","    pad_left = tf.math.floordiv(pad_len, 2)\n","    pad_right = pad_len - pad_left\n","    paddings = tf.stack([\n","        tf.constant([0, 0], dtype=tf.int32),\n","        tf.stack([pad_left, pad_right]),\n","        tf.constant([0, 0], dtype=tf.int32)\n","    ], axis=0)\n","    x_pad = tf.pad(x, paddings, mode=pad_mode)\n","    x_blocks = tf.reshape(x_pad, [B, nb, L, C])\n","    return x_blocks\n","\n","class BlockTSFExtractor(Layer):\n","    def __init__(self, num_blocks, fs, use_time, use_freq, tag_spec=None, pad_mode='SYMMETRIC', name=None, **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.num_blocks = int(num_blocks)\n","        self.tsf = TSFFeatureLayer(fs=fs, use_time=use_time, use_freq=use_freq)\n","        self.tag_spec = tag_spec\n","        self.pad_mode = pad_mode\n","        self.tag_dim = 0 if (tag_spec is None or 'axis_tags' not in tag_spec) else int(tag_spec['axis_tags'].shape[1])\n","        self.base_feat_dim = (TIME_FEATS if use_time else 0) + (FREQ_FEATS if use_freq else 0)\n","        self.out_feat_dim = self.base_feat_dim + self.tag_dim\n","\n","    def get_config(self):\n","        c = super().get_config()\n","        c.update({\n","            'num_blocks': self.num_blocks,\n","            'fs': self.tsf.fs,\n","            'use_time': self.tsf.use_time,\n","            'use_freq': self.tsf.use_freq,\n","            'pad_mode': self.pad_mode\n","        })\n","        return c\n","\n","    def call(self, x, training=None):\n","        xb = frame_signal_with_padding(x, self.num_blocks, pad_mode=self.pad_mode)  # [B,K,L,C]\n","        B = tf.shape(xb)[0]\n","        K_ = tf.shape(xb)[1]\n","        L = tf.shape(xb)[2]\n","        C = tf.shape(xb)[3]\n","        xb2 = tf.reshape(xb, [B * K_, L, C])\n","        tsf_axis = self.tsf(xb2)\n","        tsf_axis = tf.reshape(tsf_axis, [B, K_, C, self.base_feat_dim])\n","        if self.tag_dim > 0:\n","            axis_tags = tf.convert_to_tensor(self.tag_spec['axis_tags'], dtype=tsf_axis.dtype)  # [A, tag_dim]\n","            axis_tags = tf.reshape(axis_tags, [1, 1, tf.shape(tsf_axis)[2], -1])\n","            axis_tags = tf.tile(axis_tags, [B, K_, 1, 1])\n","            tsf_axis = tf.concat([tsf_axis, axis_tags], axis=-1)\n","        return tsf_axis\n","\n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape([input_shape[0], self.num_blocks, input_shape[2], self.out_feat_dim])\n","\n","# Binary gate for axis/channel selection\n","class BinaryGate(Layer):\n","    def call(self, p, training=None):\n","        p = tf.clip_by_value(p, 0.0, 1.0)\n","        hard = tf.round(p)\n","        return hard + tf.stop_gradient(p - hard)\n","\n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape(input_shape)\n","\n","# TSF-Mixer components\n","class TSFMixerSubBlock(Layer):\n","    def __init__(self, axis_hidden=128, out_hidden=128, base_depth=2, drop=0.5, wd=0.0, ln_eps=1e-7, name=None):\n","        super().__init__(name=name)\n","        self.axis_hidden = int(axis_hidden)\n","        self.out_hidden = int(out_hidden)\n","        self.base_depth = int(base_depth)\n","        self.drop = float(drop)\n","        self.wd = float(wd)\n","        self.ln_eps = float(ln_eps)\n","        self.axis_mlp_layers = []\n","        for k in range(self.base_depth - 1, -1, -1):\n","            self.axis_mlp_layers.append(Dense(self.axis_hidden * (2**k), kernel_regularizer=l2(self.wd)))\n","            self.axis_mlp_layers.append(LayerNormalization(epsilon=self.ln_eps))\n","            self.axis_mlp_layers.append(LeakyReLU())\n","            self.axis_mlp_layers.append(Dropout(self.drop))\n","        self.out_stack = MLPStack(\n","            base_kn=self.out_hidden,\n","            depth=self.base_depth,\n","            drop=self.drop,\n","            wd=self.wd,\n","            ln_eps=self.ln_eps,\n","            name=f'{self.name}_out'\n","        )\n","\n","    def call(self, x, training=None, **kwargs):\n","        Bp = tf.shape(x)[0]\n","        A = tf.shape(x)[1]\n","        F = tf.shape(x)[2]\n","        x2 = tf.reshape(x, [Bp * A, F])\n","        z = x2\n","        for lyr in self.axis_mlp_layers:\n","            z = lyr(z, training=training) if isinstance(lyr, Dropout) else lyr(z)\n","        z = tf.reshape(z, [Bp, A, self.axis_hidden])\n","        z = tf.reshape(z, [Bp, A * self.axis_hidden])\n","        z = self.out_stack(z, training=training)\n","        return z\n","\n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape([input_shape[0], self.out_stack.out_dim])\n","\n","class TSFMixerBlock(Layer):\n","    def __init__(self, feat_dim, axis_hidden=128, out_hidden=128, base_depth=2, drop=0.5,\n","                 wd=0.0, ln_eps=1e-7, use_binary=True, name=None):\n","        super().__init__(name=name)\n","        self.use_binary = bool(use_binary)\n","        self.sub = TSFMixerSubBlock(\n","            axis_hidden, out_hidden, base_depth, drop, wd, ln_eps, name=f'{name}_sub'\n","        )\n","        self.axis_gate_dense = Dense(1, activation='sigmoid', name=f'{name}_axis_gate')\n","        self.chan_gate_dense = Dense(int(feat_dim), activation='sigmoid', name=f'{name}_chan_gate')\n","        self.bin_gate = BinaryGate(name=f'{name}_bin')\n","        self.out_stack = MLPStack(\n","            base_kn=out_hidden,\n","            depth=base_depth,\n","            drop=drop,\n","            wd=wd,\n","            ln_eps=ln_eps,\n","            name=f'{name}_out'\n","        )\n","\n","    def call(self, x, training=None, **kwargs):\n","        Bp = tf.shape(x)[0]\n","        A = tf.shape(x)[1]\n","        F = tf.shape(x)[2]\n","\n","        # Channel-wise gating\n","        x_mean_axis = tf.reduce_mean(x, axis=1)\n","        p_chan = self.chan_gate_dense(x_mean_axis)\n","        p_chan = tf.reshape(p_chan, [Bp, 1, F])\n","        g_chan = self.bin_gate(p_chan, training=training) if self.use_binary else p_chan\n","        x = x * g_chan\n","\n","        # Axis-wise gating\n","        x2 = tf.reshape(x, [Bp * A, F])\n","        z = x2\n","        for lyr in self.sub.axis_mlp_layers:\n","            z = lyr(z, training=training) if isinstance(lyr, Dropout) else lyr(z)\n","        z = tf.reshape(z, [Bp, A, self.sub.axis_hidden])\n","        p_axis = self.axis_gate_dense(z)\n","        g_axis = self.bin_gate(p_axis, training=training) if self.use_binary else p_axis\n","        z = z * g_axis\n","        z = tf.reshape(z, [Bp, A * self.sub.axis_hidden])\n","\n","        # Output stack\n","        z = self.out_stack(z, training=training)\n","        return z\n","\n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape([input_shape[0], self.out_stack.out_dim])\n","\n","# Rotation parameter estimation & multi-head 3D rotation (cumulative across heads)\n","def _feat_dim_for_spec(use_time, use_freq, tag_dim):\n","    base = (TIME_FEATS if use_time else 0) + (FREQ_FEATS if use_freq else 0)\n","    return base + tag_dim\n","\n","BLOCK_SPECS = [\n","    dict(name='short', num_blocks=4, use_time=True,  use_freq=False),\n","    dict(name='long',  num_blocks=1, use_time=False, use_freq=True),\n","]\n","\n","class RotationParamEstimator(Layer):\n","    def __init__(self, block_specs, fs, mlp_base=128, mlp_depth=2, drop=0.5,\n","                 wd=0.0, ln_eps=1e-7, use_binary=True, pad_mode='SYMMETRIC', name=None):\n","        super().__init__(name=name)\n","        self.block_specs = block_specs\n","        self.fs = fs\n","        self.mlp_base = int(mlp_base)\n","        self.mlp_depth = int(mlp_depth)\n","        self.drop = float(drop)\n","        self.wd = float(wd)\n","        self.ln_eps = float(ln_eps)\n","        self.use_binary = bool(use_binary)\n","        self.pad_mode = pad_mode\n","\n","        # Axis tags: [axis_type, sensor_type]\n","        axis_tags = []\n","        for i in range(8):\n","            axis_type = i + 1\n","            sensor_type = 1 if (i <= 2 or i == 6) else 2\n","            axis_tags.append([axis_type, sensor_type])\n","        axis_tags = np.array(axis_tags, dtype=np.float32)\n","        self.tag_spec = {'axis_tags': axis_tags}\n","        tag_dim = axis_tags.shape[1]\n","\n","        self.extractors, self.td_mixers, self.flatteners = [], [], []\n","        for spec in block_specs:\n","            ext = BlockTSFExtractor(\n","                num_blocks=spec['num_blocks'], fs=fs,\n","                use_time=spec['use_time'], use_freq=spec['use_freq'],\n","                tag_spec=self.tag_spec, pad_mode=self.pad_mode,\n","                name=f'rot_ext_{spec[\"name\"]}'\n","            )\n","            self.extractors.append(ext)\n","            feat_dim = _feat_dim_for_spec(spec['use_time'], spec['use_freq'], tag_dim)\n","            mix = TSFMixerBlock(\n","                feat_dim=feat_dim, axis_hidden=self.mlp_base, out_hidden=self.mlp_base,\n","                base_depth=max(1, self.mlp_depth - 1), drop=self.drop, wd=self.wd,\n","                ln_eps=self.ln_eps, use_binary=self.use_binary, name=f'rot_mix_{spec[\"name\"]}'\n","            )\n","            self.td_mixers.append(TimeDistributed(mix, name=f'rot_td_{spec[\"name\"]}'))\n","            self.flatteners.append(Flatten(name=f'rot_flat_{spec[\"name\"]}'))\n","\n","        self.concat_sets = Concatenate(name='rot_concat_sets')\n","        self.post_stack = MLPStack(\n","            base_kn=self.mlp_base, depth=self.mlp_depth,\n","            drop=self.drop, wd=self.wd, ln_eps=self.ln_eps,\n","            name='rot_post'\n","        )\n","        self.out_head = Dense(4, activation='tanh', name='rot4_tanh')\n","        self.add_l2 = AddL2Channels()\n","\n","    def call(self, x, training=None, **kwargs):\n","        x8 = self.add_l2(x)\n","        feats_all = []\n","        for ext, td, flt in zip(self.extractors, self.td_mixers, self.flatteners):\n","            tsf_blocks = ext(x8, training=training)\n","            blk_feat = td(tsf_blocks, training=training)\n","            blk_feat = flt(blk_feat)\n","            feats_all.append(blk_feat)\n","        h = self.concat_sets(feats_all)\n","        h = self.post_stack(h, training=training)\n","        rot4 = self.out_head(h)\n","        return rot4\n","\n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape([input_shape[0], 4])\n","\n","class Multihead3DRotationOfficial(Layer):\n","    def __init__(self, head_nums=2, fs=50.0, mlp_base=128, mlp_depth=2, drop=0.5,\n","                 wd=0.0, ln_eps=1e-7, block_specs=None, use_binary=True,\n","                 pad_mode='SYMMETRIC', name=None):\n","        super().__init__(name=name)\n","        if block_specs is None:\n","            block_specs = BLOCK_SPECS\n","        self.head_nums = int(head_nums)\n","        self.estimator = RotationParamEstimator(\n","            block_specs=block_specs, fs=fs, mlp_base=mlp_base, mlp_depth=mlp_depth,\n","            drop=drop, wd=wd, ln_eps=ln_eps, use_binary=use_binary,\n","            pad_mode=pad_mode, name='rot_estimator'\n","        )\n","        self.eps = 1e-8\n","\n","    def compute_output_shape(self, input_shape):\n","        return [tf.TensorShape(input_shape) for _ in range(self.head_nums)]\n","\n","    def _axis_angle_to_R(self, axis_raw, angle_raw):\n","        axis = axis_raw / (tf.norm(axis_raw, axis=-1, keepdims=True) + self.eps)\n","        theta = angle_raw * math.pi\n","        B = tf.shape(axis)[0]\n","        ux, uy, uz = axis[:, 0], axis[:, 1], axis[:, 2]\n","        z = tf.zeros_like(ux)\n","        K = tf.stack([z, -uz, uy, uz, z, -ux, -uy, ux, z], axis=-1)\n","        K = tf.reshape(K, [B, 3, 3])\n","        I = tf.tile(tf.eye(3, dtype=axis.dtype)[None, ...], [B, 1, 1])\n","        u = tf.expand_dims(axis, -1)\n","        uuT = tf.matmul(u, u, transpose_b=True)\n","        cos = tf.reshape(tf.cos(theta), [-1, 1, 1])\n","        sin = tf.reshape(tf.sin(theta), [-1, 1, 1])\n","        R = cos * I + (1.0 - cos) * uuT + sin * K\n","        return R\n","\n","    def call(self, x, training=None, **kwargs):\n","        acc, gyr = x[:, :, :3], x[:, :, 3:6]\n","        out_list = []\n","        prev_rot4 = None\n","        for _ in range(self.head_nums):\n","            rot4 = self.estimator(x, training=training)\n","            if prev_rot4 is not None:\n","                rot4 = rot4 + prev_rot4\n","            prev_rot4 = rot4\n","            axis = rot4[:, :3]\n","            angle = tf.expand_dims(rot4[:, 3], -1)\n","            R = self._axis_angle_to_R(axis, angle)\n","            acc_t = tf.transpose(acc, [0, 2, 1])\n","            acc_rot = tf.transpose(tf.matmul(R, acc_t), [0, 2, 1])\n","            gyr_t = tf.transpose(gyr, [0, 2, 1])\n","            gyr_rot = tf.transpose(tf.matmul(R, gyr_t), [0, 2, 1])\n","            out_list.append(tf.concat([acc_rot, gyr_rot], axis=-1))\n","        return out_list\n","\n","# Main network (official architecture aligned)\n","class AddL2ChannelsPublic(Layer):\n","    def call(self, x, training=None):\n","        acc = x[:, :, :3]\n","        gyr = x[:, :, 3:6]\n","        l2_acc = tf.sqrt(tf.reduce_sum(tf.square(acc), axis=-1, keepdims=True))\n","        l2_gyr = tf.sqrt(tf.reduce_sum(tf.square(gyr), axis=-1, keepdims=True))\n","        return tf.concat([x, l2_acc, l2_gyr], axis=-1)\n","\n","def r_tsf_net_official(x_shape, n_classes,\n","                       learning_rate=LR, base_kn=MLP_BASE, depth=MLP_DEPTH, dropout_rate=DROPOUT,\n","                       imu_rot_heads=IMU_ROT_HEADS, fs=FS, use_orig_input=USE_ORIG_INPUT,\n","                       use_binary_selection=USE_BINARY_SELECTION, ln_eps=LN_EPS, pad_mode=PAD_MODE):\n","    inputs = Input(shape=x_shape[1:])\n","    x = inputs\n","\n","    # Multi-head rotation with TSF-Mixer-based parameter estimation\n","    rot_layer = Multihead3DRotationOfficial(\n","        head_nums=imu_rot_heads, fs=fs,\n","        mlp_base=base_kn, mlp_depth=max(1, depth - 1),\n","        drop=dropout_rate, wd=WEIGHT_DECAY,\n","        ln_eps=ln_eps, block_specs=BLOCK_SPECS,\n","        use_binary=use_binary_selection, pad_mode=pad_mode,\n","        name='multihead_rot_official'\n","    )\n","    rotated_list = rot_layer(x)\n","\n","    # L2-augmented streams (original + rotated)\n","    streams = []\n","    add_l2 = AddL2ChannelsPublic()\n","    if use_orig_input:\n","        streams.append(add_l2(x))\n","    for xr in rotated_list:\n","        streams.append(add_l2(xr))\n","    concat_streams = Concatenate(axis=-1, name='concat_streams')(streams)\n","\n","    # Axis/channel binary selection + block-wise TSF-Mixer backbone\n","    axis_tags_one_stream = []\n","    for i in range(8):\n","        axis_type = i + 1\n","        sensor_type = 1 if (i <= 2 or i == 6) else 2\n","        axis_tags_one_stream.append([axis_type, sensor_type])\n","    axis_tags_one_stream = np.array(axis_tags_one_stream, dtype=np.float32)\n","    num_streams = (1 if use_orig_input else 0) + imu_rot_heads\n","    axis_tags_all = np.concatenate([axis_tags_one_stream for _ in range(num_streams)], axis=0)\n","    tag_spec_main = {'axis_tags': axis_tags_all}\n","    tag_dim_main = axis_tags_all.shape[1]\n","\n","    feats_all_sets = []\n","    for spec in BLOCK_SPECS:\n","        ext = BlockTSFExtractor(\n","            num_blocks=spec['num_blocks'], fs=fs,\n","            use_time=spec['use_time'], use_freq=spec['use_freq'],\n","            tag_spec=tag_spec_main, pad_mode=pad_mode,\n","            name=f'main_ext_{spec[\"name\"]}'\n","        )\n","        feat_dim = _feat_dim_for_spec(spec['use_time'], spec['use_freq'], tag_dim_main)\n","        mix = TSFMixerBlock(\n","            feat_dim=feat_dim, axis_hidden=base_kn, out_hidden=base_kn,\n","            base_depth=max(1, depth - 1), drop=dropout_rate,\n","            wd=WEIGHT_DECAY, ln_eps=ln_eps,\n","            use_binary=use_binary_selection, name=f'main_mix_{spec[\"name\"]}'\n","        )\n","        td = TimeDistributed(mix, name=f'main_td_{spec[\"name\"]}')\n","        flt = Flatten(name=f'main_flat_{spec[\"name\"]}')\n","        tsf_blocks = ext(concat_streams)\n","        blk_feat = td(tsf_blocks)\n","        blk_feat = flt(blk_feat)\n","        feats_all_sets.append(blk_feat)\n","\n","    z = Concatenate(name='main_concat_sets')(feats_all_sets)\n","    cls_stack = MLPStack(\n","        base_kn=base_kn, depth=depth, drop=dropout_rate,\n","        wd=WEIGHT_DECAY, ln_eps=ln_eps, name='cls'\n","    )\n","    z = cls_stack(z)\n","    logits = Dense(n_classes, kernel_regularizer=l2(WEIGHT_DECAY), name='logits')(z)\n","    probs = Activation('softmax', dtype='float32', name='softmax')(logits)\n","\n","    model = Model(inputs, probs, name='rTsfNet_official_aligned')\n","    opt = Adam(learning_rate=learning_rate, amsgrad=True)\n","    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","    return model\n","\n","# ---------------- Data: prefer /content/features; otherwise generate synthetic data for demonstration ----------------\n","BASE = Path('/content')\n","features_dir = BASE / 'features'\n","models_dir = BASE / 'models'\n","logs_dir = BASE / 'logs'\n","models_dir.mkdir(parents=True, exist_ok=True)\n","logs_dir.mkdir(parents=True, exist_ok=True)\n","\n","def load_fold_data(fold_k, features_dir: Path):\n","    \"\"\"Data loading logic consistent with Step 10.\"\"\"\n","    npz_file = features_dir / f'windows_normalized_fold{fold_k}.npz'\n","    data = np.load(npz_file, allow_pickle=True)\n","    X = np.stack(\n","        [data['acc_x'], data['acc_y'], data['acc_z'],\n","         data['gyro_x'], data['gyro_y'], data['gyro_z']],\n","        axis=-1\n","    )  # [N,T,6]\n","    y = data['labels']\n","    splits = data['splits']\n","    train_mask = splits == 'train'\n","    test_mask = splits == 'test'\n","    return X[train_mask], y[train_mask], X[test_mask], y[test_mask]\n","\n","def scan_available_folds(features_dir: Path):\n","    ks = []\n","    for f in features_dir.glob(\"windows_normalized_fold*.npz\"):\n","        try:\n","            ks.append(int(f.stem.replace(\"windows_normalized_fold\", \"\")))\n","        except Exception:\n","            pass\n","    return sorted(set(ks))\n","\n","def make_synth_fold(n_train=4000, n_test=800, T=150, C=6, n_classes=8, seed=2025):\n","    rng = np.random.default_rng(seed)\n","    Xtr = rng.normal(0, 1, size=(n_train, T, C)).astype(np.float32)\n","    Xte = rng.normal(0, 1, size=(n_test, T, C)).astype(np.float32)\n","    ytr = rng.integers(0, n_classes, size=n_train).astype(np.int64)\n","    yte = rng.integers(0, n_classes, size=n_test).astype(np.int64)\n","    return Xtr, ytr, Xte, yte\n","\n","available_folds = scan_available_folds(features_dir)\n","# By default, measure 1 fold; change to list(range(15)) to cover all folds.\n","ACTIVE_FOLDS = available_folds[:1] if available_folds else [0]\n","print(f\"[Info] Available folds: {available_folds} | Planned measurement: {ACTIVE_FOLDS}\")\n","\n","# ---------------- TF inference wrapper (entire test set as one logical “call”) ----------------\n","# Optional: enable memory growth to avoid pre-allocating all GPU memory\n","for g in tf.config.list_physical_devices('GPU'):\n","    try:\n","        tf.config.experimental.set_memory_growth(g, True)\n","    except Exception:\n","        pass\n","\n","def make_tf_runner(model: tf.keras.Model, X_test_np: np.ndarray, bs: int = 256):\n","    \"\"\"Run inference over the full test set once; trailing .numpy() forces device sync for NVML boundaries.\"\"\"\n","    device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n","    with tf.device(device):\n","        X_gpu = tf.convert_to_tensor(X_test_np.astype(np.float32))\n","    N = X_test_np.shape[0]\n","\n","    @tf.function(jit_compile=False)\n","    def fwd(x):\n","        return model(x, training=False)\n","\n","    def run_once():\n","        last = None\n","        for s in range(0, N, bs):\n","            e = min(N, s + bs)\n","            last = fwd(X_gpu[s:e])\n","        _ = tf.reduce_sum(last).numpy()  # force synchronization\n","\n","    return run_once, N\n","\n","# ---------------- Idle power ----------------\n","print(\"\\n[Info] Measuring idle power for 20 s ...\")\n","P_idle_mW, _idle = sample_idle_power_mW(\n","    duration_s=20.0, dev_index=0, interval=0.02,\n","    save_csv=str(logs_dir / 'power_idle_trace_rtsfnet_official.csv')\n",")\n","print(f\"[Info] Mean idle power ~ {P_idle_mW:.1f} mW\")\n","\n","# ---------------- Main pipeline: per-fold GPU inference energy measurement (Option 1) ----------------\n","summary_rows = []\n","for k in ACTIVE_FOLDS:\n","    print(\"\\n\" + \"=\" * 72)\n","    print(f\"[rTsfNet-Official] Fold {k} — preparing data and model (original architecture/hyperparameters)\")\n","\n","    if k in available_folds:\n","        X_train, y_train, X_test, y_test = load_fold_data(k, features_dir)\n","        n_classes = int(max(y_train.max(), y_test.max()) + 1)\n","    else:\n","        print(\"[Warn] Real feature files not found; using synthetic data for demonstration (energy measurement pipeline unchanged).\")\n","        X_train, y_train, X_test, y_test = make_synth_fold()\n","        n_classes = int(max(y_train.max(), y_test.max()) + 1)\n","\n","    model = r_tsf_net_official(\n","        x_shape=X_train.shape, n_classes=n_classes,\n","        learning_rate=LR, base_kn=MLP_BASE, depth=MLP_DEPTH, dropout_rate=DROPOUT,\n","        imu_rot_heads=IMU_ROT_HEADS, fs=FS, use_orig_input=USE_ORIG_INPUT,\n","        use_binary_selection=USE_BINARY_SELECTION, ln_eps=LN_EPS, pad_mode=PAD_MODE\n","    )\n","\n","    # Load trained weights if available (affects accuracy only, not inference energy)\n","    wpath = models_dir / f\"model_fold{k}.weights.h5\"\n","    if wpath.exists():\n","        try:\n","            model.load_weights(wpath)\n","            print(f\"[Info] Loaded weights: {wpath.name}\")\n","        except Exception as e:\n","            print(f\"[Warn] Failed to load weights: {e}\")\n","\n","    # Quick accuracy check (excluded from energy measurement)\n","    try:\n","        y_prob = model.predict(X_test, batch_size=256, verbose=0)\n","        y_hat = y_prob.argmax(1)\n","        acc = (y_hat == y_test).mean()\n","        print(f\"[Check] Fold {k} quick accuracy: {acc:.3f}\")\n","    except Exception as e:\n","        print(f\"[Warn] Skipping accuracy check: {e}\")\n","\n","    # Build run_once (one forward pass over the full test set)\n","    run_once, N = make_tf_runner(model, X_test, bs=256)\n","\n","    # Warm up a few times to avoid first-time graph construction overhead\n","    for _ in range(3):\n","        run_once()\n","\n","    # Adaptive window (≥ 8 s)\n","    rep = calibrate_repeats(run_once, target_s=8.0, min_rep=3, max_rep=5000)\n","    print(f\"[Info] repeats = {rep}  (each run processes {N} samples)\")\n","\n","    # Measurement + bootstrap CI\n","    tag = f\"rtsfnet_official_fold{k}\"\n","    summ = measure_with_bootstrap(\n","        name=tag, run_once=run_once, n_items=N, repeats=rep,\n","        n_runs=5, n_boot=1000, logdir=logs_dir\n","    )\n","\n","    summary_rows.append({\n","        \"fold\": k,\n","        \"model\": f\"rTsfNet-Official (fold {k})\",\n","        \"mJ/inf_mean\": summ[\"mean_mJ_per_inf\"],\n","        \"CI95_low\": summ[\"ci95_low\"],\n","        \"CI95_high\": summ[\"ci95_high\"]\n","    })\n","\n","    # Clear graph and caches (architecture/parameters unchanged)\n","    K.clear_session()\n","    gc.collect()\n","\n","# ---------------- Summary output ----------------\n","df_sum = pd.DataFrame(summary_rows).sort_values(\"fold\").reset_index(drop=True)\n","df_sum.to_csv(logs_dir / \"energy_summary_rtsfnet_official.csv\", index=False)\n","print(\"\\n=== Completed (rTsfNet official architecture × Option 1: GPU inference energy) ===\")\n","print(df_sum)\n","print(\"\\nLog files:\")\n","print(\"- logs/power_idle_trace_rtsfnet_official.csv\")\n","print(\"- logs/power_trace_rtsfnet_official_fold*_run*.csv\")\n","print(\"- logs/energy_rtsfnet_official_fold*.json\")\n","print(\"- logs/energy_summary_rtsfnet_official.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kLZFlyx4ovLN","executionInfo":{"status":"ok","timestamp":1763396839265,"user_tz":0,"elapsed":93667,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"8443b06f-2d06-45e5-b756-c46370ad7c74"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Nov 17 16:25:46 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   50C    P0             27W /   70W |     608MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","+-----------------------------------------------------------------------------------------+\n","[Info] Available folds: [] | Planned measurement: [0]\n","\n","[Info] Measuring idle power for 20 s ...\n","[Info] Mean idle power ~ 27835.6 mW\n","\n","========================================================================\n","[rTsfNet-Official] Fold 0 — preparing data and model (original architecture/hyperparameters)\n","[Warn] Real feature files not found; using synthetic data for demonstration (energy measurement pipeline unchanged).\n","[Check] Fold 0 quick accuracy: 0.138\n","[Info] repeats = 134  (each run processes 800 samples)\n","[Measure] rtsfnet_official_fold0 run 1/5 ...\n","[Measure] rtsfnet_official_fold0 run 2/5 ...\n","[Measure] rtsfnet_official_fold0 run 3/5 ...\n","[Measure] rtsfnet_official_fold0 run 4/5 ...\n","[Measure] rtsfnet_official_fold0 run 5/5 ...\n","[Result] rtsfnet_official_fold0: 2.213 mJ/inf (95% CI [2.180, 2.238])\n","\n","=== Completed (rTsfNet official architecture × Option 1: GPU inference energy) ===\n","   fold                      model  mJ/inf_mean  CI95_low  CI95_high\n","0     0  rTsfNet-Official (fold 0)     2.213496  2.179691   2.238095\n","\n","Log files:\n","- logs/power_idle_trace_rtsfnet_official.csv\n","- logs/power_trace_rtsfnet_official_fold*_run*.csv\n","- logs/energy_rtsfnet_official_fold*.json\n","- logs/energy_summary_rtsfnet_official.csv\n"]}]}]}