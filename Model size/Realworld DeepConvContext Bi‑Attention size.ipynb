{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyNwFck1hqY5KrqssDrruG4d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# ============ DeepConvContext (RWHAR · Bi-Attention, paper-aligned) – structure & size ============\n","\n","import json\n","from pathlib import Path\n","\n","import torch\n","import torch.nn as nn\n","\n","print(\"\\n[DeepConvContext (RWHAR · Bi-Attention, paper-aligned) – structure & size]\")\n","\n","# ---------------------------\n","# 1) Determine NUM_CLASSES\n","# ---------------------------\n","BASE = Path(\"/content\")\n","CFG_DIR = BASE / \"configs\"\n","\n","if (CFG_DIR / \"classes.json\").exists():\n","    with open(CFG_DIR / \"classes.json\", \"r\") as f:\n","        classes_cfg = json.load(f)\n","    NUM_CLASSES = int(classes_cfg[\"num_classes\"])\n","    print(f\"Detected NUM_CLASSES from configs: {NUM_CLASSES}\")\n","else:\n","    # Change this default if your experiment uses a different number of classes\n","    NUM_CLASSES = 8\n","    print(\"Warning: /content/configs/classes.json not found. Using default NUM_CLASSES = 8.\")\n","    print(\"Please update NUM_CLASSES manually if this does not match your setup.\")\n","\n","# ---------------------------\n","# 2) Hyperparameters (must match your Bi-Attention script)\n","# ---------------------------\n","NUM_CHANNELS     = 6\n","SAMPLES_PER_WIN  = 150\n","STRIDE_SAMPLES   = 75\n","\n","EPOCHS        = 30\n","LEARNING_RATE = 1e-4\n","WEIGHT_DECAY  = 1e-6\n","STEP_SIZE     = 10\n","GAMMA         = 0.9\n","\n","DROPOUT_P      = 0.5\n","HIDDEN_UNITS   = 128\n","CONV_CHANNELS  = 64\n","KERNEL_SIZE    = 9\n","PROJECTION_DIM = 128\n","ATTN_HEADS     = 4\n","ATTN_LAYERS    = 3\n","MAX_CONTEXT_WINS = 200   # used for positional embedding length\n","\n","print(f\"\\nConfig for size check:\")\n","print(f\"  NUM_CLASSES      = {NUM_CLASSES}\")\n","print(f\"  NUM_CHANNELS     = {NUM_CHANNELS}\")\n","print(f\"  CONV_CHANNELS    = {CONV_CHANNELS}\")\n","print(f\"  HIDDEN_UNITS     = {HIDDEN_UNITS}\")\n","print(f\"  PROJECTION_DIM   = {PROJECTION_DIM}\")\n","print(f\"  ATTN_HEADS       = {ATTN_HEADS}\")\n","print(f\"  ATTN_LAYERS      = {ATTN_LAYERS}\")\n","print(f\"  MAX_CONTEXT_WINS = {MAX_CONTEXT_WINS}\")\n","\n","# ---------------------------\n","# 3) Model definition (identical to your training script)\n","# ---------------------------\n","class DeepConvLSTM_Intra(nn.Module):\n","    \"\"\"\n","    Intra-window branch:\n","    4×Conv1d(64, k=9) + ReLU → 1-layer LSTM(128)\n","    \"\"\"\n","    def __init__(self, in_ch: int = 6, conv_ch: int = 64, kernel_size: int = 9, hidden: int = 128):\n","        super().__init__()\n","        pad = kernel_size // 2\n","        self.conv1 = nn.Conv1d(in_ch,   conv_ch, kernel_size, padding=pad)\n","        self.conv2 = nn.Conv1d(conv_ch, conv_ch, kernel_size, padding=pad)\n","        self.conv3 = nn.Conv1d(conv_ch, conv_ch, kernel_size, padding=pad)\n","        self.conv4 = nn.Conv1d(conv_ch, conv_ch, kernel_size, padding=pad)\n","        self.relu  = nn.ReLU(inplace=True)\n","        self.lstm  = nn.LSTM(\n","            input_size=conv_ch,\n","            hidden_size=hidden,\n","            num_layers=1,\n","            batch_first=True\n","        )\n","\n","    def forward(self, x_win: torch.Tensor) -> torch.Tensor:\n","        # x_win: (B, C, T)\n","        x = self.relu(self.conv1(x_win))\n","        x = self.relu(self.conv2(x))\n","        x = self.relu(self.conv3(x))\n","        x = self.relu(self.conv4(x))        # (B, conv_ch, T)\n","        x = x.permute(0, 2, 1)              # (B, T, conv_ch)\n","        _, (h_n, _) = self.lstm(x)          # h_n: (1, B, hidden)\n","        return h_n[-1]                      # (B, hidden)\n","\n","\n","class PositionalEncoding1D(nn.Module):\n","    \"\"\"\n","    Learnable 1D positional encoding for window index within the batch-context.\n","    \"\"\"\n","    def __init__(self, d_model: int, max_len: int = 200):\n","        super().__init__()\n","        self.pos_embedding = nn.Embedding(max_len, d_model)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        # x: (B_seq, S, D)\n","        seq_len = x.size(1)\n","        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)  # (1, S)\n","        pos = self.pos_embedding(positions)                              # (1, S, D)\n","        return x + pos\n","\n","\n","class MultiHeadSelfAttentionBlock(nn.Module):\n","    \"\"\"\n","    Pure self-attention block: MHA + residual + LayerNorm\n","    (dropout=0.0 inside MHA, as in your script)\n","    \"\"\"\n","    def __init__(self, dim: int, num_heads: int):\n","        super().__init__()\n","        self.mha  = nn.MultiheadAttention(\n","            embed_dim=dim,\n","            num_heads=num_heads,\n","            dropout=0.0,\n","            batch_first=True\n","        )\n","        self.norm = nn.LayerNorm(dim)\n","\n","    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor | None = None) -> torch.Tensor:\n","        attn_out, _ = self.mha(x, x, x, attn_mask=attn_mask, need_weights=False)\n","        return self.norm(x + attn_out)\n","\n","\n","class DeepConvContext_BiAttention(nn.Module):\n","    \"\"\"\n","    DeepConvContext Bi-Attention variant (paper-aligned):\n","      Intra: Conv×4 + LSTM(128)\n","      Projection: Linear(128 → 128)\n","      Inter: 3-layer 4-head self-attention stack, batch-as-context\n","      Positional encoding + optional causal mask (here fixed to bidirectional)\n","      Dropout(0.5) → FC(num_classes)\n","    \"\"\"\n","    def __init__(self,\n","                 num_channels: int = 6,\n","                 num_classes: int = 8,\n","                 conv_channels: int = 64,\n","                 hidden_intra: int = 128,\n","                 projection_dim: int = 128,\n","                 attn_heads: int = 4,\n","                 num_attn_layers: int = 3,\n","                 max_context_len: int = 200,\n","                 dropout: float = 0.5,\n","                 bidirectional: bool = True):\n","        super().__init__()\n","\n","        self.intra = DeepConvLSTM_Intra(\n","            in_ch=num_channels,\n","            conv_ch=conv_channels,\n","            kernel_size=KERNEL_SIZE,\n","            hidden=hidden_intra\n","        )\n","\n","        self.proj = nn.Linear(hidden_intra, projection_dim)\n","        self.pos_enc = PositionalEncoding1D(projection_dim, max_len=max_context_len)\n","\n","        self.bidirectional = bidirectional\n","        self.attn_layers = nn.ModuleList([\n","            MultiHeadSelfAttentionBlock(projection_dim, attn_heads)\n","            for _ in range(num_attn_layers)\n","        ])\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc      = nn.Linear(projection_dim, num_classes)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        x: (B, C, T) — windows sorted by time; batch dimension is the context sequence.\n","        Returns: (B, num_classes)\n","        \"\"\"\n","        B, C, T = x.shape\n","\n","        feats = self.intra(x)                # (B, hidden_intra)\n","        proj  = self.proj(feats)            # (B, D)\n","\n","        seq = proj.unsqueeze(0)             # (1, B, D)\n","        seq = self.pos_enc(seq)             # (1, B, D)\n","\n","        attn_mask = None\n","        if not self.bidirectional:\n","            S_len = B\n","            attn_mask = torch.ones(\n","                S_len, S_len,\n","                device=x.device,\n","                dtype=torch.bool\n","            ).triu(diagonal=1)             # upper-triangular mask for causal attention\n","\n","        for layer in self.attn_layers:\n","            seq = layer(seq, attn_mask=attn_mask)\n","\n","        seq = self.dropout(seq)\n","        logits = self.fc(seq)               # (1, B, num_classes)\n","        return logits.squeeze(0)            # (B, num_classes)\n","\n","# ---------------------------\n","# 4) Instantiate model and compute size\n","# ---------------------------\n","model = DeepConvContext_BiAttention(\n","    num_channels=NUM_CHANNELS,\n","    num_classes=NUM_CLASSES,\n","    conv_channels=CONV_CHANNELS,\n","    hidden_intra=HIDDEN_UNITS,\n","    projection_dim=PROJECTION_DIM,\n","    attn_heads=ATTN_HEADS,\n","    num_attn_layers=ATTN_LAYERS,\n","    max_context_len=MAX_CONTEXT_WINS,\n","    dropout=DROPOUT_P,\n","    bidirectional=True   # fixed to Bi-Attention variant\n",")\n","\n","print(\"\\n====== nn.Module structure ======\\n\")\n","print(model)\n","\n","# Parameter counts\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(\"\\n====== Parameter statistics ======\")\n","print(f\"Total params:      {total_params:,}\")\n","print(f\"Trainable params:  {trainable_params:,}\")\n","\n","print(\"\\n====== Per-layer parameter counts ======\")\n","for name, p in model.named_parameters():\n","    print(f\"{name:50s} shape={tuple(p.shape)}  params={p.numel():,}\")\n","\n","# Size estimation (parameters only)\n","def fmt_mb(n_bytes: int) -> str:\n","    return f\"{n_bytes / 1024 / 1024:.2f} MB\"\n","\n","bytes_fp32 = total_params * 4   # float32: 4 bytes per parameter\n","bytes_fp16 = total_params * 2   # float16: 2 bytes per parameter\n","\n","print(\"\\n====== Model size estimate (parameters only) ======\")\n","print(f\"FP32 (float32, 4B/param): {fmt_mb(bytes_fp32)}\")\n","print(f\"FP16 (float16, 2B/param): {fmt_mb(bytes_fp16)}\")\n","\n","# Save a randomly initialised state_dict to check actual .pth size\n","models_dir = BASE / \"models\"\n","models_dir.mkdir(parents=True, exist_ok=True)\n","tmp_path = models_dir / \"deepconvcontext_biattn_rwhar_dummy.pth\"\n","torch.save(model.state_dict(), tmp_path)\n","file_bytes = tmp_path.stat().st_size\n","print(f\"\\nRandom-initialised state_dict saved to {tmp_path.name}\")\n","print(f\"Actual .pth file size: {fmt_mb(file_bytes)}\")\n","tmp_path.unlink(missing_ok=True)\n","\n","print(\"\\n[DeepConvContext (RWHAR · Bi-Attention, paper-aligned) – structure & size done]\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LAae6NpIPekE","executionInfo":{"status":"ok","timestamp":1763406937957,"user_tz":0,"elapsed":72,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"0f76b3b5-47f7-4f6c-bbed-87f44fcb529f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[DeepConvContext (RWHAR · Bi-Attention, paper-aligned) – structure & size]\n","Warning: /content/configs/classes.json not found. Using default NUM_CLASSES = 8.\n","Please update NUM_CLASSES manually if this does not match your setup.\n","\n","Config for size check:\n","  NUM_CLASSES      = 8\n","  NUM_CHANNELS     = 6\n","  CONV_CHANNELS    = 64\n","  HIDDEN_UNITS     = 128\n","  PROJECTION_DIM   = 128\n","  ATTN_HEADS       = 4\n","  ATTN_LAYERS      = 3\n","  MAX_CONTEXT_WINS = 200\n","\n","====== nn.Module structure ======\n","\n","DeepConvContext_BiAttention(\n","  (intra): DeepConvLSTM_Intra(\n","    (conv1): Conv1d(6, 64, kernel_size=(9,), stride=(1,), padding=(4,))\n","    (conv2): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(4,))\n","    (conv3): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(4,))\n","    (conv4): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(4,))\n","    (relu): ReLU(inplace=True)\n","    (lstm): LSTM(64, 128, batch_first=True)\n","  )\n","  (proj): Linear(in_features=128, out_features=128, bias=True)\n","  (pos_enc): PositionalEncoding1D(\n","    (pos_embedding): Embedding(200, 128)\n","  )\n","  (attn_layers): ModuleList(\n","    (0-2): 3 x MultiHeadSelfAttentionBlock(\n","      (mha): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","      )\n","      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (fc): Linear(in_features=128, out_features=8, bias=True)\n",")\n","\n","====== Parameter statistics ======\n","Total params:      455,688\n","Trainable params:  455,688\n","\n","====== Per-layer parameter counts ======\n","intra.conv1.weight                                 shape=(64, 6, 9)  params=3,456\n","intra.conv1.bias                                   shape=(64,)  params=64\n","intra.conv2.weight                                 shape=(64, 64, 9)  params=36,864\n","intra.conv2.bias                                   shape=(64,)  params=64\n","intra.conv3.weight                                 shape=(64, 64, 9)  params=36,864\n","intra.conv3.bias                                   shape=(64,)  params=64\n","intra.conv4.weight                                 shape=(64, 64, 9)  params=36,864\n","intra.conv4.bias                                   shape=(64,)  params=64\n","intra.lstm.weight_ih_l0                            shape=(512, 64)  params=32,768\n","intra.lstm.weight_hh_l0                            shape=(512, 128)  params=65,536\n","intra.lstm.bias_ih_l0                              shape=(512,)  params=512\n","intra.lstm.bias_hh_l0                              shape=(512,)  params=512\n","proj.weight                                        shape=(128, 128)  params=16,384\n","proj.bias                                          shape=(128,)  params=128\n","pos_enc.pos_embedding.weight                       shape=(200, 128)  params=25,600\n","attn_layers.0.mha.in_proj_weight                   shape=(384, 128)  params=49,152\n","attn_layers.0.mha.in_proj_bias                     shape=(384,)  params=384\n","attn_layers.0.mha.out_proj.weight                  shape=(128, 128)  params=16,384\n","attn_layers.0.mha.out_proj.bias                    shape=(128,)  params=128\n","attn_layers.0.norm.weight                          shape=(128,)  params=128\n","attn_layers.0.norm.bias                            shape=(128,)  params=128\n","attn_layers.1.mha.in_proj_weight                   shape=(384, 128)  params=49,152\n","attn_layers.1.mha.in_proj_bias                     shape=(384,)  params=384\n","attn_layers.1.mha.out_proj.weight                  shape=(128, 128)  params=16,384\n","attn_layers.1.mha.out_proj.bias                    shape=(128,)  params=128\n","attn_layers.1.norm.weight                          shape=(128,)  params=128\n","attn_layers.1.norm.bias                            shape=(128,)  params=128\n","attn_layers.2.mha.in_proj_weight                   shape=(384, 128)  params=49,152\n","attn_layers.2.mha.in_proj_bias                     shape=(384,)  params=384\n","attn_layers.2.mha.out_proj.weight                  shape=(128, 128)  params=16,384\n","attn_layers.2.mha.out_proj.bias                    shape=(128,)  params=128\n","attn_layers.2.norm.weight                          shape=(128,)  params=128\n","attn_layers.2.norm.bias                            shape=(128,)  params=128\n","fc.weight                                          shape=(8, 128)  params=1,024\n","fc.bias                                            shape=(8,)  params=8\n","\n","====== Model size estimate (parameters only) ======\n","FP32 (float32, 4B/param): 1.74 MB\n","FP16 (float16, 2B/param): 0.87 MB\n","\n","Random-initialised state_dict saved to deepconvcontext_biattn_rwhar_dummy.pth\n","Actual .pth file size: 1.75 MB\n","\n","[DeepConvContext (RWHAR · Bi-Attention, paper-aligned) – structure & size done]\n","\n"]}]}]}