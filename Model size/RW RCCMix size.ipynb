{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyPc8KMSy0uwGwxgOwwZbARz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# ============ RCCMix-HAR++ (Step 11, GeoContextHARV2) – structure & size ============\n","\n","import json\n","from pathlib import Path\n","\n","import torch\n","import torch.nn as nn\n","\n","print(\"\\n[RCCMix-HAR++ (Step 11, GeoContextHARV2) – structure & size]\")\n","\n","# ---------------------------\n","# 1) Determine NUM_CLASSES\n","# ---------------------------\n","BASE = Path(\"/content\")\n","CFG_DIR = BASE / \"configs\"\n","\n","if (CFG_DIR / \"classes.json\").exists():\n","    with open(CFG_DIR / \"classes.json\", \"r\") as f:\n","        classes_cfg = json.load(f)\n","    NUM_CLASSES = int(classes_cfg[\"num_classes\"])\n","    print(f\"Detected NUM_CLASSES from configs: {NUM_CLASSES}\")\n","else:\n","    # Change this default if your setup uses a different number of classes\n","    NUM_CLASSES = 8\n","    print(\"Warning: /content/configs/classes.json not found. Using default NUM_CLASSES = 8.\")\n","    print(\"Please update NUM_CLASSES manually if this does not match your setup.\")\n","\n","# ---------------------------\n","# 2) Hyperparameters (must match your Step 11 script)\n","# ---------------------------\n","IN_CHANNELS  = 6\n","D_MODEL      = 192    # must be divisible by 3 in WindowEncoderV2\n","N_HEADS      = 6\n","N_LAYERS     = 3\n","D_FF         = 4 * D_MODEL   # 768\n","DROPOUT      = 0.2\n","SEQ_LEN      = 8\n","\n","print(f\"\\nConfig for size check:\")\n","print(f\"  NUM_CLASSES = {NUM_CLASSES}\")\n","print(f\"  IN_CHANNELS = {IN_CHANNELS}\")\n","print(f\"  D_MODEL     = {D_MODEL}\")\n","print(f\"  N_HEADS     = {N_HEADS}\")\n","print(f\"  N_LAYERS    = {N_LAYERS}\")\n","print(f\"  D_FF        = {D_FF}\")\n","print(f\"  DROPOUT     = {DROPOUT}\")\n","print(f\"  SEQ_LEN     = {SEQ_LEN}\")\n","\n","# ---------------------------\n","# 3) Model definition (identical to Step 11 training code)\n","# ---------------------------\n","class DepthwiseSeparableConv1d(nn.Module):\n","    def __init__(self, in_ch, out_ch, k, dilation=1, dropout=0.0):\n","        super().__init__()\n","        pad = (k // 2) * dilation\n","        self.dw = nn.Conv1d(\n","            in_ch, in_ch, kernel_size=k,\n","            padding=pad, dilation=dilation,\n","            groups=in_ch, bias=False\n","        )\n","        self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n","        self.bn = nn.BatchNorm1d(out_ch)\n","        self.act = nn.GELU()\n","        self.drop = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.dw(x)\n","        x = self.pw(x)\n","        x = self.bn(x)\n","        x = self.act(x)\n","        return self.drop(x)\n","\n","\n","class SEBlock(nn.Module):\n","    \"\"\"Channel SE attention applied to the 8 channels.\"\"\"\n","    def __init__(self, ch, reduction=4):\n","        super().__init__()\n","        self.pool = nn.AdaptiveAvgPool1d(1)\n","        hidden = max(1, ch // reduction)\n","        self.fc = nn.Sequential(\n","            nn.Linear(ch, hidden, bias=False),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(hidden, ch, bias=False),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        # x: [B, C, T]\n","        b, c, t = x.shape\n","        s = self.pool(x).view(b, c)\n","        s = self.fc(s).view(b, c, 1)\n","        return x * s\n","\n","\n","class WindowEncoderV2(nn.Module):\n","    \"\"\"\n","    Per-window representation:\n","      - 6-axis IMU + acc_norm + gyro_norm + channel SE attention\n","      - 3-branch multi-scale depthwise conv\n","      - [avg + max] pooling -> projection to d_model\n","      - 8-dim geometric stats -> d_model\n","    \"\"\"\n","    def __init__(self, in_ch=6, d_model=192, dropout=0.2):\n","        super().__init__()\n","        self.in_ch = in_ch\n","        self.aug_ch = in_ch + 2   # + acc_norm + gyro_norm\n","        self.se = SEBlock(self.aug_ch, reduction=4)\n","\n","        b_dim = d_model // 3\n","        assert b_dim * 3 == d_model, \"D_MODEL must be divisible by 3 for WindowEncoderV2\"\n","\n","        self.b1 = DepthwiseSeparableConv1d(self.aug_ch, b_dim, k=7,  dilation=1, dropout=dropout)\n","        self.b2 = DepthwiseSeparableConv1d(self.aug_ch, b_dim, k=15, dilation=2, dropout=dropout)\n","        self.b3 = DepthwiseSeparableConv1d(self.aug_ch, b_dim, k=31, dilation=3, dropout=dropout)\n","\n","        self.mix = nn.Conv1d(d_model, d_model, kernel_size=1, bias=False)\n","        self.bn  = nn.BatchNorm1d(d_model)\n","        self.act = nn.GELU()\n","        self.drop= nn.Dropout(dropout)\n","\n","        self.token_proj = nn.Sequential(\n","            nn.Linear(2 * d_model, d_model),\n","            nn.GELU(),\n","            nn.Dropout(dropout)\n","        )\n","\n","        self.g_proj = nn.Sequential(\n","            nn.Linear(8, d_model),\n","            nn.GELU(),\n","            nn.Linear(d_model, d_model)\n","        )\n","\n","    def forward(self, x):\n","        # x: [B*L, 6, T]\n","        BL, C, T = x.shape\n","\n","        acc_norm = torch.sqrt(\n","            x[:, 0, :]**2 + x[:, 1, :]**2 + x[:, 2, :]**2 + 1e-8\n","        ).unsqueeze(1)\n","        gyr_norm = torch.sqrt(\n","            x[:, 3, :]**2 + x[:, 4, :]**2 + x[:, 5, :]**2 + 1e-8\n","        ).unsqueeze(1)\n","        x_aug = torch.cat([x, acc_norm, gyr_norm], dim=1)  # [BL, 8, T]\n","\n","        x_aug = self.se(x_aug)\n","\n","        z1 = self.b1(x_aug)\n","        z2 = self.b2(x_aug)\n","        z3 = self.b3(x_aug)\n","        z = torch.cat([z1, z2, z3], dim=1)                 # [BL, d_model, T]\n","\n","        z = self.mix(z)\n","        z = self.bn(z)\n","        z = self.act(z)\n","        z = self.drop(z)\n","\n","        avg_pool = z.mean(dim=-1)\n","        max_pool, _ = z.max(dim=-1)\n","        token = torch.cat([avg_pool, max_pool], dim=-1)\n","        token = self.token_proj(token)                     # [BL, d_model]\n","\n","        acc_rms = acc_norm.squeeze(1).pow(2).mean(dim=-1).sqrt()\n","        gyr_rms = gyr_norm.squeeze(1).pow(2).mean(dim=-1).sqrt()\n","        acc_en  = x[:, 0:3, :].pow(2).mean(dim=(1, 2)).sqrt()\n","        gyr_en  = x[:, 3:6, :].pow(2).mean(dim=(1, 2)).sqrt()\n","\n","        acc_mean = x[:, 0:3, :].mean(dim=-1)\n","        gyr_mean = x[:, 3:6, :].mean(dim=-1)\n","        acc_mean_norm = acc_mean.pow(2).sum(dim=-1).sqrt()\n","        gyr_mean_norm = gyr_mean.pow(2).sum(dim=-1).sqrt()\n","\n","        acc_var = x[:, 0:3, :].var(dim=-1).mean(dim=-1)\n","        gyr_var = x[:, 3:6, :].var(dim=-1).mean(dim=-1)\n","\n","        g_raw = torch.stack(\n","            [acc_rms, gyr_rms, acc_en, gyr_en,\n","             acc_mean_norm, gyr_mean_norm, acc_var, gyr_var],\n","            dim=-1\n","        )                                                  # [BL, 8]\n","        g = self.g_proj(g_raw)                             # [BL, d_model]\n","\n","        return token, g\n","\n","\n","class CondLayerNorm(nn.Module):\n","    \"\"\"FiLM-style conditional LayerNorm.\"\"\"\n","    def __init__(self, d_model):\n","        super().__init__()\n","        self.ln = nn.LayerNorm(d_model)\n","        self.gamma = nn.Linear(d_model, d_model)\n","        self.beta  = nn.Linear(d_model, d_model)\n","\n","    def forward(self, x, g):\n","        y = self.ln(x)\n","        return y * (1 + self.gamma(g)) + self.beta(g)\n","\n","\n","class RCCBlock(nn.Module):\n","    \"\"\"Rotation-conditioned Transformer encoder block.\"\"\"\n","    def __init__(self, d_model=192, n_heads=6, d_ff=768, dropout=0.2):\n","        super().__init__()\n","        self.condln1 = CondLayerNorm(d_model)\n","        self.mha = nn.MultiheadAttention(\n","            d_model, n_heads, dropout=dropout, batch_first=True\n","        )\n","        self.drop1 = nn.Dropout(dropout)\n","\n","        self.condln2 = CondLayerNorm(d_model)\n","        self.ff = nn.Sequential(\n","            nn.Linear(d_model, d_ff),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(d_ff, d_model)\n","        )\n","        self.drop2 = nn.Dropout(dropout)\n","\n","    def forward(self, x, g):\n","        # x, g: [B, L(+1), d_model]\n","        y = self.condln1(x, g)\n","        attn, _ = self.mha(y, y, y, need_weights=False)\n","        x = x + self.drop1(attn)\n","\n","        y = self.condln2(x, g)\n","        y = self.ff(y)\n","        x = x + self.drop2(y)\n","        return x\n","\n","\n","class GeoContextHARV2(nn.Module):\n","    \"\"\"\n","    RCCMix-HAR++ main body:\n","      - WindowEncoderV2\n","      - rotation-conditioned Transformer (n_layers=3, n_heads=6)\n","      - classifier on [CLS || mean(tokens)]\n","    \"\"\"\n","    def __init__(self, in_ch=6, d_model=192, n_layers=3, n_heads=6, d_ff=768,\n","                 dropout=0.2, seq_len=8, num_classes=8):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        self.encoder = WindowEncoderV2(in_ch=in_ch, d_model=d_model, dropout=dropout)\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n","        self.pos = nn.Parameter(torch.zeros(1, seq_len + 1, d_model))\n","        self.blocks = nn.ModuleList(\n","            [RCCBlock(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n","        )\n","        self.norm = nn.LayerNorm(d_model)\n","        self.head_drop = nn.Dropout(dropout)\n","        self.head = nn.Linear(2 * d_model, num_classes)\n","\n","        nn.init.trunc_normal_(self.pos, std=0.02)\n","        nn.init.trunc_normal_(self.cls_token, std=0.02)\n","\n","    def forward(self, x):\n","        # x: [B, L, C, T]\n","        B, L, C, T = x.shape\n","        x = x.view(B * L, C, T)\n","        token, g = self.encoder(x)          # [B*L, d_model], [B*L, d_model]\n","        token = token.view(B, L, -1)        # [B, L, d_model]\n","        g     = g.view(B, L, -1)            # [B, L, d_model]\n","\n","        cls = self.cls_token.expand(B, 1, -1)   # [B, 1, d_model]\n","        z = torch.cat([cls, token], dim=1)      # [B, L+1, d_model]\n","        g_cls = g.mean(dim=1, keepdim=True)     # [B, 1, d_model]\n","        g_all = torch.cat([g_cls, g], dim=1)    # [B, L+1, d_model]\n","\n","        z = z + self.pos\n","\n","        for blk in self.blocks:\n","            z = blk(z, g_all)\n","\n","        z = self.norm(z)\n","        cls_rep  = z[:, 0, :]              # [B, d_model]\n","        mean_rep = z[:, 1:, :].mean(dim=1) # [B, d_model]\n","        feat = torch.cat([cls_rep, mean_rep], dim=-1)\n","        feat = self.head_drop(feat)\n","        logits = self.head(feat)           # [B, num_classes]\n","        return logits\n","\n","# ---------------------------\n","# 4) Instantiate model and compute size\n","# ---------------------------\n","model = GeoContextHARV2(\n","    in_ch=IN_CHANNELS,\n","    d_model=D_MODEL,\n","    n_layers=N_LAYERS,\n","    n_heads=N_HEADS,\n","    d_ff=D_FF,\n","    dropout=DROPOUT,\n","    seq_len=SEQ_LEN,\n","    num_classes=NUM_CLASSES\n",")\n","\n","print(\"\\n====== nn.Module structure ======\\n\")\n","print(model)\n","\n","# Parameter counts\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(\"\\n====== Parameter statistics ======\")\n","print(f\"Total params:      {total_params:,}\")\n","print(f\"Trainable params:  {trainable_params:,}\")\n","\n","print(\"\\n====== Per-layer parameter counts ======\")\n","for name, p in model.named_parameters():\n","    print(f\"{name:50s} shape={tuple(p.shape)}  params={p.numel():,}\")\n","\n","# Size estimation (parameters only)\n","def fmt_mb(n_bytes: int) -> str:\n","    return f\"{n_bytes / 1024 / 1024:.2f} MB\"\n","\n","bytes_fp32 = total_params * 4\n","bytes_fp16 = total_params * 2\n","\n","print(\"\\n====== Model size estimate (parameters only) ======\")\n","print(f\"FP32 (float32, 4B/param): {fmt_mb(bytes_fp32)}\")\n","print(f\"FP16 (float16, 2B/param): {fmt_mb(bytes_fp16)}\")\n","\n","# Save a randomly initialised state_dict to check actual .pt size\n","models_dir = BASE / \"models\"\n","models_dir.mkdir(parents=True, exist_ok=True)\n","tmp_path = models_dir / \"rccmix_har_step11_dummy.pt\"\n","torch.save(model.state_dict(), tmp_path)\n","file_bytes = tmp_path.stat().st_size\n","print(f\"\\nRandom-initialised state_dict saved to: {tmp_path.name}\")\n","print(f\"Actual .pt file size:                 {fmt_mb(file_bytes)}\")\n","\n","print(\"\\n[RCCMix-HAR++ (Step 11, GeoContextHARV2) – structure & size done]\\n\")"],"metadata":{"id":"15zFllOlTK0P","executionInfo":{"status":"ok","timestamp":1763407869412,"user_tz":0,"elapsed":85,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"691896f9-3c3a-4d46-97e4-4d0e196e2825","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[RCCMix-HAR++ (Step 11, GeoContextHARV2) – structure & size]\n","Warning: /content/configs/classes.json not found. Using default NUM_CLASSES = 8.\n","Please update NUM_CLASSES manually if this does not match your setup.\n","\n","Config for size check:\n","  NUM_CLASSES = 8\n","  IN_CHANNELS = 6\n","  D_MODEL     = 192\n","  N_HEADS     = 6\n","  N_LAYERS    = 3\n","  D_FF        = 768\n","  DROPOUT     = 0.2\n","  SEQ_LEN     = 8\n","\n","====== nn.Module structure ======\n","\n","GeoContextHARV2(\n","  (encoder): WindowEncoderV2(\n","    (se): SEBlock(\n","      (pool): AdaptiveAvgPool1d(output_size=1)\n","      (fc): Sequential(\n","        (0): Linear(in_features=8, out_features=2, bias=False)\n","        (1): ReLU(inplace=True)\n","        (2): Linear(in_features=2, out_features=8, bias=False)\n","        (3): Sigmoid()\n","      )\n","    )\n","    (b1): DepthwiseSeparableConv1d(\n","      (dw): Conv1d(8, 8, kernel_size=(7,), stride=(1,), padding=(3,), groups=8, bias=False)\n","      (pw): Conv1d(8, 64, kernel_size=(1,), stride=(1,), bias=False)\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (act): GELU(approximate='none')\n","      (drop): Dropout(p=0.2, inplace=False)\n","    )\n","    (b2): DepthwiseSeparableConv1d(\n","      (dw): Conv1d(8, 8, kernel_size=(15,), stride=(1,), padding=(14,), dilation=(2,), groups=8, bias=False)\n","      (pw): Conv1d(8, 64, kernel_size=(1,), stride=(1,), bias=False)\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (act): GELU(approximate='none')\n","      (drop): Dropout(p=0.2, inplace=False)\n","    )\n","    (b3): DepthwiseSeparableConv1d(\n","      (dw): Conv1d(8, 8, kernel_size=(31,), stride=(1,), padding=(45,), dilation=(3,), groups=8, bias=False)\n","      (pw): Conv1d(8, 64, kernel_size=(1,), stride=(1,), bias=False)\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (act): GELU(approximate='none')\n","      (drop): Dropout(p=0.2, inplace=False)\n","    )\n","    (mix): Conv1d(192, 192, kernel_size=(1,), stride=(1,), bias=False)\n","    (bn): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (act): GELU(approximate='none')\n","    (drop): Dropout(p=0.2, inplace=False)\n","    (token_proj): Sequential(\n","      (0): Linear(in_features=384, out_features=192, bias=True)\n","      (1): GELU(approximate='none')\n","      (2): Dropout(p=0.2, inplace=False)\n","    )\n","    (g_proj): Sequential(\n","      (0): Linear(in_features=8, out_features=192, bias=True)\n","      (1): GELU(approximate='none')\n","      (2): Linear(in_features=192, out_features=192, bias=True)\n","    )\n","  )\n","  (blocks): ModuleList(\n","    (0-2): 3 x RCCBlock(\n","      (condln1): CondLayerNorm(\n","        (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","        (gamma): Linear(in_features=192, out_features=192, bias=True)\n","        (beta): Linear(in_features=192, out_features=192, bias=True)\n","      )\n","      (mha): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n","      )\n","      (drop1): Dropout(p=0.2, inplace=False)\n","      (condln2): CondLayerNorm(\n","        (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","        (gamma): Linear(in_features=192, out_features=192, bias=True)\n","        (beta): Linear(in_features=192, out_features=192, bias=True)\n","      )\n","      (ff): Sequential(\n","        (0): Linear(in_features=192, out_features=768, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Dropout(p=0.2, inplace=False)\n","        (3): Linear(in_features=768, out_features=192, bias=True)\n","      )\n","      (drop2): Dropout(p=0.2, inplace=False)\n","    )\n","  )\n","  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","  (head_drop): Dropout(p=0.2, inplace=False)\n","  (head): Linear(in_features=384, out_features=8, bias=True)\n",")\n","\n","====== Parameter statistics ======\n","Total params:      1,936,976\n","Trainable params:  1,936,976\n","\n","====== Per-layer parameter counts ======\n","cls_token                                          shape=(1, 1, 192)  params=192\n","pos                                                shape=(1, 9, 192)  params=1,728\n","encoder.se.fc.0.weight                             shape=(2, 8)  params=16\n","encoder.se.fc.2.weight                             shape=(8, 2)  params=16\n","encoder.b1.dw.weight                               shape=(8, 1, 7)  params=56\n","encoder.b1.pw.weight                               shape=(64, 8, 1)  params=512\n","encoder.b1.bn.weight                               shape=(64,)  params=64\n","encoder.b1.bn.bias                                 shape=(64,)  params=64\n","encoder.b2.dw.weight                               shape=(8, 1, 15)  params=120\n","encoder.b2.pw.weight                               shape=(64, 8, 1)  params=512\n","encoder.b2.bn.weight                               shape=(64,)  params=64\n","encoder.b2.bn.bias                                 shape=(64,)  params=64\n","encoder.b3.dw.weight                               shape=(8, 1, 31)  params=248\n","encoder.b3.pw.weight                               shape=(64, 8, 1)  params=512\n","encoder.b3.bn.weight                               shape=(64,)  params=64\n","encoder.b3.bn.bias                                 shape=(64,)  params=64\n","encoder.mix.weight                                 shape=(192, 192, 1)  params=36,864\n","encoder.bn.weight                                  shape=(192,)  params=192\n","encoder.bn.bias                                    shape=(192,)  params=192\n","encoder.token_proj.0.weight                        shape=(192, 384)  params=73,728\n","encoder.token_proj.0.bias                          shape=(192,)  params=192\n","encoder.g_proj.0.weight                            shape=(192, 8)  params=1,536\n","encoder.g_proj.0.bias                              shape=(192,)  params=192\n","encoder.g_proj.2.weight                            shape=(192, 192)  params=36,864\n","encoder.g_proj.2.bias                              shape=(192,)  params=192\n","blocks.0.condln1.ln.weight                         shape=(192,)  params=192\n","blocks.0.condln1.ln.bias                           shape=(192,)  params=192\n","blocks.0.condln1.gamma.weight                      shape=(192, 192)  params=36,864\n","blocks.0.condln1.gamma.bias                        shape=(192,)  params=192\n","blocks.0.condln1.beta.weight                       shape=(192, 192)  params=36,864\n","blocks.0.condln1.beta.bias                         shape=(192,)  params=192\n","blocks.0.mha.in_proj_weight                        shape=(576, 192)  params=110,592\n","blocks.0.mha.in_proj_bias                          shape=(576,)  params=576\n","blocks.0.mha.out_proj.weight                       shape=(192, 192)  params=36,864\n","blocks.0.mha.out_proj.bias                         shape=(192,)  params=192\n","blocks.0.condln2.ln.weight                         shape=(192,)  params=192\n","blocks.0.condln2.ln.bias                           shape=(192,)  params=192\n","blocks.0.condln2.gamma.weight                      shape=(192, 192)  params=36,864\n","blocks.0.condln2.gamma.bias                        shape=(192,)  params=192\n","blocks.0.condln2.beta.weight                       shape=(192, 192)  params=36,864\n","blocks.0.condln2.beta.bias                         shape=(192,)  params=192\n","blocks.0.ff.0.weight                               shape=(768, 192)  params=147,456\n","blocks.0.ff.0.bias                                 shape=(768,)  params=768\n","blocks.0.ff.3.weight                               shape=(192, 768)  params=147,456\n","blocks.0.ff.3.bias                                 shape=(192,)  params=192\n","blocks.1.condln1.ln.weight                         shape=(192,)  params=192\n","blocks.1.condln1.ln.bias                           shape=(192,)  params=192\n","blocks.1.condln1.gamma.weight                      shape=(192, 192)  params=36,864\n","blocks.1.condln1.gamma.bias                        shape=(192,)  params=192\n","blocks.1.condln1.beta.weight                       shape=(192, 192)  params=36,864\n","blocks.1.condln1.beta.bias                         shape=(192,)  params=192\n","blocks.1.mha.in_proj_weight                        shape=(576, 192)  params=110,592\n","blocks.1.mha.in_proj_bias                          shape=(576,)  params=576\n","blocks.1.mha.out_proj.weight                       shape=(192, 192)  params=36,864\n","blocks.1.mha.out_proj.bias                         shape=(192,)  params=192\n","blocks.1.condln2.ln.weight                         shape=(192,)  params=192\n","blocks.1.condln2.ln.bias                           shape=(192,)  params=192\n","blocks.1.condln2.gamma.weight                      shape=(192, 192)  params=36,864\n","blocks.1.condln2.gamma.bias                        shape=(192,)  params=192\n","blocks.1.condln2.beta.weight                       shape=(192, 192)  params=36,864\n","blocks.1.condln2.beta.bias                         shape=(192,)  params=192\n","blocks.1.ff.0.weight                               shape=(768, 192)  params=147,456\n","blocks.1.ff.0.bias                                 shape=(768,)  params=768\n","blocks.1.ff.3.weight                               shape=(192, 768)  params=147,456\n","blocks.1.ff.3.bias                                 shape=(192,)  params=192\n","blocks.2.condln1.ln.weight                         shape=(192,)  params=192\n","blocks.2.condln1.ln.bias                           shape=(192,)  params=192\n","blocks.2.condln1.gamma.weight                      shape=(192, 192)  params=36,864\n","blocks.2.condln1.gamma.bias                        shape=(192,)  params=192\n","blocks.2.condln1.beta.weight                       shape=(192, 192)  params=36,864\n","blocks.2.condln1.beta.bias                         shape=(192,)  params=192\n","blocks.2.mha.in_proj_weight                        shape=(576, 192)  params=110,592\n","blocks.2.mha.in_proj_bias                          shape=(576,)  params=576\n","blocks.2.mha.out_proj.weight                       shape=(192, 192)  params=36,864\n","blocks.2.mha.out_proj.bias                         shape=(192,)  params=192\n","blocks.2.condln2.ln.weight                         shape=(192,)  params=192\n","blocks.2.condln2.ln.bias                           shape=(192,)  params=192\n","blocks.2.condln2.gamma.weight                      shape=(192, 192)  params=36,864\n","blocks.2.condln2.gamma.bias                        shape=(192,)  params=192\n","blocks.2.condln2.beta.weight                       shape=(192, 192)  params=36,864\n","blocks.2.condln2.beta.bias                         shape=(192,)  params=192\n","blocks.2.ff.0.weight                               shape=(768, 192)  params=147,456\n","blocks.2.ff.0.bias                                 shape=(768,)  params=768\n","blocks.2.ff.3.weight                               shape=(192, 768)  params=147,456\n","blocks.2.ff.3.bias                                 shape=(192,)  params=192\n","norm.weight                                        shape=(192,)  params=192\n","norm.bias                                          shape=(192,)  params=192\n","head.weight                                        shape=(8, 384)  params=3,072\n","head.bias                                          shape=(8,)  params=8\n","\n","====== Model size estimate (parameters only) ======\n","FP32 (float32, 4B/param): 7.39 MB\n","FP16 (float16, 2B/param): 3.69 MB\n","\n","Random-initialised state_dict saved to: rccmix_har_step11_dummy.pt\n","Actual .pt file size:                 7.43 MB\n","\n","[RCCMix-HAR++ (Step 11, GeoContextHARV2) – structure & size done]\n","\n"]}]}]}