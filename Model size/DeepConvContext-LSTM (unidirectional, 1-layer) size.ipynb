{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyOT+yfRXucfZpPghwC1ucjy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# ============ DeepConvContext (official-structure, from-scratch) structure & size ============\n","\n","import json\n","from pathlib import Path\n","\n","import torch\n","import torch.nn as nn\n","\n","print(\"\\n[DeepConvContext (official-structure, from-scratch) – structure & size]\")\n","\n","# ---------------------------\n","# 1) Determine NUM_CLASSES\n","# ---------------------------\n","BASE = Path(\"/content\")\n","CFG_DIR = BASE / \"configs\"\n","\n","if (CFG_DIR / \"classes.json\").exists():\n","    with open(CFG_DIR / \"classes.json\", \"r\") as f:\n","        classes_cfg = json.load(f)\n","    NUM_CLASSES = int(classes_cfg[\"num_classes\"])\n","    print(f\"Detected NUM_CLASSES from configs: {NUM_CLASSES}\")\n","else:\n","    # Change this default if your experiment uses a different number of classes\n","    NUM_CLASSES = 8\n","    print(\"Warning: /content/configs/classes.json not found. Using default NUM_CLASSES = 8.\")\n","    print(\"Please update NUM_CLASSES manually if this does not match your setup.\")\n","\n","# ---------------------------\n","# 2) Hyperparameters (must match your Step 10 from-scratch script)\n","# ---------------------------\n","NUM_CHANNELS      = 6\n","SAMPLES_PER_WIN   = 150\n","WIN_OVERLAP       = 0.5\n","STRIDE_SAMPLES    = int(SAMPLES_PER_WIN * (1 - WIN_OVERLAP))  # 75\n","CONTEXT_LEN_WINS  = 100\n","\n","EPOCHS        = 30          # does not affect structure\n","LEARNING_RATE = 1e-4\n","WEIGHT_DECAY  = 1e-6\n","STEP_SIZE     = 10\n","GAMMA         = 0.9\n","DROPOUT_P     = 0.5\n","BIDIRECTIONAL = False       # from-scratch variant uses unidirectional LSTM\n","HIDDEN_UNITS  = 128\n","CONV_CHANNELS = 64\n","KERNEL_SIZE   = 9\n","PADDING       = KERNEL_SIZE // 2\n","\n","print(f\"\\nConfig for size check:\")\n","print(f\"  NUM_CLASSES      = {NUM_CLASSES}\")\n","print(f\"  NUM_CHANNELS     = {NUM_CHANNELS}\")\n","print(f\"  SAMPLES_PER_WIN  = {SAMPLES_PER_WIN}\")\n","print(f\"  CONTEXT_LEN_WINS = {CONTEXT_LEN_WINS}\")\n","print(f\"  CONV_CHANNELS    = {CONV_CHANNELS}\")\n","print(f\"  HIDDEN_UNITS     = {HIDDEN_UNITS}\")\n","print(f\"  BIDIRECTIONAL    = {BIDIRECTIONAL}\")\n","\n","# ---------------------------\n","# 3) Model definition (must match Step 10 exactly)\n","# ---------------------------\n","class DeepConvLSTM_Intra(nn.Module):\n","    def __init__(self, in_ch=6, conv_ch=64, kernel_size=9, hidden=128):\n","        super().__init__()\n","        pad = kernel_size // 2\n","        self.conv1 = nn.Conv1d(in_ch,   conv_ch, kernel_size, padding=pad)\n","        self.conv2 = nn.Conv1d(conv_ch, conv_ch, kernel_size, padding=pad)\n","        self.conv3 = nn.Conv1d(conv_ch, conv_ch, kernel_size, padding=pad)\n","        self.conv4 = nn.Conv1d(conv_ch, conv_ch, kernel_size, padding=pad)\n","        self.relu  = nn.ReLU(inplace=True)\n","        self.lstm  = nn.LSTM(\n","            input_size=conv_ch,\n","            hidden_size=hidden,\n","            num_layers=1,\n","            batch_first=True\n","        )\n","\n","    def forward(self, x_win):  # x_win: (N, C, T)\n","        x = self.relu(self.conv1(x_win))\n","        x = self.relu(self.conv2(x))\n","        x = self.relu(self.conv3(x))\n","        x = self.relu(self.conv4(x))\n","        x = x.permute(0, 2, 1)  # (N, T, C)\n","        _, (h_n, _) = self.lstm(x)\n","        return h_n[-1]          # (N, hidden)\n","\n","\n","class DeepConvContext(nn.Module):\n","    def __init__(self,\n","                 num_channels=6,\n","                 num_classes=8,\n","                 conv_channels=64,\n","                 hidden_intra=128,\n","                 hidden_inter=128,\n","                 dropout=0.5,\n","                 bidirectional=False):\n","        super().__init__()\n","        self.intra = DeepConvLSTM_Intra(\n","            in_ch=num_channels,\n","            conv_ch=conv_channels,\n","            kernel_size=KERNEL_SIZE,\n","            hidden=hidden_intra\n","        )\n","\n","        self.inter = nn.LSTM(\n","            input_size=hidden_intra,\n","            hidden_size=hidden_inter,\n","            num_layers=1,\n","            batch_first=True,\n","            bidirectional=bidirectional\n","        )\n","        inter_out = hidden_inter * (2 if bidirectional else 1)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc      = nn.Linear(inter_out, num_classes)\n","\n","    def forward(self, x):               # x: (B, S, C, T)\n","        B, S, C, T = x.shape\n","        x = x.reshape(B * S, C, T)\n","        feats = self.intra(x)           # (B*S, hidden_intra)\n","        feats = feats.view(B, S, -1)    # (B, S, hidden_intra)\n","        inter_out, _ = self.inter(feats)  # (B, S, inter_out)\n","        inter_out = self.dropout(inter_out)\n","        logits = self.fc(inter_out)     # (B, S, NUM_CLASSES)\n","        return logits\n","\n","# ---------------------------\n","# 4) Instantiate model and compute size\n","# ---------------------------\n","model = DeepConvContext(\n","    num_channels=NUM_CHANNELS,\n","    num_classes=NUM_CLASSES,\n","    conv_channels=CONV_CHANNELS,\n","    hidden_intra=HIDDEN_UNITS,\n","    hidden_inter=HIDDEN_UNITS,\n","    dropout=DROPOUT_P,\n","    bidirectional=BIDIRECTIONAL\n",")\n","\n","print(\"\\n====== nn.Module structure ======\")\n","print(model)\n","\n","# Parameter counts\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(\"\\n====== Parameter statistics ======\")\n","print(f\"Total params:      {total_params:,}\")\n","print(f\"Trainable params:  {trainable_params:,}\")\n","\n","print(\"\\n====== Per-layer parameter counts ======\")\n","for name, p in model.named_parameters():\n","    print(f\"{name:40s} shape={tuple(p.shape)}  params={p.numel():,}\")\n","\n","# Size estimation (weights only)\n","def fmt_mb(n_bytes: int) -> str:\n","    return f\"{n_bytes / 1024 / 1024:.2f} MB\"\n","\n","bytes_fp32 = total_params * 4   # float32: 4 bytes per parameter\n","bytes_fp16 = total_params * 2   # float16: 2 bytes per parameter\n","\n","print(\"\\n====== Model size estimate (parameters only) ======\")\n","print(f\"FP32 (float32, 4B/param): {fmt_mb(bytes_fp32)}\")\n","print(f\"FP16 (float16, 2B/param): {fmt_mb(bytes_fp16)}\")\n","\n","# Save a randomly initialised state_dict to check actual .pth size\n","models_dir = BASE / \"models\"\n","models_dir.mkdir(parents=True, exist_ok=True)\n","tmp_path = models_dir / \"deepconvcontext_from_scratch_dummy.pth\"\n","torch.save(model.state_dict(), tmp_path)\n","file_bytes = tmp_path.stat().st_size\n","print(f\"\\nRandom initialised state_dict saved to {tmp_path.name}\")\n","print(f\"Actual .pth file size: {fmt_mb(file_bytes)}\")\n","tmp_path.unlink(missing_ok=True)\n","\n","print(\"\\n[DeepConvContext (from-scratch) structure & size – done]\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vg_5h8QbO4jA","executionInfo":{"status":"ok","timestamp":1763406745945,"user_tz":0,"elapsed":30,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"2480a4fd-6703-4ed9-8da1-d7a4f1850934"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[DeepConvContext (official-structure, from-scratch) – structure & size]\n","Warning: /content/configs/classes.json not found. Using default NUM_CLASSES = 8.\n","Please update NUM_CLASSES manually if this does not match your setup.\n","\n","Config for size check:\n","  NUM_CLASSES      = 8\n","  NUM_CHANNELS     = 6\n","  SAMPLES_PER_WIN  = 150\n","  CONTEXT_LEN_WINS = 100\n","  CONV_CHANNELS    = 64\n","  HIDDEN_UNITS     = 128\n","  BIDIRECTIONAL    = False\n","\n","====== nn.Module structure ======\n","DeepConvContext(\n","  (intra): DeepConvLSTM_Intra(\n","    (conv1): Conv1d(6, 64, kernel_size=(9,), stride=(1,), padding=(4,))\n","    (conv2): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(4,))\n","    (conv3): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(4,))\n","    (conv4): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(4,))\n","    (relu): ReLU(inplace=True)\n","    (lstm): LSTM(64, 128, batch_first=True)\n","  )\n","  (inter): LSTM(128, 128, batch_first=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (fc): Linear(in_features=128, out_features=8, bias=True)\n",")\n","\n","====== Parameter statistics ======\n","Total params:      346,760\n","Trainable params:  346,760\n","\n","====== Per-layer parameter counts ======\n","intra.conv1.weight                       shape=(64, 6, 9)  params=3,456\n","intra.conv1.bias                         shape=(64,)  params=64\n","intra.conv2.weight                       shape=(64, 64, 9)  params=36,864\n","intra.conv2.bias                         shape=(64,)  params=64\n","intra.conv3.weight                       shape=(64, 64, 9)  params=36,864\n","intra.conv3.bias                         shape=(64,)  params=64\n","intra.conv4.weight                       shape=(64, 64, 9)  params=36,864\n","intra.conv4.bias                         shape=(64,)  params=64\n","intra.lstm.weight_ih_l0                  shape=(512, 64)  params=32,768\n","intra.lstm.weight_hh_l0                  shape=(512, 128)  params=65,536\n","intra.lstm.bias_ih_l0                    shape=(512,)  params=512\n","intra.lstm.bias_hh_l0                    shape=(512,)  params=512\n","inter.weight_ih_l0                       shape=(512, 128)  params=65,536\n","inter.weight_hh_l0                       shape=(512, 128)  params=65,536\n","inter.bias_ih_l0                         shape=(512,)  params=512\n","inter.bias_hh_l0                         shape=(512,)  params=512\n","fc.weight                                shape=(8, 128)  params=1,024\n","fc.bias                                  shape=(8,)  params=8\n","\n","====== Model size estimate (parameters only) ======\n","FP32 (float32, 4B/param): 1.32 MB\n","FP16 (float16, 2B/param): 0.66 MB\n","\n","Random initialised state_dict saved to deepconvcontext_from_scratch_dummy.pth\n","Actual .pth file size: 1.33 MB\n","\n","[DeepConvContext (from-scratch) structure & size – done]\n","\n"]}]}]}