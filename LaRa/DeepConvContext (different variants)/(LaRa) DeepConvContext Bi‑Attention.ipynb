{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMU01tspDyj/dpRrJIM7eCY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPQXnr2K9ypV","executionInfo":{"status":"ok","timestamp":1763470189778,"user_tz":0,"elapsed":4467,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"81ce6762-c437-4eb1-bd03-abe28d20375f"},"outputs":[{"output_type":"stream","name":"stdout","text":["⚠️ Note: In Jupyter/Colab, PYTHONHASHSEED must be set before the kernel starts\n","   Suggestion: After setting environment variables, restart the runtime, then run the main code\n","\n","Configured random seeds: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","Generating SEEDS.yaml...\n","Generating requirements.txt...\n","Generating env.txt...\n","Generating environment.yml...\n","Collecting hardware information...\n","Collecting Git information...\n","Saving PyTorch build information...\n","Generating data checksums...\n","  data/ directory does not exist; skipping checksums\n","Computing environment hashes...\n","\n","============================================================\n","Step 0 complete - Reproducible environment configuration (top-conf/journal grade)\n","============================================================\n","Output directory: artifacts/env/\n","  ✓ SEEDS.yaml (seeds: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","  ✓ requirements.txt\n","  ✓ env.txt (with system summary)\n","  ✓ environment.yml\n","  ✓ hardware_log.json\n","  ✓ git_info.json (dirty=False)\n","  ✓ torch_build.txt\n","  ✓ ENV.SHA256 (covers all key files)\n","\n","Strict determinism configuration:\n","  - torch.use_deterministic_algorithms: True (warn_only=False)\n","  - cudnn.deterministic: True\n","  - cudnn.benchmark: False\n","  - TF32 disabled: False\n","  - Environment variables set:\n","    PYTHONHASHSEED: 0\n","    CUBLAS_WORKSPACE_CONFIG: :4096:8\n","    Thread control: OMP/MKL/OPENBLAS/NUMEXPR=1\n","============================================================\n"]}],"source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 0: Reproducible Environment (Colab/Jupyter adapted - top-conf/journal grade)\n","Generate a complete reproducible environment configuration\n","\"\"\"\n","\n","# ===== Set environment variables directly (Colab/Jupyter env) =====\n","import os\n","import sys\n","\n","os.environ[\"PYTHONHASHSEED\"] = \"0\"\n","os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n","os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n","os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n","os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n","os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n","\n","print(\"⚠️ Note: In Jupyter/Colab, PYTHONHASHSEED must be set before the kernel starts\")\n","print(\"   Suggestion: After setting environment variables, restart the runtime, then run the main code\\n\")\n","\n","# ===== Environment variables set; continue normal flow =====\n","import json\n","import hashlib\n","import subprocess\n","from pathlib import Path\n","from datetime import datetime, timezone\n","from contextlib import redirect_stdout\n","import io\n","\n","# Check Python version\n","assert sys.version_info >= (3, 10), f\"Require Python ≥ 3.10, current: {sys.version}\"\n","\n","# Create output directory\n","output_dir = Path(\"artifacts/env\")\n","output_dir.mkdir(parents=True, exist_ok=True)\n","\n","# 1. Multiple random seeds (0–9)\n","SEEDS = list(range(10))\n","print(f\"Configured random seeds: {SEEDS}\")\n","\n","# Import and configure\n","import random\n","import numpy as np\n","import torch\n","\n","# Initialize with the first seed\n","random.seed(SEEDS[0])\n","np.random.seed(SEEDS[0])\n","torch.manual_seed(SEEDS[0])\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEEDS[0])\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # Disable TF32\n","    torch.backends.cuda.matmul.allow_tf32 = False\n","    torch.backends.cudnn.allow_tf32 = False\n","\n","# Enable strict deterministic algorithms (not using warn_only)\n","torch.use_deterministic_algorithms(True)\n","\n","# Set matmul precision\n","if hasattr(torch, 'set_float32_matmul_precision'):\n","    torch.set_float32_matmul_precision(\"high\")\n","\n","# 2. Generate SEEDS.yaml (with fallback)\n","print(\"Generating SEEDS.yaml...\")\n","seeds_config = {\n","    \"seeds\": SEEDS,\n","    \"default_seed\": SEEDS[0],\n","    \"description\": \"Random seeds for python, numpy, torch, sklearn\"\n","}\n","try:\n","    import yaml\n","    with open(output_dir / \"SEEDS.yaml\", \"w\") as f:\n","        yaml.dump(seeds_config, f, default_flow_style=False)\n","except ImportError:\n","    # Fallback if PyYAML is not installed\n","    yaml_content = f\"\"\"seeds: {SEEDS}\n","default_seed: {SEEDS[0]}\n","description: Random seeds for python, numpy, torch, sklearn\n","\"\"\"\n","    with open(output_dir / \"SEEDS.yaml\", \"w\") as f:\n","        f.write(yaml_content)\n","\n","# 3. Generate requirements.txt (frozen versions)\n","print(\"Generating requirements.txt...\")\n","result = subprocess.run(\n","    [sys.executable, \"-m\", \"pip\", \"freeze\"],\n","    capture_output=True, text=True\n",")\n","requirements = result.stdout\n","with open(output_dir / \"requirements.txt\", \"w\") as f:\n","    f.write(requirements)\n","\n","# 4. Collect system info (for env.txt header)\n","import platform\n","system_info = []\n","system_info.append(\"=\"*60)\n","system_info.append(\"Environment Snapshot - System Overview\")\n","system_info.append(\"=\"*60)\n","system_info.append(f\"Time (UTC): {datetime.now(timezone.utc).isoformat()}\")\n","system_info.append(f\"Python: {sys.version}\")\n","system_info.append(f\"Platform: {platform.system()} {platform.release()} ({platform.machine()})\")\n","\n","try:\n","    import psutil\n","    system_info.append(f\"CPU: {psutil.cpu_count(logical=False)} cores / {psutil.cpu_count(logical=True)} threads\")\n","    system_info.append(f\"Memory: {round(psutil.virtual_memory().total / (1024**3), 2)} GB\")\n","except ImportError:\n","    pass\n","\n","system_info.append(f\"PyTorch: {torch.__version__}\")\n","if torch.cuda.is_available():\n","    system_info.append(f\"CUDA: {torch.version.cuda}\")\n","    system_info.append(f\"cuDNN: {torch.backends.cudnn.version()}\")\n","    try:\n","        out = subprocess.run(\n","            [\"nvidia-smi\", \"--query-gpu=driver_version\", \"--format=csv,noheader\"],\n","            capture_output=True, text=True\n","        )\n","        if out.returncode == 0 and out.stdout.strip():\n","            system_info.append(f\"NVIDIA driver: {out.stdout.strip().splitlines()[0]}\")\n","    except:\n","        pass\n","\n","system_info.append(\"\\nEnvironment variables:\")\n","for key in [\"PYTHONHASHSEED\", \"CUBLAS_WORKSPACE_CONFIG\", \"OMP_NUM_THREADS\",\n","            \"MKL_NUM_THREADS\", \"OPENBLAS_NUM_THREADS\", \"NUMEXPR_NUM_THREADS\"]:\n","    system_info.append(f\"  {key}={os.environ.get(key, 'N/A')}\")\n","\n","system_info.append(\"\\n\" + \"=\"*60)\n","system_info.append(\"Installed packages list\")\n","system_info.append(\"=\"*60 + \"\\n\")\n","\n","# 5. Generate env.txt (human-readable + system summary)\n","print(\"Generating env.txt...\")\n","result = subprocess.run(\n","    [sys.executable, \"-m\", \"pip\", \"list\"],\n","    capture_output=True, text=True\n",")\n","with open(output_dir / \"env.txt\", \"w\") as f:\n","    f.write(\"\\n\".join(system_info))\n","    f.write(result.stdout)\n","\n","# 6. Generate environment.yml\n","print(\"Generating environment.yml...\")\n","env_yml = f\"\"\"name: har_lara\n","channels:\n","  - defaults\n","  - conda-forge\n","dependencies:\n","  - python={sys.version_info.major}.{sys.version_info.minor}\n","  - pip\n","  - pip:\n","\"\"\"\n","for line in requirements.strip().split(\"\\n\"):\n","    if line and not line.startswith(\"#\"):\n","        env_yml += f\"      - {line}\\n\"\n","\n","with open(output_dir / \"environment.yml\", \"w\") as f:\n","    f.write(env_yml)\n","\n","# 7. Collect complete hardware information\n","print(\"Collecting hardware information...\")\n","hardware_info = {\n","    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n","    \"python_version\": sys.version,\n","    \"python_executable\": sys.executable,\n","    \"platform\": sys.platform,\n","    \"os\": platform.system(),\n","    \"os_release\": platform.release(),\n","    \"os_version\": platform.version(),\n","    \"machine\": platform.machine(),\n","    \"processor\": platform.processor(),\n","}\n","\n","try:\n","    import psutil\n","    hardware_info[\"cpu_count_physical\"] = psutil.cpu_count(logical=False)\n","    hardware_info[\"cpu_count_logical\"] = psutil.cpu_count(logical=True)\n","    hardware_info[\"memory_total_gb\"] = round(psutil.virtual_memory().total / (1024**3), 2)\n","except ImportError:\n","    pass\n","\n","hardware_info[\"torch_version\"] = torch.__version__\n","\n","if torch.cuda.is_available():\n","    hardware_info[\"gpu_available\"] = True\n","    hardware_info[\"gpu_count\"] = torch.cuda.device_count()\n","    hardware_info[\"gpu_names\"] = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n","    hardware_info[\"cuda_version\"] = torch.version.cuda\n","    hardware_info[\"cudnn_version\"] = torch.backends.cudnn.version()\n","\n","    gpu_details = []\n","    for i in range(torch.cuda.device_count()):\n","        props = torch.cuda.get_device_properties(i)\n","        gpu_details.append({\n","            \"id\": i,\n","            \"name\": props.name,\n","            \"compute_capability\": f\"{props.major}.{props.minor}\",\n","            \"total_memory_gb\": round(props.total_memory / (1024**3), 2),\n","            \"multi_processor_count\": props.multi_processor_count\n","        })\n","    hardware_info[\"gpu_details\"] = gpu_details\n","\n","    try:\n","        out = subprocess.run(\n","            [\"nvidia-smi\", \"--query-gpu=driver_version\", \"--format=csv,noheader\"],\n","            capture_output=True, text=True\n","        )\n","        if out.returncode == 0 and out.stdout.strip():\n","            hardware_info[\"nvidia_driver_version\"] = out.stdout.strip().splitlines()[0]\n","    except:\n","        pass\n","else:\n","    hardware_info[\"gpu_available\"] = False\n","\n","hardware_info[\"deterministic_config\"] = {\n","    \"cudnn_deterministic\": torch.backends.cudnn.deterministic,\n","    \"cudnn_benchmark\": torch.backends.cudnn.benchmark,\n","    \"use_deterministic_algorithms\": True,\n","    \"warn_only\": False,\n","    \"tf32_disabled\": not torch.backends.cuda.matmul.allow_tf32 if torch.cuda.is_available() else \"N/A\",\n","    \"float32_matmul_precision\": \"high\" if hasattr(torch, 'set_float32_matmul_precision') else \"N/A\",\n","    \"PYTHONHASHSEED\": os.environ.get(\"PYTHONHASHSEED\"),\n","    \"CUBLAS_WORKSPACE_CONFIG\": os.environ.get(\"CUBLAS_WORKSPACE_CONFIG\"),\n","    \"OMP_NUM_THREADS\": os.environ.get(\"OMP_NUM_THREADS\"),\n","    \"MKL_NUM_THREADS\": os.environ.get(\"MKL_NUM_THREADS\"),\n","    \"OPENBLAS_NUM_THREADS\": os.environ.get(\"OPENBLAS_NUM_THREADS\"),\n","    \"NUMEXPR_NUM_THREADS\": os.environ.get(\"NUMEXPR_NUM_THREADS\"),\n","}\n","\n","with open(output_dir / \"hardware_log.json\", \"w\") as f:\n","    json.dump(hardware_info, f, indent=2)\n","\n","# 8. Git commit + dirty flag\n","print(\"Collecting Git information...\")\n","git_info = {}\n","try:\n","    git_commit = subprocess.run(\n","        [\"git\", \"rev-parse\", \"HEAD\"],\n","        capture_output=True, text=True, check=True\n","    ).stdout.strip()\n","    git_info[\"commit\"] = git_commit\n","\n","    git_branch = subprocess.run(\n","        [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n","        capture_output=True, text=True, check=True\n","    ).stdout.strip()\n","    git_info[\"branch\"] = git_branch\n","\n","    dirty = subprocess.run(\n","        [\"git\", \"status\", \"--porcelain\"],\n","        capture_output=True, text=True\n","    ).stdout.strip()\n","    git_info[\"dirty\"] = bool(dirty)\n","except:\n","    git_info[\"commit\"] = \"N/A (not a git repo)\"\n","    git_info[\"dirty\"] = False\n","\n","with open(output_dir / \"git_info.json\", \"w\") as f:\n","    json.dump(git_info, f, indent=2)\n","\n","# 9. PyTorch build information\n","print(\"Saving PyTorch build information...\")\n","try:\n","    buf = io.StringIO()\n","    with redirect_stdout(buf):\n","        torch.__config__.show()\n","    (output_dir / \"torch_build.txt\").write_text(buf.getvalue(), encoding=\"utf-8\")\n","except:\n","    pass\n","\n","# 10. Data checksums (only original archives)\n","print(\"Generating data checksums...\")\n","data_dir = Path(\"data\")\n","if data_dir.exists():\n","    sha256sums = []\n","    archive_exts = {'.zip', '.tar', '.gz', '.tgz', '.bz2', '.xz', '.7z', '.rar'}\n","    for file_path in sorted(data_dir.rglob(\"*\")):\n","        if file_path.is_file() and file_path.suffix.lower() in archive_exts:\n","            sha256 = hashlib.sha256()\n","            with open(file_path, \"rb\") as f:\n","                for chunk in iter(lambda: f.read(65536), b\"\"):\n","                    sha256.update(chunk)\n","            rel_path = file_path.relative_to(data_dir)\n","            sha256sums.append(f\"{sha256.hexdigest()}  {rel_path}\")\n","\n","    if sha256sums:\n","        with open(output_dir / \"data_SHA256SUMS.txt\", \"w\") as f:\n","            f.write(\"\\n\".join(sha256sums))\n","        print(f\"  Generated checksums for {len(sha256sums)} archives\")\n","    else:\n","        print(\"  No archives in data/ directory; skipping checksums\")\n","else:\n","    print(\"  data/ directory does not exist; skipping checksums\")\n","\n","# 11. Compute environment hashes of all key files\n","print(\"Computing environment hashes...\")\n","env_files = [\n","    \"requirements.txt\",\n","    \"environment.yml\",\n","    \"env.txt\",\n","    \"SEEDS.yaml\",\n","    \"hardware_log.json\",\n","    \"git_info.json\"\n","]\n","sha256_lines = []\n","for filename in env_files:\n","    filepath = output_dir / filename\n","    if filepath.exists():\n","        sha256 = hashlib.sha256()\n","        with open(filepath, \"rb\") as f:\n","            sha256.update(f.read())\n","        sha256_lines.append(f\"{sha256.hexdigest()}  {filename}\")\n","\n","with open(output_dir / \"ENV.SHA256\", \"w\") as f:\n","    f.write(\"\\n\".join(sha256_lines))\n","\n","# Output summary\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 0 complete - Reproducible environment configuration (top-conf/journal grade)\")\n","print(\"=\"*60)\n","print(f\"Output directory: {output_dir}/\")\n","print(f\"  ✓ SEEDS.yaml (seeds: {SEEDS})\")\n","print(f\"  ✓ requirements.txt\")\n","print(f\"  ✓ env.txt (with system summary)\")\n","print(f\"  ✓ environment.yml\")\n","print(f\"  ✓ hardware_log.json\")\n","print(f\"  ✓ git_info.json (dirty={git_info.get('dirty', False)})\")\n","print(f\"  ✓ torch_build.txt\")\n","print(f\"  ✓ ENV.SHA256 (covers all key files)\")\n","if (output_dir / \"data_SHA256SUMS.txt\").exists():\n","    print(f\"  ✓ data_SHA256SUMS.txt (archives only)\")\n","\n","print(f\"\\nStrict determinism configuration:\")\n","print(f\"  - torch.use_deterministic_algorithms: True (warn_only=False)\")\n","print(f\"  - cudnn.deterministic: {torch.backends.cudnn.deterministic}\")\n","print(f\"  - cudnn.benchmark: {torch.backends.cudnn.benchmark}\")\n","if torch.cuda.is_available():\n","    print(f\"  - TF32 disabled: {not torch.backends.cuda.matmul.allow_tf32}\")\n","print(f\"  - Environment variables set:\")\n","print(f\"    PYTHONHASHSEED: {os.environ.get('PYTHONHASHSEED')}\")\n","print(f\"    CUBLAS_WORKSPACE_CONFIG: {os.environ.get('CUBLAS_WORKSPACE_CONFIG')}\")\n","print(f\"    Thread control: OMP/MKL/OPENBLAS/NUMEXPR=1\")\n","print(\"=\"*60)"]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Steps 1–2: Data Acquisition & Unpack Standardization (top-conf/journal grade)\n","Process the uploaded LARa MbientLab IMU archive\n","\"\"\"\n","\n","import os\n","import hashlib\n","import zipfile\n","import shutil\n","import json\n","import re\n","import numpy as np\n","from pathlib import Path\n","from datetime import datetime, timezone\n","import pandas as pd\n","\n","# ========== Helper functions ==========\n","def read_any_csv(path, nrows=None):\n","    \"\"\"CSV reader with auto delimiter detection\"\"\"\n","    try:\n","        return pd.read_csv(path, nrows=nrows, sep=None, engine=\"python\")\n","    except Exception:\n","        return pd.read_csv(path, nrows=nrows)\n","\n","def infer_sampling_rate(df):\n","    \"\"\"Infer sampling rate; auto-handle ns/μs/ms/s time units\"\"\"\n","    cols = [c.lower() for c in df.columns]\n","    time_cols = [c for c in df.columns if re.search(r\"(time|timestamp|epoch)\", c.lower())]\n","    if not time_cols:\n","        return None\n","\n","    c = time_cols[0]\n","    t = pd.to_numeric(df[c], errors=\"coerce\").dropna().to_numpy()\n","    if t.size < 3:\n","        return None\n","\n","    # Infer time unit by magnitude\n","    max_val = np.nanmax(np.abs(t[:1000])) if t.size else 0\n","    if max_val >= 1e12:      # nanoseconds\n","        scale = 1e-9\n","    elif max_val >= 1e9:     # nanoseconds\n","        scale = 1e-9\n","    elif max_val >= 1e6:     # microseconds\n","        scale = 1e-6\n","    elif max_val >= 1e3:     # milliseconds\n","        scale = 1e-3\n","    else:                    # seconds\n","        scale = 1.0\n","\n","    t_sec = t * scale\n","    dt = np.diff(t_sec)\n","    dt = dt[dt > 0]\n","    if dt.size == 0:\n","        return None\n","\n","    # Use median for robustness\n","    return float(np.round(1.0 / np.median(dt), 3))\n","\n","def infer_sensor_type(cols_lower, filename):\n","    \"\"\"Infer sensor type\"\"\"\n","    if 'label' in filename.lower() or 'activity' in filename.lower():\n","        return \"labels\"\n","\n","    sensors = []\n","    if any((\"acc\" in c) or (\"accelerom\" in c) for c in cols_lower):\n","        sensors.append(\"acc\")\n","    if any((\"gyro\" in c) or re.search(r\"\\bgyr\", c) for c in cols_lower):\n","        sensors.append(\"gyro\")\n","    if any((\"mag\" in c) or (\"magnetom\" in c) for c in cols_lower):\n","        sensors.append(\"mag\")\n","\n","    return \"+\".join(sensors) if sensors else \"unknown\"\n","\n","# LARa placement mapping (per official docs)\n","PLACEMENT_MAP = {\n","    \"L01\": \"lwrist\",      # Left wrist\n","    \"L02\": \"rwrist\",      # Right wrist\n","    \"L03\": \"chest\",       # Chest\n","    \"L04\": \"belt\",        # Belt\n","    \"L05\": \"lankle\",      # Left ankle\n","    \"L06\": \"pocket\",      # Pocket\n","    \"L07\": \"lforearm\",    # Left forearm\n","    \"L08\": \"lupperarm\",   # Left upper arm\n","}\n","\n","# ========== Step 1: Acquire & verify ==========\n","print(\"=\"*60)\n","print(\"Step 1: Data acquisition & verification\")\n","print(\"=\"*60)\n","\n","# Create directory structure\n","raw_dir = Path(\"data/lara/mbientlab/raw\")\n","raw_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Find uploaded zip files (prefer annotated versions)\n","uploaded_files = list(Path(\".\").glob(\"*annotated*MbientLab*.zip\"))\n","if not uploaded_files:\n","    uploaded_files = list(Path(\".\").glob(\"*MbientLab*.zip\"))\n","if not uploaded_files:\n","    uploaded_files = list(Path(\".\").glob(\"*.zip\"))\n","\n","if not uploaded_files:\n","    raise FileNotFoundError(\"No MbientLab data archive found; please upload a zip file first\")\n","\n","if len(uploaded_files) > 1:\n","    print(f\"Warning: found multiple candidate files: {[f.name for f in uploaded_files]}\")\n","    print(f\"Using the first: {uploaded_files[0].name}\")\n","\n","zip_file = uploaded_files[0]\n","print(f\"Found archive: {zip_file}\")\n","\n","# Move to raw data directory\n","target_zip = raw_dir / zip_file.name\n","if not target_zip.exists():\n","    shutil.copy2(zip_file, target_zip)\n","    print(f\"Copied to: {target_zip}\")\n","else:\n","    print(f\"File already exists: {target_zip}\")\n","\n","# Compute SHA256 checksum\n","print(\"Computing SHA256 checksum...\")\n","sha256_hash = hashlib.sha256()\n","with open(target_zip, \"rb\") as f:\n","    for chunk in iter(lambda: f.read(65536), b\"\"):\n","        sha256_hash.update(chunk)\n","\n","checksum = sha256_hash.hexdigest()\n","print(f\"SHA256: {checksum}\")\n","\n","# Save checksum\n","sha256_file = raw_dir / \"SHA256SUMS.txt\"\n","with open(sha256_file, \"w\") as f:\n","    f.write(f\"{checksum}  {target_zip.name}\\n\")\n","print(f\"Saved checksum: {sha256_file}\")\n","\n","# Record provenance (traceability)\n","provenance = {\n","    \"dataset\": \"LARa IMU-only / MbientLab\",\n","    \"origin\": \"manual-upload\",\n","    \"official_url\": \"https://sensor.informatik.uni-mannheim.de/#dataset_lara\",\n","    \"retrieved_at_utc\": datetime.now(timezone.utc).isoformat(),\n","    \"archive\": target_zip.name,\n","    \"sha256\": checksum\n","}\n","(raw_dir / \"PROVENANCE.json\").write_text(\n","    json.dumps(provenance, indent=2, ensure_ascii=False),\n","    encoding=\"utf-8\"\n",")\n","print(f\"Recorded provenance info: {raw_dir / 'PROVENANCE.json'}\")\n","\n","# Set raw archive to read-only\n","os.chmod(target_zip, 0o444)\n","print(f\"Set read-only permission: {target_zip}\")\n","\n","# ========== Step 2: Unpack & directory standardization ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 2: Unpack & directory standardization\")\n","print(\"=\"*60)\n","\n","# Extract to temp directory\n","temp_extract = raw_dir / \"temp_extract\"\n","temp_extract.mkdir(exist_ok=True)\n","\n","print(f\"Extracting {target_zip.name}...\")\n","with zipfile.ZipFile(target_zip, 'r') as zip_ref:\n","    zip_ref.extractall(temp_extract)\n","\n","# Scan extracted files and normalize\n","file_records = []\n","problems = []  # record files that failed to parse\n","\n","# Recursively scan all CSV/TSV files\n","for file_path in temp_extract.rglob(\"*\"):\n","    if not file_path.is_file():\n","        continue\n","\n","    # Process only data files\n","    if file_path.suffix.lower() not in ['.csv', '.tsv', '.txt']:\n","        continue\n","\n","    # Parse filename: LARa pattern L01_S07_R01.csv\n","    filename = file_path.stem\n","\n","    # Extract L01/L02/L03 (placement)\n","    placement_match = re.search(r'L(\\d+)', filename)\n","    placement_raw = f\"L{placement_match.group(1).zfill(2)}\" if placement_match else \"L00\"\n","    placement = PLACEMENT_MAP.get(placement_raw, placement_raw)\n","\n","    # Extract S07 (subject)\n","    subject_match = re.search(r'S(\\d+)', filename)\n","    subject_id = f\"S{subject_match.group(1).zfill(2)}\" if subject_match else \"S00\"\n","\n","    # Extract R01 (session)\n","    session_match = re.search(r'R(\\d+)', filename)\n","    session_id = f\"R{session_match.group(1).zfill(2)}\" if session_match else \"R01\"\n","\n","    # Detect parse failures (avoid LOSO leakage)\n","    if subject_id == \"S00\" or session_id == \"R01\":\n","        if not re.search(r'R01', filename):  # exclude real R01\n","            problems.append(str(file_path.relative_to(temp_extract)))\n","\n","    # Create standardized directory structure\n","    std_dir = raw_dir / subject_id / session_id / placement\n","    std_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # Standardized filename (lowercase, underscores)\n","    std_filename = file_path.name.lower().replace(' ', '_').replace('-', '_')\n","    std_path = std_dir / std_filename\n","\n","    # Copy to standardized location\n","    if not std_path.exists():\n","        shutil.copy2(file_path, std_path)\n","\n","    # Get file info\n","    file_size = file_path.stat().st_size\n","    num_rows = 0\n","    sampling_rate = None\n","    duration = None\n","    sensor_type = \"unknown\"\n","\n","    try:\n","        # Read sample\n","        df_sample = read_any_csv(file_path, nrows=2000)\n","        columns_lower = [c.lower() for c in df_sample.columns]\n","\n","        # Infer sensor type\n","        sensor_type = infer_sensor_type(columns_lower, filename)\n","\n","        # Infer sampling rate (skip for labels)\n","        if sensor_type != \"labels\":\n","            sampling_rate = infer_sampling_rate(df_sample)\n","\n","        # Count total rows (streaming to avoid loading big files)\n","        with open(file_path, \"rb\") as fh:\n","            num_rows = sum(1 for _ in fh) - 1  # minus header\n","\n","        # Compute duration\n","        if sampling_rate and num_rows > 0:\n","            duration = round(num_rows / sampling_rate, 2)\n","\n","    except Exception:\n","        pass  # silently skip files that cannot be parsed\n","\n","    # Record file info\n","    file_records.append({\n","        \"subject_id\": subject_id,\n","        \"session_id\": session_id,\n","        \"placement\": placement,\n","        \"placement_raw\": placement_raw,\n","        \"sensor_type\": sensor_type,\n","        \"original_path\": str(file_path.relative_to(temp_extract)),\n","        \"standardized_path\": str(std_path.relative_to(raw_dir)),\n","        \"filename\": std_filename,\n","        \"file_size_bytes\": file_size,\n","        \"num_rows\": num_rows,\n","        \"sampling_rate_hz\": sampling_rate,\n","        \"duration_sec\": duration,\n","    })\n","\n","print(f\"Processed {len(file_records)} files\")\n","\n","# Check parse failures\n","if problems:\n","    problems_file = raw_dir / \"PROBLEMS.log\"\n","    problems_file.write_text(\n","        \"The following files could not parse subject/session (would break LOSO):\\n\" +\n","        \"\\n\".join(problems) + \"\\n\",\n","        encoding=\"utf-8\"\n","    )\n","    raise RuntimeError(\n","        f\"Found {len(problems)} files with unparsed subject/session; \"\n","        f\"please check {problems_file} and fix\"\n","    )\n","\n","# Remove temp extraction directory\n","shutil.rmtree(temp_extract)\n","print(\"Removed temporary files\")\n","\n","# Generate file_index (Parquet preferred; fallback to CSV)\n","if file_records:\n","    file_index = pd.DataFrame(file_records)\n","\n","    # Sort\n","    file_index = file_index.sort_values(\n","        ['subject_id', 'session_id', 'placement', 'sensor_type']\n","    )\n","\n","    # Save index\n","    index_file = raw_dir / \"file_index.parquet\"\n","    try:\n","        file_index.to_parquet(index_file, index=False)\n","        saved_index = index_file\n","        print(f\"\\nGenerated file index: {saved_index}\")\n","    except Exception as e:\n","        print(f\"Warning: Parquet write failed ({e}); falling back to CSV\")\n","        index_file_csv = raw_dir / \"file_index.csv\"\n","        file_index.to_csv(index_file_csv, index=False)\n","        saved_index = index_file_csv\n","        print(f\"Generated file index: {saved_index}\")\n","\n","    # Show dataset statistics\n","    print(\"\\nDataset statistics:\")\n","    print(f\"  Number of subjects: {file_index['subject_id'].nunique()}\")\n","    print(f\"  Number of sessions: {file_index.groupby('subject_id')['session_id'].nunique().sum()}\")\n","    print(f\"  Placements: {sorted(file_index['placement'].unique().tolist())}\")\n","    print(f\"  Sensor types: {sorted(file_index['sensor_type'].unique().tolist())}\")\n","    print(f\"  Total files: {len(file_index)}\")\n","\n","    # Sampling rate stats\n","    sensor_files = file_index[file_index['sensor_type'] != 'labels']\n","    if not sensor_files.empty:\n","        rates = sensor_files['sampling_rate_hz'].dropna()\n","        if not rates.empty:\n","            print(f\"  Sampling rate range: {rates.min():.1f} - {rates.max():.1f} Hz\")\n","            print(f\"  Median sampling rate: {rates.median():.1f} Hz\")\n","\n","    # Preview first records\n","    print(\"\\nFile index preview:\")\n","    print(file_index.head(10).to_string())\n","else:\n","    print(\"Warning: No data files found\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Steps 1–2 complete (top-conf/journal grade)\")\n","print(\"=\"*60)\n","print(f\"Raw data: {raw_dir}/\")\n","print(f\"Checksum: {sha256_file}\")\n","print(f\"Provenance record: {raw_dir / 'PROVENANCE.json'}\")\n","print(f\"File index: {saved_index}\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"esro3-Wq_AzJ","executionInfo":{"status":"ok","timestamp":1763470221435,"user_tz":0,"elapsed":31653,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"e0926d91-11b8-44cc-f7f3-9c3742c87e3e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 1: Data acquisition & verification\n","============================================================\n","Found archive: IMU data (annotated) _ MbientLab.zip\n","Copied to: data/lara/mbientlab/raw/IMU data (annotated) _ MbientLab.zip\n","Computing SHA256 checksum...\n","SHA256: 70968b6b8874375e96671af67e31c27ccb63793f31191f86e732d40f24ac3106\n","Saved checksum: data/lara/mbientlab/raw/SHA256SUMS.txt\n","Recorded provenance info: data/lara/mbientlab/raw/PROVENANCE.json\n","Set read-only permission: data/lara/mbientlab/raw/IMU data (annotated) _ MbientLab.zip\n","\n","============================================================\n","Step 2: Unpack & directory standardization\n","============================================================\n","Extracting IMU data (annotated) _ MbientLab.zip...\n","Processed 386 files\n","Removed temporary files\n","\n","Generated file index: data/lara/mbientlab/raw/file_index.parquet\n","\n","Dataset statistics:\n","  Number of subjects: 8\n","  Number of sessions: 193\n","  Placements: ['chest', 'lwrist', 'rwrist']\n","  Sensor types: ['acc+gyro', 'labels']\n","  Total files: 386\n","\n","File index preview:\n","    subject_id session_id placement placement_raw sensor_type                                                original_path                      standardized_path                filename  file_size_bytes  num_rows sampling_rate_hz duration_sec\n","363        S07        R01    lwrist           L01    acc+gyro         IMU data (annotated) _ MbientLab/S07/L01_S07_R01.csv         S07/R01/lwrist/l01_s07_r01.csv         l01_s07_r01.csv          7227132     11824             None         None\n","375        S07        R01    lwrist           L01      labels  IMU data (annotated) _ MbientLab/S07/L01_S07_R01_labels.csv  S07/R01/lwrist/l01_s07_r01_labels.csv  l01_s07_r01_labels.csv           485055     11824             None         None\n","364        S07        R02    lwrist           L01    acc+gyro         IMU data (annotated) _ MbientLab/S07/L01_S07_R02.csv         S07/R02/lwrist/l01_s07_r02.csv         l01_s07_r02.csv          7196022     11776             None         None\n","353        S07        R02    lwrist           L01      labels  IMU data (annotated) _ MbientLab/S07/L01_S07_R02_labels.csv  S07/R02/lwrist/l01_s07_r02_labels.csv  l01_s07_r02_labels.csv           483087     11776             None         None\n","330        S07        R03    rwrist           L02    acc+gyro         IMU data (annotated) _ MbientLab/S07/L02_S07_R03.csv         S07/R03/rwrist/l02_s07_r03.csv         l02_s07_r03.csv          7193816     11758             None         None\n","361        S07        R03    rwrist           L02      labels  IMU data (annotated) _ MbientLab/S07/L02_S07_R03_labels.csv  S07/R03/rwrist/l02_s07_r03_labels.csv  l02_s07_r03_labels.csv           482349     11758             None         None\n","360        S07        R05    rwrist           L02    acc+gyro         IMU data (annotated) _ MbientLab/S07/L02_S07_R05.csv         S07/R05/rwrist/l02_s07_r05.csv         l02_s07_r05.csv          7197459     11766             None         None\n","329        S07        R05    rwrist           L02      labels  IMU data (annotated) _ MbientLab/S07/L02_S07_R05_labels.csv  S07/R05/rwrist/l02_s07_r05_labels.csv  l02_s07_r05_labels.csv           482677     11766             None         None\n","384        S07        R06    rwrist           L02    acc+gyro         IMU data (annotated) _ MbientLab/S07/L02_S07_R06.csv         S07/R06/rwrist/l02_s07_r06.csv         l02_s07_r06.csv          7238567     11838             None         None\n","349        S07        R06    rwrist           L02      labels  IMU data (annotated) _ MbientLab/S07/L02_S07_R06_labels.csv  S07/R06/rwrist/l02_s07_r06_labels.csv  l02_s07_r06_labels.csv           485629     11838             None         None\n","\n","============================================================\n","Steps 1–2 complete (top-conf/journal grade)\n","============================================================\n","Raw data: data/lara/mbientlab/raw/\n","Checksum: data/lara/mbientlab/raw/SHA256SUMS.txt\n","Provenance record: data/lara/mbientlab/raw/PROVENANCE.json\n","File index: data/lara/mbientlab/raw/file_index.parquet\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 3: Metadata & Quality Audit (top-conf/journal grade - final)\n","Parse subjects, activity set, sampling rate, placement, session time; empty-window cleanup\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","from datetime import datetime, timezone\n","import json\n","import re\n","\n","# ========== Config ==========\n","MISSING_THRESHOLD = 0.05      # Missing-rate threshold 5%\n","GAP_THRESHOLD = 2.0           # Single-gap absolute threshold (seconds)\n","GAP_RATIO_THRESHOLD = 0.05    # Gap ratio threshold 5%\n","\n","print(\"=\"*60)\n","print(\"Step 3: Metadata & Quality Audit\")\n","print(\"=\"*60)\n","\n","# Load file index\n","raw_dir = Path(\"data/lara/mbientlab/raw\")\n","index_file = raw_dir / \"file_index.parquet\"\n","if not index_file.exists():\n","    index_file = raw_dir / \"file_index.csv\"\n","\n","print(f\"Loading file index: {index_file}\")\n","file_index = pd.read_parquet(index_file) if index_file.suffix == '.parquet' else pd.read_csv(index_file)\n","\n","# Initialize variables (avoid undefined in edge cases)\n","subject_agg = pd.DataFrame()\n","meta_subjects_file = None\n","meta_sessions_file = None\n","keep_sessions_file = None\n","\n","# ========== Helper functions ==========\n","def pick_scale(med_raw, sr_hint=None):\n","    \"\"\"Smartly pick time unit (s/ms/μs/ns → seconds)\"\"\"\n","    cands = [1.0, 1e-3, 1e-6, 1e-9]\n","\n","    if sr_hint and sr_hint > 0:\n","        target_dt = 1.0 / sr_hint\n","        return min(cands, key=lambda s: abs(med_raw * s - target_dt))\n","\n","    # Without hint: prefer median interval mapping into 5-400 Hz, bias toward ~50 Hz\n","    best, err = 1.0, float(\"inf\")\n","    for s in cands:\n","        dt = med_raw * s\n","        if dt <= 0:\n","            continue\n","        sr = 1.0 / dt\n","        score = 0 if 5 <= sr <= 400 else abs(sr - 50) * 10\n","        if score < err:\n","            best, err = s, score\n","    return best\n","\n","def extract_time_range_and_gaps(file_path, sampling_rate_hint=None, head_rows=20000, chunksize=200000):\n","    \"\"\"Read time column in chunks; extract range and gaps (incl. inter-chunk gaps, memory-friendly)\"\"\"\n","    try:\n","        # Infer time column & unit from a small sample\n","        df_head = pd.read_csv(file_path, nrows=head_rows, sep=None, engine=\"python\")\n","        time_cols = [c for c in df_head.columns if re.search(r\"(time|timestamp|epoch|ts)\", c, re.I)]\n","        if not time_cols:\n","            return None, None, 0.0, 0.0, 0.0\n","\n","        c = time_cols[0]\n","        s = pd.to_numeric(df_head[c], errors=\"coerce\").dropna().to_numpy()\n","\n","        # Numeric timestamp branch\n","        if s.size >= 3:\n","            diffs = np.diff(s)\n","            diffs = diffs[np.isfinite(diffs) & (diffs > 0)]\n","            if diffs.size > 0:\n","                med = float(np.median(diffs))\n","                scale = pick_scale(med, sampling_rate_hint)\n","                expected = (1.0 / sampling_rate_hint) if (sampling_rate_hint and sampling_rate_hint > 0) else (med * scale)\n","\n","                # OR logic: two independent thresholds\n","                rel_threshold = 10.0 * expected  # Relative threshold: 10× expected interval\n","                abs_threshold = GAP_THRESHOLD    # Absolute threshold: 2 s\n","\n","                first = None\n","                last = None\n","                prev = None\n","                gap_sec = 0.0\n","                max_gap = 0.0\n","\n","                for chunk in pd.read_csv(file_path, usecols=[c], sep=None, engine=\"python\", chunksize=chunksize):\n","                    v = pd.to_numeric(chunk[c], errors=\"coerce\").dropna().to_numpy()\n","                    if v.size == 0:\n","                        continue\n","\n","                    if first is None:\n","                        first = v[0]\n","\n","                    # Inter-chunk gaps (fix: use max as baseline)\n","                    if prev is not None:\n","                        delta = (v[0] - prev) * scale\n","                        cond_rel = delta > rel_threshold\n","                        cond_abs = delta > abs_threshold\n","\n","                        if cond_rel or cond_abs:\n","                            # If both trigger, use max (more lenient); if only one, use that one\n","                            if cond_rel and cond_abs:\n","                                base = max(rel_threshold, abs_threshold)\n","                            elif cond_rel:\n","                                base = rel_threshold\n","                            else:\n","                                base = abs_threshold\n","\n","                            gap_this = delta - base\n","                            gap_sec += gap_this\n","                            max_gap = max(max_gap, gap_this)\n","\n","                    # Intra-chunk gaps (fix: shape + baseline)\n","                    d = np.diff(v) * scale\n","                    mask_rel = d > rel_threshold\n","                    mask_abs = d > abs_threshold\n","                    mask = mask_rel | mask_abs\n","\n","                    if mask.any():\n","                        # Vectorized: choose the threshold triggered by each gap (use max if both)\n","                        both_triggered = mask_rel & mask_abs\n","                        thr_used = np.where(\n","                            both_triggered,\n","                            max(rel_threshold, abs_threshold),\n","                            np.where(mask_rel, rel_threshold, abs_threshold)\n","                        )\n","                        gaps = d[mask] - thr_used[mask]  # Fix: also index thr_used\n","                        gap_sec += float(gaps.sum())\n","                        max_gap = max(max_gap, float(gaps.max()))\n","\n","                    prev = v[-1]\n","                    last = v[-1]\n","\n","                if first is not None and last is not None:\n","                    start_sec = float(first * scale)\n","                    end_sec = float(last * scale)\n","                    total = end_sec - start_sec\n","                    ratio = float(gap_sec / total) if total > 0 else 0.0\n","                    return start_sec, end_sec, float(round(gap_sec, 2)), float(round(ratio, 4)), float(round(max_gap, 2))\n","\n","        # Fallback branch: datetime strings\n","        t_head = pd.to_datetime(df_head[c], utc=True, errors=\"coerce\").dropna()\n","        if t_head.size >= 3:\n","            med = float(t_head.diff().dt.total_seconds().dropna().median())\n","            if med > 0:\n","                expected = (1.0 / sampling_rate_hint) if (sampling_rate_hint and sampling_rate_hint > 0) else med\n","\n","                # OR logic\n","                rel_threshold = 10.0 * expected\n","                abs_threshold = GAP_THRESHOLD\n","\n","                first = None\n","                last = None\n","                prev = None\n","                gap_sec = 0.0\n","                max_gap = 0.0\n","\n","                for chunk in pd.read_csv(file_path, usecols=[c], sep=None, engine=\"python\", chunksize=chunksize):\n","                    tt = pd.to_datetime(chunk[c], utc=True, errors=\"coerce\").dropna()\n","                    if tt.empty:\n","                        continue\n","\n","                    if first is None:\n","                        first = tt.iloc[0]\n","\n","                    # Inter-chunk gaps (fix: use max as baseline)\n","                    if prev is not None:\n","                        delta = (tt.iloc[0] - prev).total_seconds()\n","                        cond_rel = delta > rel_threshold\n","                        cond_abs = delta > abs_threshold\n","\n","                        if cond_rel or cond_abs:\n","                            if cond_rel and cond_abs:\n","                                base = max(rel_threshold, abs_threshold)\n","                            elif cond_rel:\n","                                base = rel_threshold\n","                            else:\n","                                base = abs_threshold\n","\n","                            gap_this = delta - base\n","                            gap_sec += gap_this\n","                            max_gap = max(max_gap, gap_this)\n","\n","                    # Intra-chunk gaps (fix: shape + baseline)\n","                    d = tt.diff().dt.total_seconds().dropna()\n","                    mask_rel = d > rel_threshold\n","                    mask_abs = d > abs_threshold\n","                    mask = mask_rel | mask_abs\n","\n","                    if not mask.empty and mask.any():\n","                        both_triggered = mask_rel & mask_abs\n","                        thr_used = np.where(\n","                            both_triggered,\n","                            max(rel_threshold, abs_threshold),\n","                            np.where(mask_rel, rel_threshold, abs_threshold)\n","                        )\n","                        gaps = d[mask].values - thr_used[mask]\n","                        gap_sec += float(gaps.sum())\n","                        max_gap = max(max_gap, float(gaps.max()))\n","\n","                    prev = tt.iloc[-1]\n","                    last = tt.iloc[-1]\n","\n","                if first is not None and last is not None:\n","                    total = (last - first).total_seconds()\n","                    ratio = float(gap_sec / total) if total > 0 else 0.0\n","                    return first.timestamp(), last.timestamp(), float(round(gap_sec, 2)), float(round(ratio, 4)), float(round(max_gap, 2))\n","\n","        return None, None, 0.0, 0.0, 0.0\n","\n","    except Exception:\n","        return None, None, 0.0, 0.0, 0.0\n","\n","def safe_float(x, default=0.0):\n","    \"\"\"Safely cast to float, handling NaN/Inf\"\"\"\n","    try:\n","        if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))):\n","            return default\n","        return float(x)\n","    except:\n","        return default\n","\n","# ========== 1. Parse sensor data metadata ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Parse sensor data metadata\")\n","print(\"=\"*60)\n","\n","# Determine label files directly from filenames (more reliable)\n","label_files = file_index[\n","    file_index['filename'].str.contains('label', case=False, na=False)\n","].copy()\n","sensor_files = file_index[\n","    ~file_index['filename'].str.contains('label', case=False, na=False)\n","].copy()\n","\n","print(f\"Sensor files: {len(sensor_files)}\")\n","print(f\"Label files: {len(label_files)}\")\n","\n","# Extract time ranges for sensor files (receive 5 return values)\n","print(\"Extracting time spans and gap statistics (chunked)...\")\n","time_records = []\n","for idx, row in sensor_files.iterrows():\n","    file_path = raw_dir / row['standardized_path']\n","    start, end, gap_sec, gap_ratio, max_gap = extract_time_range_and_gaps(\n","        file_path,\n","        row['sampling_rate_hz']\n","    )\n","    time_records.append({\n","        'subject_id': row['subject_id'],\n","        'session_id': row['session_id'],\n","        'placement': row['placement'],\n","        'start_time': start,\n","        'end_time': end,\n","        'gap_seconds': gap_sec,\n","        'gap_ratio': gap_ratio,\n","        'max_gap_seconds': max_gap,\n","    })\n","\n","df_time_ranges = pd.DataFrame(time_records)\n","\n","# Aggregate time ranges by session (includes max_gap)\n","session_time_agg = df_time_ranges.groupby(['subject_id', 'session_id']).agg({\n","    'start_time': 'min',\n","    'end_time': 'max',\n","    'gap_seconds': 'sum',\n","    'max_gap_seconds': 'max',\n","}).reset_index()\n","\n","session_time_agg['session_duration_sec'] = (\n","    session_time_agg['end_time'] - session_time_agg['start_time']\n",")\n","session_time_agg['gap_ratio'] = (\n","    session_time_agg['gap_seconds'] / session_time_agg['session_duration_sec']\n",").fillna(0.0).infer_objects(copy=False)\n","\n","session_time_agg.rename(columns={\n","    'start_time': 'session_start_time',\n","    'end_time': 'session_end_time'\n","}, inplace=True)\n","\n","# Add ISO8601 (human-readable) times\n","def to_iso(x):\n","    try:\n","        if pd.notna(x):\n","            return datetime.fromtimestamp(float(x), tz=timezone.utc).isoformat()\n","    except:\n","        pass\n","    return None\n","\n","session_time_agg['session_start_utc'] = session_time_agg['session_start_time'].apply(to_iso)\n","session_time_agg['session_end_utc'] = session_time_agg['session_end_time'].apply(to_iso)\n","\n","print(f\"Extracted time spans for {len(session_time_agg)} sessions\")\n","\n","# ========== 2. Parse labels & activity statistics ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Parse labels & activity statistics\")\n","print(\"=\"*60)\n","\n","activity_stats = []\n","session_records = []\n","\n","for idx, label_row in label_files.iterrows():\n","    label_path = raw_dir / label_row['standardized_path']\n","\n","    if not label_path.exists():\n","        continue\n","\n","    try:\n","        # Read label file\n","        df_label = pd.read_csv(label_path, sep=None, engine='python')\n","\n","        # Find label column (LARa dataset uses 'Class')\n","        if 'Class' in df_label.columns:\n","            label_col = 'Class'\n","        elif 'class' in df_label.columns:\n","            label_col = 'class'\n","        else:\n","            label_cols = [c for c in df_label.columns if 'label' in c.lower() or 'activity' in c.lower()]\n","            if not label_cols:\n","                print(f\"  No label column ({df_label.columns.tolist()}): {label_path.name}\")\n","                continue\n","            label_col = label_cols[0]\n","\n","        # Count activity distribution\n","        activity_counts = df_label[label_col].value_counts()\n","        total_samples = len(df_label)\n","\n","        # Check missing\n","        missing_count = df_label[label_col].isna().sum()\n","        missing_rate = missing_count / total_samples if total_samples > 0 else 0\n","\n","        # Record session info\n","        session_info = {\n","            'subject_id': label_row['subject_id'],\n","            'session_id': label_row['session_id'],\n","            'placement': label_row['placement'],\n","            'total_samples': total_samples,\n","            'missing_samples': missing_count,\n","            'missing_rate': round(missing_rate, 4),\n","            'num_activities': len(activity_counts),\n","        }\n","\n","        # Add per-activity stats\n","        for activity, count in activity_counts.items():\n","            activity_stats.append({\n","                'subject_id': label_row['subject_id'],\n","                'session_id': label_row['session_id'],\n","                'placement': label_row['placement'],\n","                'activity': str(activity),\n","                'count': int(count),\n","                'percentage': round(count / total_samples * 100, 2)\n","            })\n","\n","        session_records.append(session_info)\n","\n","    except Exception as e:\n","        print(f\"  Warning: failed to parse {label_path.name}: {e}\")\n","        continue\n","\n","print(f\"Parsed {len(session_records)} sessions\")\n","\n","# ========== 2.1 Orphan session check ==========\n","print(\"\\nChecking orphan sessions...\")\n","sess_from_sensors = set(zip(sensor_files['subject_id'], sensor_files['session_id']))\n","sess_from_labels = set(zip(label_files['subject_id'], label_files['session_id']))\n","orphans = sess_from_sensors - sess_from_labels\n","\n","if orphans:\n","    orphan_file = raw_dir / \"QA_ISSUES.log\"\n","    with open(orphan_file, \"a\", encoding=\"utf-8\") as f:\n","        f.write(\"\\nSessions with sensors but no labels (orphan sessions):\\n\")\n","        for s, r in sorted(orphans):\n","            f.write(f\"  {s}-{r}\\n\")\n","    print(f\"⚠️  Found {len(orphans)} orphan sessions; logged to QA_ISSUES.log\")\n","\n","# ========== 3. Merge session metadata ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Merge session metadata\")\n","print(\"=\"*60)\n","\n","df_sessions = pd.DataFrame(session_records)\n","df_activities = pd.DataFrame(activity_stats)\n","\n","# Merge time info\n","if not df_sessions.empty and not session_time_agg.empty:\n","    df_sessions = df_sessions.merge(\n","        session_time_agg,\n","        on=['subject_id', 'session_id'],\n","        how='left'\n","    )\n","    print(f\"Merged time span info\")\n","\n","# ========== 4. Data quality checks & empty-window cleanup ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Data quality checks & empty-window cleanup\")\n","print(\"=\"*60)\n","\n","if not df_sessions.empty:\n","    # Generate keep flag\n","    df_sessions['keep'] = True\n","    df_sessions['reject_reason'] = ''\n","\n","    # Check missing-rate exceeds threshold\n","    high_missing_mask = df_sessions['missing_rate'] > MISSING_THRESHOLD\n","    if high_missing_mask.any():\n","        df_sessions.loc[high_missing_mask, 'keep'] = False\n","        df_sessions.loc[high_missing_mask, 'reject_reason'] = 'high_missing_rate'\n","        print(f\"⚠️  {high_missing_mask.sum()} sessions marked not kept due to high missing rate\")\n","\n","    # Check time-gap ratio exceeds threshold\n","    if 'gap_ratio' in df_sessions.columns:\n","        high_gap_mask = df_sessions['gap_ratio'] > GAP_RATIO_THRESHOLD\n","        if high_gap_mask.any():\n","            # Append reason if already rejected; otherwise mark alone\n","            for idx in df_sessions[high_gap_mask].index:\n","                if df_sessions.loc[idx, 'keep']:\n","                    df_sessions.loc[idx, 'keep'] = False\n","                    df_sessions.loc[idx, 'reject_reason'] = 'high_gap_ratio'\n","                else:\n","                    df_sessions.loc[idx, 'reject_reason'] += '+high_gap_ratio'\n","            print(f\"⚠️  {high_gap_mask.sum()} sessions marked not kept due to high gap ratio\")\n","\n","    # Summary\n","    keep_count = df_sessions['keep'].sum()\n","    reject_count = (~df_sessions['keep']).sum()\n","    print(f\"✓ QC result: keep {keep_count} sessions, reject {reject_count} sessions\")\n","\n","    # Save keep list\n","    keep_sessions_file = raw_dir / \"qa_keep_sessions.csv\"\n","    df_sessions[['subject_id', 'session_id', 'placement', 'keep', 'reject_reason',\n","                 'missing_rate', 'gap_ratio']].to_csv(keep_sessions_file, index=False)\n","    print(f\"✓ Saved: {keep_sessions_file}\")\n","\n","    # Log rejection details\n","    if reject_count > 0:\n","        rejected = df_sessions[~df_sessions['keep']]\n","        qa_issues = raw_dir / \"QA_ISSUES.log\"\n","        with open(qa_issues, \"a\") as f:\n","            f.write(f\"\\nSessions rejected by QC (total {reject_count}):\\n\\n\")\n","            f.write(rejected[['subject_id', 'session_id', 'placement', 'reject_reason',\n","                             'missing_rate', 'gap_ratio']].to_string(index=False))\n","        print(f\"  Details logged to: {qa_issues}\")\n","\n","# ========== 4.1 Generate file-level empty-window list ==========\n","print(\"\\nGenerating file-level empty-window list...\")\n","if not df_time_ranges.empty:\n","    empty_segments = df_time_ranges[\n","        df_time_ranges['gap_ratio'].notna() &\n","        (df_time_ranges['gap_ratio'] > GAP_RATIO_THRESHOLD)\n","    ].copy()\n","\n","    if not empty_segments.empty:\n","        empty_todo_file = raw_dir / \"EMPTY_SEGMENTS_TODO.csv\"\n","        empty_segments[['subject_id', 'session_id', 'placement',\n","                       'gap_seconds', 'gap_ratio', 'max_gap_seconds']].to_csv(empty_todo_file, index=False)\n","        print(f\"⚠️  Generated empty-segment list: {empty_todo_file} ({len(empty_segments)} files)\")\n","\n","# ========== 5. Generate subject-level metadata ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Generate subject-level metadata\")\n","print(\"=\"*60)\n","\n","if not df_sessions.empty:\n","    # Only count kept sessions\n","    df_keep = df_sessions[df_sessions['keep']]\n","\n","    if not df_keep.empty:\n","        # Aggregate by subject\n","        subject_agg = df_keep.groupby('subject_id').agg({\n","            'session_id': 'nunique',\n","            'total_samples': 'sum',\n","            'missing_samples': 'sum',\n","            'session_duration_sec': 'sum',\n","            'num_activities': 'sum',\n","        }).reset_index()\n","\n","        subject_agg.columns = ['subject_id', 'num_sessions', 'total_samples',\n","                               'total_missing', 'total_duration_sec', 'total_activities']\n","\n","        # Compute overall missing rate\n","        subject_agg['overall_missing_rate'] = (\n","            subject_agg['total_missing'] / subject_agg['total_samples']\n","        ).round(4)\n","\n","        # Add placement coverage\n","        placement_coverage = df_keep.groupby('subject_id')['placement'].apply(\n","            lambda x: ','.join(sorted(set(x)))\n","        ).reset_index()\n","        placement_coverage.columns = ['subject_id', 'placements']\n","\n","        subject_agg = subject_agg.merge(placement_coverage, on='subject_id')\n","\n","        # Save subject metadata\n","        meta_subjects_file = raw_dir / \"meta_subjects.csv\"\n","        subject_agg.to_csv(meta_subjects_file, index=False)\n","        print(f\"✓ Saved: {meta_subjects_file}\")\n","        print(f\"  Number of subjects: {len(subject_agg)}\")\n","\n","# ========== 6. Generate session-level metadata ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Generate session-level metadata\")\n","print(\"=\"*60)\n","\n","if not df_sessions.empty:\n","    # Add activity list\n","    if not df_activities.empty:\n","        activity_list = df_activities.groupby(['subject_id', 'session_id'])['activity'].apply(\n","            lambda x: ','.join(sorted(set(x)))\n","        ).reset_index()\n","        activity_list.columns = ['subject_id', 'session_id', 'activities']\n","\n","        df_sessions_full = df_sessions.merge(\n","            activity_list,\n","            on=['subject_id', 'session_id'],\n","            how='left'\n","        )\n","    else:\n","        df_sessions_full = df_sessions\n","\n","    # Save session metadata\n","    meta_sessions_file = raw_dir / \"meta_sessions.csv\"\n","    df_sessions_full.to_csv(meta_sessions_file, index=False)\n","    print(f\"✓ Saved: {meta_sessions_file}\")\n","    print(f\"  Number of sessions: {len(df_sessions_full)}\")\n","\n","# ========== 7. Generate quality audit report ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"7. Generate quality audit report\")\n","print(\"=\"*60)\n","\n","qa_report = []\n","qa_report.append(\"=\"*70)\n","qa_report.append(\"LARa MbientLab IMU Dataset - Quality Audit Report\")\n","qa_report.append(\"=\"*70)\n","qa_report.append(f\"Generated at: {datetime.now(timezone.utc).isoformat()}\")\n","qa_report.append(f\"Data path: {raw_dir}\")\n","qa_report.append(\"\")\n","\n","# Overall stats\n","qa_report.append(\"[1. Dataset overview]\")\n","qa_report.append(\"-\"*70)\n","if not subject_agg.empty:\n","    total_hours = safe_float(subject_agg['total_duration_sec'].sum() / 3600)\n","    qa_report.append(f\"Number of subjects: {len(subject_agg)}\")\n","    qa_report.append(f\"Total sessions: {subject_agg['num_sessions'].sum()}\")\n","    qa_report.append(f\"Total duration: {total_hours:.2f} hours\")\n","    qa_report.append(f\"Total samples: {subject_agg['total_samples'].sum():,}\")\n","qa_report.append(\"\")\n","\n","# Sampling rate stats\n","qa_report.append(\"[2. Sampling rate statistics]\")\n","qa_report.append(\"-\"*70)\n","if not sensor_files.empty:\n","    rates = sensor_files['sampling_rate_hz'].dropna()\n","    if not rates.empty:\n","        qa_report.append(f\"Sampling rate range: {rates.min():.2f} - {rates.max():.2f} Hz\")\n","        qa_report.append(f\"Median sampling rate: {rates.median():.2f} Hz\")\n","        qa_report.append(f\"Mode sampling rate: {rates.mode().values[0]:.2f} Hz\")\n","qa_report.append(\"\")\n","\n","# Placement coverage\n","qa_report.append(\"[3. Sensor placement coverage]\")\n","qa_report.append(\"-\"*70)\n","if not df_sessions.empty:\n","    df_keep = df_sessions[df_sessions['keep']]\n","    if not df_keep.empty:\n","        placement_dist = df_keep['placement'].value_counts()\n","        for placement, count in placement_dist.items():\n","            percentage = count / len(df_keep) * 100\n","            qa_report.append(f\"  {placement:15s}: {count:3d} sessions ({percentage:5.1f}%)\")\n","qa_report.append(\"\")\n","\n","# Activity distribution\n","qa_report.append(\"[4. Activity distribution]\")\n","qa_report.append(\"-\"*70)\n","if not df_activities.empty:\n","    activity_total = df_activities.groupby('activity').agg({\n","        'count': 'sum',\n","    }).sort_values('count', ascending=False)\n","\n","    total_count = activity_total['count'].sum()\n","    qa_report.append(f\"Number of activity classes: {len(activity_total)}\")\n","    qa_report.append(f\"Total samples: {total_count:,}\")\n","    qa_report.append(\"\")\n","    qa_report.append(\"Per-activity share:\")\n","    for activity, row in activity_total.iterrows():\n","        percentage = row['count'] / total_count * 100\n","        qa_report.append(f\"  {str(activity):30s}: {row['count']:8,} ({percentage:5.2f}%)\")\n","qa_report.append(\"\")\n","\n","# Data quality (incl. max_gap stats)\n","qa_report.append(\"[5. Data quality assessment]\")\n","qa_report.append(\"-\"*70)\n","if not df_sessions.empty:\n","    qa_report.append(f\"Missing-rate threshold: {MISSING_THRESHOLD*100}%\")\n","    qa_report.append(f\"Gap absolute threshold: {GAP_THRESHOLD} s\")\n","    qa_report.append(f\"Gap relative threshold: 10× expected interval\")\n","    qa_report.append(f\"Gap ratio threshold: {GAP_RATIO_THRESHOLD*100}%\")\n","\n","    avg_miss = safe_float(df_sessions['missing_rate'].mean())\n","    max_miss = safe_float(df_sessions['missing_rate'].max())\n","    med_miss = safe_float(df_sessions['missing_rate'].median())\n","\n","    qa_report.append(f\"Overall average missing rate: {avg_miss*100:.2f}%\")\n","    qa_report.append(f\"Max missing rate: {max_miss*100:.2f}%\")\n","    qa_report.append(f\"Median missing rate: {med_miss*100:.2f}%\")\n","\n","    if 'gap_ratio' in df_sessions.columns:\n","        avg_gap = safe_float(df_sessions['gap_ratio'].mean())\n","        max_gap_ratio = safe_float(df_sessions['gap_ratio'].max())\n","        qa_report.append(f\"Average gap ratio: {avg_gap*100:.2f}%\")\n","        qa_report.append(f\"Max gap ratio: {max_gap_ratio*100:.2f}%\")\n","\n","    if 'max_gap_seconds' in df_sessions.columns:\n","        max_single_gap = safe_float(df_sessions['max_gap_seconds'].max())\n","        qa_report.append(f\"Max single gap: {max_single_gap:.2f} s\")\n","\n","    keep_count = df_sessions['keep'].sum()\n","    total_count = len(df_sessions)\n","    pass_rate = keep_count / total_count * 100 if total_count > 0 else 0\n","    qa_report.append(f\"\")\n","    qa_report.append(f\"Sessions passing QC: {keep_count}/{total_count} ({pass_rate:.1f}%)\")\n","\n","if (raw_dir / \"EMPTY_SEGMENTS_TODO.csv\").exists():\n","    qa_report.append(\"\")\n","    qa_report.append(\"[Note] Empty/abnormal segments found; see: EMPTY_SEGMENTS_TODO.csv (exclude during later sliding-window segmentation)\")\n","\n","qa_report.append(\"\")\n","\n","# Per-subject details\n","qa_report.append(\"[6. Subject-level details]\")\n","qa_report.append(\"-\"*70)\n","if not subject_agg.empty:\n","    for _, subj in subject_agg.iterrows():\n","        qa_report.append(f\"Subject {subj['subject_id']}:\")\n","        qa_report.append(f\"  # sessions: {subj['num_sessions']}\")\n","        qa_report.append(f\"  Total duration: {subj['total_duration_sec']/60:.1f} minutes\")\n","        qa_report.append(f\"  Total samples: {subj['total_samples']:,}\")\n","        qa_report.append(f\"  Missing rate: {subj['overall_missing_rate']*100:.2f}%\")\n","        qa_report.append(f\"  Placements: {subj['placements']}\")\n","        qa_report.append(\"\")\n","\n","qa_report.append(\"=\"*70)\n","qa_report.append(\"End of report\")\n","qa_report.append(\"=\"*70)\n","\n","# Save QA report\n","qa_report_file = raw_dir / \"QA_REPORT.txt\"\n","with open(qa_report_file, \"w\", encoding=\"utf-8\") as f:\n","    f.write(\"\\n\".join(qa_report))\n","\n","print(f\"✓ Saved quality report: {qa_report_file}\")\n","\n","# Also print to console\n","print(\"\\n\" + \"\\n\".join(qa_report))\n","\n","# ========== 8. Generate summary JSON ==========\n","summary = {\n","    \"generated_at_utc\": datetime.now(timezone.utc).isoformat(),\n","    \"num_subjects\": int(len(subject_agg)) if not subject_agg.empty else 0,\n","    \"num_sessions_total\": len(df_sessions) if not df_sessions.empty else 0,\n","    \"num_sessions_keep\": int(df_sessions['keep'].sum()) if not df_sessions.empty else 0,\n","    \"total_duration_hours\": safe_float(subject_agg['total_duration_sec'].sum() / 3600) if not subject_agg.empty else 0.0,\n","    \"missing_threshold\": MISSING_THRESHOLD,\n","    \"gap_threshold_sec\": GAP_THRESHOLD,\n","    \"gap_ratio_threshold\": GAP_RATIO_THRESHOLD,\n","    \"avg_missing_rate\": safe_float(df_sessions['missing_rate'].mean()) if not df_sessions.empty else 0.0,\n","    \"avg_gap_ratio\": safe_float(df_sessions['gap_ratio'].mean()) if not df_sessions.empty and 'gap_ratio' in df_sessions.columns else 0.0,\n","    \"max_single_gap_seconds\": safe_float(df_sessions['max_gap_seconds'].max()) if not df_sessions.empty and 'max_gap_seconds' in df_sessions.columns else 0.0,\n","    \"num_activities\": int(len(activity_total)) if not df_activities.empty else 0,\n","    \"placements\": sorted(df_sessions[df_sessions['keep']]['placement'].unique().tolist()) if not df_sessions.empty and df_sessions['keep'].any() else [],\n","}\n","\n","summary_file = raw_dir / \"qa_summary.json\"\n","with open(summary_file, \"w\") as f:\n","    json.dump(summary, f, indent=2)\n","\n","print(f\"\\n✓ Saved summary: {summary_file}\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 3 complete - Metadata & Quality Audit (top-conf/journal grade)\")\n","print(\"=\"*60)\n","print(f\"Output files:\")\n","if meta_subjects_file:\n","    print(f\"  - {meta_subjects_file}\")\n","if meta_sessions_file:\n","    print(f\"  - {meta_sessions_file}\")\n","if keep_sessions_file:\n","    print(f\"  - {keep_sessions_file}\")\n","print(f\"  - {qa_report_file}\")\n","print(f\"  - {summary_file}\")\n","if (raw_dir / \"EMPTY_SEGMENTS_TODO.csv\").exists():\n","    print(f\"  - {raw_dir / 'EMPTY_SEGMENTS_TODO.csv'} (file-level empty-window list)\")\n","if (raw_dir / \"QA_ISSUES.log\").exists():\n","    print(f\"  - {raw_dir / 'QA_ISSUES.log'} (quality issue details)\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ca2YKaE2_Fap","executionInfo":{"status":"ok","timestamp":1763470348836,"user_tz":0,"elapsed":127386,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"489f46b4-62ca-411f-c3ff-c15b7d8d996a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 3: Metadata & Quality Audit\n","============================================================\n","Loading file index: data/lara/mbientlab/raw/file_index.parquet\n","\n","============================================================\n","1. Parse sensor data metadata\n","============================================================\n","Sensor files: 193\n","Label files: 193\n","Extracting time spans and gap statistics (chunked)...\n","Extracted time spans for 193 sessions\n","\n","============================================================\n","2. Parse labels & activity statistics\n","============================================================\n","Parsed 193 sessions\n","\n","Checking orphan sessions...\n","\n","============================================================\n","3. Merge session metadata\n","============================================================\n","Merged time span info\n","\n","============================================================\n","4. Data quality checks & empty-window cleanup\n","============================================================\n","✓ QC result: keep 193 sessions, reject 0 sessions\n","✓ Saved: data/lara/mbientlab/raw/qa_keep_sessions.csv\n","\n","Generating file-level empty-window list...\n","\n","============================================================\n","5. Generate subject-level metadata\n","============================================================\n","✓ Saved: data/lara/mbientlab/raw/meta_subjects.csv\n","  Number of subjects: 8\n","\n","============================================================\n","6. Generate session-level metadata\n","============================================================\n","✓ Saved: data/lara/mbientlab/raw/meta_sessions.csv\n","  Number of sessions: 193\n","\n","============================================================\n","7. Generate quality audit report\n","============================================================\n","✓ Saved quality report: data/lara/mbientlab/raw/QA_REPORT.txt\n","\n","======================================================================\n","LARa MbientLab IMU Dataset - Quality Audit Report\n","======================================================================\n","Generated at: 2025-11-18T12:52:29.550348+00:00\n","Data path: data/lara/mbientlab/raw\n","\n","[1. Dataset overview]\n","----------------------------------------------------------------------\n","Number of subjects: 8\n","Total sessions: 193\n","Total duration: 6.18 hours\n","Total samples: 2,224,452\n","\n","[2. Sampling rate statistics]\n","----------------------------------------------------------------------\n","\n","[3. Sensor placement coverage]\n","----------------------------------------------------------------------\n","  rwrist         :  96 sessions ( 49.7%)\n","  chest          :  85 sessions ( 44.0%)\n","  lwrist         :  12 sessions (  6.2%)\n","\n","[4. Activity distribution]\n","----------------------------------------------------------------------\n","Number of activity classes: 8\n","Total samples: 2,224,452\n","\n","Per-activity share:\n","  4                             : 1,198,354 (53.87%)\n","  0                             :  231,084 (10.39%)\n","  2                             :  197,087 ( 8.86%)\n","  1                             :  182,687 ( 8.21%)\n","  3                             :  150,685 ( 6.77%)\n","  5                             :  112,818 ( 5.07%)\n","  7                             :  107,298 ( 4.82%)\n","  6                             :   44,439 ( 2.00%)\n","\n","[5. Data quality assessment]\n","----------------------------------------------------------------------\n","Missing-rate threshold: 5.0%\n","Gap absolute threshold: 2.0 s\n","Gap relative threshold: 10× expected interval\n","Gap ratio threshold: 5.0%\n","Overall average missing rate: 0.00%\n","Max missing rate: 0.00%\n","Median missing rate: 0.00%\n","Average gap ratio: 0.00%\n","Max gap ratio: 0.00%\n","Max single gap: 0.00 s\n","\n","Sessions passing QC: 193/193 (100.0%)\n","\n","[6. Subject-level details]\n","----------------------------------------------------------------------\n","Subject S07:\n","  # sessions: 29\n","  Total duration: 57.1 minutes\n","  Total samples: 342,376\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","Subject S08:\n","  # sessions: 24\n","  Total duration: 47.4 minutes\n","  Total samples: 284,329\n","  Missing rate: 0.00%\n","  Placements: chest,rwrist\n","\n","Subject S09:\n","  # sessions: 29\n","  Total duration: 54.5 minutes\n","  Total samples: 326,680\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","Subject S10:\n","  # sessions: 23\n","  Total duration: 43.2 minutes\n","  Total samples: 259,019\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","Subject S11:\n","  # sessions: 14\n","  Total duration: 27.7 minutes\n","  Total samples: 165,980\n","  Missing rate: 0.00%\n","  Placements: lwrist,rwrist\n","\n","Subject S12:\n","  # sessions: 17\n","  Total duration: 30.5 minutes\n","  Total samples: 183,024\n","  Missing rate: 0.00%\n","  Placements: chest,rwrist\n","\n","Subject S13:\n","  # sessions: 29\n","  Total duration: 56.7 minutes\n","  Total samples: 340,084\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","Subject S14:\n","  # sessions: 28\n","  Total duration: 53.8 minutes\n","  Total samples: 322,960\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","======================================================================\n","End of report\n","======================================================================\n","\n","✓ Saved summary: data/lara/mbientlab/raw/qa_summary.json\n","\n","============================================================\n","Step 3 complete - Metadata & Quality Audit (top-conf/journal grade)\n","============================================================\n","Output files:\n","  - data/lara/mbientlab/raw/meta_subjects.csv\n","  - data/lara/mbientlab/raw/meta_sessions.csv\n","  - data/lara/mbientlab/raw/qa_keep_sessions.csv\n","  - data/lara/mbientlab/raw/QA_REPORT.txt\n","  - data/lara/mbientlab/raw/qa_summary.json\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 4: Channel & Placement Strategy Selection (top-conf/journal grade)\n","Select placement, raw channels, derived channels; generate config file\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import re\n","\n","print(\"=\"*60)\n","print(\"Step 4: Channel & Placement Strategy Selection\")\n","print(\"=\"*60)\n","\n","# ========== Placement → Prefix allowlist (eradicate cross-placement leakage) ==========\n","PREFIX_ALLOWLIST = {\n","    \"rwrist\": [\"RA_\"],\n","    \"lwrist\": [\"LA_\"],\n","    \"chest\":  [\"N_\"],\n","    # Extensible: \"rleg\": [\"RL_\"], \"lleg\": [\"LL_\"]\n","}\n","\n","REQ_SUFFIX = {\n","    \"ax\": \"AccelerometerX\", \"ay\": \"AccelerometerY\", \"az\": \"AccelerometerZ\",\n","    \"gx\": \"GyroscopeX\",     \"gy\": \"GyroscopeY\",     \"gz\": \"GyroscopeZ\",\n","}\n","\n","# Coverage threshold: required column presence ratio across files (1.0=100%, 0.95=95%)\n","MIN_COVERAGE = 1.0\n","\n","# Load metadata\n","raw_dir = Path(\"data/lara/mbientlab/raw\")\n","configs_dir = Path(\"configs\")\n","configs_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Load subject metadata\n","meta_subjects = pd.read_csv(raw_dir / \"meta_subjects.csv\")\n","print(f\"\\nLoaded subject metadata: {len(meta_subjects)} subjects\")\n","\n","# Load file index\n","index_file = raw_dir / \"file_index.parquet\"\n","if not index_file.exists():\n","    index_file = raw_dir / \"file_index.csv\"\n","file_index = pd.read_parquet(index_file) if index_file.suffix == '.parquet' else pd.read_csv(index_file)\n","\n","# Keep only sensor files (more robust: filter by sensor_type and filename)\n","if 'sensor_type' in file_index.columns:\n","    sensor_files = file_index[\n","        (file_index['sensor_type'].isin(['acc+gyro', 'acc', 'gyro'])) &\n","        ~file_index['filename'].str.contains('label', case=False, na=False)\n","    ].copy()\n","else:\n","    sensor_files = file_index[\n","        ~file_index['filename'].str.contains('label', case=False, na=False)\n","    ].copy()\n","\n","print(f\"Number of sensor files: {len(sensor_files)}\")\n","\n","# ========== 1. Analyze placement coverage ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Analyze placement coverage\")\n","print(\"=\"*60)\n","\n","# Count data volume per placement\n","placement_stats = sensor_files.groupby('placement').agg({\n","    'subject_id': 'nunique',\n","    'session_id': 'nunique',\n","    'file_size_bytes': 'sum',\n","    'num_rows': 'sum',\n","}).reset_index()\n","placement_stats.columns = ['placement', 'num_subjects', 'num_sessions', 'total_bytes', 'total_samples']\n","placement_stats = placement_stats.sort_values('total_samples', ascending=False)\n","\n","print(\"\\nPlacement statistics (sorted by sample count):\")\n","print(placement_stats.to_string(index=False))\n","\n","# Fix selection to right wrist (this round)\n","selected_placement = \"rwrist\"\n","print(f\"\\nFixed placement for this round: {selected_placement}\")\n","\n","# Check whether placement exists\n","if selected_placement not in placement_stats['placement'].values:\n","    raise ValueError(f\"Specified placement '{selected_placement}' does not exist in the data\")\n","\n","# Check which subjects have that placement\n","subjects_with_selected = sensor_files[sensor_files['placement'] == selected_placement]['subject_id'].unique()\n","print(f\"Subjects with {selected_placement} data: {len(subjects_with_selected)}/{len(meta_subjects)}\")\n","\n","# ========== 2. Allowlist validation & channel check ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Allowlist validation & channel check\")\n","print(\"=\"*60)\n","\n","# Read only from files of selected placement\n","placement_files = sensor_files[sensor_files['placement'] == selected_placement]\n","print(f\"Number of files for selected placement '{selected_placement}': {len(placement_files)}\")\n","\n","# Get allowlist prefixes\n","allowed_prefixes = PREFIX_ALLOWLIST.get(selected_placement, [])\n","assert allowed_prefixes, f\"Prefix allowlist for '{selected_placement}' not configured; please add it in PREFIX_ALLOWLIST\"\n","print(f\"\\nUsing placement→prefix allowlist: {selected_placement} → {allowed_prefixes}\")\n","\n","# Robust header-reading function\n","def read_cols(fp):\n","    \"\"\"Read column names (with fallback)\"\"\"\n","    try:\n","        return pd.read_csv(fp, nrows=5, sep=None, engine='python').columns.tolist()\n","    except Exception:\n","        return pd.read_csv(fp, nrows=5, sep=\",\").columns.tolist()\n","\n","# Read headers of all files\n","print(f\"\\nRead headers of all {len(placement_files)} files to check consistency...\")\n","all_columns_by_file = []\n","\n","for _, row in placement_files.iterrows():\n","    fp = raw_dir / row['standardized_path']\n","    cols = read_cols(fp)\n","    data_cols = [c for c in cols if not re.search(r'(time|timestamp|epoch|index|id|class|label)', c, re.I)]\n","    all_columns_by_file.append(data_cols)\n","\n","# Assert all files were read successfully\n","assert len(all_columns_by_file) == len(placement_files), \\\n","    f\"{len(placement_files)-len(all_columns_by_file)} '{selected_placement}' files failed header reading; fix or exclude these files first\"\n","\n","print(f\"✓ Successfully read {len(all_columns_by_file)} files\")\n","\n","# Show columns of the first file as a reference\n","if all_columns_by_file:\n","    print(f\"\\nData columns of the first file:\")\n","    for col in all_columns_by_file[0]:\n","        print(f\"  {col}\")\n","\n","# ========== 3. Build strict channel mapping (allowlist + consistency assertions) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Build strict channel mapping (allowlist + consistency assertions)\")\n","print(\"=\"*60)\n","\n","def extract_prefix(col):\n","    \"\"\"Extract column prefix\"\"\"\n","    m = re.match(r'^([A-Z]{1,}_)', col)\n","    return m.group(1) if m else None\n","\n","def build_mapping_from_allowlist(allowed_prefixes, all_cols_by_file, min_coverage=1.0):\n","    \"\"\"Compose column names from allowlist × suffix and check coverage\"\"\"\n","    mapping = {}\n","    missing_files = {}\n","\n","    for std, suf in REQ_SUFFIX.items():\n","        chosen = None\n","        for pfx in allowed_prefixes:\n","            cand = f\"{pfx}{suf}\"\n","            # Count in how many files this column exists\n","            present_files = [i for i, cols in enumerate(all_cols_by_file) if cand in cols]\n","            coverage = len(present_files) / len(all_cols_by_file)\n","\n","            if coverage >= min_coverage:\n","                chosen = cand\n","                if coverage < 1.0:\n","                    # Record indices of files missing this column (for later inspection)\n","                    missing_idx = [i for i in range(len(all_cols_by_file)) if i not in present_files]\n","                    missing_files[std] = missing_idx\n","                break\n","\n","        if not chosen:\n","            raise RuntimeError(\n","                f\"[Consistency assertion failed] {std}: Under prefixes {allowed_prefixes}, no '{suf}' meets {min_coverage*100:.0f}% coverage. \"\n","                f\"Check raw column names or change placement/prefix allowlist.\"\n","            )\n","\n","        mapping[std] = chosen\n","\n","    # Prefix consistency check: all mapped columns must come from allowlist\n","    used_prefixes = {extract_prefix(v) for v in mapping.values()}\n","    if not used_prefixes.issubset(set(allowed_prefixes)):\n","        raise RuntimeError(\n","            f\"[Consistency assertion failed] Final mapping prefixes {used_prefixes} are not all within allowlist {allowed_prefixes}\"\n","        )\n","\n","    return mapping, used_prefixes, missing_files\n","\n","# Build mapping\n","final_mapping, used_prefixes, missing_files = build_mapping_from_allowlist(\n","    allowed_prefixes, all_columns_by_file, MIN_COVERAGE\n",")\n","\n","print(\"\\nFinal channel mapping (standard_name <- original_column):\")\n","for std, orig in sorted(final_mapping.items()):\n","    print(f\"  {std} <- {orig}\")\n","\n","# Explicit hard assertions\n","assert len(used_prefixes) == 1, f\"A single prefix should be used; got {used_prefixes}\"\n","assert list(used_prefixes)[0] in set(PREFIX_ALLOWLIST[selected_placement]), \\\n","    f\"Source prefix {used_prefixes} not in allowlist {PREFIX_ALLOWLIST[selected_placement]} for {selected_placement}\"\n","\n","print(f\"\\n✓ Consistency assertions passed:\")\n","print(f\"  - Using a single prefix: {sorted(used_prefixes)}\")\n","print(f\"  - Prefix is in the allowlist: {PREFIX_ALLOWLIST[selected_placement]}\")\n","print(f\"  - Number of files checked: {len(all_columns_by_file)}\")\n","print(f\"  - Coverage requirement: {MIN_COVERAGE*100:.0f}%\")\n","\n","# If there are missing, print warnings\n","if missing_files:\n","    print(f\"\\n⚠️  The following channels are missing in some files (coverage threshold set to {MIN_COVERAGE*100:.0f}%):\")\n","    for std, idx_list in missing_files.items():\n","        print(f\"  {std}: missing in {len(idx_list)} files\")\n","\n","# ========== 4. Generate channel & placement config ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Generate channel & placement config\")\n","print(\"=\"*60)\n","\n","# Config content\n","config = {\n","    'dataset': 'LARa_MbientLab_IMU',\n","    'strategy': 'single_placement_baseline',\n","\n","    # Placement configuration\n","    'placements': {\n","        'selected': [selected_placement],\n","        'available': placement_stats['placement'].tolist(),\n","        'rationale': f'Fixed selection {selected_placement}, covering {len(subjects_with_selected)} subjects',\n","    },\n","\n","    # Raw channel configuration\n","    'channels': {\n","        'raw': ['ax', 'ay', 'az', 'gx', 'gy', 'gz'],\n","        'mapping': final_mapping,\n","        'prefix_allowlist': PREFIX_ALLOWLIST,\n","        'source_prefix': sorted(used_prefixes)[0],\n","        'min_coverage': MIN_COVERAGE,\n","        'description': {\n","            'ax': 'Accelerometer X axis (m/s² or g)',\n","            'ay': 'Accelerometer Y axis (m/s² or g)',\n","            'az': 'Accelerometer Z axis (m/s² or g)',\n","            'gx': 'Gyroscope X axis (rad/s or deg/s)',\n","            'gy': 'Gyroscope Y axis (rad/s or deg/s)',\n","            'gz': 'Gyroscope Z axis (rad/s or deg/s)',\n","        }\n","    },\n","\n","    # Derived channel configuration\n","    'derived_channels': {\n","        'acc_mag': {\n","            'formula': 'sqrt(ax^2 + ay^2 + az^2)',\n","            'description': 'Accelerometer vector magnitude',\n","        },\n","        'gyr_mag': {\n","            'formula': 'sqrt(gx^2 + gy^2 + gz^2)',\n","            'description': 'Gyroscope vector magnitude',\n","        }\n","    },\n","\n","    # Final channel order\n","    'final_channels': ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag'],\n","\n","    # Multi-placement fusion (reserved; currently disabled)\n","    'multi_placement_fusion': {\n","        'enabled': False,\n","        'strategy': None,\n","        'warning': 'If enabling multi-placement fusion, you must select the fusion strategy independently within each training fold to avoid cross-fold leakage',\n","    },\n","\n","    # Rigor notes\n","    'notes': [\n","        'Single-placement baseline: avoid cross-placement information leakage',\n","        'Channel mapping uses \"placement→prefix allowlist + consistency assertions\"; no cross-prefix voting',\n","        f'Consistency checked over all {len(all_columns_by_file)} {selected_placement} files',\n","        f'Coverage requirement: {MIN_COVERAGE*100:.0f}% (tunable tolerance)',\n","        'Derived channels are computed at feature-extraction stage to preserve raw data integrity',\n","        'Any multi-placement fusion must be chosen & validated within each LOSO fold',\n","    ]\n","}\n","\n","# Save config\n","config_file = configs_dir / \"channels.yaml\"\n","with open(config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","print(f\"✓ Saved config: {config_file}\")\n","\n","# ========== 5. Validate config (random multi-file sampling) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Validate config\")\n","print(\"=\"*60)\n","\n","# Verify coverage of selected placement across all sensor files\n","files_with_placement = sensor_files[sensor_files['placement'] == selected_placement]\n","\n","print(f\"\\nValidate placement '{selected_placement}':\")\n","print(f\"  Files: {len(files_with_placement)}\")\n","print(f\"  Subjects: {files_with_placement['subject_id'].nunique()}\")\n","print(f\"  Sessions: {files_with_placement['session_id'].nunique()}\")\n","\n","# Validate channel mapping: randomly sample multiple files\n","verify_sample_size = min(5, len(files_with_placement))\n","verify_df = files_with_placement.sample(n=verify_sample_size, random_state=0)\n","\n","print(f\"\\nValidate channel mapping (random sample of {verify_sample_size} files):\")\n","for idx, sample_file in verify_df.iterrows():\n","    sample_path = raw_dir / sample_file['standardized_path']\n","    try:\n","        df_verify = pd.read_csv(sample_path, nrows=100, sep=None, engine='python')\n","\n","        print(f\"\\nFile: {sample_file['filename']}\")\n","        all_found = True\n","        for std_name in ['ax', 'ay', 'az', 'gx', 'gy', 'gz']:\n","            if std_name in final_mapping:\n","                orig_name = final_mapping[std_name]\n","                if orig_name in df_verify.columns:\n","                    sample_val = df_verify[orig_name].iloc[0]\n","                    print(f\"  ✓ {std_name} <- {orig_name} (sample value: {sample_val:.4f})\")\n","                else:\n","                    print(f\"  ✗ {std_name} <- {orig_name} (column not found)\")\n","                    all_found = False\n","            else:\n","                print(f\"  ✗ {std_name} (not mapped)\")\n","                all_found = False\n","\n","        if not all_found:\n","            print(f\"  ⚠️  This file failed validation\")\n","\n","    except Exception as e:\n","        print(f\"\\nFile: {sample_file['filename']}\")\n","        print(f\"  ✗ Error during validation: {e}\")\n","\n","# Compute derived-channel examples on the first successfully validated file\n","for idx, sample_file in verify_df.iterrows():\n","    sample_path = raw_dir / sample_file['standardized_path']\n","    try:\n","        df_verify = pd.read_csv(sample_path, nrows=100, sep=None, engine='python')\n","        if all(final_mapping[ch] in df_verify.columns for ch in ['ax', 'ay', 'az', 'gx', 'gy', 'gz']):\n","            acc_mag = np.sqrt(\n","                df_verify[final_mapping['ax']].values**2 +\n","                df_verify[final_mapping['ay']].values**2 +\n","                df_verify[final_mapping['az']].values**2\n","            )\n","            gyr_mag = np.sqrt(\n","                df_verify[final_mapping['gx']].values**2 +\n","                df_verify[final_mapping['gy']].values**2 +\n","                df_verify[final_mapping['gz']].values**2\n","            )\n","\n","            print(f\"\\nDerived-channel example values (file: {sample_file['filename']}):\")\n","            print(f\"  acc_mag: min={acc_mag.min():.4f}, max={acc_mag.max():.4f}, mean={acc_mag.mean():.4f}\")\n","            print(f\"  gyr_mag: min={gyr_mag.min():.4f}, max={gyr_mag.max():.4f}, mean={gyr_mag.mean():.4f}\")\n","            break\n","    except:\n","        continue\n","\n","# ========== 6. Fuse check (reload config for verification) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Fuse check (reload config for verification)\")\n","print(\"=\"*60)\n","\n","with open(config_file, \"r\", encoding=\"utf-8\") as f:\n","    cfg = yaml.safe_load(f)\n","\n","# Extract prefixes of all mapped columns\n","srcs = list(cfg[\"channels\"][\"mapping\"].values())\n","pfxs = {re.match(r'^([A-Za-z]+_)', s).group(1) for s in srcs if re.match(r'^([A-Za-z]+_)', s)}\n","\n","# Assertion: all channels use the same prefix\n","assert len(pfxs) == 1, f\"ax..gz not using a single prefix: {pfxs}\"\n","\n","# Assertion: prefix in allowlist\n","sel = cfg[\"placements\"][\"selected\"][0]\n","allow = set(cfg[\"channels\"][\"prefix_allowlist\"][sel])\n","assert list(pfxs)[0] in allow, f\"Prefix {pfxs} not in {sel} allowlist {allow}\"\n","\n","print(f\"✓ Config fuse check passed:\")\n","print(f\"  - Reloaded config: {config_file}\")\n","print(f\"  - All channels use a single prefix: {pfxs}\")\n","print(f\"  - Prefix is in {sel} allowlist: {allow}\")\n","\n","# ========== 7. Summary ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 4 complete - Channels & Placement Strategy\")\n","print(\"=\"*60)\n","print(f\"\\nConfig summary:\")\n","print(f\"  Strategy: single-placement baseline\")\n","print(f\"  Fixed placement: {config['placements']['selected']}\")\n","print(f\"  Raw channels: {config['channels']['raw']}\")\n","print(f\"  Derived channels: {list(config['derived_channels'].keys())}\")\n","print(f\"  Final number of channels: {len(config['final_channels'])}\")\n","print(f\"  Prefix used: {sorted(used_prefixes)}\")\n","print(f\"  Coverage requirement: {MIN_COVERAGE*100:.0f}%\")\n","print(f\"\\nConfig file: {config_file}\")\n","print(f\"\\nRigor guarantees:\")\n","print(f\"  1. ✓ Use placement→prefix allowlist (hard-coded)\")\n","print(f\"  2. ✓ Consistency assertions across all files ({len(all_columns_by_file)} files)\")\n","print(f\"  3. ✓ No cross-prefix voting; avoid mis-selection\")\n","print(f\"  4. ✓ Error out if column names don't match allowlist\")\n","print(f\"  5. ✓ Explicit assertions: single prefix + within allowlist\")\n","print(f\"  6. ✓ Abort if header reading fails\")\n","print(f\"  7. ✓ Randomly sample {verify_sample_size} files to validate mapping\")\n","print(f\"  8. ✓ Fuse check: reload config and verify prefix\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YPvkQ4Hp_HOa","executionInfo":{"status":"ok","timestamp":1763470349095,"user_tz":0,"elapsed":255,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"e7cd3573-c189-4146-f0e8-e780d2576db7"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 4: Channel & Placement Strategy Selection\n","============================================================\n","\n","Loaded subject metadata: 8 subjects\n","Number of sensor files: 193\n","\n","============================================================\n","1. Analyze placement coverage\n","============================================================\n","\n","Placement statistics (sorted by sample count):\n","placement  num_subjects  num_sessions  total_bytes  total_samples\n","   rwrist             8            14    685467785        1120045\n","    chest             7            14    595662725         972496\n","   lwrist             6             2     80626579         131911\n","\n","Fixed placement for this round: rwrist\n","Subjects with rwrist data: 8/8\n","\n","============================================================\n","2. Allowlist validation & channel check\n","============================================================\n","Number of files for selected placement 'rwrist': 96\n","\n","Using placement→prefix allowlist: rwrist → ['RA_']\n","\n","Read headers of all 96 files to check consistency...\n","✓ Successfully read 96 files\n","\n","Data columns of the first file:\n","  LA_AccelerometerX\n","  LA_AccelerometerY\n","  LA_AccelerometerZ\n","  LA_GyroscopeX\n","  LA_GyroscopeY\n","  LA_GyroscopeZ\n","  LL_AccelerometerX\n","  LL_AccelerometerY\n","  LL_AccelerometerZ\n","  LL_GyroscopeX\n","  LL_GyroscopeY\n","  LL_GyroscopeZ\n","  N_AccelerometerX\n","  N_AccelerometerY\n","  N_AccelerometerZ\n","  N_GyroscopeX\n","  N_GyroscopeY\n","  N_GyroscopeZ\n","  RA_AccelerometerX\n","  RA_AccelerometerY\n","  RA_AccelerometerZ\n","  RA_GyroscopeX\n","  RA_GyroscopeY\n","  RA_GyroscopeZ\n","  RL_AccelerometerX\n","  RL_AccelerometerY\n","  RL_AccelerometerZ\n","  RL_GyroscopeX\n","  RL_GyroscopeY\n","  RL_GyroscopeZ\n","\n","============================================================\n","3. Build strict channel mapping (allowlist + consistency assertions)\n","============================================================\n","\n","Final channel mapping (standard_name <- original_column):\n","  ax <- RA_AccelerometerX\n","  ay <- RA_AccelerometerY\n","  az <- RA_AccelerometerZ\n","  gx <- RA_GyroscopeX\n","  gy <- RA_GyroscopeY\n","  gz <- RA_GyroscopeZ\n","\n","✓ Consistency assertions passed:\n","  - Using a single prefix: ['RA_']\n","  - Prefix is in the allowlist: ['RA_']\n","  - Number of files checked: 96\n","  - Coverage requirement: 100%\n","\n","============================================================\n","4. Generate channel & placement config\n","============================================================\n","✓ Saved config: configs/channels.yaml\n","\n","============================================================\n","5. Validate config\n","============================================================\n","\n","Validate placement 'rwrist':\n","  Files: 96\n","  Subjects: 8\n","  Sessions: 14\n","\n","Validate channel mapping (random sample of 5 files):\n","\n","File: l02_s09_r05.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.3215)\n","  ✓ ay <- RA_AccelerometerY (sample value: -0.9254)\n","  ✓ az <- RA_AccelerometerZ (sample value: 0.2765)\n","  ✓ gx <- RA_GyroscopeX (sample value: -1.1439)\n","  ✓ gy <- RA_GyroscopeY (sample value: -1.3566)\n","  ✓ gz <- RA_GyroscopeZ (sample value: -0.7625)\n","\n","File: l02_s14_r03.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.2813)\n","  ✓ ay <- RA_AccelerometerY (sample value: -0.9499)\n","  ✓ az <- RA_AccelerometerZ (sample value: 0.1393)\n","  ✓ gx <- RA_GyroscopeX (sample value: 1.2430)\n","  ✓ gy <- RA_GyroscopeY (sample value: 12.4323)\n","  ✓ gz <- RA_GyroscopeZ (sample value: 4.8893)\n","\n","File: l02_s07_r06.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.1674)\n","  ✓ ay <- RA_AccelerometerY (sample value: -0.9645)\n","  ✓ az <- RA_AccelerometerZ (sample value: -0.0895)\n","  ✓ gx <- RA_GyroscopeX (sample value: -1.0865)\n","  ✓ gy <- RA_GyroscopeY (sample value: 9.2328)\n","  ✓ gz <- RA_GyroscopeZ (sample value: 55.7498)\n","\n","File: l02_s11_r06.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.5316)\n","  ✓ ay <- RA_AccelerometerY (sample value: -0.8636)\n","  ✓ az <- RA_AccelerometerZ (sample value: 0.1911)\n","  ✓ gx <- RA_GyroscopeX (sample value: 8.0598)\n","  ✓ gy <- RA_GyroscopeY (sample value: 32.2562)\n","  ✓ gz <- RA_GyroscopeZ (sample value: -1.3813)\n","\n","File: l02_s12_r15.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.9424)\n","  ✓ ay <- RA_AccelerometerY (sample value: -1.0324)\n","  ✓ az <- RA_AccelerometerZ (sample value: 0.9479)\n","  ✓ gx <- RA_GyroscopeX (sample value: 59.7400)\n","  ✓ gy <- RA_GyroscopeY (sample value: 145.9488)\n","  ✓ gz <- RA_GyroscopeZ (sample value: -34.0791)\n","\n","Derived-channel example values (file: l02_s09_r05.csv):\n","  acc_mag: min=0.9377, max=1.0473, mean=1.0126\n","  gyr_mag: min=0.4164, max=9.8511, mean=3.7085\n","\n","============================================================\n","6. Fuse check (reload config for verification)\n","============================================================\n","✓ Config fuse check passed:\n","  - Reloaded config: configs/channels.yaml\n","  - All channels use a single prefix: {'RA_'}\n","  - Prefix is in rwrist allowlist: {'RA_'}\n","\n","============================================================\n","Step 4 complete - Channels & Placement Strategy\n","============================================================\n","\n","Config summary:\n","  Strategy: single-placement baseline\n","  Fixed placement: ['rwrist']\n","  Raw channels: ['ax', 'ay', 'az', 'gx', 'gy', 'gz']\n","  Derived channels: ['acc_mag', 'gyr_mag']\n","  Final number of channels: 8\n","  Prefix used: ['RA_']\n","  Coverage requirement: 100%\n","\n","Config file: configs/channels.yaml\n","\n","Rigor guarantees:\n","  1. ✓ Use placement→prefix allowlist (hard-coded)\n","  2. ✓ Consistency assertions across all files (96 files)\n","  3. ✓ No cross-prefix voting; avoid mis-selection\n","  4. ✓ Error out if column names don't match allowlist\n","  5. ✓ Explicit assertions: single prefix + within allowlist\n","  6. ✓ Abort if header reading fails\n","  7. ✓ Randomly sample 5 files to validate mapping\n","  8. ✓ Fuse check: reload config and verify prefix\n","============================================================\n"]}]},{"cell_type":"code","source":["import os\n","\n","\"\"\"\n","Step 5: Timeline Unification & Resampling (top-conf/journal grade - flawless)\n","Unify to 50 Hz; linear interpolation/forward-fill; align start/end\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import re\n","import json\n","\n","# ========== Config ==========\n","TARGET_FREQ_HZ = 50.0           # Target sampling rate\n","MAX_INTERP_GAP_MS = 20.0        # Maximum interpolation gap (milliseconds)\n","MAX_INTERP_RATIO = 0.15         # Gap coverage threshold 15% (constant; applied globally)\n","\n","print(\"=\"*60)\n","print(\"Step 5: Timeline Unification & Resampling\")\n","print(\"=\"*60)\n","\n","# Load config and metadata\n","raw_dir = Path(\"data/lara/mbientlab/raw\")\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","proc_dir.mkdir(parents=True, exist_ok=True)\n","\n","configs_dir = Path(\"configs\")\n","with open(configs_dir / \"channels.yaml\", 'r', encoding='utf-8') as f:\n","    channel_config = yaml.safe_load(f)\n","\n","selected_placement = channel_config['placements']['selected'][0]\n","channel_mapping = channel_config['channels']['mapping']\n","print(f\"\\nTarget sampling rate: {TARGET_FREQ_HZ} Hz\")\n","print(f\"Selected placement: {selected_placement}\")\n","\n","# Load QC results (all kept sessions)\n","qa_keep = pd.read_csv(raw_dir / \"qa_keep_sessions.csv\")\n","keep_sessions = qa_keep[qa_keep['keep'] == True].copy()\n","keep_sessions = keep_sessions[keep_sessions['placement'] == selected_placement].copy()\n","\n","# Global processing (no per-fold dependency)\n","print(f\"\\nGlobal resampling over all kept sessions (no per-fold markers)\")\n","print(f\"  Total sessions: {len(keep_sessions)}\")\n","\n","# Prune switch: always ON (remove sessions with excessive gaps globally)\n","APPLY_PRUNE = True\n","keep_sessions['is_train'] = False  # Kept for compatibility with stats / logs\n","\n","# Load file index\n","index_file = raw_dir / \"file_index.parquet\"\n","if not index_file.exists():\n","    index_file = raw_dir / \"file_index.csv\"\n","file_index = pd.read_parquet(index_file) if index_file.suffix == '.parquet' else pd.read_csv(index_file)\n","\n","# ========== Helper functions ==========\n","def detect_time_column(df):\n","    \"\"\"Detect time column (avoid false positive matches on 'ts' substring)\"\"\"\n","    time_cols = [c for c in df.columns\n","                 if re.search(r'(^|_)(time|timestamp|epoch|ts)($|_)', c, re.I)]\n","    return time_cols[0] if time_cols else None\n","\n","def parse_time_to_seconds(time_series):\n","    \"\"\"Convert time to seconds (correctly infer Unix timestamp units)\"\"\"\n","    numeric = pd.to_numeric(time_series, errors='coerce')\n","    if numeric.notna().sum() > len(time_series) * 0.9:\n","        vals = numeric.dropna().values\n","        max_val = np.abs(vals[:1000]).max() if len(vals) else 0\n","\n","        # Infer by 2025 Unix timestamp magnitude\n","        if max_val > 1e17:      # nanoseconds\n","            return numeric * 1e-9\n","        elif max_val > 1e14:    # microseconds\n","            return numeric * 1e-6\n","        elif max_val > 1e11:    # milliseconds\n","            return numeric * 1e-3\n","        else:                   # seconds\n","            return numeric\n","\n","    dt = pd.to_datetime(time_series, utc=True, errors='coerce')\n","    if dt.notna().sum() > len(time_series) * 0.9:\n","        epoch = pd.Timestamp(\"1970-01-01\", tz='UTC')\n","        return (dt - epoch).dt.total_seconds()\n","\n","    return None\n","\n","def resample_sensor_data(df, time_col, data_cols, target_freq_hz=50.0, max_gap_ms=20.0):\n","    \"\"\"Resample sensor data (return cleaned time for labels)\"\"\"\n","    time_sec = parse_time_to_seconds(df[time_col])\n","    if time_sec is None:\n","        raise ValueError(\"Unable to parse time column\")\n","\n","    valid_mask = time_sec.notna() & df[data_cols].notna().all(axis=1)\n","    time_clean = time_sec[valid_mask].values\n","    data_clean = df.loc[valid_mask, data_cols].values\n","\n","    if len(time_clean) < 2:\n","        return None, 0.0, 0, 0.0, 0.0, None\n","\n","    # De-duplicate + sort\n","    unique_idx = np.unique(time_clean, return_index=True)[1]\n","    time_clean = time_clean[unique_idx]\n","    data_clean = data_clean[unique_idx]\n","\n","    order = np.argsort(time_clean)\n","    time_clean = time_clean[order]\n","    data_clean = data_clean[order]\n","\n","    # Original frequency\n","    dt_orig = np.median(np.diff(time_clean))\n","    orig_freq_hz = 1.0 / dt_orig if dt_orig > 0 else 0.0\n","\n","    # Build target timeline with integer number of samples\n","    dt = 1.0 / target_freq_hz\n","    t_start = time_clean[0]\n","    t_end = time_clean[-1]\n","    n_samples = int(np.round((t_end - t_start) / dt))\n","    target_time = t_start + np.arange(n_samples + 1) * dt\n","\n","    # Linear interpolation\n","    resampled_data = np.zeros((len(target_time), len(data_cols)))\n","    for i in range(len(data_cols)):\n","        resampled_data[:, i] = np.interp(target_time, time_clean, data_clean[:, i])\n","\n","    # Large-gap detection (account for jitter)\n","    max_gap_sec = max(max_gap_ms / 1000.0, 1.25 * dt)\n","    time_diffs = np.diff(time_clean)\n","    gap_mask = time_diffs > max_gap_sec\n","\n","    is_in_gap = np.zeros(len(target_time), dtype=int)\n","    is_forced_nan = np.zeros(len(target_time), dtype=int)\n","    actual_interp_count = 0\n","    total_gap_time = 0.0\n","\n","    if gap_mask.any():\n","        for i in range(len(time_clean) - 1):\n","            if gap_mask[i]:\n","                t_gap_start = time_clean[i]\n","                t_gap_end = time_clean[i + 1]\n","                gap_duration = t_gap_end - t_gap_start\n","                total_gap_time += gap_duration\n","\n","                idxs = np.where((target_time > t_gap_start) & (target_time < t_gap_end))[0]\n","\n","                if idxs.size > 0:\n","                    is_in_gap[idxs] = 1\n","                    actual_interp_count += 1\n","\n","                    if idxs.size > 1:\n","                        forced_nan_idxs = idxs[1:]\n","                        is_forced_nan[forced_nan_idxs] = 1\n","                        resampled_data[forced_nan_idxs, :] = np.nan\n","\n","    # Gap coverage\n","    gap_points = int(is_in_gap.sum())\n","    interp_ratio = gap_points / len(target_time) if len(target_time) > 0 else 0.0\n","\n","    # Gap time fraction\n","    total_duration = t_end - t_start\n","    gap_time_fraction = total_gap_time / total_duration if total_duration > 0 else 0.0\n","\n","    resampled_df = pd.DataFrame(resampled_data, columns=data_cols)\n","    resampled_df.insert(0, 'time_sec', target_time)\n","    resampled_df['is_in_gap'] = is_in_gap\n","    resampled_df['is_forced_nan'] = is_forced_nan\n","\n","    return resampled_df, interp_ratio, gap_points, gap_time_fraction, orig_freq_hz, time_clean\n","\n","def resample_labels(df_label, df_sensor_time_clean, label_col, target_time, label_time_col=None):\n","    \"\"\"Resample labels (boundary NaN + sorting)\"\"\"\n","    if label_time_col is not None:\n","        time_sec = parse_time_to_seconds(df_label[label_time_col])\n","        if time_sec is None:\n","            raise ValueError(\"Unable to parse label time column\")\n","\n","        valid_mask = time_sec.notna() & df_label[label_col].notna()\n","        time_clean = time_sec[valid_mask].values\n","        labels_clean = df_label.loc[valid_mask, label_col].values\n","    else:\n","        # Use cleaned sensor time as reference\n","        sensor_time_original = df_sensor_time_clean\n","        if sensor_time_original is None:\n","            raise ValueError(\"Labels have no time column and no sensor time provided\")\n","\n","        min_len = min(len(df_label), len(sensor_time_original))\n","        if abs(len(df_label) - len(sensor_time_original)) > min_len * 0.01:\n","            raise ValueError(\n","                f\"Label rows ({len(df_label)}) differ too much from sensor rows ({len(sensor_time_original)})\"\n","            )\n","\n","        time_clean = sensor_time_original[:min_len]\n","        labels_clean = df_label[label_col].iloc[:min_len].values\n","\n","        valid_mask = pd.notna(labels_clean)\n","        time_clean = time_clean[valid_mask]\n","        labels_clean = labels_clean[valid_mask]\n","\n","    if len(time_clean) == 0:\n","        return np.full(len(target_time), np.nan)\n","\n","    # Explicit sorting\n","    order = np.argsort(time_clean)\n","    time_clean = time_clean[order]\n","    labels_clean = labels_clean[order]\n","\n","    idx = np.searchsorted(time_clean, target_time, side='right') - 1\n","    idx = np.clip(idx, 0, len(time_clean) - 1)\n","\n","    labels = labels_clean[idx].copy()\n","\n","    # Fix: cast integers to float to allow NaN\n","    if labels.dtype.kind in ['i', 'u']:  # integer or unsigned integer\n","        labels = labels.astype('float64')\n","\n","    # Boundary NaNs\n","    mask_before = target_time < time_clean[0]\n","    mask_after = target_time > time_clean[-1]\n","    labels[mask_before | mask_after] = np.nan\n","\n","    return labels\n","\n","# ========== 1. Process all sessions ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Resampling\")\n","print(\"=\"*60)\n","\n","resampled_records = []\n","interp_stats = []\n","issues = []\n","\n","for idx, session in keep_sessions.iterrows():\n","    subject_id = session['subject_id']\n","    session_id = session['session_id']\n","    placement = session['placement']\n","    is_train = session['is_train']\n","\n","    print(f\"\\nProcessing {subject_id}/{session_id}/{placement} {'[TRAIN]' if is_train else '[TEST]'}...\")\n","\n","    sensor_file = file_index[\n","        (file_index['subject_id'] == subject_id) &\n","        (file_index['session_id'] == session_id) &\n","        (file_index['placement'] == placement) &\n","        (~file_index['filename'].str.contains('label', case=False, na=False))\n","    ]\n","\n","    label_file = file_index[\n","        (file_index['subject_id'] == subject_id) &\n","        (file_index['session_id'] == session_id) &\n","        (file_index['placement'] == placement) &\n","        (file_index['filename'].str.contains('label', case=False, na=False))\n","    ]\n","\n","    if sensor_file.empty or label_file.empty:\n","        print(f\"  Skip: missing files\")\n","        continue\n","\n","    sensor_path = raw_dir / sensor_file.iloc[0]['standardized_path']\n","    label_path = raw_dir / label_file.iloc[0]['standardized_path']\n","\n","    try:\n","        df_sensor = pd.read_csv(sensor_path, sep=None, engine='python')\n","        time_col = detect_time_column(df_sensor)\n","        if not time_col:\n","            print(f\"  Skip: no time column\")\n","            continue\n","\n","        data_cols = [channel_mapping[std] for std in ['ax', 'ay', 'az', 'gx', 'gy', 'gz']]\n","        missing_cols = [c for c in data_cols if c not in df_sensor.columns]\n","        if missing_cols:\n","            print(f\"  Skip: missing columns {missing_cols}\")\n","            continue\n","\n","        print(f\"  Resampling sensors ({len(df_sensor)} rows)...\")\n","        result = resample_sensor_data(\n","            df_sensor, time_col, data_cols, TARGET_FREQ_HZ, MAX_INTERP_GAP_MS\n","        )\n","\n","        if result[0] is None:\n","            print(f\"  Skip: resampling failed\")\n","            continue\n","\n","        # Receive cleaned time for labels\n","        resampled_sensor, interp_ratio, gap_points, gap_time_frac, orig_freq, sensor_time_clean = result\n","\n","        valid_samples = resampled_sensor[data_cols].notna().all(axis=1).sum()\n","        nan_samples = len(resampled_sensor) - valid_samples\n","        forced_nan_points = int(resampled_sensor['is_forced_nan'].sum())\n","\n","        print(f\"  → {len(resampled_sensor)} rows, gap coverage: {interp_ratio*100:.2f}%, NaN: {nan_samples}\")\n","\n","        # Prune based on global switch\n","        if interp_ratio > MAX_INTERP_RATIO:\n","            msg = f\"Gap coverage too high ({interp_ratio*100:.1f}%)\"\n","            print(f\"  ⚠️  {msg}\")\n","            issues.append({\n","                'subject_id': subject_id,\n","                'session_id': session_id,\n","                'placement': placement,\n","                'is_train': is_train,\n","                'issue': 'high_gap_coverage',\n","                'gap_coverage': round(interp_ratio, 4),\n","            })\n","            if APPLY_PRUNE:\n","                continue\n","\n","        interp_stats.append({\n","            'subject_id': subject_id,\n","            'session_id': session_id,\n","            'placement': placement,\n","            'is_train': is_train,\n","            'original_samples': len(df_sensor),\n","            'original_freq_hz': round(orig_freq, 2),\n","            'resampled_samples': len(resampled_sensor),\n","            'valid_samples': valid_samples,\n","            'nan_samples': nan_samples,\n","            'gap_points': gap_points,\n","            'gap_coverage': round(interp_ratio, 4),\n","            'gap_time_fraction': round(gap_time_frac, 4),\n","            'forced_nan_points': forced_nan_points,\n","        })\n","\n","        df_label = pd.read_csv(label_path, sep=None, engine='python')\n","\n","        label_col = None\n","        for col_candidate in ['Class', 'class', 'label', 'Label', 'activity', 'Activity']:\n","            if col_candidate in df_label.columns:\n","                label_col = col_candidate\n","                break\n","\n","        if not label_col:\n","            for col in df_label.columns:\n","                if any(kw in col.lower() for kw in ['label', 'activity', 'class', 'action']):\n","                    label_col = col\n","                    break\n","\n","        if not label_col:\n","            print(f\"  Skip: no label column\")\n","            issues.append({\n","                'subject_id': subject_id,\n","                'session_id': session_id,\n","                'placement': placement,\n","                'is_train': is_train,\n","                'issue': 'no_label_column',\n","            })\n","            continue\n","\n","        label_time_col = detect_time_column(df_label)\n","        target_time = resampled_sensor['time_sec'].values\n","\n","        print(f\"  Resampling labels...\")\n","        try:\n","            if label_time_col:\n","                resampled_labels = resample_labels(\n","                    df_label, sensor_time_clean, label_col, target_time,\n","                    label_time_col=label_time_col\n","                )\n","            else:\n","                resampled_labels = resample_labels(\n","                    df_label, sensor_time_clean, label_col, target_time\n","                )\n","\n","            resampled_sensor['label'] = resampled_labels\n","\n","        except Exception as e:\n","            print(f\"  Skip: label resampling failed - {e}\")\n","            issues.append({\n","                'subject_id': subject_id,\n","                'session_id': session_id,\n","                'placement': placement,\n","                'is_train': is_train,\n","                'issue': 'label_resample_error',\n","                'error': str(e),  # include error details\n","            })\n","            continue\n","\n","        resampled_sensor.rename(columns={\n","            channel_mapping['ax']: 'ax',\n","            channel_mapping['ay']: 'ay',\n","            channel_mapping['az']: 'az',\n","            channel_mapping['gx']: 'gx',\n","            channel_mapping['gy']: 'gy',\n","            channel_mapping['gz']: 'gz',\n","        }, inplace=True)\n","\n","        resampled_sensor.insert(0, 'subject_id', subject_id)\n","        resampled_sensor.insert(1, 'session_id', session_id)\n","        resampled_sensor.insert(2, 'placement', placement)\n","\n","        resampled_records.append(resampled_sensor)\n","        print(f\"  ✓ Done\")\n","\n","    except Exception as e:\n","        print(f\"  ✗ Error: {e}\")\n","        issues.append({\n","            'subject_id': subject_id,\n","            'session_id': session_id,\n","            'placement': placement,\n","            'is_train': is_train,\n","            'issue': 'processing_error',\n","            'error': str(e),  # include error details\n","        })\n","\n","print(f\"\\nSuccessfully processed: {len(resampled_records)} sessions\")\n","print(f\"Skipped/failed: {len(issues)} sessions\")\n","\n","# ========== 2. Combine & save ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Combine & Save\")\n","print(\"=\"*60)\n","\n","if resampled_records:\n","    df_all = pd.concat(resampled_records, ignore_index=True)\n","\n","    # Optimization: cast dtypes (reduce size)\n","    for c in ['ax', 'ay', 'az', 'gx', 'gy', 'gz']:\n","        df_all[c] = df_all[c].astype('float32')\n","    df_all['time_sec'] = df_all['time_sec'].astype('float64')  # Keep high precision for time\n","\n","    output_file = proc_dir / \"resampled.parquet\"\n","\n","    if output_file.exists():\n","        import shutil\n","        if output_file.is_dir():\n","            shutil.rmtree(output_file)\n","        else:\n","            output_file.unlink()\n","        print(f\"Removed old data: {output_file}\")\n","\n","    df_all.to_parquet(\n","        output_file,\n","        index=False,\n","        partition_cols=['subject_id', 'placement'],\n","        engine='pyarrow'\n","    )\n","    print(f\"✓ Saved: {output_file}\")\n","    print(f\"  Total rows: {len(df_all):,}\")\n","    print(f\"  # subjects: {df_all['subject_id'].nunique()}\")\n","    print(f\"  # sessions: {df_all.groupby(['subject_id', 'session_id']).ngroups}\")\n","\n","    valid_mask = df_all[['ax', 'ay', 'az', 'gx', 'gy', 'gz']].notna().all(axis=1)\n","    print(f\"  Valid samples: {valid_mask.sum():,} ({valid_mask.sum()/len(df_all)*100:.1f}%)\")\n","    print(f\"  Samples with NaN: {(~valid_mask).sum():,}\")\n","\n","    print(\"\\nData preview:\")\n","    print(df_all.head(10).to_string())\n","\n","    print(\"\\nNumeric column stats (valid samples):\")\n","    numeric_cols = ['ax', 'ay', 'az', 'gx', 'gy', 'gz']\n","    print(df_all.loc[valid_mask, numeric_cols].describe().round(4))\n","else:\n","    print(\"Warning: No data to save\")\n","\n","# ========== 3. Save statistics ==========\n","if interp_stats:\n","    df_interp = pd.DataFrame(interp_stats)\n","    interp_file = proc_dir / \"resample_stats.csv\"\n","    df_interp.to_csv(interp_file, index=False)\n","    print(f\"\\n✓ Saved stats: {interp_file}\")\n","\n","    if 'is_train' in df_interp.columns and df_interp['is_train'].any():\n","        train_stats = df_interp[df_interp['is_train']]\n","        print(f\"\\nGap statistics (train fold):\")\n","        print(f\"  Mean gap coverage: {train_stats['gap_coverage'].mean()*100:.2f}%\")\n","        print(f\"  Max gap coverage: {train_stats['gap_coverage'].max()*100:.2f}%\")\n","        print(f\"  Mean gap time fraction: {train_stats['gap_time_fraction'].mean()*100:.2f}%\")\n","\n","        print(f\"\\nGap statistics (overall):\")\n","        print(f\"  Mean gap coverage: {df_interp['gap_coverage'].mean()*100:.2f}%\")\n","        print(f\"  Max gap coverage: {df_interp['gap_coverage'].max()*100:.2f}%\")\n","    else:\n","        print(f\"\\nGap statistics:\")\n","        print(f\"  Mean gap coverage: {df_interp['gap_coverage'].mean()*100:.2f}%\")\n","        print(f\"  Max gap coverage: {df_interp['gap_coverage'].max()*100:.2f}%\")\n","\n","if issues:\n","    df_issues = pd.DataFrame(issues)\n","    issues_file = proc_dir / \"resample_issues.csv\"\n","    df_issues.to_csv(issues_file, index=False)\n","    print(f\"\\n⚠️  Saved issue records: {issues_file} ({len(issues)} items)\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 5 complete - Flawless version\")\n","print(\"=\"*60)\n","print(f\"\\nFinal fixes:\")\n","print(f\"  1. ✓ Prune switch (always ON; global high-gap sessions removed)\")\n","print(f\"  2. ✓ Label time harmonized (reuse cleaned time)\")\n","print(f\"  3. ✓ Complete error information (\\\"error\\\" field)\")\n","print(f\"  4. ✓ Comment fix (constant threshold 0.15)\")\n","print(f\"  5. ✓ Type optimization (float32/float64)\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"STGCB6nr_JAz","executionInfo":{"status":"ok","timestamp":1763470624188,"user_tz":0,"elapsed":51302,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"1a2b8d87-c73e-45f0-aebd-6e184cec57fe"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 5: Timeline Unification & Resampling\n","============================================================\n","\n","Target sampling rate: 50.0 Hz\n","Selected placement: rwrist\n","\n","Global resampling over all kept sessions (no per-fold markers)\n","  Total sessions: 96\n","\n","============================================================\n","1. Resampling\n","============================================================\n","\n","Processing S07/R03/rwrist [TEST]...\n","  Resampling sensors (11758 rows)...\n","  → 5879 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R05/rwrist [TEST]...\n","  Resampling sensors (11766 rows)...\n","  → 5883 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R06/rwrist [TEST]...\n","  Resampling sensors (11838 rows)...\n","  → 5919 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R07/rwrist [TEST]...\n","  Resampling sensors (11795 rows)...\n","  → 5898 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R08/rwrist [TEST]...\n","  Resampling sensors (11804 rows)...\n","  → 5902 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R09/rwrist [TEST]...\n","  Resampling sensors (11779 rows)...\n","  → 5890 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R10/rwrist [TEST]...\n","  Resampling sensors (11777 rows)...\n","  → 5889 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R11/rwrist [TEST]...\n","  Resampling sensors (11775 rows)...\n","  → 5888 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R12/rwrist [TEST]...\n","  Resampling sensors (11814 rows)...\n","  → 5908 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R13/rwrist [TEST]...\n","  Resampling sensors (11842 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R14/rwrist [TEST]...\n","  Resampling sensors (11806 rows)...\n","  → 5903 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R15/rwrist [TEST]...\n","  Resampling sensors (11774 rows)...\n","  → 5888 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R16/rwrist [TEST]...\n","  Resampling sensors (11863 rows)...\n","  → 5932 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R03/rwrist [TEST]...\n","  Resampling sensors (11868 rows)...\n","  → 5935 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R04/rwrist [TEST]...\n","  Resampling sensors (11843 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R06/rwrist [TEST]...\n","  Resampling sensors (11864 rows)...\n","  → 5932 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R07/rwrist [TEST]...\n","  Resampling sensors (11901 rows)...\n","  → 5951 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R08/rwrist [TEST]...\n","  Resampling sensors (11856 rows)...\n","  → 5929 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R10/rwrist [TEST]...\n","  Resampling sensors (11926 rows)...\n","  → 5963 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R11/rwrist [TEST]...\n","  Resampling sensors (11903 rows)...\n","  → 5952 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R12/rwrist [TEST]...\n","  Resampling sensors (11924 rows)...\n","  → 5963 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R13/rwrist [TEST]...\n","  Resampling sensors (11925 rows)...\n","  → 5963 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R15/rwrist [TEST]...\n","  Resampling sensors (11270 rows)...\n","  → 5636 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R16/rwrist [TEST]...\n","  Resampling sensors (11540 rows)...\n","  → 5770 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R03/rwrist [TEST]...\n","  Resampling sensors (11866 rows)...\n","  → 5934 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R04/rwrist [TEST]...\n","  Resampling sensors (11907 rows)...\n","  → 5954 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R05/rwrist [TEST]...\n","  Resampling sensors (11895 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R06/rwrist [TEST]...\n","  Resampling sensors (11916 rows)...\n","  → 5959 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R07/rwrist [TEST]...\n","  Resampling sensors (11860 rows)...\n","  → 5931 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R08/rwrist [TEST]...\n","  Resampling sensors (11909 rows)...\n","  → 5955 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R09/rwrist [TEST]...\n","  Resampling sensors (11906 rows)...\n","  → 5953 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R10/rwrist [TEST]...\n","  Resampling sensors (11886 rows)...\n","  → 5944 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R11/rwrist [TEST]...\n","  Resampling sensors (11880 rows)...\n","  → 5941 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R12/rwrist [TEST]...\n","  Resampling sensors (11866 rows)...\n","  → 5933 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R13/rwrist [TEST]...\n","  Resampling sensors (11904 rows)...\n","  → 5953 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R14/rwrist [TEST]...\n","  Resampling sensors (4035 rows)...\n","  → 2018 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R15/rwrist [TEST]...\n","  Resampling sensors (11895 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R16/rwrist [TEST]...\n","  Resampling sensors (11895 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R03/rwrist [TEST]...\n","  Resampling sensors (11795 rows)...\n","  → 5898 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R04/rwrist [TEST]...\n","  Resampling sensors (11806 rows)...\n","  → 5903 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R05/rwrist [TEST]...\n","  Resampling sensors (11815 rows)...\n","  → 5908 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R06/rwrist [TEST]...\n","  Resampling sensors (11767 rows)...\n","  → 5884 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R07/rwrist [TEST]...\n","  Resampling sensors (11869 rows)...\n","  → 5935 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R08/rwrist [TEST]...\n","  Resampling sensors (11794 rows)...\n","  → 5898 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R09/rwrist [TEST]...\n","  Resampling sensors (11791 rows)...\n","  → 5896 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R10/rwrist [TEST]...\n","  Resampling sensors (11863 rows)...\n","  → 5932 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R11/rwrist [TEST]...\n","  Resampling sensors (11822 rows)...\n","  → 5911 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R12/rwrist [TEST]...\n","  Resampling sensors (11845 rows)...\n","  → 5923 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R13/rwrist [TEST]...\n","  Resampling sensors (11792 rows)...\n","  → 5897 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R14/rwrist [TEST]...\n","  Resampling sensors (11798 rows)...\n","  → 5900 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R15/rwrist [TEST]...\n","  Resampling sensors (11803 rows)...\n","  → 5902 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R16/rwrist [TEST]...\n","  Resampling sensors (11751 rows)...\n","  → 5876 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R03/rwrist [TEST]...\n","  Resampling sensors (11858 rows)...\n","  → 5929 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R04/rwrist [TEST]...\n","  Resampling sensors (11809 rows)...\n","  → 5905 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R05/rwrist [TEST]...\n","  Resampling sensors (11821 rows)...\n","  → 5911 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R06/rwrist [TEST]...\n","  Resampling sensors (11843 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R07/rwrist [TEST]...\n","  Resampling sensors (11897 rows)...\n","  → 5949 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R08/rwrist [TEST]...\n","  Resampling sensors (11889 rows)...\n","  → 5945 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R09/rwrist [TEST]...\n","  Resampling sensors (11906 rows)...\n","  → 5953 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R10/rwrist [TEST]...\n","  Resampling sensors (11880 rows)...\n","  → 5940 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R11/rwrist [TEST]...\n","  Resampling sensors (11859 rows)...\n","  → 5930 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R12/rwrist [TEST]...\n","  Resampling sensors (11885 rows)...\n","  → 5943 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R13/rwrist [TEST]...\n","  Resampling sensors (11841 rows)...\n","  → 5921 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R15/rwrist [TEST]...\n","  Resampling sensors (11879 rows)...\n","  → 5940 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R11/rwrist [TEST]...\n","  Resampling sensors (11894 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R12/rwrist [TEST]...\n","  Resampling sensors (2817 rows)...\n","  → 1409 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R13/rwrist [TEST]...\n","  Resampling sensors (11905 rows)...\n","  → 5953 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R14/rwrist [TEST]...\n","  Resampling sensors (11489 rows)...\n","  → 5745 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R15/rwrist [TEST]...\n","  Resampling sensors (11927 rows)...\n","  → 5964 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R16/rwrist [TEST]...\n","  Resampling sensors (11843 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R03/rwrist [TEST]...\n","  Resampling sensors (11845 rows)...\n","  → 5923 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R04/rwrist [TEST]...\n","  Resampling sensors (11878 rows)...\n","  → 5940 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R05/rwrist [TEST]...\n","  Resampling sensors (11910 rows)...\n","  → 5956 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R06/rwrist [TEST]...\n","  Resampling sensors (11902 rows)...\n","  → 5951 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R07/rwrist [TEST]...\n","  Resampling sensors (11903 rows)...\n","  → 5952 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R08/rwrist [TEST]...\n","  Resampling sensors (11896 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R09/rwrist [TEST]...\n","  Resampling sensors (11910 rows)...\n","  → 5955 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R10/rwrist [TEST]...\n","  Resampling sensors (11918 rows)...\n","  → 5959 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R11/rwrist [TEST]...\n","  Resampling sensors (11861 rows)...\n","  → 5931 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R12/rwrist [TEST]...\n","  Resampling sensors (11912 rows)...\n","  → 5956 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R13/rwrist [TEST]...\n","  Resampling sensors (11894 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R14/rwrist [TEST]...\n","  Resampling sensors (11884 rows)...\n","  → 5943 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R15/rwrist [TEST]...\n","  Resampling sensors (11892 rows)...\n","  → 5946 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R16/rwrist [TEST]...\n","  Resampling sensors (11870 rows)...\n","  → 5935 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R03/rwrist [TEST]...\n","  Resampling sensors (11849 rows)...\n","  → 5925 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R04/rwrist [TEST]...\n","  Resampling sensors (11760 rows)...\n","  → 5881 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R06/rwrist [TEST]...\n","  Resampling sensors (11856 rows)...\n","  → 5928 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R07/rwrist [TEST]...\n","  Resampling sensors (11863 rows)...\n","  → 5932 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R08/rwrist [TEST]...\n","  Resampling sensors (11921 rows)...\n","  → 5961 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R09/rwrist [TEST]...\n","  Resampling sensors (11871 rows)...\n","  → 5936 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R10/rwrist [TEST]...\n","  Resampling sensors (11911 rows)...\n","  → 5956 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R11/rwrist [TEST]...\n","  Resampling sensors (11843 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R12/rwrist [TEST]...\n","  Resampling sensors (11787 rows)...\n","  → 5894 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R14/rwrist [TEST]...\n","  Resampling sensors (11851 rows)...\n","  → 5926 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R15/rwrist [TEST]...\n","  Resampling sensors (11823 rows)...\n","  → 5912 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R16/rwrist [TEST]...\n","  Resampling sensors (11851 rows)...\n","  → 5926 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Successfully processed: 96 sessions\n","Skipped/failed: 0 sessions\n","\n","============================================================\n","2. Combine & Save\n","============================================================\n","Removed old data: data/lara/mbientlab/proc/resampled.parquet\n","✓ Saved: data/lara/mbientlab/proc/resampled.parquet\n","  Total rows: 560,070\n","  # subjects: 8\n","  # sessions: 96\n","  Valid samples: 560,070 (100.0%)\n","  Samples with NaN: 0\n","\n","Data preview:\n","  subject_id session_id placement      time_sec        ax        ay        az         gx         gy         gz  is_in_gap  is_forced_nan  label\n","0        S07        R03    rwrist  1.564739e+09 -0.281438 -0.924541  0.325248   1.414270  24.960403  -9.089332          0              0    6.0\n","1        S07        R03    rwrist  1.564739e+09 -0.298464 -0.935255  0.334803   3.294115  27.049623 -12.293148          0              0    6.0\n","2        S07        R03    rwrist  1.564739e+09 -0.263180 -1.002837  0.456556   2.767826  14.701869 -19.894005          0              0    6.0\n","3        S07        R03    rwrist  1.564739e+09 -0.174738 -1.060583  0.545962   5.506332   5.940687 -22.905399          0              0    6.0\n","4        S07        R03    rwrist  1.564739e+09 -0.125629 -1.083030  0.620053  16.071184  10.412006 -25.808487          0              0    6.0\n","5        S07        R03    rwrist  1.564739e+09 -0.148535 -1.093232  0.639439  25.785658  20.021729 -29.624060          0              0    6.0\n","6        S07        R03    rwrist  1.564739e+09 -0.210257 -1.095191  0.675926  40.849041  25.070871 -35.659191          0              0    6.0\n","7        S07        R03    rwrist  1.564739e+09 -0.222465 -1.095854  0.701550  45.951775  22.461361 -37.891891          0              0    6.0\n","8        S07        R03    rwrist  1.564739e+09 -0.214872 -1.125343  0.729511  57.344635  10.825806 -42.137077          0              0    6.0\n","9        S07        R03    rwrist  1.564739e+09 -0.218806 -1.212497  0.748468  68.151352   9.044179 -45.126099          0              0    6.0\n","\n","Numeric column stats (valid samples):\n","                ax           ay           az           gx           gy  \\\n","count  560070.0000  560070.0000  560070.0000  560070.0000  560070.0000   \n","mean       -0.6569      -0.1522       0.3342      -1.0943       0.6134   \n","std         0.4212       0.5260       0.4697      77.1153      72.2905   \n","min       -11.1498      -9.2003     -27.0872   -4185.1846   -1426.9553   \n","25%        -0.9339      -0.5590       0.1043     -19.9812     -22.0506   \n","50%        -0.7474      -0.0536       0.3487       0.1252      -0.3429   \n","75%        -0.4270       0.2280       0.6074      20.5185      21.7400   \n","max        32.7536      11.5694      14.6606    1110.7982    1697.6891   \n","\n","                gz  \n","count  560070.0000  \n","mean        0.5667  \n","std        72.9628  \n","min     -3227.1677  \n","25%       -19.0527  \n","50%         0.5039  \n","75%        22.0198  \n","max       736.3111  \n","\n","✓ Saved stats: data/lara/mbientlab/proc/resample_stats.csv\n","\n","Gap statistics:\n","  Mean gap coverage: 0.00%\n","  Max gap coverage: 0.00%\n","\n","============================================================\n","Step 5 complete - Flawless version\n","============================================================\n","\n","Final fixes:\n","  1. ✓ Prune switch (always ON; global high-gap sessions removed)\n","  2. ✓ Label time harmonized (reuse cleaned time)\n","  3. ✓ Complete error information (\"error\" field)\n","  4. ✓ Comment fix (constant threshold 0.15)\n","  5. ✓ Type optimization (float32/float64)\n","============================================================\n"]}]},{"cell_type":"code","source":["import os\n","\n","\"\"\"\n","Step 6: Sensor Preprocessing (top-conf/journal grade - final fixed version)\n","Accelerometer high-pass to remove gravity; gyroscope denoising; adaptive ±Nσ clipping (target 1%)\n","Global version: no FOLD_ID required; thresholds estimated on all data.\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import json\n","from scipy import signal\n","\n","# ========== Config ==========\n","# Accelerometer high-pass (remove gravity)\n","ACC_HPF_CUTOFF_HZ = 0.3      # Cutoff frequency\n","ACC_HPF_ORDER = 2            # Filter order\n","\n","# Gyroscope low-pass (denoise)\n","GYR_LPF_CUTOFF_HZ = 20.0     # Cutoff frequency\n","GYR_LPF_ORDER = 2            # Filter order\n","\n","# Adaptive clipping threshold (auto-tuned to target clipping rate)\n","TARGET_CLIP_RATE = 0.01      # Target clipping rate 1% (sum of both tails)\n","\n","# Sampling rate (from Step 5)\n","SAMPLING_RATE_HZ = 50.0\n","\n","# Unit conversions\n","DEG2RAD = np.pi / 180.0\n","G_TO_MS2 = 9.80665\n","\n","print(\"=\"*60)\n","print(\"Step 6: Sensor Preprocessing\")\n","print(\"=\"*60)\n","\n","# Load data\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","\n","print(f\"\\nLoading resampled data: {proc_dir / 'resampled.parquet'}\")\n","df = pd.read_parquet(proc_dir / \"resampled.parquet\")\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Number of subjects: {df['subject_id'].nunique()}\")\n","print(f\"Number of sessions: {df.groupby(['subject_id', 'session_id'], observed=True).ngroups}\")\n","\n","# ========== 0. Unit normalization ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"0. Unit normalization\")\n","print(\"=\"*60)\n","\n","acc_channels = ['ax', 'ay', 'az']\n","print(f\"\\nAccelerometer unit conversion: g → m/s²\")\n","for ch in acc_channels:\n","    if ch in df.columns:\n","        mask = df[ch].notna()\n","        df.loc[mask, ch] = df.loc[mask, ch] * G_TO_MS2\n","print(f\"✓ Conversion factor: {G_TO_MS2:.5f}\")\n","\n","gyr_channels = ['gx', 'gy', 'gz']\n","print(f\"\\nGyroscope unit conversion: deg/s → rad/s\")\n","for ch in gyr_channels:\n","    if ch in df.columns:\n","        mask = df[ch].notna()\n","        df.loc[mask, ch] = df.loc[mask, ch] * DEG2RAD\n","print(f\"✓ Conversion factor: π/180 = {DEG2RAD:.6f}\")\n","\n","# ========== Helper functions ==========\n","def design_highpass_filter(cutoff_hz, fs_hz, order=2):\n","    \"\"\"Design a high-pass Butterworth filter\"\"\"\n","    nyq = 0.5 * fs_hz\n","    normal_cutoff = cutoff_hz / nyq\n","    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n","    return b, a\n","\n","def design_lowpass_filter(cutoff_hz, fs_hz, order=2):\n","    \"\"\"Design a low-pass Butterworth filter\"\"\"\n","    nyq = 0.5 * fs_hz\n","    normal_cutoff = cutoff_hz / nyq\n","    b, a = signal.butter(order, normal_cutoff, btype='low', analog=False)\n","    return b, a\n","\n","def filtfilt_nan_safe(x, b, a):\n","    \"\"\"Zero-phase filtering tolerant to NaN (filter each contiguous non-NaN run)\"\"\"\n","    y = x.copy()\n","    good = np.isfinite(x)\n","\n","    if not good.any():\n","        return x\n","\n","    idx = np.where(good)[0]\n","    cuts = np.where(np.diff(idx) > 1)[0] + 1\n","    runs = np.split(idx, cuts)\n","\n","    padlen = 3 * (max(len(a), len(b)) - 1)\n","\n","    for run in runs:\n","        seg = x[run]\n","\n","        if len(seg) > padlen:\n","            y[run] = signal.filtfilt(b, a, seg, method=\"pad\")\n","        else:\n","            tmp = signal.lfilter(b, a, seg)\n","            y[run] = signal.lfilter(b, a, tmp[::-1])[::-1]\n","\n","    return y\n","\n","def apply_filter_by_session(df, channels, b, a):\n","    \"\"\"Apply zero-phase filtering grouped by session (include placement grouping + sorting)\"\"\"\n","    filtered_data = []\n","\n","    for (subj, sess, plc), group in df.groupby(['subject_id', 'session_id', 'placement'], observed=True):\n","        group = group.sort_values('time_sec').copy()\n","\n","        for ch in channels:\n","            if ch not in group.columns:\n","                continue\n","\n","            data = group[ch].values\n","            filtered = filtfilt_nan_safe(data, b, a)\n","            group[ch] = filtered\n","\n","        filtered_data.append(group)\n","\n","    return pd.concat(filtered_data, ignore_index=True)\n","\n","def compute_clip_thresholds_target(df, channels, target_rate=0.01, use_robust=True):\n","    \"\"\"Adaptive thresholds to a target clipping rate (Scheme A)\n","\n","    Args:\n","        target_rate: target total clipping rate for both tails (e.g., 0.01 = 1%)\n","        use_robust: if True, use Median±k·(1.4826·MAD); otherwise Mean±k·Std\n","    \"\"\"\n","    eps = 1e-6\n","    thresholds = {}\n","\n","    for ch in channels:\n","        if ch not in df.columns:\n","            continue\n","\n","        x = df[ch].dropna().values\n","        if x.size == 0:\n","            continue\n","\n","        if use_robust:\n","            # Robust estimate: Median ± k·(1.4826·MAD)\n","            median = np.median(x)\n","            mad = np.median(np.abs(x - median))\n","            robust_std = max(1.4826 * mad, eps)\n","\n","            deviations = np.abs(x - median) / robust_std\n","            k = np.quantile(deviations, 1 - target_rate)\n","\n","            lower = median - k * robust_std\n","            upper = median + k * robust_std\n","\n","            thresholds[ch] = {\n","                'center': float(median),\n","                'scale': float(robust_std),\n","                'k': float(k),\n","                'lower': float(lower),\n","                'upper': float(upper),\n","                'method': f'Median±k·MAD (k={k:.3f}, both tails total {target_rate*100:.1f}%)',\n","            }\n","        else:\n","            # Conventional estimate: Mean ± k·Std\n","            mean = np.mean(x)\n","            std = max(np.std(x), eps)\n","\n","            deviations = np.abs(x - mean) / std\n","            k = np.quantile(deviations, 1 - target_rate)\n","\n","            lower = mean - k * std\n","            upper = mean + k * std\n","\n","            thresholds[ch] = {\n","                'center': float(mean),\n","                'scale': float(std),\n","                'k': float(k),\n","                'lower': float(lower),\n","                'upper': float(upper),\n","                'method': f'Mean±k·Std (k={k:.3f}, both tails total {target_rate*100:.1f}%)',\n","            }\n","\n","    return thresholds\n","\n","def apply_clip(df, channels, thresholds):\n","    \"\"\"Apply clipping and compute actual clipping rate\"\"\"\n","    df_clipped = df.copy()\n","    clip_stats = {}\n","\n","    for ch in channels:\n","        if ch not in df_clipped.columns or ch not in thresholds:\n","            continue\n","\n","        lower = thresholds[ch]['lower']\n","        upper = thresholds[ch]['upper']\n","\n","        mask = df_clipped[ch].notna()\n","        total = mask.sum()\n","\n","        if total > 0:\n","            outliers = ((df_clipped.loc[mask, ch] < lower) | (df_clipped.loc[mask, ch] > upper)).sum()\n","            clip_rate = outliers / total\n","            clip_stats[ch] = {\n","                'outliers': int(outliers),\n","                'total': int(total),\n","                'rate': float(clip_rate),\n","            }\n","\n","        df_clipped.loc[mask, ch] = df_clipped.loc[mask, ch].clip(lower, upper)\n","\n","    return df_clipped, clip_stats\n","\n","# ========== 1. Design filters ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Design filters\")\n","print(\"=\"*60)\n","\n","print(f\"\\nAccelerometer high-pass filter:\")\n","print(f\"  Cutoff frequency: {ACC_HPF_CUTOFF_HZ} Hz\")\n","print(f\"  Order: {ACC_HPF_ORDER}\")\n","acc_b, acc_a = design_highpass_filter(ACC_HPF_CUTOFF_HZ, SAMPLING_RATE_HZ, ACC_HPF_ORDER)\n","\n","print(f\"\\nGyroscope low-pass filter:\")\n","print(f\"  Cutoff frequency: {GYR_LPF_CUTOFF_HZ} Hz\")\n","print(f\"  Order: {GYR_LPF_ORDER}\")\n","gyr_b, gyr_a = design_lowpass_filter(GYR_LPF_CUTOFF_HZ, SAMPLING_RATE_HZ, GYR_LPF_ORDER)\n","\n","# ========== 2. Apply filters (by session + placement) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Apply filters (by session + placement, zero-phase)\")\n","print(\"=\"*60)\n","\n","print(\"\\nApplying accelerometer high-pass (remove gravity)...\")\n","df_filtered = apply_filter_by_session(df, acc_channels, acc_b, acc_a)\n","print(\"✓ Done\")\n","\n","print(\"\\nApplying gyroscope low-pass (denoise)...\")\n","df_filtered = apply_filter_by_session(df_filtered, gyr_channels, gyr_b, gyr_a)\n","print(\"✓ Done\")\n","\n","# ========== 3. Compute clipping thresholds (adaptive to target rate) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Compute adaptive clipping thresholds (target clipping rate)\")\n","print(\"=\"*60)\n","\n","print(\"Estimate clipping thresholds on all data\")\n","print(f\"  Target clip rate: {TARGET_CLIP_RATE*100:.1f}%\")\n","\n","all_channels = acc_channels + gyr_channels\n","df_for_stats = df_filtered  # global estimation on all subjects\n","clip_thresholds = compute_clip_thresholds_target(\n","    df_for_stats, all_channels, TARGET_CLIP_RATE, use_robust=True\n",")\n","\n","print(f\"\\nClipping thresholds (adaptive robust estimation):\")\n","for ch, thresh in clip_thresholds.items():\n","    print(f\"  {ch}:\")\n","    print(f\"    center: {thresh['center']:.4f}\")\n","    print(f\"    scale: {thresh['scale']:.4f}\")\n","    print(f\"    k: {thresh['k']:.3f}\")\n","    print(f\"    range: [{thresh['lower']:.4f}, {thresh['upper']:.4f}]\")\n","\n","# ========== 4. Apply clipping ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Apply adaptive clipping\")\n","print(\"=\"*60)\n","\n","df_clipped, clip_stats = apply_clip(df_filtered, all_channels, clip_thresholds)\n","\n","print(\"\\nActual clipping statistics:\")\n","for ch, stats in clip_stats.items():\n","    print(f\"  {ch}: {stats['outliers']:,} / {stats['total']:,} ({stats['rate']*100:.2f}%)\")\n","\n","# ========== 5. Cast to float32 to save memory ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Data type optimization\")\n","print(\"=\"*60)\n","\n","numeric_cols = ['ax', 'ay', 'az', 'gx', 'gy', 'gz']\n","for col in numeric_cols:\n","    if col in df_clipped.columns:\n","        df_clipped[col] = df_clipped[col].astype('float32')\n","\n","print(f\"✓ Sensor columns cast to float32\")\n","print(f\"✓ time_sec kept as float64\")\n","\n","# ========== 6. Save results ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Save results\")\n","print(\"=\"*60)\n","\n","output_file = proc_dir / \"filtered.parquet\"\n","\n","if output_file.exists():\n","    import shutil\n","    if output_file.is_dir():\n","        shutil.rmtree(output_file)\n","    else:\n","        output_file.unlink()\n","    print(f\"Removed old data: {output_file}\")\n","\n","df_clipped.to_parquet(\n","    output_file,\n","    index=False,\n","    partition_cols=['subject_id', 'placement'],\n","    engine='pyarrow'\n",")\n","print(f\"✓ Saved: {output_file}\")\n","print(f\"  Data shape: {df_clipped.shape}\")\n","\n","print(\"\\nData preview:\")\n","print(df_clipped.head(10).to_string())\n","\n","print(\"\\nPost-filter numeric column stats:\")\n","valid_mask = df_clipped[numeric_cols].notna().all(axis=1)\n","print(df_clipped.loc[valid_mask, numeric_cols].describe().round(4))\n","\n","# ========== 7. Save filter configuration ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"7. Save filter configuration\")\n","print(\"=\"*60)\n","\n","filter_config = {\n","    'sampling_rate_hz': SAMPLING_RATE_HZ,\n","\n","    'units': {\n","        'accelerometer': 'm/s² (converted from g)',\n","        'gyroscope': 'rad/s (converted from deg/s)',\n","        'conversion': {\n","            'accelerometer_g_to_ms2': G_TO_MS2,\n","            'gyroscope_deg_to_rad': DEG2RAD,\n","        }\n","    },\n","\n","    'dtypes': {\n","        'sensor_channels': 'float32',\n","        'time_sec': 'float64',\n","    },\n","\n","    'accelerometer': {\n","        'filter_type': 'highpass',\n","        'purpose': 'detrend (remove gravity)',\n","        'method': 'Butterworth',\n","        'cutoff_hz': ACC_HPF_CUTOFF_HZ,\n","        'order': ACC_HPF_ORDER,\n","        'coefficients': {\n","            'b': acc_b.tolist(),\n","            'a': acc_a.tolist(),\n","        },\n","        'zero_phase': True,\n","    },\n","\n","    'gyroscope': {\n","        'filter_type': 'lowpass',\n","        'purpose': 'denoise',\n","        'method': 'Butterworth',\n","        'cutoff_hz': GYR_LPF_CUTOFF_HZ,\n","        'order': GYR_LPF_ORDER,\n","        'coefficients': {\n","            'b': gyr_b.tolist(),\n","            'a': gyr_a.tolist(),\n","        },\n","        'zero_phase': True,\n","    },\n","\n","    'clipping': {\n","        'method': 'Adaptive robust estimation (Median±k·MAD, Scheme A)',\n","        'target_clip_rate': TARGET_CLIP_RATE,\n","        'estimated_on': 'all_data',\n","        'fold_id': None,\n","        'thresholds': clip_thresholds,\n","        'actual_clip_stats': clip_stats,\n","        'rationale': (\n","            f'Auto-adjust k so the global clipping rate reaches the target '\n","            f'{TARGET_CLIP_RATE*100:.1f}% over all subjects'\n","        ),\n","    },\n","\n","    'notes': [\n","        'All filters use filtfilt for zero phase',\n","        'Filtering is grouped by session + placement, sorted by time_sec; avoid crossing session boundaries',\n","        'filtfilt_nan_safe filters each contiguous non-NaN run separately',\n","        'Accelerometer converted from g to m/s² (×9.80665)',\n","        'Gyroscope converted from deg/s to rad/s (×π/180)',\n","        f'Adaptive clipping thresholds: determine k on all data so clipping ≈ {TARGET_CLIP_RATE*100:.1f}%, then apply consistently to all data',\n","        'NaNs remain unchanged',\n","        'Sensor columns are float32; time_sec is float64',\n","    ]\n","}\n","\n","filter_config_file = configs_dir / \"filter.yaml\"\n","with open(filter_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(filter_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","print(f\"✓ Saved filter configuration: {filter_config_file}\")\n","\n","filter_config_json = configs_dir / \"filter.json\"\n","with open(filter_config_json, 'w', encoding='utf-8') as f:\n","    json.dump(filter_config, f, indent=2)\n","print(f\"✓ Saved filter configuration: {filter_config_json}\")\n","\n","# ========== 8. Summary ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 6 complete - Sensor preprocessing (global version)\")\n","print(\"=\"*60)\n","print(f\"\\nConfig:\")\n","print(f\"  Units: Acc g→m/s², Gyro deg/s→rad/s\")\n","print(f\"  Accelerometer: high-pass {ACC_HPF_CUTOFF_HZ} Hz (remove gravity)\")\n","print(f\"  Gyroscope: low-pass {GYR_LPF_CUTOFF_HZ} Hz (denoise)\")\n","print(f\"  Clipping: adaptive ±k·MAD (target {TARGET_CLIP_RATE*100:.1f}%)\")\n","print(f\"  Clipping thresholds estimated on: all data\")\n","print(f\"\\nResults:\")\n","print(f\"  Output file: {output_file}\")\n","print(f\"  Config file: {filter_config_file}\")\n","print(f\"  Data shape: {df_clipped.shape}\")\n","print(\"\\nFinal fixes:\")\n","print(f\"  ✓ Adaptive clipping thresholds (Scheme A)\")\n","print(f\"  ✓ Target clipping rate {TARGET_CLIP_RATE*100:.1f}%, auto-solve k (global)\")\n","print(f\"  ✓ Group by placement + sort by time_sec\")\n","print(f\"  ✓ Write actual clipping rate into config\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qAIfeneq_LrJ","executionInfo":{"status":"ok","timestamp":1763470626295,"user_tz":0,"elapsed":2104,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"a719a44a-ea4f-4499-fd5c-bcaee150f20e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 6: Sensor Preprocessing\n","============================================================\n","\n","Loading resampled data: data/lara/mbientlab/proc/resampled.parquet\n","Data shape: (560070, 13)\n","Number of subjects: 8\n","Number of sessions: 96\n","\n","============================================================\n","0. Unit normalization\n","============================================================\n","\n","Accelerometer unit conversion: g → m/s²\n","✓ Conversion factor: 9.80665\n","\n","Gyroscope unit conversion: deg/s → rad/s\n","✓ Conversion factor: π/180 = 0.017453\n","\n","============================================================\n","1. Design filters\n","============================================================\n","\n","Accelerometer high-pass filter:\n","  Cutoff frequency: 0.3 Hz\n","  Order: 2\n","\n","Gyroscope low-pass filter:\n","  Cutoff frequency: 20.0 Hz\n","  Order: 2\n","\n","============================================================\n","2. Apply filters (by session + placement, zero-phase)\n","============================================================\n","\n","Applying accelerometer high-pass (remove gravity)...\n","✓ Done\n","\n","Applying gyroscope low-pass (denoise)...\n","✓ Done\n","\n","============================================================\n","3. Compute adaptive clipping thresholds (target clipping rate)\n","============================================================\n","Estimate clipping thresholds on all data\n","  Target clip rate: 1.0%\n","\n","Clipping thresholds (adaptive robust estimation):\n","  ax:\n","    center: 0.0152\n","    scale: 1.3530\n","    k: 6.712\n","    range: [-9.0660, 9.0964]\n","  ay:\n","    center: 0.0141\n","    scale: 1.3815\n","    k: 5.790\n","    range: [-7.9844, 8.0127]\n","  az:\n","    center: 0.0019\n","    scale: 1.3427\n","    k: 5.934\n","    range: [-7.9661, 7.9699]\n","  gx:\n","    center: 0.0021\n","    scale: 0.5227\n","    k: 6.479\n","    range: [-3.3845, 3.3888]\n","  gy:\n","    center: -0.0064\n","    scale: 0.5632\n","    k: 8.447\n","    range: [-4.7639, 4.7512]\n","  gz:\n","    center: 0.0088\n","    scale: 0.5295\n","    k: 7.474\n","    range: [-3.9489, 3.9665]\n","\n","============================================================\n","4. Apply adaptive clipping\n","============================================================\n","\n","Actual clipping statistics:\n","  ax: 5,601 / 560,070 (1.00%)\n","  ay: 5,601 / 560,070 (1.00%)\n","  az: 5,601 / 560,070 (1.00%)\n","  gx: 5,601 / 560,070 (1.00%)\n","  gy: 5,601 / 560,070 (1.00%)\n","  gz: 5,601 / 560,070 (1.00%)\n","\n","============================================================\n","5. Data type optimization\n","============================================================\n","✓ Sensor columns cast to float32\n","✓ time_sec kept as float64\n","\n","============================================================\n","6. Save results\n","============================================================\n","Removed old data: data/lara/mbientlab/proc/filtered.parquet\n","✓ Saved: data/lara/mbientlab/proc/filtered.parquet\n","  Data shape: (560070, 13)\n","\n","Data preview:\n","  session_id      time_sec        ax        ay        az        gx        gy        gz  is_in_gap  is_forced_nan  label subject_id placement\n","0        R03  1.564739e+09  0.891079 -2.660401  0.238257  0.025291  0.435949 -0.158735          0              0    6.0        S07    rwrist\n","1        R03  1.564739e+09  0.715069 -2.840439  0.239453  0.059089  0.468073 -0.219117          0              0    6.0        S07    rwrist\n","2        R03  1.564739e+09  1.051161 -3.582292  1.341616  0.042692  0.260707 -0.339956          0              0    6.0        S07    rwrist\n","3        R03  1.564739e+09  1.907651 -4.231893  2.127376  0.107476  0.100950 -0.407764          0              0    6.0        S07    rwrist\n","4        R03  1.564739e+09  2.377473 -4.539616  2.763897  0.263063  0.184827 -0.442223          0              0    6.0        S07    rwrist\n","5        R03  1.564739e+09  2.140075 -4.731589  2.865023  0.472918  0.345074 -0.525765          0              0    6.0        S07    rwrist\n","6        R03  1.564739e+09  1.521002 -4.847136  3.135042  0.686333  0.442941 -0.613507          0              0    6.0        S07    rwrist\n","7        R03  1.564739e+09  1.386402 -4.954428  3.299844  0.829848  0.386559 -0.669225          0              0    6.0        S07    rwrist\n","8        R03  1.564739e+09  1.444855 -5.348888  3.488989  0.973541  0.191755 -0.729564          0              0    6.0        S07    rwrist\n","9        R03  1.564739e+09  1.389077 -6.313359  3.591348  1.214971  0.159118 -0.790919          0              0    6.0        S07    rwrist\n","\n","Post-filter numeric column stats:\n","                ax           ay           az           gx           gy  \\\n","count  560070.0000  560070.0000  560070.0000  560070.0000  560070.0000   \n","mean       -0.0088       0.0070       0.0014      -0.0023       0.0101   \n","std         2.2740       2.1459       2.0872       0.8858       1.1601   \n","min        -9.0660      -7.9844      -7.9661      -3.3845      -4.7639   \n","25%        -0.9098      -0.9143      -0.9075      -0.3482      -0.3829   \n","50%         0.0152       0.0141       0.0019       0.0021      -0.0064   \n","75%         0.9153       0.9493       0.9042       0.3572       0.3771   \n","max         9.0964       8.0127       7.9699       3.3888       4.7512   \n","\n","                gz  \n","count  560070.0000  \n","mean        0.0233  \n","std         0.9875  \n","min        -3.9489  \n","25%        -0.3321  \n","50%         0.0088  \n","75%         0.3837  \n","max         3.9665  \n","\n","============================================================\n","7. Save filter configuration\n","============================================================\n","✓ Saved filter configuration: configs/filter.yaml\n","✓ Saved filter configuration: configs/filter.json\n","\n","============================================================\n","Step 6 complete - Sensor preprocessing (global version)\n","============================================================\n","\n","Config:\n","  Units: Acc g→m/s², Gyro deg/s→rad/s\n","  Accelerometer: high-pass 0.3 Hz (remove gravity)\n","  Gyroscope: low-pass 20.0 Hz (denoise)\n","  Clipping: adaptive ±k·MAD (target 1.0%)\n","  Clipping thresholds estimated on: all data\n","\n","Results:\n","  Output file: data/lara/mbientlab/proc/filtered.parquet\n","  Config file: configs/filter.yaml\n","  Data shape: (560070, 13)\n","\n","Final fixes:\n","  ✓ Adaptive clipping thresholds (Scheme A)\n","  ✓ Target clipping rate 1.0%, auto-solve k (global)\n","  ✓ Group by placement + sort by time_sec\n","  ✓ Write actual clipping rate into config\n","============================================================\n"]}]},{"cell_type":"code","source":["import os\n","\n","\"\"\"\n","Step 7: Coordinate/Magnitude Normalization (top-conf/journal grade)\n","Compute magnitude channels; z-score standardization (global statistics)\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import json\n","import pickle\n","\n","# ========== Config ==========\n","EPSILON = 1e-8  # Prevent division by zero\n","\n","print(\"=\"*60)\n","print(\"Step 7: Coordinate/Magnitude Normalization\")\n","print(\"=\"*60)\n","\n","# Load data\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","\n","print(f\"\\nLoading filtered data: {proc_dir / 'filtered.parquet'}\")\n","df = pd.read_parquet(proc_dir / \"filtered.parquet\")\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Number of subjects: {df['subject_id'].nunique()}\")\n","print(f\"Number of sessions: {df.groupby(['subject_id', 'session_id'], observed=True).ngroups}\")\n","\n","# ========== 1. Compute derived channels (magnitude) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Compute derived channels (magnitude)\")\n","print(\"=\"*60)\n","\n","# Accelerometer magnitude\n","print(\"\\nComputing acc_mag = sqrt(ax² + ay² + az²)...\")\n","df['acc_mag'] = np.sqrt(\n","    df['ax'].values**2 +\n","    df['ay'].values**2 +\n","    df['az'].values**2\n",").astype('float32')\n","\n","# Gyroscope magnitude\n","print(\"Computing gyr_mag = sqrt(gx² + gy² + gz²)...\")\n","df['gyr_mag'] = np.sqrt(\n","    df['gx'].values**2 +\n","    df['gy'].values**2 +\n","    df['gz'].values**2\n",").astype('float32')\n","\n","print(f\"✓ Added derived channels: acc_mag, gyr_mag\")\n","\n","# Show derived-channel stats\n","print(\"\\nDerived channel statistics (post-filter):\")\n","for col in ['acc_mag', 'gyr_mag']:\n","    valid_data = df[col].dropna()\n","    if len(valid_data) > 0:\n","        print(f\"  {col}:\")\n","        print(f\"    Mean: {valid_data.mean():.4f}\")\n","        print(f\"    Std: {valid_data.std():.4f}\")\n","        print(f\"    Range: [{valid_data.min():.4f}, {valid_data.max():.4f}]\")\n","\n","# ========== 2. Determine training set (global) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Determine training set (global)\")\n","print(\"=\"*60)\n","\n","# In this simplified global version, we use ALL subjects to estimate statistics\n","df_train = df\n","train_subjects = set(df['subject_id'].unique())\n","test_subjects = set()  # no explicit test set at this step\n","\n","print(\"Compute statistics on all data (no per-fold split)\")\n","print(f\"  Samples: {len(df):,}\")\n","print(f\"  Subjects: {len(train_subjects)}\")\n","\n","# ========== 3. Compute z-score parameters (global) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Compute z-score parameters (global)\")\n","print(\"=\"*60)\n","\n","# Channels to standardize\n","channels_to_normalize = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","# Compute mean and std (valid data only)\n","scaler_params = {}\n","\n","print(\"\\nz-score parameters (global):\")\n","for ch in channels_to_normalize:\n","    if ch not in df_train.columns:\n","        continue\n","\n","    valid_data = df_train[ch].dropna().values\n","\n","    if len(valid_data) > 0:\n","        mean = float(np.mean(valid_data))\n","        std = float(np.std(valid_data))\n","\n","        # Guard against zero std\n","        if std < EPSILON:\n","            std = 1.0\n","\n","        scaler_params[ch] = {\n","            'mean': mean,\n","            'std': std,\n","        }\n","\n","        print(f\"  {ch}:\")\n","        print(f\"    Mean: {mean:.6f}\")\n","        print(f\"    Std: {std:.6f}\")\n","\n","# ========== 4. Apply z-score standardization ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Apply z-score standardization\")\n","print(\"=\"*60)\n","\n","df_normalized = df.copy()\n","\n","for ch in channels_to_normalize:\n","    if ch not in scaler_params:\n","        continue\n","\n","    mean = scaler_params[ch]['mean']\n","    std = scaler_params[ch]['std']\n","\n","    # Standardize non-NaN values only; cast to float32 to avoid warnings\n","    mask = df_normalized[ch].notna()\n","    normalized_values = ((df_normalized.loc[mask, ch] - mean) / (std + EPSILON)).astype('float32')\n","    df_normalized.loc[mask, ch] = normalized_values\n","\n","print(f\"✓ Standardized {len(scaler_params)} channels\")\n","\n","# Show post-standardization stats (global)\n","print(\"\\nPost-standardization stats (global):\")\n","for ch in channels_to_normalize:\n","    if ch not in scaler_params:\n","        continue\n","\n","    valid_data = df_normalized[ch].dropna()\n","    if len(valid_data) > 0:\n","        print(f\"  {ch}:\")\n","        print(f\"    Mean: {valid_data.mean():.6f} (should be near 0)\")\n","        print(f\"    Std: {valid_data.std():.6f} (should be near 1)\")\n","\n","# ========== 5. Save results ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Save results\")\n","print(\"=\"*60)\n","\n","# Save normalized data\n","output_file = proc_dir / \"normalized.parquet\"\n","\n","# Delete existing directory/file (avoid duplicate appends)\n","if output_file.exists():\n","    import shutil\n","    if output_file.is_dir():\n","        shutil.rmtree(output_file)\n","    else:\n","        output_file.unlink()\n","    print(f\"Removed old data: {output_file}\")\n","\n","df_normalized.to_parquet(\n","    output_file,\n","    index=False,\n","    partition_cols=['subject_id', 'placement'],\n","    engine='pyarrow'\n",")\n","print(f\"✓ Saved: {output_file}\")\n","print(f\"  Data shape: {df_normalized.shape}\")\n","\n","# Show data preview\n","print(\"\\nData preview:\")\n","display_cols = ['subject_id', 'session_id', 'ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag', 'label']\n","available_cols = [c for c in display_cols if c in df_normalized.columns]\n","print(df_normalized[available_cols].head(10).to_string())\n","\n","# Post-standardization numeric stats (overall)\n","print(\"\\nPost-standardization numeric column stats (overall):\")\n","numeric_cols = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","valid_mask = df_normalized[numeric_cols].notna().all(axis=1)\n","print(df_normalized.loc[valid_mask, numeric_cols].describe().round(4))\n","\n","# ========== 6. Save scaler parameters ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Save scaler parameters\")\n","print(\"=\"*60)\n","\n","scaler_info = {\n","    'fold_id': None,             # global\n","    'epsilon': EPSILON,\n","    'train_subjects': sorted(list(train_subjects)),\n","    'test_subjects': None,       # not defined at this step\n","    'channels': channels_to_normalize,\n","    'params': scaler_params,\n","    'notes': [\n","        'z-score standardization: (x - mean) / (std + ε)',\n","        'Mean and std computed from all available samples (global statistics)',\n","        'If std < ε, set std = 1.0 to avoid divide-by-zero',\n","        'NaN values are excluded from stats and remain NaN after normalization',\n","    ]\n","}\n","\n","# Save as pickle (global)\n","scaler_file = proc_dir / \"standardization.pkl\"\n","with open(scaler_file, 'wb') as f:\n","    pickle.dump(scaler_info, f)\n","print(f\"✓ Saved scaler: {scaler_file}\")\n","\n","# Also save as JSON (human-readable)\n","scaler_json = proc_dir / \"standardization.json\"\n","with open(scaler_json, 'w') as f:\n","    json.dump(scaler_info, f, indent=2)\n","print(f\"✓ Saved scaler: {scaler_json}\")\n","\n","# ========== 7. Validate standardization ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"7. Validate standardization (global)\")\n","print(\"=\"*60)\n","\n","for ch in channels_to_normalize[:3]:  # check first 3 channels only\n","    if ch in scaler_params:\n","        valid_data = df_normalized[ch].dropna()\n","        if len(valid_data) > 0:\n","            mean_check = valid_data.mean()\n","            std_check = valid_data.std()\n","            print(f\"  {ch}: mean={mean_check:.6f}, std={std_check:.6f}\")\n","\n","# ========== 8. Summary ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 7 complete - Coordinate/Magnitude Normalization (global)\")\n","print(\"=\"*60)\n","print(f\"\\nConfig:\")\n","print(f\"  Method: z-score standardization (global)\")\n","print(f\"  ε (avoid divide-by-zero): {EPSILON}\")\n","print(f\"  Standardized channels: {len(scaler_params)}\")\n","print(f\"\\nResults:\")\n","print(f\"  Output data: {output_file}\")\n","print(f\"  Scaler (pkl): {scaler_file}\")\n","print(f\"  Scaler (json): {scaler_json}\")\n","print(f\"  Data shape: {df_normalized.shape}\")\n","print(f\"  New columns: acc_mag, gyr_mag\")\n","print(\"\\nRigor guarantees:\")\n","print(\"  1. ✓ Mean/std computed once on all data (global stats)\")\n","print(\"  2. ✓ NaNs remain unchanged\")\n","print(\"  3. ✓ ε={} prevents divide-by-zero\".format(EPSILON))\n","print(\"  4. ✓ Derived channels acc_mag, gyr_mag\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cxoWJ07C_Q3p","executionInfo":{"status":"ok","timestamp":1763470626920,"user_tz":0,"elapsed":611,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"5b082cba-85ea-432f-cd4d-00fee5d8d87d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 7: Coordinate/Magnitude Normalization\n","============================================================\n","\n","Loading filtered data: data/lara/mbientlab/proc/filtered.parquet\n","Data shape: (560070, 13)\n","Number of subjects: 8\n","Number of sessions: 96\n","\n","============================================================\n","1. Compute derived channels (magnitude)\n","============================================================\n","\n","Computing acc_mag = sqrt(ax² + ay² + az²)...\n","Computing gyr_mag = sqrt(gx² + gy² + gz²)...\n","✓ Added derived channels: acc_mag, gyr_mag\n","\n","Derived channel statistics (post-filter):\n","  acc_mag:\n","    Mean: 2.9151\n","    Std: 2.3751\n","    Range: [0.0073, 14.5074]\n","  gyr_mag:\n","    Mean: 1.3122\n","    Std: 1.1772\n","    Range: [0.0015, 7.0648]\n","\n","============================================================\n","2. Determine training set (global)\n","============================================================\n","Compute statistics on all data (no per-fold split)\n","  Samples: 560,070\n","  Subjects: 8\n","\n","============================================================\n","3. Compute z-score parameters (global)\n","============================================================\n","\n","z-score parameters (global):\n","  ax:\n","    Mean: -0.008808\n","    Std: 2.274599\n","  ay:\n","    Mean: 0.006988\n","    Std: 2.146420\n","  az:\n","    Mean: 0.001384\n","    Std: 2.087749\n","  gx:\n","    Mean: -0.002287\n","    Std: 0.886048\n","  gy:\n","    Mean: 0.010105\n","    Std: 1.160388\n","  gz:\n","    Mean: 0.023344\n","    Std: 0.987726\n","  acc_mag:\n","    Mean: 2.915099\n","    Std: 2.375277\n","  gyr_mag:\n","    Mean: 1.312169\n","    Std: 1.177305\n","\n","============================================================\n","4. Apply z-score standardization\n","============================================================\n","✓ Standardized 8 channels\n","\n","Post-standardization stats (global):\n","  ax:\n","    Mean: -0.000000 (should be near 0)\n","    Std: 0.999755 (should be near 1)\n","  ay:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999796 (should be near 1)\n","  az:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999753 (should be near 1)\n","  gx:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999748 (should be near 1)\n","  gy:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999680 (should be near 1)\n","  gz:\n","    Mean: -0.000000 (should be near 0)\n","    Std: 0.999743 (should be near 1)\n","  acc_mag:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999905 (should be near 1)\n","  gyr_mag:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999909 (should be near 1)\n","\n","============================================================\n","5. Save results\n","============================================================\n","Removed old data: data/lara/mbientlab/proc/normalized.parquet\n","✓ Saved: data/lara/mbientlab/proc/normalized.parquet\n","  Data shape: (560070, 15)\n","\n","Data preview:\n","  subject_id session_id        ax        ay        az        gx        gy        gz   acc_mag   gyr_mag  label\n","0        S07        R03  0.395624 -1.242715  0.113459  0.031125  0.366984 -0.184342 -0.041821 -0.719891    6.0\n","1        S07        R03  0.318244 -1.326593  0.114031  0.069269  0.394669 -0.245474  0.009993 -0.672706    6.0\n","2        S07        R03  0.466002 -1.672217  0.641951  0.050763  0.215964 -0.367815  0.442886 -0.748857    6.0\n","3        S07        R03  0.842548 -1.974861  1.018318  0.123879  0.078289 -0.436465  0.922482 -0.746250    6.0\n","4        S07        R03  1.049100 -2.118226  1.323202  0.299475  0.150572 -0.471352  1.223958 -0.650154    6.0\n","5        S07        R03  0.944730 -2.207665  1.371639  0.536319  0.288670 -0.555932  1.269686 -0.446192    6.0\n","6        S07        R03  0.672562 -2.261497  1.500974  0.777181  0.373010 -0.644765  1.285975 -0.246819    6.0\n","7        S07        R03  0.613387 -2.311484  1.579912  0.939153  0.324421 -0.701175  1.345937 -0.151343    6.0\n","8        S07        R03  0.639085 -2.495260  1.670510  1.101326  0.156542 -0.762265  1.529302 -0.068443    6.0\n","9        S07        R03  0.614563 -2.944599  1.719538  1.373805  0.128417 -0.824382  1.886048  0.124236    6.0\n","\n","Post-standardization numeric column stats (overall):\n","                ax           ay           az           gx           gy  \\\n","count  560070.0000  560070.0000  560070.0000  560070.0000  560070.0000   \n","mean       -0.0000       0.0000       0.0000       0.0000       0.0000   \n","std         0.9998       0.9998       0.9998       0.9997       0.9997   \n","min        -3.9819      -3.7231      -3.8163      -3.8172      -4.1142   \n","25%        -0.3961      -0.4292      -0.4354      -0.3904      -0.3387   \n","50%         0.0106       0.0033       0.0002       0.0050      -0.0142   \n","75%         0.4063       0.4390       0.4324       0.4057       0.3163   \n","max         4.0030       3.7298       3.8168       3.8272       4.0858   \n","\n","                gz      acc_mag      gyr_mag  \n","count  560070.0000  560070.0000  560070.0000  \n","mean       -0.0000       0.0000       0.0000  \n","std         0.9997       0.9999       0.9999  \n","min        -4.0216      -1.2242      -1.1133  \n","25%        -0.3598      -0.7161      -0.7281  \n","50%        -0.0147      -0.2953      -0.3247  \n","75%         0.3648       0.4121       0.4094  \n","max         3.9921       4.8804       4.8863  \n","\n","============================================================\n","6. Save scaler parameters\n","============================================================\n","✓ Saved scaler: data/lara/mbientlab/proc/standardization.pkl\n","✓ Saved scaler: data/lara/mbientlab/proc/standardization.json\n","\n","============================================================\n","7. Validate standardization (global)\n","============================================================\n","  ax: mean=-0.000000, std=0.999755\n","  ay: mean=0.000000, std=0.999796\n","  az: mean=0.000000, std=0.999753\n","\n","============================================================\n","Step 7 complete - Coordinate/Magnitude Normalization (global)\n","============================================================\n","\n","Config:\n","  Method: z-score standardization (global)\n","  ε (avoid divide-by-zero): 1e-08\n","  Standardized channels: 8\n","\n","Results:\n","  Output data: data/lara/mbientlab/proc/normalized.parquet\n","  Scaler (pkl): data/lara/mbientlab/proc/standardization.pkl\n","  Scaler (json): data/lara/mbientlab/proc/standardization.json\n","  Data shape: (560070, 15)\n","  New columns: acc_mag, gyr_mag\n","\n","Rigor guarantees:\n","  1. ✓ Mean/std computed once on all data (global stats)\n","  2. ✓ NaNs remain unchanged\n","  3. ✓ ε=1e-08 prevents divide-by-zero\n","  4. ✓ Derived channels acc_mag, gyr_mag\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\n","\"\"\"\n","Step 8: Label Alignment & Cleaning (top-conf/journal grade - revised)\n","Clean NULL/transition, unify to a standard label set, and record mappings\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import json\n","from collections import Counter\n","\n","# ========== Config ==========\n","\n","# Label cleaning strategy\n","NULL_STRATEGY = \"remove\"  # \"remove\" or \"merge_to_transition\"\n","TRANSITION_STRATEGY = \"merge_to_nearest\"  # \"remove\" or \"merge_to_nearest\"\n","\n","# Unmapped label threshold (abort if exceeded)\n","UNMAPPED_THRESHOLD = 0.01  # 1%\n","\n","print(\"=\"*60)\n","print(\"Step 8: Label Alignment & Cleaning\")\n","print(\"=\"*60)\n","\n","# Create directories\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","reports_dir = Path(\"reports\")\n","reports_dir.mkdir(parents=True, exist_ok=True)\n","\n","print(f\"\\nLoading normalized data: {proc_dir / 'normalized.parquet'}\")\n","df = pd.read_parquet(proc_dir / \"normalized.parquet\")\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Number of subjects: {df['subject_id'].nunique()}\")\n","\n","# ========== 1. Analyze original label distribution ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Analyze original label distribution\")\n","print(\"=\"*60)\n","\n","# Count all labels\n","label_counts = df['label'].value_counts(dropna=False)\n","total_samples = len(df)\n","null_count = df['label'].isna().sum()\n","\n","print(f\"\\nOriginal label stats:\")\n","print(f\"  Total samples: {total_samples:,}\")\n","print(f\"  NULL samples: {null_count:,} ({null_count/total_samples*100:.2f}%)\")\n","print(f\"  Number of label classes: {df['label'].nunique(dropna=True)}\")\n","\n","print(f\"\\nLabel distribution (top 20):\")\n","for label, count in label_counts.head(20).items():\n","    pct = count / total_samples * 100\n","    print(f\"  {str(label):30s}: {count:8,} ({pct:5.2f}%)\")\n","\n","# ========== 2. Define label mapping rules ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Define label mapping rules\")\n","print(\"=\"*60)\n","\n","# Map LARa dataset labels to a cross-dataset unified label superset\n","# Covers LARa / RealWorld / SHL\n","LABEL_MAPPING = {\n","    # Basic activities (shared by RealWorld + LARa)\n","    1: {\"original\": \"walking\", \"mapped\": \"walking\", \"category\": \"locomotion\"},\n","    2: {\"original\": \"running\", \"mapped\": \"running\", \"category\": \"locomotion\"},\n","    3: {\"original\": \"shuffling\", \"mapped\": \"walking\", \"category\": \"locomotion\"},  # merge into walking\n","    4: {\"original\": \"stairs (ascending)\", \"mapped\": \"upstairs\", \"category\": \"locomotion\"},\n","    5: {\"original\": \"stairs (descending)\", \"mapped\": \"downstairs\", \"category\": \"locomotion\"},\n","    6: {\"original\": \"standing\", \"mapped\": \"standing\", \"category\": \"static\"},\n","    7: {\"original\": \"sitting\", \"mapped\": \"sitting\", \"category\": \"static\"},\n","    8: {\"original\": \"lying\", \"mapped\": \"lying\", \"category\": \"static\"},\n","\n","    # Transport (specific to LARa; not in RealWorld)\n","    13: {\"original\": \"cycling (sit)\", \"mapped\": \"cycling\", \"category\": \"transport\"},\n","    14: {\"original\": \"cycling (stand)\", \"mapped\": \"cycling\", \"category\": \"transport\"},\n","    130: {\"original\": \"cycling\", \"mapped\": \"cycling\", \"category\": \"transport\"},\n","\n","    17: {\"original\": \"car\", \"mapped\": \"car\", \"category\": \"transport\"},\n","    18: {\"original\": \"bus\", \"mapped\": \"bus\", \"category\": \"transport\"},\n","    19: {\"original\": \"train\", \"mapped\": \"train\", \"category\": \"transport\"},\n","    20: {\"original\": \"subway\", \"mapped\": \"subway\", \"category\": \"transport\"},\n","\n","    # Transition label\n","    0: {\"original\": \"transition\", \"mapped\": \"transition\", \"category\": \"transition\"},\n","}\n","\n","# Cross-dataset unified label superset (LARa + RealWorld + SHL)\n","UNIFIED_LABELS = {\n","    \"walking\": 1,\n","    \"running\": 2,\n","    \"sitting\": 3,\n","    \"standing\": 4,\n","    \"upstairs\": 5,\n","    \"downstairs\": 6,\n","    \"lying\": 7,\n","    \"cycling\": 8,\n","    \"car\": 9,\n","    \"bus\": 10,\n","    \"train\": 11,\n","    \"subway\": 12,\n","    \"transition\": 0,  # kept or cleaned\n","}\n","\n","print(f\"\\nDefined mapping rules: {len(LABEL_MAPPING)} original labels\")\n","print(f\"Unified label set: {len(UNIFIED_LABELS)} labels (cross-dataset superset)\")\n","\n","print(f\"\\nMapping examples:\")\n","for orig_id, info in list(LABEL_MAPPING.items())[:10]:\n","    print(f\"  {orig_id} ({info['original']}) -> {info['mapped']}\")\n","\n","# ========== 3. Audit assertion: check unmapped labels ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Audit assertion: check unmapped labels\")\n","print(\"=\"*60)\n","\n","# Find all original label IDs (excluding NULL)\n","orig_ids = set(df['label'].dropna().astype(int).unique())\n","covered_ids = set(LABEL_MAPPING.keys())\n","unmapped_ids = sorted(orig_ids - covered_ids)\n","\n","if unmapped_ids:\n","    # Count samples for unmapped labels\n","    unmapped_counts = []\n","    for uid in unmapped_ids:\n","        count = (df['label'] == uid).sum()\n","        pct = count / total_samples\n","        unmapped_counts.append({\n","            'original_label_id': uid,\n","            'sample_count': count,\n","            'percentage': round(pct * 100, 4),\n","        })\n","\n","    df_unmapped = pd.DataFrame(unmapped_counts)\n","    total_unmapped = df_unmapped['sample_count'].sum()\n","    unmapped_ratio = total_unmapped / total_samples\n","\n","    # Save list of unmapped labels\n","    unmapped_file = reports_dir / \"unmapped_labels.csv\"\n","    df_unmapped.to_csv(unmapped_file, index=False)\n","\n","    print(f\"\\n⚠️ Found unmapped labels: {len(unmapped_ids)}\")\n","    print(f\"  Unmapped sample count: {total_unmapped:,} ({unmapped_ratio*100:.2f}%)\")\n","    print(f\"  Details saved to: {unmapped_file}\")\n","    print(f\"\\nList of unmapped labels:\")\n","    print(df_unmapped.to_string(index=False))\n","\n","    # Abort if threshold exceeded\n","    if unmapped_ratio > UNMAPPED_THRESHOLD:\n","        raise RuntimeError(\n","            f\"Unmapped label ratio {unmapped_ratio*100:.2f}% exceeds threshold {UNMAPPED_THRESHOLD*100}%. \"\n","            f\"Please check {unmapped_file} and extend LABEL_MAPPING.\"\n","        )\n","    else:\n","        print(f\"\\n✓ Unmapped label ratio does not exceed threshold {UNMAPPED_THRESHOLD*100}%; continuing (will mark as NULL)\")\n","else:\n","    print(f\"\\n✓ All original labels are covered\")\n","\n","# ========== 4. Apply label mapping ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Apply label mapping\")\n","print(\"=\"*60)\n","\n","df_mapped = df.copy()\n","\n","# Keep a copy of original labels (nullable integer)\n","df_mapped['label_original'] = df_mapped['label'].astype('Int32')\n","\n","# Apply mapping\n","def map_label(label):\n","    \"\"\"Map a single label\"\"\"\n","    if pd.isna(label):\n","        return np.nan\n","\n","    label = int(label)\n","    if label in LABEL_MAPPING:\n","        mapped_name = LABEL_MAPPING[label]['mapped']\n","        return UNIFIED_LABELS[mapped_name]\n","    else:\n","        # Unknown labels marked as NaN\n","        return np.nan\n","\n","df_mapped['label'] = df_mapped['label_original'].apply(map_label)\n","\n","# Stats after mapping\n","mapped_label_counts = df_mapped['label'].value_counts(dropna=False)\n","null_after_mapping = df_mapped['label'].isna().sum()\n","\n","print(f\"\\nPost-mapping label stats:\")\n","print(f\"  NULL samples: {null_after_mapping:,} ({null_after_mapping/total_samples*100:.2f}%)\")\n","print(f\"  Number of valid label classes: {df_mapped['label'].nunique(dropna=True)}\")\n","\n","print(f\"\\nPost-mapping distribution:\")\n","for label, count in mapped_label_counts.head(15).items():\n","    pct = count / total_samples * 100\n","    # find label name\n","    label_name = \"NULL\"\n","    if not pd.isna(label):\n","        label_name = [k for k, v in UNIFIED_LABELS.items() if v == int(label)][0]\n","    print(f\"  {label_name:15s} ({str(label):2s}): {count:8,} ({pct:5.2f}%)\")\n","\n","# ========== 5. Clean NULL and transition labels (true nearest neighbor) ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Clean NULL and transition labels (true nearest neighbor)\")\n","print(\"=\"*60)\n","\n","df_cleaned = df_mapped.copy()\n","\n","# Handle NULL labels\n","if NULL_STRATEGY == \"remove\":\n","    null_mask = df_cleaned['label'].isna()\n","    removed_null = null_mask.sum()\n","    df_cleaned = df_cleaned[~null_mask].copy()\n","    print(f\"\\nNULL handling: removed {removed_null:,} samples\")\n","elif NULL_STRATEGY == \"merge_to_transition\":\n","    null_mask = df_cleaned['label'].isna()\n","    df_cleaned.loc[null_mask, 'label'] = UNIFIED_LABELS['transition']\n","    print(f\"\\nNULL handling: merged into transition ({null_mask.sum():,} samples)\")\n","\n","# Detect time column\n","time_col = None\n","for candidate in ['time_sec', 'timestamp', 'timestamp_ms', 'time', 'epoch_ms']:\n","    if candidate in df_cleaned.columns:\n","        time_col = candidate\n","        break\n","\n","if time_col:\n","    print(f\"\\nDetected time column: {time_col}\")\n","else:\n","    print(f\"\\nNo time column detected; will process by index order\")\n","\n","# Handle transition label (true nearest neighbor)\n","transition_value = UNIFIED_LABELS['transition']\n","if TRANSITION_STRATEGY == \"remove\":\n","    trans_mask = df_cleaned['label'] == transition_value\n","    removed_trans = trans_mask.sum()\n","    df_cleaned = df_cleaned[~trans_mask].copy()\n","    print(f\"Transition handling: removed {removed_trans:,} samples\")\n","\n","elif TRANSITION_STRATEGY == \"merge_to_nearest\":\n","    trans_mask = df_cleaned['label'] == transition_value\n","    trans_count = trans_mask.sum()\n","\n","    if trans_count > 0:\n","        print(f\"Transition handling: merge {trans_count:,} samples using nearest-neighbor interpolation\")\n","\n","        # Sort by time (ensure nearest-neighbor semantics)\n","        if time_col:\n","            df_cleaned = df_cleaned.sort_values(\n","                ['subject_id', 'session_id', 'placement', time_col],\n","                kind='stable'\n","            ).copy()\n","            print(f\"  ✓ Sorted by [{time_col}]\")\n","        else:\n","            df_cleaned = df_cleaned.sort_index(kind='stable').copy()\n","            print(f\"  ⚠️ Sorted by index (no time column)\")\n","\n","        # True nearest-neighbor merge\n","        merged_count = 0\n","        for (subj, sess, plc), group in df_cleaned.groupby(\n","            ['subject_id', 'session_id', 'placement'], observed=True\n","        ):\n","            idx = group.index\n","            labels = df_cleaned.loc[idx, 'label'].copy()\n","\n","            # Replace transition with NaN\n","            labels_with_nan = labels.replace(transition_value, np.nan).astype('float')\n","\n","            if labels_with_nan.isna().any():\n","                # Use nearest interpolation (true nearest neighbor)\n","                labels_filled = labels_with_nan.interpolate(\n","                    method='nearest',\n","                    limit_direction='both'\n","                )\n","\n","                # Count successfully merged items\n","                was_trans = (labels == transition_value)\n","                now_filled = labels_filled.notna()\n","                merged_this_group = (was_trans & now_filled).sum()\n","                merged_count += merged_this_group\n","\n","                # Update labels (round then cast to int)\n","                df_cleaned.loc[idx, 'label'] = labels_filled.round()\n","\n","        print(f\"  ✓ Successfully merged {merged_count:,} transition samples to nearest labels\")\n","\n","        # Remove transitions that could not be merged (entire segments are transition)\n","        remaining_trans = (df_cleaned['label'] == transition_value).sum()\n","        if remaining_trans > 0:\n","            df_cleaned = df_cleaned[df_cleaned['label'] != transition_value].copy()\n","            print(f\"  ✓ Removed remaining {remaining_trans:,} transition samples that could not be merged\")\n","\n","# Remove remaining NaNs\n","final_nan = df_cleaned['label'].isna().sum()\n","if final_nan > 0:\n","    df_cleaned = df_cleaned[df_cleaned['label'].notna()].copy()\n","    print(f\"\\nRemoved final residual NaN samples: {final_nan:,}\")\n","\n","# Cast to int32\n","df_cleaned['label'] = df_cleaned['label'].astype('int32')\n","\n","# Reset index\n","df_cleaned = df_cleaned.reset_index(drop=True)\n","\n","print(f\"\\nData after cleaning:\")\n","print(f\"  Samples: {len(df_cleaned):,}\")\n","print(f\"  Number of label classes: {df_cleaned['label'].nunique()}\")\n","print(f\"  Retention rate: {len(df_cleaned)/total_samples*100:.2f}%\")\n","\n","# ========== 6. Audit assertion: verify final label set ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Audit assertion: verify final label set\")\n","print(\"=\"*60)\n","\n","# Determine allowed label set\n","allowed_labels = set(UNIFIED_LABELS.values())\n","if TRANSITION_STRATEGY == \"remove\":\n","    allowed_labels.discard(UNIFIED_LABELS['transition'])\n","\n","# Check actual label set\n","actual_labels = set(df_cleaned['label'].unique())\n","unexpected = sorted(actual_labels - allowed_labels)\n","\n","if unexpected:\n","    raise RuntimeError(\n","        f\"Illegal labels found after cleaning: {unexpected}\\n\"\n","        f\"Allowed labels: {sorted(allowed_labels)}\"\n","    )\n","else:\n","    print(f\"✓ Final label set validation passed\")\n","    print(f\"  Allowed labels: {sorted(allowed_labels)}\")\n","    print(f\"  Actual labels: {sorted(actual_labels)}\")\n","\n","# ========== 7. Final label distribution ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"7. Final label distribution\")\n","print(\"=\"*60)\n","\n","final_label_counts = df_cleaned['label'].value_counts()\n","\n","print(f\"\\nFinal label distribution:\")\n","for label_id, count in final_label_counts.items():\n","    pct = count / len(df_cleaned) * 100\n","    label_name = [k for k, v in UNIFIED_LABELS.items() if v == int(label_id)][0]\n","    print(f\"  {label_name:15s} ({int(label_id):2d}): {count:8,} ({pct:5.2f}%)\")\n","\n","# By-category statistics\n","category_stats = {}\n","for label_id, count in final_label_counts.items():\n","    label_name = [k for k, v in UNIFIED_LABELS.items() if v == int(label_id)][0]\n","    # Find category\n","    category = None\n","    for orig_id, info in LABEL_MAPPING.items():\n","        if info['mapped'] == label_name:\n","            category = info['category']\n","            break\n","\n","    if category:\n","        category_stats[category] = category_stats.get(category, 0) + count\n","\n","print(f\"\\nBy-category statistics:\")\n","for category, count in sorted(category_stats.items()):\n","    pct = count / len(df_cleaned) * 100\n","    print(f\"  {category:15s}: {count:8,} ({pct:5.2f}%)\")\n","\n","# ========== 8. Save results ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"8. Save results\")\n","print(\"=\"*60)\n","\n","# Save cleaned data (using directory layout)\n","output_dir = proc_dir / \"labeled\"\n","if output_dir.exists():\n","    import shutil\n","    shutil.rmtree(output_dir)\n","\n","df_cleaned.to_parquet(\n","    output_dir,\n","    index=False,\n","    partition_cols=['subject_id', 'placement'],\n","    engine='pyarrow'\n",")\n","\n","print(f\"✓ Saved: {output_dir}/\")\n","print(f\"  Data shape: {df_cleaned.shape}\")\n","print(f\"  Partitions: subject_id / placement\")\n","\n","# ========== 9. Save label mapping config (rich) ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"9. Save label mapping config (rich)\")\n","print(\"=\"*60)\n","\n","# Build labels_map with more info\n","labels_map_data = []\n","for label_name, label_id in sorted(UNIFIED_LABELS.items(), key=lambda x: x[1]):\n","    if label_name == \"transition\" and TRANSITION_STRATEGY == \"remove\":\n","        continue  # exclude removed transition\n","\n","    # Find original label IDs and names\n","    original_ids = []\n","    original_names = []\n","    category = None\n","\n","    for orig_id, info in LABEL_MAPPING.items():\n","        if info['mapped'] == label_name:\n","            original_ids.append(str(orig_id))\n","            original_names.append(info['original'])\n","            if category is None:\n","                category = info['category']\n","\n","    # Actual sample count\n","    sample_count = final_label_counts.get(label_id, 0)\n","\n","    labels_map_data.append({\n","        'label_id': label_id,\n","        'label_name': label_name,\n","        'category': category or 'unknown',\n","        'sample_count': int(sample_count),\n","        'percentage': round(sample_count / len(df_cleaned) * 100, 2) if len(df_cleaned) > 0 else 0.0,\n","        'original_label_ids': ','.join(original_ids) if original_ids else '',\n","        'original_label_names': '; '.join(original_names) if original_names else '',\n","        'source_dataset': 'LARa-MbientLab',\n","        'description': f\"{label_name} activity\",\n","    })\n","\n","df_labels_map = pd.DataFrame(labels_map_data)\n","labels_map_file = proc_dir / \"labels_map.csv\"\n","df_labels_map.to_csv(labels_map_file, index=False)\n","\n","print(f\"✓ Saved label mapping: {labels_map_file}\")\n","print(f\"\\nLabel mapping table:\")\n","print(df_labels_map.to_string(index=False))\n","\n","# Save detailed configuration\n","label_config = {\n","    'dataset': 'LARa-MbientLab',\n","    'label_system': 'Cross-dataset unified label superset (covers LARa/RealWorld/SHL)',\n","    'unified_labels': UNIFIED_LABELS,\n","    'label_mapping': LABEL_MAPPING,\n","    'cleaning_strategy': {\n","        'null_strategy': NULL_STRATEGY,\n","        'transition_strategy': TRANSITION_STRATEGY,\n","        'transition_method': 'nearest-neighbor interpolation (true nearest neighbor)' if TRANSITION_STRATEGY == 'merge_to_nearest' else 'remove',\n","        'time_sorted': time_col is not None,\n","        'time_column': time_col,\n","        'unmapped_threshold': UNMAPPED_THRESHOLD,\n","    },\n","    'statistics': {\n","        'original_samples': int(total_samples),\n","        'cleaned_samples': int(len(df_cleaned)),\n","        'removed_samples': int(total_samples - len(df_cleaned)),\n","        'removal_rate': float((total_samples - len(df_cleaned)) / total_samples),\n","        'original_label_count': int(df['label'].nunique(dropna=True)),\n","        'final_label_count': int(df_cleaned['label'].nunique()),\n","        'unmapped_label_count': len(unmapped_ids) if unmapped_ids else 0,\n","    },\n","    'label_distribution': {\n","        label_name: int(final_label_counts.get(label_id, 0))\n","        for label_name, label_id in UNIFIED_LABELS.items()\n","        if label_name != 'transition' or TRANSITION_STRATEGY != 'remove'\n","    },\n","    'notes': [\n","        'Label mapping based on cross-dataset unified label superset (LARa + RealWorld + SHL)',\n","        f'NULL label strategy: {NULL_STRATEGY}',\n","        f'Transition label strategy: {TRANSITION_STRATEGY} (true nearest-neighbor interpolation)',\n","        'Unmapped original labels are automatically marked as NULL',\n","        f'Unmapped label threshold: {UNMAPPED_THRESHOLD*100}%',\n","        f'Sorted by time column: {time_col if time_col else \"No (by index)\"}',\n","        'Mapping table saved at proc/labels_map.csv',\n","        'label_original column uses nullable integer Int32',\n","        'Includes audit assertions to ensure label set integrity',\n","    ]\n","}\n","\n","label_config_file = configs_dir / \"labels.yaml\"\n","with open(label_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(label_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","print(f\"✓ Saved config: {label_config_file}\")\n","\n","label_config_json = configs_dir / \"labels.json\"\n","with open(label_config_json, 'w', encoding='utf-8') as f:\n","    json.dump(label_config, f, indent=2)\n","\n","print(f\"✓ Saved config: {label_config_json}\")\n","\n","# ========== 10. Summary ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 8 complete - Label alignment & cleaning (top-tier revised)\")\n","print(\"=\"*60)\n","\n","print(f\"\\nConfig:\")\n","print(f\"  Label system: cross-dataset unified superset (LARa/RealWorld/SHL)\")\n","print(f\"  NULL strategy: {NULL_STRATEGY}\")\n","print(f\"  Transition strategy: {TRANSITION_STRATEGY} (true nearest neighbor)\")\n","print(f\"  Unmapped threshold: {UNMAPPED_THRESHOLD*100}%\")\n","print(f\"  Time column: {time_col if time_col else 'No (by index)'}\")\n","\n","print(f\"\\nResults:\")\n","print(f\"  Original samples: {total_samples:,}\")\n","print(f\"  Cleaned samples: {len(df_cleaned):,}\")\n","print(f\"  Removed samples: {total_samples - len(df_cleaned):,}\")\n","print(f\"  Retention rate: {len(df_cleaned)/total_samples*100:.2f}%\")\n","\n","print(f\"\\nLabel stats:\")\n","print(f\"  Original label classes: {df['label'].nunique(dropna=True)}\")\n","print(f\"  Final label classes: {df_cleaned['label'].nunique()}\")\n","print(f\"  Unmapped labels: {len(unmapped_ids) if unmapped_ids else 0}\")\n","\n","print(f\"\\nOutputs:\")\n","print(f\"  Data: {output_dir}/\")\n","print(f\"  Mapping table: {labels_map_file}\")\n","print(f\"  Config: {label_config_file}\")\n","if unmapped_ids:\n","    print(f\"  Unmapped list: {reports_dir / 'unmapped_labels.csv'}\")\n","\n","print(\"\\nKey fixes (top-tier):\")\n","print(\"  1. ✓ True nearest-neighbor merge (interpolate method='nearest')\")\n","print(\"  2. ✓ Sort by time before processing (correct semantics)\")\n","print(\"  3. ✓ Record unmapped labels to reports/unmapped_labels.csv\")\n","print(\"  4. ✓ label_original uses nullable Int32\")\n","print(\"  5. ✓ Removed irrelevant MAJORITY_VOTE_THRESHOLD\")\n","print(\"  6. ✓ Audit assertions (fail-fast)\")\n","print(\"  7. ✓ labels_map.csv includes original names and source\")\n","print(\"  8. ✓ Label system described as cross-dataset superset\")\n","print(\"  9. ✓ Output directory changed to labeled/\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NZd1vu8R_TAP","executionInfo":{"status":"ok","timestamp":1763470628465,"user_tz":0,"elapsed":1539,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"4cea8f0c-038e-428f-c182-069e05f528f8"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 8: Label Alignment & Cleaning\n","============================================================\n","\n","Loading normalized data: data/lara/mbientlab/proc/normalized.parquet\n","Data shape: (560070, 15)\n","Number of subjects: 8\n","\n","============================================================\n","1. Analyze original label distribution\n","============================================================\n","\n","Original label stats:\n","  Total samples: 560,070\n","  NULL samples: 28 (0.00%)\n","  Number of label classes: 8\n","\n","Label distribution (top 20):\n","  4.0                           :  215,988 (38.56%)\n","  2.0                           :   86,132 (15.38%)\n","  0.0                           :   73,180 (13.07%)\n","  7.0                           :   48,927 ( 8.74%)\n","  5.0                           :   43,966 ( 7.85%)\n","  1.0                           :   42,039 ( 7.51%)\n","  3.0                           :   38,224 ( 6.82%)\n","  6.0                           :   11,586 ( 2.07%)\n","  nan                           :       28 ( 0.00%)\n","\n","============================================================\n","2. Define label mapping rules\n","============================================================\n","\n","Defined mapping rules: 16 original labels\n","Unified label set: 13 labels (cross-dataset superset)\n","\n","Mapping examples:\n","  1 (walking) -> walking\n","  2 (running) -> running\n","  3 (shuffling) -> walking\n","  4 (stairs (ascending)) -> upstairs\n","  5 (stairs (descending)) -> downstairs\n","  6 (standing) -> standing\n","  7 (sitting) -> sitting\n","  8 (lying) -> lying\n","  13 (cycling (sit)) -> cycling\n","  14 (cycling (stand)) -> cycling\n","\n","============================================================\n","3. Audit assertion: check unmapped labels\n","============================================================\n","\n","✓ All original labels are covered\n","\n","============================================================\n","4. Apply label mapping\n","============================================================\n","\n","Post-mapping label stats:\n","  NULL samples: 28 (0.00%)\n","  Number of valid label classes: 7\n","\n","Post-mapping distribution:\n","  upstairs        (5.0):  215,988 (38.56%)\n","  running         (2.0):   86,132 (15.38%)\n","  walking         (1.0):   80,263 (14.33%)\n","  transition      (0.0):   73,180 (13.07%)\n","  sitting         (3.0):   48,927 ( 8.74%)\n","  downstairs      (6.0):   43,966 ( 7.85%)\n","  standing        (4.0):   11,586 ( 2.07%)\n","  NULL            (nan):       28 ( 0.00%)\n","\n","============================================================\n","5. Clean NULL and transition labels (true nearest neighbor)\n","============================================================\n","\n","NULL handling: removed 28 samples\n","\n","Detected time column: time_sec\n","Transition handling: merge 73,180 samples using nearest-neighbor interpolation\n","  ✓ Sorted by [time_sec]\n","  ✓ Successfully merged 69,642 transition samples to nearest labels\n","\n","Removed final residual NaN samples: 3,538\n","\n","Data after cleaning:\n","  Samples: 556,504\n","  Number of label classes: 6\n","  Retention rate: 99.36%\n","\n","============================================================\n","6. Audit assertion: verify final label set\n","============================================================\n","✓ Final label set validation passed\n","  Allowed labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n","  Actual labels: [np.int32(1), np.int32(2), np.int32(3), np.int32(4), np.int32(5), np.int32(6)]\n","\n","============================================================\n","7. Final label distribution\n","============================================================\n","\n","Final label distribution:\n","  upstairs        ( 5):  255,038 (45.83%)\n","  walking         ( 1):   96,957 (17.42%)\n","  running         ( 2):   89,302 (16.05%)\n","  sitting         ( 3):   51,931 ( 9.33%)\n","  downstairs      ( 6):   50,391 ( 9.05%)\n","  standing        ( 4):   12,885 ( 2.32%)\n","\n","By-category statistics:\n","  locomotion     :  491,688 (88.35%)\n","  static         :   64,816 (11.65%)\n","\n","============================================================\n","8. Save results\n","============================================================\n","✓ Saved: data/lara/mbientlab/proc/labeled/\n","  Data shape: (556504, 16)\n","  Partitions: subject_id / placement\n","\n","============================================================\n","9. Save label mapping config (rich)\n","============================================================\n","✓ Saved label mapping: data/lara/mbientlab/proc/labels_map.csv\n","\n","Label mapping table:\n"," label_id label_name   category  sample_count  percentage original_label_ids                    original_label_names source_dataset         description\n","        0 transition transition             0        0.00                  0                              transition LARa-MbientLab transition activity\n","        1    walking locomotion         96957       17.42                1,3                      walking; shuffling LARa-MbientLab    walking activity\n","        2    running locomotion         89302       16.05                  2                                 running LARa-MbientLab    running activity\n","        3    sitting     static         51931        9.33                  7                                 sitting LARa-MbientLab    sitting activity\n","        4   standing     static         12885        2.32                  6                                standing LARa-MbientLab   standing activity\n","        5   upstairs locomotion        255038       45.83                  4                      stairs (ascending) LARa-MbientLab   upstairs activity\n","        6 downstairs locomotion         50391        9.05                  5                     stairs (descending) LARa-MbientLab downstairs activity\n","        7      lying     static             0        0.00                  8                                   lying LARa-MbientLab      lying activity\n","        8    cycling  transport             0        0.00          13,14,130 cycling (sit); cycling (stand); cycling LARa-MbientLab    cycling activity\n","        9        car  transport             0        0.00                 17                                     car LARa-MbientLab        car activity\n","       10        bus  transport             0        0.00                 18                                     bus LARa-MbientLab        bus activity\n","       11      train  transport             0        0.00                 19                                   train LARa-MbientLab      train activity\n","       12     subway  transport             0        0.00                 20                                  subway LARa-MbientLab     subway activity\n","✓ Saved config: configs/labels.yaml\n","✓ Saved config: configs/labels.json\n","\n","============================================================\n","Step 8 complete - Label alignment & cleaning (top-tier revised)\n","============================================================\n","\n","Config:\n","  Label system: cross-dataset unified superset (LARa/RealWorld/SHL)\n","  NULL strategy: remove\n","  Transition strategy: merge_to_nearest (true nearest neighbor)\n","  Unmapped threshold: 1.0%\n","  Time column: time_sec\n","\n","Results:\n","  Original samples: 560,070\n","  Cleaned samples: 556,504\n","  Removed samples: 3,566\n","  Retention rate: 99.36%\n","\n","Label stats:\n","  Original label classes: 8\n","  Final label classes: 6\n","  Unmapped labels: 0\n","\n","Outputs:\n","  Data: data/lara/mbientlab/proc/labeled/\n","  Mapping table: data/lara/mbientlab/proc/labels_map.csv\n","  Config: configs/labels.yaml\n","\n","Key fixes (top-tier):\n","  1. ✓ True nearest-neighbor merge (interpolate method='nearest')\n","  2. ✓ Sort by time before processing (correct semantics)\n","  3. ✓ Record unmapped labels to reports/unmapped_labels.csv\n","  4. ✓ label_original uses nullable Int32\n","  5. ✓ Removed irrelevant MAJORITY_VOTE_THRESHOLD\n","  6. ✓ Audit assertions (fail-fast)\n","  7. ✓ labels_map.csv includes original names and source\n","  8. ✓ Label system described as cross-dataset superset\n","  9. ✓ Output directory changed to labeled/\n","============================================================\n"]}]},{"cell_type":"code","source":["import os\n","\n","\"\"\"\n","Step 9: Sliding-window Slicing (top-conf/journal grade - multi-fold version)\n","Slice with fixed window length/step; assign window label by majority label\n","For each fold in configs/splits.json, generate windows/{fold_xx}/X_train.npy, X_test.npy, etc.\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import json\n","from collections import Counter\n","\n","# ========== Config ==========\n","\n","# Sliding-window parameters\n","SAMPLING_RATE_HZ = 50.0\n","WINDOW_SIZE_SEC = 3.0\n","OVERLAP_RATIO = 0.5\n","\n","# Compute sample counts\n","WINDOW_SIZE = int(WINDOW_SIZE_SEC * SAMPLING_RATE_HZ)        # 150 samples\n","STEP_SIZE = int(WINDOW_SIZE * (1 - OVERLAP_RATIO))           # 75 samples\n","\n","# Majority label threshold\n","DOMINANT_THRESHOLD = 0.8\n","\n","# Feature columns (8 channels)\n","FEATURE_COLS = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","print(\"=\"*60)\n","print(\"Step 9: Sliding-window slicing (multi-fold)\")\n","print(\"=\"*60)\n","\n","# Base directories\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","windows_root = proc_dir / \"windows\"\n","windows_root.mkdir(parents=True, exist_ok=True)\n","\n","print(f\"\\nSliding-window parameters:\")\n","print(f\"  Window length: {WINDOW_SIZE_SEC} s = {WINDOW_SIZE} samples @ {SAMPLING_RATE_HZ} Hz\")\n","print(f\"  Step size: {STEP_SIZE} samples (overlap {OVERLAP_RATIO*100:.0f}%)\")\n","print(f\"  Dominant label threshold: {DOMINANT_THRESHOLD*100:.0f}%\")\n","print(f\"  Feature columns: {FEATURE_COLS}\")\n","\n","# ========== 1. Load data ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Load cleaned & labeled data\")\n","print(\"=\"*60)\n","\n","labeled_dir = proc_dir / \"labeled\"\n","print(f\"Loading data from: {labeled_dir}/\")\n","df = pd.read_parquet(labeled_dir)\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Number of subjects: {df['subject_id'].nunique()}\")\n","print(f\"Number of label classes: {df['label'].nunique()}\")\n","\n","# Check required columns\n","required_cols = ['subject_id', 'session_id', 'placement', 'label'] + FEATURE_COLS\n","missing_cols = [c for c in required_cols if c not in df.columns]\n","if missing_cols:\n","    raise ValueError(f\"Missing required columns: {missing_cols}\")\n","\n","# Detect time column\n","time_col = 'time_sec' if 'time_sec' in df.columns else None\n","if time_col:\n","    print(f\"Time column: {time_col}\")\n","else:\n","    print(\"No time column detected; will sort by index\")\n","\n","# ========== 2. Load splits and determine folds ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Load train/test splits (folds)\")\n","print(\"=\"*60)\n","\n","splits_path = configs_dir / \"splits.json\"\n","fold_ids = []\n","splits = None\n","\n","if splits_path.exists():\n","    with open(splits_path, \"r\") as f:\n","        splits = json.load(f)\n","\n","    # Expect keys like \"0\",\"1\",\"2\",...\n","    fold_ids = sorted(int(k) for k in splits.keys())\n","    print(f\"Detected {len(fold_ids)} folds from {splits_path}: {fold_ids}\")\n","else:\n","    # Fallback: single \"all\" fold (no LOSO config)\n","    print(\"⚠️ splits.json not found; will treat all data as a single 'all' fold\")\n","    fold_ids = [None]\n","\n","# ========== 3. Sliding-window function (with time continuity check) ==========\n","\n","def sliding_window_extract(df_subset, window_size, step_size, dominant_threshold, time_col=None):\n","    \"\"\"\n","    Perform sliding-window slicing grouped by session.\n","\n","    Returns:\n","        windows_list: list of window feature arrays\n","        metadata_list: list of window metadata dicts\n","    \"\"\"\n","    windows_list = []\n","    metadata_list = []\n","    window_id = 0\n","\n","    # Group by session + placement\n","    for (subj, sess, plc), group in df_subset.groupby(\n","        ['subject_id', 'session_id', 'placement'], observed=True\n","    ):\n","        # Sort by time column (preferred), otherwise by index\n","        if time_col and time_col in group.columns:\n","            group = group.sort_values(time_col, kind='stable').copy()\n","        else:\n","            group = group.sort_index(kind='stable').copy()\n","\n","        # Extract features and labels\n","        features = group[FEATURE_COLS].values\n","        labels = group['label'].values\n","\n","        # Extract timestamps (if any)\n","        if time_col and time_col in group.columns:\n","            timestamps = group[time_col].values\n","        else:\n","            timestamps = None\n","\n","        # Sliding-window slicing\n","        n_samples = len(group)\n","        for start_idx in range(0, n_samples - window_size + 1, step_size):\n","            end_idx = start_idx + window_size\n","\n","            # Extract window\n","            window_features = features[start_idx:end_idx]\n","            window_labels = labels[start_idx:end_idx]\n","\n","            # Check NaNs\n","            if np.isnan(window_features).any():\n","                continue\n","\n","            # Time continuity check (if timestamps exist)\n","            if timestamps is not None:\n","                expected_duration = (window_size - 1) / SAMPLING_RATE_HZ\n","                actual_duration = timestamps[end_idx - 1] - timestamps[start_idx]\n","                # Allow 10% jitter\n","                if abs(actual_duration - expected_duration) > 0.1 * expected_duration:\n","                    continue\n","\n","            # Compute dominant label\n","            label_counts = Counter(window_labels)\n","            dominant_label, dominant_count = label_counts.most_common(1)[0]\n","            dominant_ratio = dominant_count / window_size\n","\n","            # Keep only windows that meet the threshold\n","            if dominant_ratio < dominant_threshold:\n","                continue\n","\n","            # Extract time range\n","            if timestamps is not None:\n","                time_start = timestamps[start_idx]\n","                time_end = timestamps[end_idx - 1]\n","                time_range = f\"{time_start:.3f}-{time_end:.3f}\"\n","            else:\n","                time_range = f\"{start_idx}-{end_idx-1}\"\n","\n","            # Save window\n","            windows_list.append(window_features)\n","\n","            # Save metadata\n","            metadata_list.append({\n","                'window_id': window_id,\n","                'subject_id': subj,\n","                'session_id': sess,\n","                'placement': plc,\n","                'label': int(dominant_label),\n","                'label_purity': round(dominant_ratio, 4),\n","                'time_range': time_range,\n","                'start_idx': start_idx,\n","                'end_idx': end_idx,\n","            })\n","\n","            window_id += 1\n","\n","    return windows_list, metadata_list\n","\n","# ========== 4. Loop over folds and extract windows ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Extract windows for each fold\")\n","print(\"=\"*60)\n","\n","fold_stats = {}  # for global config (per-fold statistics)\n","\n","for fold_id in fold_ids:\n","    if fold_id is None:\n","        fold_tag = \"all\"\n","        print(f\"\\n--- Processing pseudo-fold: {fold_tag} (all data) ---\")\n","\n","        train_subjects = set(df['subject_id'].unique())\n","        test_subjects = set()\n","        df_train = df.copy()\n","        df_test = pd.DataFrame()\n","    else:\n","        fold_tag = f\"fold_{fold_id:02d}\"\n","        print(f\"\\n--- Processing fold {fold_id} ({fold_tag}) ---\")\n","\n","        fold_cfg = splits[str(fold_id)]\n","        train_subjects = set(fold_cfg[\"train_subjects\"])\n","        test_subjects = set(fold_cfg[\"test_subjects\"])\n","\n","        # Split data\n","        df_train = df[df['subject_id'].isin(train_subjects)].copy()\n","        df_test = df[df['subject_id'].isin(test_subjects)].copy()\n","\n","        print(f\"  Train subjects: {len(train_subjects)}\")\n","        print(f\"  Test subjects:  {len(test_subjects)}\")\n","        print(f\"  Train samples:  {len(df_train):,}\")\n","        print(f\"  Test samples:   {len(df_test):,}\")\n","\n","    # Create output directory for this fold\n","    windows_dir = windows_root / fold_tag\n","    windows_dir.mkdir(parents=True, exist_ok=True)\n","    print(f\"  Output directory: {windows_dir}\")\n","\n","    # ----- 4.1 Extract training-set windows -----\n","    print(\"\\n  [Train] Sliding-window extraction\")\n","\n","    train_windows = []\n","    df_train_meta = pd.DataFrame()\n","    train_label_counts = pd.Series(dtype=int)\n","\n","    if not df_train.empty:\n","        print(f\"  Processing train set ({len(df_train):,} samples)...\")\n","\n","        train_windows, train_metadata = sliding_window_extract(\n","            df_train, WINDOW_SIZE, STEP_SIZE, DOMINANT_THRESHOLD, time_col\n","        )\n","\n","        print(f\"  ✓ Extracted train windows: {len(train_windows):,}\")\n","\n","        if train_windows:\n","            # To numpy array\n","            X_train = np.array(train_windows, dtype='float32')  # (n_windows, window_size, n_features)\n","            df_train_meta = pd.DataFrame(train_metadata)\n","\n","            print(f\"    X_train shape: {X_train.shape}\")\n","            print(f\"    Feature dims : {X_train.shape[2]} channels × {X_train.shape[1]} timesteps\")\n","\n","            # Label distribution\n","            train_label_counts = df_train_meta['label'].value_counts().sort_index()\n","            print(f\"\\n    Train-set label distribution:\")\n","            for label, count in train_label_counts.items():\n","                pct = count / len(df_train_meta) * 100\n","                print(f\"      Label {label}: {count:6,} windows ({pct:5.2f}%)\")\n","\n","            # Label purity stats\n","            avg_purity = df_train_meta['label_purity'].mean()\n","            min_purity = df_train_meta['label_purity'].min()\n","            print(f\"\\n    Train-set label purity:\")\n","            print(f\"      Mean: {avg_purity*100:.2f}%\")\n","            print(f\"      Min:  {min_purity*100:.2f}%\")\n","\n","            # Save train set\n","            print(f\"\\n    Saving train set...\")\n","\n","            # Save features (numpy)\n","            X_train_npy_file = windows_dir / \"X_train.npy\"\n","            np.save(X_train_npy_file, X_train)\n","            print(f\"      ✓ {X_train_npy_file} (feature tensor)\")\n","\n","            # Save metadata (Parquet)\n","            X_train_meta_file = windows_dir / \"X_train.parquet\"\n","            df_train_meta[['window_id', 'subject_id', 'session_id', 'placement',\n","                           'label', 'label_purity', 'time_range', 'start_idx', 'end_idx']].to_parquet(\n","                X_train_meta_file, index=False\n","            )\n","            print(f\"      ✓ {X_train_meta_file} (metadata)\")\n","\n","            # Save label vector\n","            y_train = df_train_meta['label'].values.astype('int32')\n","            y_train_file = windows_dir / \"y_train.npy\"\n","            np.save(y_train_file, y_train)\n","            print(f\"      ✓ {y_train_file}\")\n","\n","            # Export label distribution snapshot (for audit)\n","            train_label_counts.to_csv(windows_dir / \"train_label_counts.csv\", header=['count'])\n","            print(f\"      ✓ train_label_counts.csv\")\n","        else:\n","            print(\"  ⚠️ No train windows extracted\")\n","    else:\n","        print(\"  Train set is empty; skipping\")\n","\n","    # ----- 4.2 Extract test-set windows -----\n","    print(\"\\n  [Test] Sliding-window extraction\")\n","\n","    test_windows = []\n","    df_test_meta = pd.DataFrame()\n","    test_label_counts = pd.Series(dtype=int)\n","\n","    if not df_test.empty:\n","        print(f\"  Processing test set ({len(df_test):,} samples)...\")\n","\n","        test_windows, test_metadata = sliding_window_extract(\n","            df_test, WINDOW_SIZE, STEP_SIZE, DOMINANT_THRESHOLD, time_col\n","        )\n","\n","        print(f\"  ✓ Extracted test windows: {len(test_windows):,}\")\n","\n","        if test_windows:\n","            # To numpy array\n","            X_test = np.array(test_windows, dtype='float32')\n","            df_test_meta = pd.DataFrame(test_metadata)\n","\n","            print(f\"    X_test shape: {X_test.shape}\")\n","\n","            # Label distribution\n","            test_label_counts = df_test_meta['label'].value_counts().sort_index()\n","            print(f\"\\n    Test-set label distribution:\")\n","            for label, count in test_label_counts.items():\n","                pct = count / len(df_test_meta) * 100\n","                print(f\"      Label {label}: {count:6,} windows ({pct:5.2f}%)\")\n","\n","            # Label purity stats\n","            avg_purity = df_test_meta['label_purity'].mean()\n","            min_purity = df_test_meta['label_purity'].min()\n","            print(f\"\\n    Test-set label purity:\")\n","            print(f\"      Mean: {avg_purity*100:.2f}%\")\n","            print(f\"      Min:  {min_purity*100:.2f}%\")\n","\n","            # Save test set\n","            print(f\"\\n    Saving test set...\")\n","\n","            # Save features (numpy)\n","            X_test_npy_file = windows_dir / \"X_test.npy\"\n","            np.save(X_test_npy_file, X_test)\n","            print(f\"      ✓ {X_test_npy_file} (feature tensor)\")\n","\n","            # Save metadata (Parquet)\n","            X_test_meta_file = windows_dir / \"X_test.parquet\"\n","            df_test_meta[['window_id', 'subject_id', 'session_id', 'placement',\n","                          'label', 'label_purity', 'time_range', 'start_idx', 'end_idx']].to_parquet(\n","                X_test_meta_file, index=False\n","            )\n","            print(f\"      ✓ {X_test_meta_file} (metadata)\")\n","\n","            # Save label vector\n","            y_test = df_test_meta['label'].values.astype('int32')\n","            y_test_file = windows_dir / \"y_test.npy\"\n","            np.save(y_test_file, y_test)\n","            print(f\"      ✓ {y_test_file}\")\n","\n","            # Export label distribution snapshot (for audit)\n","            test_label_counts.to_csv(windows_dir / \"test_label_counts.csv\", header=['count'])\n","            print(f\"      ✓ test_label_counts.csv\")\n","        else:\n","            print(\"  ⚠️ No test windows extracted\")\n","    else:\n","        print(\"  Test set is empty; skipping\")\n","\n","    # ----- 4.3 Collect statistics for this fold -----\n","    fold_key = \"all\" if fold_id is None else str(fold_id)\n","    fold_stats[fold_key] = {}\n","\n","    if train_windows:\n","        fold_stats[fold_key]['train'] = {\n","            'n_windows': int(len(train_windows)),\n","            'n_subjects': int(df_train_meta['subject_id'].nunique()),\n","            'n_sessions': int(df_train_meta.groupby(['subject_id', 'session_id']).ngroups),\n","            'label_distribution': {int(k): int(v) for k, v in train_label_counts.items()},\n","            'avg_label_purity': round(float(df_train_meta['label_purity'].mean()), 4),\n","            'min_label_purity': round(float(df_train_meta['label_purity'].min()), 4),\n","        }\n","\n","    if test_windows:\n","        fold_stats[fold_key]['test'] = {\n","            'n_windows': int(len(test_windows)),\n","            'n_subjects': int(df_test_meta['subject_id'].nunique()),\n","            'n_sessions': int(df_test_meta.groupby(['subject_id', 'session_id']).ngroups),\n","            'label_distribution': {int(k): int(v) for k, v in test_label_counts.items()},\n","            'avg_label_purity': round(float(df_test_meta['label_purity'].mean()), 4),\n","            'min_label_purity': round(float(df_test_meta['label_purity'].min()), 4),\n","        }\n","\n","# ========== 5. Save window configuration (global, multi-fold) ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Save window configuration (global)\")\n","print(\"=\"*60)\n","\n","fold_ids_str = [\"all\" if fid is None else str(fid) for fid in fold_ids]\n","\n","window_config = {\n","    'window_parameters': {\n","        'sampling_rate_hz': SAMPLING_RATE_HZ,\n","        'window_size_sec': WINDOW_SIZE_SEC,\n","        'window_size_samples': WINDOW_SIZE,\n","        'overlap_ratio': OVERLAP_RATIO,\n","        'step_size_samples': STEP_SIZE,\n","        'dominant_threshold': DOMINANT_THRESHOLD,\n","    },\n","    'features': {\n","        'channels': FEATURE_COLS,\n","        'n_channels': len(FEATURE_COLS),\n","        'description': '8-channel IMU features (ax,ay,az,gx,gy,gz,acc_mag,gyr_mag)',\n","    },\n","    'dataset_split': {\n","        'num_folds': len(fold_ids),\n","        'fold_ids': fold_ids_str,\n","        'source': str(splits_path) if splits_path.exists() else None,\n","    },\n","    'statistics': fold_stats,\n","    'notes': [\n","        f'Window parameters: {WINDOW_SIZE_SEC}s @ {SAMPLING_RATE_HZ}Hz = {WINDOW_SIZE} samples',\n","        f'Step size: {STEP_SIZE} samples (overlap {OVERLAP_RATIO*100:.0f}%)',\n","        f'Dominant label threshold: {DOMINANT_THRESHOLD*100:.0f}% (discard windows below threshold)',\n","        'Features: 8 channels (3-axis accelerometer + 3-axis gyroscope + 2 magnitudes)',\n","        'Data formats: X_*.npy (float32 tensor), X_*.parquet (metadata), y_*.npy (int32)',\n","        'Metadata includes: window_id/time_range/label/label_purity, etc.',\n","        'Slice per session to ensure temporal continuity',\n","        f'Order by {time_col if time_col else \"index\"}',\n","        'Discard windows containing NaN',\n","        'Time continuity check (allow 10% jitter)',\n","        'Persist by fold: windows/fold_xx/ (avoid overwrite when looping over folds)',\n","    ]\n","}\n","\n","window_config_file = configs_dir / \"windows.yaml\"\n","with open(window_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(window_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","print(f\"✓ Saved config: {window_config_file}\")\n","\n","window_config_json = configs_dir / \"windows.json\"\n","with open(window_config_json, 'w', encoding='utf-8') as f:\n","    json.dump(window_config, f, indent=2)\n","print(f\"✓ Saved config: {window_config_json}\")\n","\n","# ========== 6. Summary ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 9 complete - Sliding-window slicing (multi-fold)\")\n","print(\"=\"*60)\n","\n","print(f\"\\nWindow parameters:\")\n","print(f\"  Window length: {WINDOW_SIZE_SEC} s = {WINDOW_SIZE} samples\")\n","print(f\"  Step size: {STEP_SIZE} samples (overlap {OVERLAP_RATIO*100:.0f}%)\")\n","print(f\"  Dominant threshold: {DOMINANT_THRESHOLD*100:.0f}%\")\n","print(f\"  Feature dimension: {len(FEATURE_COLS)} channels\")\n","print(f\"  Sort order: {time_col if time_col else 'index'}\")\n","\n","print(\"\\nPer-fold window statistics:\")\n","for fold_key, stats in fold_stats.items():\n","    print(f\"\\n  Fold {fold_key}:\")\n","    if 'train' in stats:\n","        tr = stats['train']\n","        print(f\"    Train: {tr['n_windows']} windows, \"\n","              f\"{tr['n_subjects']} subjects, {tr['n_sessions']} sessions, \"\n","              f\"avg purity {tr['avg_label_purity']*100:.2f}%\")\n","    else:\n","        print(\"    Train: (no windows)\")\n","    if 'test' in stats:\n","        te = stats['test']\n","        print(f\"    Test : {te['n_windows']} windows, \"\n","              f\"{te['n_subjects']} subjects, {te['n_sessions']} sessions, \"\n","              f\"avg purity {te['avg_label_purity']*100:.2f}%\")\n","    else:\n","        print(\"    Test : (no windows)\")\n","\n","print(f\"\\nOutputs per fold:\")\n","print(f\"  Root directory: {windows_root}/\")\n","print(f\"  For each fold: X_train.npy, X_train.parquet, y_train.npy, \"\n","      f\"train_label_counts.csv (+ test equivalents when applicable)\")\n","print(f\"  Global config: {window_config_file}, {window_config_json}\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qca7Nw24_Uyp","executionInfo":{"status":"ok","timestamp":1763470633383,"user_tz":0,"elapsed":4884,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"55a28861-df12-4836-d496-816e4c1be3f8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 9: Sliding-window slicing (multi-fold)\n","============================================================\n","\n","Sliding-window parameters:\n","  Window length: 3.0 s = 150 samples @ 50.0 Hz\n","  Step size: 75 samples (overlap 50%)\n","  Dominant label threshold: 80%\n","  Feature columns: ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","============================================================\n","1. Load cleaned & labeled data\n","============================================================\n","Loading data from: data/lara/mbientlab/proc/labeled/\n","Data shape: (556504, 16)\n","Number of subjects: 8\n","Number of label classes: 6\n","Time column: time_sec\n","\n","============================================================\n","2. Load train/test splits (folds)\n","============================================================\n","Detected 8 folds from configs/splits.json: [0, 1, 2, 3, 4, 5, 6, 7]\n","\n","============================================================\n","3. Extract windows for each fold\n","============================================================\n","\n","--- Processing fold 0 (fold_00) ---\n","  Train subjects: 7\n","  Test subjects:  1\n","  Train samples:  479,982\n","  Test samples:   76,522\n","  Output directory: data/lara/mbientlab/proc/windows/fold_00\n","\n","  [Train] Sliding-window extraction\n","  Processing train set (479,982 samples)...\n","  ✓ Extracted train windows: 4,965\n","    X_train shape: (4965, 150, 8)\n","    Feature dims : 8 channels × 150 timesteps\n","\n","    Train-set label distribution:\n","      Label 1:    812 windows (16.35%)\n","      Label 2:    849 windows (17.10%)\n","      Label 3:    554 windows (11.16%)\n","      Label 4:     47 windows ( 0.95%)\n","      Label 5:  2,359 windows (47.51%)\n","      Label 6:    344 windows ( 6.93%)\n","\n","    Train-set label purity:\n","      Mean: 98.47%\n","      Min:  80.00%\n","\n","    Saving train set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_00/X_train.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_00/X_train.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_00/y_train.npy\n","      ✓ train_label_counts.csv\n","\n","  [Test] Sliding-window extraction\n","  Processing test set (76,522 samples)...\n","  ✓ Extracted test windows: 766\n","    X_test shape: (766, 150, 8)\n","\n","    Test-set label distribution:\n","      Label 1:     92 windows (12.01%)\n","      Label 2:    145 windows (18.93%)\n","      Label 3:     49 windows ( 6.40%)\n","      Label 4:     19 windows ( 2.48%)\n","      Label 5:    346 windows (45.17%)\n","      Label 6:    115 windows (15.01%)\n","\n","    Test-set label purity:\n","      Mean: 98.44%\n","      Min:  80.00%\n","\n","    Saving test set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_00/X_test.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_00/X_test.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_00/y_test.npy\n","      ✓ test_label_counts.csv\n","\n","--- Processing fold 1 (fold_01) ---\n","  Train subjects: 7\n","  Test subjects:  1\n","  Train samples:  491,647\n","  Test samples:   64,857\n","  Output directory: data/lara/mbientlab/proc/windows/fold_01\n","\n","  [Train] Sliding-window extraction\n","  Processing train set (491,647 samples)...\n","  ✓ Extracted train windows: 5,072\n","    X_train shape: (5072, 150, 8)\n","    Feature dims : 8 channels × 150 timesteps\n","\n","    Train-set label distribution:\n","      Label 1:    794 windows (15.65%)\n","      Label 2:    891 windows (17.57%)\n","      Label 3:    517 windows (10.19%)\n","      Label 4:     65 windows ( 1.28%)\n","      Label 5:  2,378 windows (46.88%)\n","      Label 6:    427 windows ( 8.42%)\n","\n","    Train-set label purity:\n","      Mean: 98.46%\n","      Min:  80.00%\n","\n","    Saving train set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_01/X_train.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_01/X_train.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_01/y_train.npy\n","      ✓ train_label_counts.csv\n","\n","  [Test] Sliding-window extraction\n","  Processing test set (64,857 samples)...\n","  ✓ Extracted test windows: 659\n","    X_test shape: (659, 150, 8)\n","\n","    Test-set label distribution:\n","      Label 1:    110 windows (16.69%)\n","      Label 2:    103 windows (15.63%)\n","      Label 3:     86 windows (13.05%)\n","      Label 4:      1 windows ( 0.15%)\n","      Label 5:    327 windows (49.62%)\n","      Label 6:     32 windows ( 4.86%)\n","\n","    Test-set label purity:\n","      Mean: 98.53%\n","      Min:  80.00%\n","\n","    Saving test set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_01/X_test.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_01/X_test.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_01/y_test.npy\n","      ✓ test_label_counts.csv\n","\n","--- Processing fold 2 (fold_02) ---\n","  Train subjects: 7\n","  Test subjects:  1\n","  Train samples:  478,803\n","  Test samples:   77,701\n","  Output directory: data/lara/mbientlab/proc/windows/fold_02\n","\n","  [Train] Sliding-window extraction\n","  Processing train set (478,803 samples)...\n","  ✓ Extracted train windows: 4,960\n","    X_train shape: (4960, 150, 8)\n","    Feature dims : 8 channels × 150 timesteps\n","\n","    Train-set label distribution:\n","      Label 1:    770 windows (15.52%)\n","      Label 2:    921 windows (18.57%)\n","      Label 3:    547 windows (11.03%)\n","      Label 4:     60 windows ( 1.21%)\n","      Label 5:  2,229 windows (44.94%)\n","      Label 6:    433 windows ( 8.73%)\n","\n","    Train-set label purity:\n","      Mean: 98.49%\n","      Min:  80.00%\n","\n","    Saving train set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_02/X_train.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_02/X_train.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_02/y_train.npy\n","      ✓ train_label_counts.csv\n","\n","  [Test] Sliding-window extraction\n","  Processing test set (77,701 samples)...\n","  ✓ Extracted test windows: 771\n","    X_test shape: (771, 150, 8)\n","\n","    Test-set label distribution:\n","      Label 1:    134 windows (17.38%)\n","      Label 2:     73 windows ( 9.47%)\n","      Label 3:     56 windows ( 7.26%)\n","      Label 4:      6 windows ( 0.78%)\n","      Label 5:    476 windows (61.74%)\n","      Label 6:     26 windows ( 3.37%)\n","\n","    Test-set label purity:\n","      Mean: 98.37%\n","      Min:  80.00%\n","\n","    Saving test set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_02/X_test.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_02/X_test.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_02/y_test.npy\n","      ✓ test_label_counts.csv\n","\n","--- Processing fold 3 (fold_03) ---\n","  Train subjects: 7\n","  Test subjects:  1\n","  Train samples:  473,845\n","  Test samples:   82,659\n","  Output directory: data/lara/mbientlab/proc/windows/fold_03\n","\n","  [Train] Sliding-window extraction\n","  Processing train set (473,845 samples)...\n","  ✓ Extracted train windows: 4,858\n","    X_train shape: (4858, 150, 8)\n","    Feature dims : 8 channels × 150 timesteps\n","\n","    Train-set label distribution:\n","      Label 1:    791 windows (16.28%)\n","      Label 2:    852 windows (17.54%)\n","      Label 3:    439 windows ( 9.04%)\n","      Label 4:     55 windows ( 1.13%)\n","      Label 5:  2,327 windows (47.90%)\n","      Label 6:    394 windows ( 8.11%)\n","\n","    Train-set label purity:\n","      Mean: 98.47%\n","      Min:  80.00%\n","\n","    Saving train set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_03/X_train.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_03/X_train.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_03/y_train.npy\n","      ✓ train_label_counts.csv\n","\n","  [Test] Sliding-window extraction\n","  Processing test set (82,659 samples)...\n","  ✓ Extracted test windows: 873\n","    X_test shape: (873, 150, 8)\n","\n","    Test-set label distribution:\n","      Label 1:    113 windows (12.94%)\n","      Label 2:    142 windows (16.27%)\n","      Label 3:    164 windows (18.79%)\n","      Label 4:     11 windows ( 1.26%)\n","      Label 5:    378 windows (43.30%)\n","      Label 6:     65 windows ( 7.45%)\n","\n","    Test-set label purity:\n","      Mean: 98.47%\n","      Min:  80.00%\n","\n","    Saving test set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_03/X_test.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_03/X_test.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_03/y_test.npy\n","      ✓ test_label_counts.csv\n","\n","--- Processing fold 4 (fold_04) ---\n","  Train subjects: 7\n","  Test subjects:  1\n","  Train samples:  486,094\n","  Test samples:   70,410\n","  Output directory: data/lara/mbientlab/proc/windows/fold_04\n","\n","  [Train] Sliding-window extraction\n","  Processing train set (486,094 samples)...\n","  ✓ Extracted train windows: 4,985\n","    X_train shape: (4985, 150, 8)\n","    Feature dims : 8 channels × 150 timesteps\n","\n","    Train-set label distribution:\n","      Label 1:    767 windows (15.39%)\n","      Label 2:    829 windows (16.63%)\n","      Label 3:    547 windows (10.97%)\n","      Label 4:     53 windows ( 1.06%)\n","      Label 5:  2,387 windows (47.88%)\n","      Label 6:    402 windows ( 8.06%)\n","\n","    Train-set label purity:\n","      Mean: 98.47%\n","      Min:  80.00%\n","\n","    Saving train set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_04/X_train.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_04/X_train.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_04/y_train.npy\n","      ✓ train_label_counts.csv\n","\n","  [Test] Sliding-window extraction\n","  Processing test set (70,410 samples)...\n","  ✓ Extracted test windows: 746\n","    X_test shape: (746, 150, 8)\n","\n","    Test-set label distribution:\n","      Label 1:    137 windows (18.36%)\n","      Label 2:    165 windows (22.12%)\n","      Label 3:     56 windows ( 7.51%)\n","      Label 4:     13 windows ( 1.74%)\n","      Label 5:    318 windows (42.63%)\n","      Label 6:     57 windows ( 7.64%)\n","\n","    Test-set label purity:\n","      Mean: 98.48%\n","      Min:  80.00%\n","\n","    Saving test set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_04/X_test.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_04/X_test.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_04/y_test.npy\n","      ✓ test_label_counts.csv\n","\n","--- Processing fold 5 (fold_05) ---\n","  Train subjects: 7\n","  Test subjects:  1\n","  Train samples:  525,581\n","  Test samples:   30,923\n","  Output directory: data/lara/mbientlab/proc/windows/fold_05\n","\n","  [Train] Sliding-window extraction\n","  Processing train set (525,581 samples)...\n","  ✓ Extracted train windows: 5,442\n","    X_train shape: (5442, 150, 8)\n","    Feature dims : 8 channels × 150 timesteps\n","\n","    Train-set label distribution:\n","      Label 1:    824 windows (15.14%)\n","      Label 2:    945 windows (17.36%)\n","      Label 3:    578 windows (10.62%)\n","      Label 4:     57 windows ( 1.05%)\n","      Label 5:  2,599 windows (47.76%)\n","      Label 6:    439 windows ( 8.07%)\n","\n","    Train-set label purity:\n","      Mean: 98.49%\n","      Min:  80.00%\n","\n","    Saving train set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_05/X_train.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_05/X_train.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_05/y_train.npy\n","      ✓ train_label_counts.csv\n","\n","  [Test] Sliding-window extraction\n","  Processing test set (30,923 samples)...\n","  ✓ Extracted test windows: 289\n","    X_test shape: (289, 150, 8)\n","\n","    Test-set label distribution:\n","      Label 1:     80 windows (27.68%)\n","      Label 2:     49 windows (16.96%)\n","      Label 3:     25 windows ( 8.65%)\n","      Label 4:      9 windows ( 3.11%)\n","      Label 5:    106 windows (36.68%)\n","      Label 6:     20 windows ( 6.92%)\n","\n","    Test-set label purity:\n","      Mean: 98.00%\n","      Min:  80.00%\n","\n","    Saving test set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_05/X_test.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_05/X_test.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_05/y_test.npy\n","      ✓ test_label_counts.csv\n","\n","--- Processing fold 6 (fold_06) ---\n","  Train subjects: 7\n","  Test subjects:  1\n","  Train samples:  474,169\n","  Test samples:   82,335\n","  Output directory: data/lara/mbientlab/proc/windows/fold_06\n","\n","  [Train] Sliding-window extraction\n","  Processing train set (474,169 samples)...\n","  ✓ Extracted train windows: 4,853\n","    X_train shape: (4853, 150, 8)\n","    Feature dims : 8 channels × 150 timesteps\n","\n","    Train-set label distribution:\n","      Label 1:    785 windows (16.18%)\n","      Label 2:    789 windows (16.26%)\n","      Label 3:    507 windows (10.45%)\n","      Label 4:     64 windows ( 1.32%)\n","      Label 5:  2,291 windows (47.21%)\n","      Label 6:    417 windows ( 8.59%)\n","\n","    Train-set label purity:\n","      Mean: 98.43%\n","      Min:  80.00%\n","\n","    Saving train set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_06/X_train.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_06/X_train.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_06/y_train.npy\n","      ✓ train_label_counts.csv\n","\n","  [Test] Sliding-window extraction\n","  Processing test set (82,335 samples)...\n","  ✓ Extracted test windows: 878\n","    X_test shape: (878, 150, 8)\n","\n","    Test-set label distribution:\n","      Label 1:    119 windows (13.55%)\n","      Label 2:    205 windows (23.35%)\n","      Label 3:     96 windows (10.93%)\n","      Label 4:      2 windows ( 0.23%)\n","      Label 5:    414 windows (47.15%)\n","      Label 6:     42 windows ( 4.78%)\n","\n","    Test-set label purity:\n","      Mean: 98.71%\n","      Min:  80.00%\n","\n","    Saving test set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_06/X_test.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_06/X_test.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_06/y_test.npy\n","      ✓ test_label_counts.csv\n","\n","--- Processing fold 7 (fold_07) ---\n","  Train subjects: 7\n","  Test subjects:  1\n","  Train samples:  485,407\n","  Test samples:   71,097\n","  Output directory: data/lara/mbientlab/proc/windows/fold_07\n","\n","  [Train] Sliding-window extraction\n","  Processing train set (485,407 samples)...\n","  ✓ Extracted train windows: 4,982\n","    X_train shape: (4982, 150, 8)\n","    Feature dims : 8 channels × 150 timesteps\n","\n","    Train-set label distribution:\n","      Label 1:    785 windows (15.76%)\n","      Label 2:    882 windows (17.70%)\n","      Label 3:    532 windows (10.68%)\n","      Label 4:     61 windows ( 1.22%)\n","      Label 5:  2,365 windows (47.47%)\n","      Label 6:    357 windows ( 7.17%)\n","\n","    Train-set label purity:\n","      Mean: 98.48%\n","      Min:  80.00%\n","\n","    Saving train set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_07/X_train.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_07/X_train.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_07/y_train.npy\n","      ✓ train_label_counts.csv\n","\n","  [Test] Sliding-window extraction\n","  Processing test set (71,097 samples)...\n","  ✓ Extracted test windows: 749\n","    X_test shape: (749, 150, 8)\n","\n","    Test-set label distribution:\n","      Label 1:    119 windows (15.89%)\n","      Label 2:    112 windows (14.95%)\n","      Label 3:     71 windows ( 9.48%)\n","      Label 4:      5 windows ( 0.67%)\n","      Label 5:    340 windows (45.39%)\n","      Label 6:    102 windows (13.62%)\n","\n","    Test-set label purity:\n","      Mean: 98.43%\n","      Min:  80.00%\n","\n","    Saving test set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_07/X_test.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_07/X_test.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_07/y_test.npy\n","      ✓ test_label_counts.csv\n","\n","============================================================\n","4. Save window configuration (global)\n","============================================================\n","✓ Saved config: configs/windows.yaml\n","✓ Saved config: configs/windows.json\n","\n","============================================================\n","Step 9 complete - Sliding-window slicing (multi-fold)\n","============================================================\n","\n","Window parameters:\n","  Window length: 3.0 s = 150 samples\n","  Step size: 75 samples (overlap 50%)\n","  Dominant threshold: 80%\n","  Feature dimension: 8 channels\n","  Sort order: time_sec\n","\n","Per-fold window statistics:\n","\n","  Fold 0:\n","    Train: 4965 windows, 7 subjects, 83 sessions, avg purity 98.47%\n","    Test : 766 windows, 1 subjects, 13 sessions, avg purity 98.44%\n","\n","  Fold 1:\n","    Train: 5072 windows, 7 subjects, 85 sessions, avg purity 98.46%\n","    Test : 659 windows, 1 subjects, 11 sessions, avg purity 98.53%\n","\n","  Fold 2:\n","    Train: 4960 windows, 7 subjects, 82 sessions, avg purity 98.49%\n","    Test : 771 windows, 1 subjects, 14 sessions, avg purity 98.37%\n","\n","  Fold 3:\n","    Train: 4858 windows, 7 subjects, 82 sessions, avg purity 98.47%\n","    Test : 873 windows, 1 subjects, 14 sessions, avg purity 98.47%\n","\n","  Fold 4:\n","    Train: 4985 windows, 7 subjects, 84 sessions, avg purity 98.47%\n","    Test : 746 windows, 1 subjects, 12 sessions, avg purity 98.48%\n","\n","  Fold 5:\n","    Train: 5442 windows, 7 subjects, 90 sessions, avg purity 98.49%\n","    Test : 289 windows, 1 subjects, 6 sessions, avg purity 98.00%\n","\n","  Fold 6:\n","    Train: 4853 windows, 7 subjects, 82 sessions, avg purity 98.43%\n","    Test : 878 windows, 1 subjects, 14 sessions, avg purity 98.71%\n","\n","  Fold 7:\n","    Train: 4982 windows, 7 subjects, 84 sessions, avg purity 98.48%\n","    Test : 749 windows, 1 subjects, 12 sessions, avg purity 98.43%\n","\n","Outputs per fold:\n","  Root directory: data/lara/mbientlab/proc/windows/\n","  For each fold: X_train.npy, X_train.parquet, y_train.npy, train_label_counts.csv (+ test equivalents when applicable)\n","  Global config: configs/windows.yaml, configs/windows.json\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\n","\"\"\"\n","Step 10: LOSO Split (top-conf/journal grade)\n","Leave-One-Subject-Out: 1 subject for test per fold, the rest for training\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import json\n","import yaml\n","from collections import defaultdict\n","\n","print(\"=\"*60)\n","print(\"Step 10: LOSO split\")\n","print(\"=\"*60)\n","\n","# Path configuration\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","configs_dir.mkdir(parents=True, exist_ok=True)\n","\n","# ========== 1. Load data and get subject list ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Load data and get subject list\")\n","print(\"=\"*60)\n","\n","labeled_dir = proc_dir / \"labeled\"\n","print(f\"Loading data: {labeled_dir}/\")\n","\n","df = pd.read_parquet(labeled_dir)\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Total samples: {len(df):,}\")\n","\n","# Extract all subjects\n","all_subjects = sorted(df['subject_id'].unique().tolist())\n","n_subjects = len(all_subjects)\n","\n","print(f\"\\nSubject list:\")\n","print(f\"  Total: {n_subjects} subjects\")\n","print(f\"  IDs: {all_subjects}\")\n","\n","# Sample count per subject\n","subject_sample_counts = df['subject_id'].value_counts().sort_index()\n","print(f\"\\nSample count per subject:\")\n","for subj in all_subjects:\n","    count = subject_sample_counts.get(subj, 0)\n","    pct = count / len(df) * 100\n","    print(f\"  {subj}: {count:8,} samples ({pct:5.2f}%)\")\n","\n","# ========== 2. Generate LOSO split ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Generate LOSO split\")\n","print(\"=\"*60)\n","\n","print(f\"\\nLOSO strategy: Leave-One-Subject-Out\")\n","print(f\"  #folds = #subjects = {n_subjects}\")\n","print(f\"  Per fold: 1 subject for test, {n_subjects-1} subjects for train\")\n","\n","# Create split dict\n","splits = {}\n","\n","for fold_id, test_subject in enumerate(all_subjects):\n","    # Test set: current subject\n","    test_subjects = [test_subject]\n","\n","    # Train set: all other subjects\n","    train_subjects = [s for s in all_subjects if s != test_subject]\n","\n","    # Save split\n","    splits[str(fold_id)] = {\n","        \"fold_id\": fold_id,\n","        \"test_subject\": test_subject,\n","        \"test_subjects\": test_subjects,  # list for compatibility\n","        \"train_subjects\": train_subjects,\n","        \"n_train\": len(train_subjects),\n","        \"n_test\": len(test_subjects),\n","    }\n","\n","    print(f\"  Fold {fold_id}: test {test_subject}, train {len(train_subjects)} subjects\")\n","\n","print(f\"\\n✓ Generated {len(splits)} LOSO folds\")\n","\n","# ========== 3. Validate split integrity ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Validate split integrity\")\n","print(\"=\"*60)\n","\n","# Check 1: each subject appears exactly once in the test set\n","test_subject_appearances = defaultdict(int)\n","for fold_id, fold_info in splits.items():\n","    for subj in fold_info['test_subjects']:\n","        test_subject_appearances[subj] += 1\n","\n","print(f\"\\nCheck 1: times each subject appears as test\")\n","all_once = True\n","for subj in all_subjects:\n","    count = test_subject_appearances[subj]\n","    status = \"✓\" if count == 1 else \"✗\"\n","    print(f\"  {status} {subj}: {count} time(s)\")\n","    if count != 1:\n","        all_once = False\n","\n","if all_once:\n","    print(f\"  ✓ All subjects appear exactly once\")\n","else:\n","    raise RuntimeError(\"Split validation failed: subject test appearances not equal to 1\")\n","\n","# Check 2: train and test sets are disjoint\n","print(f\"\\nCheck 2: train and test sets are disjoint\")\n","all_disjoint = True\n","for fold_id, fold_info in splits.items():\n","    train_set = set(fold_info['train_subjects'])\n","    test_set = set(fold_info['test_subjects'])\n","    overlap = train_set & test_set\n","\n","    if overlap:\n","        print(f\"  ✗ Fold {fold_id}: overlap exists {overlap}\")\n","        all_disjoint = False\n","\n","if all_disjoint:\n","    print(f\"  ✓ Train/test sets are completely disjoint for all folds\")\n","else:\n","    raise RuntimeError(\"Split validation failed: train and test sets have overlap\")\n","\n","# Check 3: all subjects covered\n","print(f\"\\nCheck 3: all subjects covered\")\n","covered_subjects = set()\n","for fold_id, fold_info in splits.items():\n","    covered_subjects.update(fold_info['train_subjects'])\n","    covered_subjects.update(fold_info['test_subjects'])\n","\n","missing = set(all_subjects) - covered_subjects\n","extra = covered_subjects - set(all_subjects)\n","\n","if not missing and not extra:\n","    print(f\"  ✓ All subjects are covered; no missing or extra subjects\")\n","else:\n","    if missing:\n","        print(f\"  ✗ Missing subjects: {missing}\")\n","    if extra:\n","        print(f\"  ✗ Extra subjects: {extra}\")\n","    raise RuntimeError(\"Split validation failed: subject coverage incomplete\")\n","\n","# Check 4: sample count stats\n","print(f\"\\nCheck 4: per-fold sample counts\")\n","fold_sample_stats = []\n","for fold_id, fold_info in splits.items():\n","    train_subjects = fold_info['train_subjects']\n","    test_subjects = fold_info['test_subjects']\n","\n","    n_train_samples = df[df['subject_id'].isin(train_subjects)].shape[0]\n","    n_test_samples = df[df['subject_id'].isin(test_subjects)].shape[0]\n","\n","    fold_sample_stats.append({\n","        'fold_id': int(fold_id),\n","        'test_subject': fold_info['test_subject'],\n","        'n_train_samples': n_train_samples,\n","        'n_test_samples': n_test_samples,\n","        'train_ratio': round(n_train_samples / len(df), 4),\n","        'test_ratio': round(n_test_samples / len(df), 4),\n","    })\n","\n","df_fold_stats = pd.DataFrame(fold_sample_stats)\n","\n","print(f\"\\nPer-fold sample distribution:\")\n","print(df_fold_stats.to_string(index=False))\n","\n","# Summary\n","print(f\"\\nSample distribution summary:\")\n","print(f\"  Train sample count: {df_fold_stats['n_train_samples'].min():,} ~ {df_fold_stats['n_train_samples'].max():,}\")\n","print(f\"  Test sample count: {df_fold_stats['n_test_samples'].min():,} ~ {df_fold_stats['n_test_samples'].max():,}\")\n","print(f\"  Average train ratio: {df_fold_stats['train_ratio'].mean()*100:.2f}%\")\n","print(f\"  Average test ratio: {df_fold_stats['test_ratio'].mean()*100:.2f}%\")\n","\n","print(f\"\\n✓ All validations passed\")\n","\n","# ========== 4. Save split configuration ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Save split configuration\")\n","print(\"=\"*60)\n","\n","# Save splits.json\n","splits_file = configs_dir / \"splits.json\"\n","with open(splits_file, 'w', encoding='utf-8') as f:\n","    json.dump(splits, f, indent=2)\n","\n","print(f\"✓ Saved: {splits_file}\")\n","\n","# Save detailed config (with metadata)\n","loso_config = {\n","    'strategy': 'LOSO (Leave-One-Subject-Out)',\n","    'description': 'One subject for test in each fold; remaining subjects for training',\n","    'n_folds': n_subjects,\n","    'n_subjects': n_subjects,\n","    'all_subjects': all_subjects,\n","    'fold_statistics': {\n","        'train_samples_min': int(df_fold_stats['n_train_samples'].min()),\n","        'train_samples_max': int(df_fold_stats['n_train_samples'].max()),\n","        'train_samples_mean': int(df_fold_stats['n_train_samples'].mean()),\n","        'test_samples_min': int(df_fold_stats['n_test_samples'].min()),\n","        'test_samples_max': int(df_fold_stats['n_test_samples'].max()),\n","        'test_samples_mean': int(df_fold_stats['n_test_samples'].mean()),\n","        'avg_train_ratio': round(float(df_fold_stats['train_ratio'].mean()), 4),\n","        'avg_test_ratio': round(float(df_fold_stats['test_ratio'].mean()), 4),\n","    },\n","    'validation': {\n","        'no_subject_overlap': True,\n","        'all_subjects_covered': True,\n","        'each_subject_tested_once': True,\n","    },\n","    'anti_leakage_principles': [\n","        'Train and test sets are completely separated by subject',\n","        'Window slicing is performed after splitting to ensure no cross-fold leakage',\n","        'Statistics (mean/std) are computed from the training fold only',\n","        'Feature engineering is performed independently within each fold',\n","        'Hyperparameter tuning uses training-fold data only (nested CV optional)',\n","        'Final model evaluation is strictly based on the corresponding fold’s test set',\n","        'When aggregating results across folds, use metrics from independent test sets',\n","    ],\n","    'notes': [\n","        f'LOSO split: {n_subjects} folds; 1 subject per fold for test',\n","        'Ensure each subject appears exactly once in the test set',\n","        'Train/test sets are mutually exclusive with no subject overlap',\n","        'Suitable for small-sample settings with large inter-subject variability',\n","        'Report mean and standard deviation across all folds',\n","    ]\n","}\n","\n","loso_config_file = configs_dir / \"loso.yaml\"\n","with open(loso_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(loso_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","print(f\"✓ Saved: {loso_config_file}\")\n","\n","loso_config_json = configs_dir / \"loso.json\"\n","with open(loso_config_json, 'w', encoding='utf-8') as f:\n","    json.dump(loso_config, f, indent=2)\n","\n","print(f\"✓ Saved: {loso_config_json}\")\n","\n","# Save per-fold sample stats\n","fold_stats_file = configs_dir / \"loso_fold_stats.csv\"\n","df_fold_stats.to_csv(fold_stats_file, index=False)\n","print(f\"✓ Saved: {fold_stats_file}\")\n","\n","# ========== 5. Generate usage example ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Generate usage example\")\n","print(\"=\"*60)\n","\n","example_code = '''\n","# ========== LOSO Usage Example ==========\n","\n","import json\n","from pathlib import Path\n","\n","# 1. Load splits\n","with open(\"configs/splits.json\", \"r\") as f:\n","    splits = json.load(f)\n","\n","# 2. Iterate over folds\n","for fold_id in range(len(splits)):\n","    print(f\"\\\\n========== Fold {fold_id} ==========\")\n","\n","    # Get current fold split\n","    fold = splits[str(fold_id)]\n","    train_subjects = fold[\"train_subjects\"]\n","    test_subject = fold[\"test_subject\"]\n","\n","    print(f\"Train: {len(train_subjects)} subjects\")\n","    print(f\"Test: {test_subject}\")\n","\n","    # 3. Set environment variable (used by later steps)\n","    import os\n","    os.environ[\"FOLD_ID\"] = str(fold_id)\n","\n","    # 4. Run training pipeline\n","    # - Step 6: per-fold clipping (statistics from train only)\n","    # - Step 7: per-fold standardization (statistics from train only)\n","    # - Step 9: per-fold windowing\n","    # - Train model (training windows only)\n","    # - Evaluate model (test windows only)\n","\n","    # 5. Save results of current fold\n","    # results[fold_id] = {\"accuracy\": acc, \"f1\": f1, ...}\n","\n","# 6. Aggregate results across folds\n","# mean_acc = np.mean([r[\"accuracy\"] for r in results.values()])\n","# std_acc = np.std([r[\"accuracy\"] for r in results.values()])\n","# print(f\"Mean accuracy: {mean_acc:.4f} ± {std_acc:.4f}\")\n","\n","# ========== Anti-leakage Checklist ==========\n","# ✓ Train/test separated by subject\n","# ✓ Statistics (mean/std) computed from training set only\n","# ✓ Feature scaling uses parameters from training set\n","# ✓ Windowing performed after splitting\n","# ✓ Hyperparameter tuning uses training data only\n","# ✓ Test set used strictly for final evaluation\n","'''\n","\n","example_file = configs_dir / \"loso_usage_example.py\"\n","with open(example_file, 'w', encoding='utf-8') as f:\n","    f.write(example_code)\n","\n","print(f\"✓ Generated usage example: {example_file}\")\n","\n","print(\"\\nHow to use:\")\n","print(\"  1. export FOLD_ID=0  # set current fold\")\n","print(\"  2. Run steps 6–9 (they will use the corresponding fold automatically)\")\n","print(\"  3. Train the model and evaluate\")\n","print(\"  4. Repeat steps 1–3 for all folds\")\n","print(\"  5. Aggregate results (mean ± std)\")\n","\n","# ========== 6. Split visualization info ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Split visualization info\")\n","print(\"=\"*60)\n","\n","print(f\"\\nLOSO split matrix (first 5 folds):\")\n","print(f\"{'Fold':<6} {'TestSubject':<12} {'#TrainSubs':<12} {'#TestSamples':<12} {'#TrainSamples':<12}\")\n","print(\"-\" * 60)\n","\n","for i in range(min(5, len(splits))):\n","    fold = splits[str(i)]\n","    stats = df_fold_stats[df_fold_stats['fold_id'] == i].iloc[0]\n","    print(f\"{i:<6} {fold['test_subject']:<12} {fold['n_train']:<12} \"\n","          f\"{stats['n_test_samples']:<12} {stats['n_train_samples']:<12}\")\n","\n","if len(splits) > 5:\n","    print(f\"... (total {len(splits)} folds)\")\n","\n","# ========== 7. Summary ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 10 complete - LOSO split\")\n","print(\"=\"*60)\n","\n","print(f\"\\nSplit strategy:\")\n","print(f\"  Method: LOSO (Leave-One-Subject-Out)\")\n","print(f\"  #folds: {n_subjects}\")\n","print(f\"  #subjects: {n_subjects}\")\n","print(f\"  Train per fold: {n_subjects-1} subjects\")\n","print(f\"  Test per fold: 1 subject\")\n","\n","print(f\"\\nData distribution:\")\n","print(f\"  Total samples: {len(df):,}\")\n","print(f\"  Train ratio (avg): {df_fold_stats['train_ratio'].mean()*100:.2f}%\")\n","print(f\"  Test ratio (avg): {df_fold_stats['test_ratio'].mean()*100:.2f}%\")\n","\n","print(f\"\\nValidation results:\")\n","print(f\"  ✓ No subject overlap\")\n","print(f\"  ✓ All subjects covered\")\n","print(f\"  ✓ Each subject tested exactly once\")\n","print(f\"  ✓ Train/test sets are disjoint\")\n","\n","print(f\"\\nOutput files:\")\n","print(f\"  Main config: {splits_file}\")\n","print(f\"  Detailed config: {loso_config_file}\")\n","print(f\"  Fold stats: {fold_stats_file}\")\n","print(f\"  Usage example: {example_file}\")\n","\n","print(\"\\nAnti-leakage principles:\")\n","print(\"  1. ✓ Fully separated by subject\")\n","print(\"  2. ✓ Statistics computed from training fold only\")\n","print(\"  3. ✓ Feature engineering is fold-internal\")\n","print(\"  4. ✓ Window slicing performed after splitting\")\n","print(\"  5. ✓ Hyperparameter tuning limited to training data\")\n","print(\"  6. ✓ Test set used strictly for independent evaluation\")\n","print(\"  7. ✓ Cross-fold aggregation uses independent metrics\")\n","\n","print(\"\\nNext steps:\")\n","print(\"  - Set export FOLD_ID=<fold_id>\")\n","print(\"  - Re-run steps 6–9 (per-fold processing)\")\n","print(\"  - Train and evaluate models\")\n","print(\"  - Iterate all folds and aggregate results\")\n","\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hxf_5NEI_W8k","executionInfo":{"status":"ok","timestamp":1763470633752,"user_tz":0,"elapsed":347,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"b90d5708-e007-450c-b249-6a3038307897"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 10: LOSO split\n","============================================================\n","\n","============================================================\n","1. Load data and get subject list\n","============================================================\n","Loading data: data/lara/mbientlab/proc/labeled/\n","Data shape: (556504, 16)\n","Total samples: 556,504\n","\n","Subject list:\n","  Total: 8 subjects\n","  IDs: ['S07', 'S08', 'S09', 'S10', 'S11', 'S12', 'S13', 'S14']\n","\n","Sample count per subject:\n","  S07:   76,522 samples (13.75%)\n","  S08:   64,857 samples (11.65%)\n","  S09:   77,701 samples (13.96%)\n","  S10:   82,659 samples (14.85%)\n","  S11:   70,410 samples (12.65%)\n","  S12:   30,923 samples ( 5.56%)\n","  S13:   82,335 samples (14.80%)\n","  S14:   71,097 samples (12.78%)\n","\n","============================================================\n","2. Generate LOSO split\n","============================================================\n","\n","LOSO strategy: Leave-One-Subject-Out\n","  #folds = #subjects = 8\n","  Per fold: 1 subject for test, 7 subjects for train\n","  Fold 0: test S07, train 7 subjects\n","  Fold 1: test S08, train 7 subjects\n","  Fold 2: test S09, train 7 subjects\n","  Fold 3: test S10, train 7 subjects\n","  Fold 4: test S11, train 7 subjects\n","  Fold 5: test S12, train 7 subjects\n","  Fold 6: test S13, train 7 subjects\n","  Fold 7: test S14, train 7 subjects\n","\n","✓ Generated 8 LOSO folds\n","\n","============================================================\n","3. Validate split integrity\n","============================================================\n","\n","Check 1: times each subject appears as test\n","  ✓ S07: 1 time(s)\n","  ✓ S08: 1 time(s)\n","  ✓ S09: 1 time(s)\n","  ✓ S10: 1 time(s)\n","  ✓ S11: 1 time(s)\n","  ✓ S12: 1 time(s)\n","  ✓ S13: 1 time(s)\n","  ✓ S14: 1 time(s)\n","  ✓ All subjects appear exactly once\n","\n","Check 2: train and test sets are disjoint\n","  ✓ Train/test sets are completely disjoint for all folds\n","\n","Check 3: all subjects covered\n","  ✓ All subjects are covered; no missing or extra subjects\n","\n","Check 4: per-fold sample counts\n","\n","Per-fold sample distribution:\n"," fold_id test_subject  n_train_samples  n_test_samples  train_ratio  test_ratio\n","       0          S07           479982           76522       0.8625      0.1375\n","       1          S08           491647           64857       0.8835      0.1165\n","       2          S09           478803           77701       0.8604      0.1396\n","       3          S10           473845           82659       0.8515      0.1485\n","       4          S11           486094           70410       0.8735      0.1265\n","       5          S12           525581           30923       0.9444      0.0556\n","       6          S13           474169           82335       0.8520      0.1480\n","       7          S14           485407           71097       0.8722      0.1278\n","\n","Sample distribution summary:\n","  Train sample count: 473,845 ~ 525,581\n","  Test sample count: 30,923 ~ 82,659\n","  Average train ratio: 87.50%\n","  Average test ratio: 12.50%\n","\n","✓ All validations passed\n","\n","============================================================\n","4. Save split configuration\n","============================================================\n","✓ Saved: configs/splits.json\n","✓ Saved: configs/loso.yaml\n","✓ Saved: configs/loso.json\n","✓ Saved: configs/loso_fold_stats.csv\n","\n","============================================================\n","5. Generate usage example\n","============================================================\n","✓ Generated usage example: configs/loso_usage_example.py\n","\n","How to use:\n","  1. export FOLD_ID=0  # set current fold\n","  2. Run steps 6–9 (they will use the corresponding fold automatically)\n","  3. Train the model and evaluate\n","  4. Repeat steps 1–3 for all folds\n","  5. Aggregate results (mean ± std)\n","\n","============================================================\n","6. Split visualization info\n","============================================================\n","\n","LOSO split matrix (first 5 folds):\n","Fold   TestSubject  #TrainSubs   #TestSamples #TrainSamples\n","------------------------------------------------------------\n","0      S07          7            76522        479982      \n","1      S08          7            64857        491647      \n","2      S09          7            77701        478803      \n","3      S10          7            82659        473845      \n","4      S11          7            70410        486094      \n","... (total 8 folds)\n","\n","============================================================\n","Step 10 complete - LOSO split\n","============================================================\n","\n","Split strategy:\n","  Method: LOSO (Leave-One-Subject-Out)\n","  #folds: 8\n","  #subjects: 8\n","  Train per fold: 7 subjects\n","  Test per fold: 1 subject\n","\n","Data distribution:\n","  Total samples: 556,504\n","  Train ratio (avg): 87.50%\n","  Test ratio (avg): 12.50%\n","\n","Validation results:\n","  ✓ No subject overlap\n","  ✓ All subjects covered\n","  ✓ Each subject tested exactly once\n","  ✓ Train/test sets are disjoint\n","\n","Output files:\n","  Main config: configs/splits.json\n","  Detailed config: configs/loso.yaml\n","  Fold stats: configs/loso_fold_stats.csv\n","  Usage example: configs/loso_usage_example.py\n","\n","Anti-leakage principles:\n","  1. ✓ Fully separated by subject\n","  2. ✓ Statistics computed from training fold only\n","  3. ✓ Feature engineering is fold-internal\n","  4. ✓ Window slicing performed after splitting\n","  5. ✓ Hyperparameter tuning limited to training data\n","  6. ✓ Test set used strictly for independent evaluation\n","  7. ✓ Cross-fold aggregation uses independent metrics\n","\n","Next steps:\n","  - Set export FOLD_ID=<fold_id>\n","  - Re-run steps 6–9 (per-fold processing)\n","  - Train and evaluate models\n","  - Iterate all folds and aggregate results\n","============================================================\n"]}]},{"cell_type":"code","source":["# ================ Step 10: DeepConvContext (RWHAR · Bi-Attention, paper-aligned · LARa MbientLab) ================\n","import os\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # GPU memory-friendly setting\n","\n","import json\n","import numpy as np\n","from pathlib import Path\n","from typing import List, Tuple, Dict, Optional\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","# ---------------------------\n","# 0) Basic configuration: aligned with the official DeepConvContext training protocol\n","# ---------------------------\n","torch.backends.cudnn.benchmark = True\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"\\n\\nStep 10: DeepConvContext (Bi-Attention, paper-aligned variant · LARa MbientLab)\")\n","print(\"=\" * 100)\n","print(f\"Device in use: {device}\")\n","\n","# Fix the random seed (paper averages over 3 seeds; here we use seed=1 by default)\n","SEED = 1\n","\n","def set_seed(seed: int = SEED):\n","    import random\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","set_seed(SEED)\n","\n","# ---------------------------\n","# 0.1) Paths and LARa configuration\n","# ---------------------------\n","BASE_DIR = Path(\".\")\n","CFG_DIR = BASE_DIR / \"configs\"\n","PROC_DIR = BASE_DIR / \"data/lara/mbientlab/proc\"\n","WINDOWS_ROOT = PROC_DIR / \"windows\"\n","\n","splits_path = CFG_DIR / \"splits.json\"\n","labels_path = CFG_DIR / \"labels.json\"\n","\n","if not splits_path.exists():\n","    raise FileNotFoundError(f\"Missing splits.json at {splits_path}. Please run LARa LOSO split first.\")\n","if not labels_path.exists():\n","    raise FileNotFoundError(f\"Missing labels.json at {labels_path}. Please run LARa label alignment step first.\")\n","\n","with open(splits_path, \"r\") as f:\n","    splits_cfg = json.load(f)\n","with open(labels_path, \"r\") as f:\n","    labels_cfg = json.load(f)\n","\n","unified_labels: Dict[str, int] = labels_cfg[\"unified_labels\"]\n","label_distribution: Dict[str, int] = labels_cfg.get(\"label_distribution\", {})\n","\n","# Target class subset for LARa (6 base activities)\n","TARGET_CLASS_NAMES = [\"walking\", \"running\", \"sitting\", \"standing\", \"upstairs\", \"downstairs\"]\n","\n","standard_names: List[str] = []\n","for name in TARGET_CLASS_NAMES:\n","    if (name in label_distribution and label_distribution[name] > 0) or name not in label_distribution:\n","        standard_names.append(name)\n","\n","if not standard_names:\n","    standard_names = TARGET_CLASS_NAMES\n","\n","STANDARD_NAMES = standard_names\n","NUM_CLASSES = len(STANDARD_NAMES)\n","\n","print(\"\\nDetected classes for LARa:\")\n","print(f\"  num_classes = {NUM_CLASSES}\")\n","print(f\"  class order = {STANDARD_NAMES}\")\n","\n","# Map unified label IDs -> contiguous indices [0, NUM_CLASSES)\n","class_label_ids: List[int] = []\n","for name in STANDARD_NAMES:\n","    if name not in unified_labels:\n","        raise KeyError(f\"Class name '{name}' not found in unified_labels of labels.json.\")\n","    class_label_ids.append(int(unified_labels[name]))\n","\n","label_id_to_index: Dict[int, int] = {raw_id: idx for idx, raw_id in enumerate(class_label_ids)}\n","\n","print(\"\\nLabel ID mapping (raw_id -> index):\")\n","for idx, raw_id in enumerate(class_label_ids):\n","    print(f\"  {raw_id} -> {idx} ({STANDARD_NAMES[idx]})\")\n","\n","# ---------------------------\n","# Window settings: 3 s @ 50 Hz = 150 samples; 50% overlap → stride = 75\n","# ---------------------------\n","NUM_CHANNELS     = 6      # Use 6 channels (ACC + GYRO)\n","SAMPLES_PER_WIN  = 150\n","WIN_OVERLAP      = 0.5\n","STRIDE_SAMPLES   = int(SAMPLES_PER_WIN * (1 - WIN_OVERLAP))  # = 75\n","\n","# Training hyperparameters — aligned with section 3.4 of the paper\n","EPOCHS        = 30\n","LEARNING_RATE = 1e-4\n","WEIGHT_DECAY  = 1e-6\n","STEP_SIZE     = 10\n","GAMMA         = 0.9\n","\n","DROPOUT_P        = 0.5\n","HIDDEN_UNITS     = 128\n","CONV_CHANNELS    = 64\n","KERNEL_SIZE      = 9      # RealWorld2016 / RWHAR uses 9\n","PROJECTION_DIM   = 128\n","ATTN_HEADS       = 4      # Attention variant: use 4 heads\n","ATTN_LAYERS      = 3      # Attention-based architectures: 3 layers\n","MAX_CONTEXT_WINS = 200    # Maximum context length used for positional encoding\n","\n","# DataLoader batch size (both training and testing treat the batch as the context)\n","TRAIN_BATCH  = 100\n","EVAL_BATCH   = 100\n","\n","print(f\"\\nParams: num_classes={NUM_CLASSES}, num_channels={NUM_CHANNELS}, samples_per_window={SAMPLES_PER_WIN}\")\n","print(\n","    f\"Context length (windows) = train_batch = {TRAIN_BATCH}, \"\n","    f\"epochs={EPOCHS}, lr={LEARNING_RATE}, wd={WEIGHT_DECAY}, step@{STEP_SIZE}x{GAMMA}\\n\"\n",")\n","\n","# ---------------------------\n","# 1) Model: DeepConvContext (Intra LSTM + Bi-Attention Inter, batch-as-context)\n","# ---------------------------\n","class DeepConvLSTM_Intra(nn.Module):\n","    \"\"\"\n","    Intra-window branch:\n","    4xConv1d(64, k=9) + ReLU → 1-layer LSTM(128)\n","    Feature extraction part aligned with the original DeepConvLSTM\n","    \"\"\"\n","    def __init__(self, in_ch: int = 6, conv_ch: int = 64, kernel_size: int = 9, hidden: int = 128):\n","        super().__init__()\n","        pad = kernel_size // 2\n","        self.conv1 = nn.Conv1d(in_ch,   conv_ch, kernel_size, padding=pad)\n","        self.conv2 = nn.Conv1d(conv_ch, conv_ch, kernel_size, padding=pad)\n","        self.conv3 = nn.Conv1d(conv_ch, conv_ch, kernel_size, padding=pad)\n","        self.conv4 = nn.Conv1d(conv_ch, conv_ch, kernel_size, padding=pad)\n","        self.relu  = nn.ReLU(inplace=True)\n","        self.lstm  = nn.LSTM(\n","            input_size=conv_ch,\n","            hidden_size=hidden,\n","            num_layers=1,\n","            batch_first=True\n","        )\n","\n","    def forward(self, x_win: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        x_win: (B, C, T) — B windows\n","        return: (B, hidden)\n","        \"\"\"\n","        x = self.relu(self.conv1(x_win))\n","        x = self.relu(self.conv2(x))\n","        x = self.relu(self.conv3(x))\n","        x = self.relu(self.conv4(x))        # (B, conv_ch, T)\n","        x = x.permute(0, 2, 1)              # (B, T, conv_ch)\n","        _, (h_n, _) = self.lstm(x)          # h_n: (1, B, hidden)\n","        return h_n[-1]                      # (B, hidden)\n","\n","\n","class PositionalEncoding1D(nn.Module):\n","    \"\"\"\n","    Learnable positional encoding for 1D sequences of windows.\n","    \"\"\"\n","    def __init__(self, d_model: int, max_len: int = 200):\n","        super().__init__()\n","        self.pos_embedding = nn.Embedding(max_len, d_model)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        x: (B_seq, S, D)\n","        \"\"\"\n","        seq_len = x.size(1)\n","        if seq_len > self.pos_embedding.num_embeddings:\n","            raise ValueError(\n","                f\"Sequence length {seq_len} exceeds max_len {self.pos_embedding.num_embeddings} \"\n","                \"used for positional encoding.\"\n","            )\n","        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)  # (1, S)\n","        pos = self.pos_embedding(positions)                              # (1, S, D)\n","        return x + pos\n","\n","\n","class MultiHeadSelfAttentionBlock(nn.Module):\n","    \"\"\"\n","    Self-attention block: Multi-head attention + residual + LayerNorm.\n","    Internal dropout is set to 0.0 to match the paper (no dropout inside attention).\n","    \"\"\"\n","    def __init__(self, dim: int, num_heads: int):\n","        super().__init__()\n","        self.mha  = nn.MultiheadAttention(\n","            embed_dim=dim,\n","            num_heads=num_heads,\n","            dropout=0.0,\n","            batch_first=True\n","        )\n","        self.norm = nn.LayerNorm(dim)\n","\n","    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n","        # x: (B_seq, S, D)\n","        attn_out, _ = self.mha(x, x, x, attn_mask=attn_mask, need_weights=False)\n","        return self.norm(x + attn_out)\n","\n","\n","class DeepConvContext_BiAttention(nn.Module):\n","    \"\"\"\n","    DeepConvContext attention variant (paper-aligned):\n","      Intra: Conv x4 + 1-layer LSTM(128)\n","      Projection: FC(128 → 128)\n","      Inter: 3-layer 4-head multi-head self-attention stack\n","             - bidirectional: no mask (model can attend to all windows in the context)\n","             - unidirectional: optional causal mask\n","      Positional encoding is added before the inter-window attention.\n","      Final classifier: Dropout(0.5) → FC(num_classes)\n","    \"\"\"\n","    def __init__(self,\n","                 num_channels: int = 6,\n","                 num_classes: int = 8,\n","                 conv_channels: int = 64,\n","                 hidden_intra: int = 128,\n","                 projection_dim: int = 128,\n","                 attn_heads: int = 4,\n","                 num_attn_layers: int = 3,\n","                 max_context_len: int = 200,\n","                 dropout: float = 0.5,\n","                 bidirectional: bool = True):\n","        super().__init__()\n","\n","        # Intra-window branch\n","        self.intra = DeepConvLSTM_Intra(\n","            in_ch=num_channels,\n","            conv_ch=conv_channels,\n","            kernel_size=KERNEL_SIZE,\n","            hidden=hidden_intra\n","        )\n","\n","        # Projection layer\n","        self.proj = nn.Linear(hidden_intra, projection_dim)\n","\n","        # Positional encoding\n","        self.pos_enc = PositionalEncoding1D(projection_dim, max_len=max_context_len)\n","\n","        # Inter-window Attention stack\n","        self.bidirectional = bidirectional\n","        self.attn_layers = nn.ModuleList([\n","            MultiHeadSelfAttentionBlock(projection_dim, attn_heads)\n","            for _ in range(num_attn_layers)\n","        ])\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc      = nn.Linear(projection_dim, num_classes)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        x: (B, C, T) — B time-ordered windows, the batch is treated as the context\n","        return: (B, num_classes)\n","        \"\"\"\n","        B, C, T = x.shape\n","\n","        # Intra-window: Conv+LSTM\n","        feats = self.intra(x)          # (B, hidden_intra)\n","        proj  = self.proj(feats)       # (B, D)\n","\n","        # Treat the batch dimension as a sequence of length B\n","        seq = proj.unsqueeze(0)        # (1, B, D)\n","        seq = self.pos_enc(seq)        # (1, B, D)\n","\n","        # Inter-window Attention\n","        attn_mask = None\n","        if not self.bidirectional:\n","            # Unidirectional attention: causal mask\n","            S_len = B\n","            attn_mask = torch.ones(\n","                S_len, S_len,\n","                device=x.device,\n","                dtype=torch.bool\n","            ).triu(diagonal=1)         # True above diagonal → masked\n","\n","        for layer in self.attn_layers:\n","            seq = layer(seq, attn_mask=attn_mask)   # (1, B, D)\n","\n","        seq = self.dropout(seq)\n","        logits = self.fc(seq)         # (1, B, num_classes)\n","        return logits.squeeze(0)      # (B, num_classes)\n","\n","# ---------------------------\n","# 2) Dataset: window-level, batch as context sequence (LARa MbientLab)\n","# ---------------------------\n","class HARWindowDataset(Dataset):\n","    \"\"\"\n","    Window-level dataset for LARa MbientLab.\n","    Uses pre-sliced windows stored under:\n","      data/lara/mbientlab/proc/windows/fold_xx/X_{train,test}.npy and corresponding parquet metadata.\n","    The DataLoader batch (with shuffle=False) is interpreted as one context sequence.\n","    \"\"\"\n","    def __init__(\n","        self,\n","        windows_dir: Path,\n","        split: str = \"train\",\n","        num_channels: int = NUM_CHANNELS,\n","        label_id_to_index: Optional[Dict[int, int]] = None,\n","    ):\n","        super().__init__()\n","        assert split in {\"train\", \"test\"}\n","        if label_id_to_index is None:\n","            raise ValueError(\"label_id_to_index must be provided.\")\n","\n","        if split == \"train\":\n","            X_path = windows_dir / \"X_train.npy\"\n","            meta_path = windows_dir / \"X_train.parquet\"\n","        else:\n","            X_path = windows_dir / \"X_test.npy\"\n","            meta_path = windows_dir / \"X_test.parquet\"\n","\n","        if not X_path.exists() or not meta_path.exists():\n","            raise FileNotFoundError(f\"Missing data files for split='{split}' under {windows_dir}\")\n","\n","        X = np.load(X_path)  # (N_windows, T, C_total)\n","        meta = pd.read_parquet(meta_path)\n","\n","        if len(meta) != X.shape[0]:\n","            raise ValueError(f\"Metadata length ({len(meta)}) does not match X shape[0] ({X.shape[0]}).\")\n","\n","        # Filter by target labels (unified label IDs)\n","        label_ids_raw = meta[\"label\"].astype(int).to_numpy()\n","        keep_mask = np.isin(label_ids_raw, list(label_id_to_index.keys()))\n","        X = X[keep_mask]\n","        meta = meta.loc[keep_mask].reset_index(drop=True)\n","        label_ids_raw = label_ids_raw[keep_mask]\n","\n","        if X.size == 0:\n","            raise RuntimeError(f\"No windows remain for split='{split}' after filtering to selected classes.\")\n","\n","        labels = np.array([label_id_to_index[int(l)] for l in label_ids_raw], dtype=np.int64)\n","\n","        # Sort windows by (subject_id, session_id, start_idx) to preserve temporal order\n","        meta = meta.copy()\n","        meta[\"subject_id_str\"] = meta[\"subject_id\"].astype(str)\n","        meta[\"session_id_str\"] = meta[\"session_id\"].astype(str)\n","        order = np.lexsort(\n","            (\n","                meta[\"start_idx\"].to_numpy(),\n","                meta[\"session_id_str\"].to_numpy(),\n","                meta[\"subject_id_str\"].to_numpy(),\n","            )\n","        )\n","\n","        X = X[order]\n","        labels = labels[order]\n","\n","        # Keep first num_channels (ax, ay, az, gx, gy, gz) and transpose to (N, C, T)\n","        if X.shape[-1] < num_channels:\n","            raise ValueError(f\"Expected at least {num_channels} channels, got {X.shape[-1]}.\")\n","        wins = X[..., :num_channels].astype(np.float32)  # (N, T, C)\n","        wins = np.transpose(wins, (0, 2, 1))             # (N, C, T)\n","\n","        self.wins = wins\n","        self.labels = labels\n","\n","        print(\n","            f\"[HARWindowDataset] split={split} | windows={self.wins.shape[0]}, \"\n","            f\"channels={self.wins.shape[1]}, samples={self.wins.shape[2]}\"\n","        )\n","\n","    def __len__(self) -> int:\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx: int):\n","        x = torch.from_numpy(self.wins[idx])  # (C, T)\n","        y = torch.tensor(self.labels[idx], dtype=torch.long)\n","        return x, y\n","\n","# ---------------------------\n","# 3) Evaluation utilities: window → sample + mAP@tIoU\n","# ---------------------------\n","def unwindow_to_samples(\n","    seq_probs: np.ndarray,\n","    seq_labels: np.ndarray,\n","    window_size: int,\n","    stride: int,\n","    num_classes: int,\n",") -> Tuple[np.ndarray, np.ndarray]:\n","    \"\"\"\n","    Aggregate window-level probabilities to the sample level (use voting + averaging in overlapping regions),\n","    and return sample-level labels.\n","    All windows from one fold are concatenated into one long time sequence.\n","    \"\"\"\n","    if seq_probs.size == 0:\n","        return np.empty((0, num_classes)), np.empty((0,), dtype=int)\n","\n","    Nw = seq_probs.shape[0]\n","    total_samples = (Nw - 1) * stride + window_size\n","\n","    votes = np.zeros((total_samples, num_classes), dtype=np.float64)\n","    counts = np.zeros(total_samples, dtype=np.int32)\n","    label_votes = np.zeros((total_samples, num_classes), dtype=np.int32)\n","\n","    for i in range(Nw):\n","        s = i * stride\n","        e = min(s + window_size, total_samples)\n","        votes[s:e] += seq_probs[i]\n","        counts[s:e] += 1\n","        label_votes[s:e, seq_labels[i]] += 1\n","\n","    counts[counts == 0] = 1\n","    probs_samples  = votes / counts[:, None]\n","    labels_samples = label_votes.argmax(axis=1)\n","    return probs_samples, labels_samples\n","\n","\n","def compute_tiou_mAP(\n","    probs: np.ndarray,\n","    labels: np.ndarray,\n","    num_classes: int,\n","    thresholds: List[float] = [0.1, 0.3, 0.5, 0.7, 0.9],\n",") -> float:\n","    \"\"\"\n","    Compute mAP over multiple tIoU thresholds.\n","    \"\"\"\n","    if probs.size == 0:\n","        return 0.0\n","\n","    hard = probs.argmax(axis=1)\n","\n","    def segments_from_seq(seq, cls, scores=None):\n","        segs, in_seg, st = [], False, 0\n","        for i, v in enumerate(seq):\n","            if v == cls and not in_seg:\n","                in_seg, st = True, i\n","            elif v != cls and in_seg:\n","                sc = scores[st:i].mean() if (scores is not None and i > st) else 0.0\n","                segs.append((st, i - 1, sc))\n","                in_seg = False\n","        if in_seg:\n","            sc = scores[st:].mean() if (scores is not None and len(scores[st:]) > 0) else 0.0\n","            segs.append((st, len(seq) - 1, sc))\n","        return segs\n","\n","    APs = []\n","    for c in range(num_classes):\n","        gt = [(s, e) for (s, e, _) in segments_from_seq(labels, c)]\n","        if not gt:\n","            continue\n","        pred = segments_from_seq(hard, c, probs[:, c])\n","        if not pred:\n","            continue\n","        pred = sorted(pred, key=lambda x: x[2], reverse=True)\n","\n","        APs_t = []\n","        for thr in thresholds:\n","            matched = set()\n","            tp = []\n","            fp = []\n","            for (ps, pe, sc) in pred:\n","                ok = False\n","                for gi, (gs, ge) in enumerate(gt):\n","                    if gi in matched:\n","                        continue\n","                    inter = max(0, min(pe, ge) - max(ps, gs) + 1)\n","                    union = max(pe, ge) - min(ps, gs) + 1\n","                    tiou = inter / union if union > 0 else 0.0\n","                    if tiou >= thr:\n","                        ok = True\n","                        matched.add(gi)\n","                        break\n","                tp.append(1 if ok else 0)\n","                fp.append(0 if ok else 1)\n","            if not tp:\n","                APs_t.append(0.0)\n","                continue\n","            tp = np.cumsum(np.asarray(tp))\n","            fp = np.cumsum(np.asarray(fp))\n","            recalls    = tp / max(1, len(gt))\n","            precisions = tp / np.maximum(1, tp + fp)\n","            ap = np.sum((recalls - np.concatenate([[0.0], recalls[:-1]])) * precisions)\n","            APs_t.append(ap)\n","        APs.append(float(np.mean(APs_t)))\n","\n","    return float(np.mean(APs)) if APs else 0.0\n","\n","\n","@torch.no_grad()\n","def evaluate(\n","    model: nn.Module,\n","    loader: DataLoader,\n","    criterion: nn.Module,\n","):\n","    model.eval()\n","    total_loss = 0.0\n","    all_probs: List[np.ndarray] = []\n","    all_labels: List[np.ndarray] = []\n","\n","    for x, y in loader:\n","        x = x.to(device, non_blocking=True)  # (B, C, T)\n","        y = y.to(device, non_blocking=True)  # (B,)\n","\n","        logits = model(x)                    # (B, K)\n","        loss = criterion(logits, y)\n","        total_loss += loss.item()\n","\n","        probs = torch.softmax(logits, dim=-1).cpu().numpy()\n","        all_probs.append(probs)\n","        all_labels.append(y.cpu().numpy())\n","\n","    probs_win = np.concatenate(all_probs, axis=0) if all_probs else np.empty((0, NUM_CLASSES))\n","    labels_win = np.concatenate(all_labels, axis=0) if all_labels else np.empty((0,), dtype=int)\n","\n","    # window → sample\n","    probs_s, labels_s = unwindow_to_samples(\n","        probs_win, labels_win,\n","        SAMPLES_PER_WIN, STRIDE_SAMPLES, NUM_CLASSES\n","    )\n","\n","    if probs_s.size > 0:\n","        preds_s = probs_s.argmax(1)\n","        acc  = accuracy_score(labels_s, preds_s)\n","        f1   = f1_score(labels_s, preds_s, average='macro', zero_division=0)\n","        cm   = confusion_matrix(labels_s, preds_s, labels=list(range(NUM_CLASSES)))\n","        mAP  = compute_tiou_mAP(probs_s, labels_s, NUM_CLASSES)\n","    else:\n","        acc, f1, mAP = 0.0, 0.0, 0.0\n","        cm = np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=int)\n","\n","    return total_loss / max(1, len(loader)), acc, f1, mAP, cm\n","\n","\n","def train_epoch(\n","    model: nn.Module,\n","    loader: DataLoader,\n","    criterion: nn.Module,\n","    optimizer: optim.Optimizer,\n","):\n","    model.train()\n","    total_loss = 0.0\n","    all_preds: List[int] = []\n","    all_labels: List[int] = []\n","\n","    for x, y in loader:\n","        x = x.to(device, non_blocking=True)  # (B, C, T)\n","        y = y.to(device, non_blocking=True)  # (B,)\n","\n","        logits = model(x)                    # (B, K)\n","        loss = criterion(logits, y)\n","\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","\n","        pred = logits.argmax(-1).detach().cpu().numpy()\n","        all_preds.extend(pred.tolist())\n","        all_labels.extend(y.detach().cpu().numpy().tolist())\n","        total_loss += loss.item()\n","\n","    acc = accuracy_score(all_labels, all_preds) if all_preds else 0.0\n","    f1  = f1_score(all_labels, all_preds, average='macro', zero_division=0) if all_preds else 0.0\n","    return total_loss / max(1, len(loader)), acc, f1\n","\n","# ---------------------------\n","# 4) Main pipeline: LOSO cross-validation (batch as context, LARa)\n","# ---------------------------\n","models_dir   = BASE_DIR / \"models_lara_deepconvcontext_biattn\"\n","fig_dir      = BASE_DIR / \"figures_lara_deepconvcontext_biattn\"\n","logs_dir     = BASE_DIR / \"logs\"\n","models_dir.mkdir(exist_ok=True, parents=True)\n","fig_dir.mkdir(exist_ok=True, parents=True)\n","logs_dir.mkdir(exist_ok=True, parents=True)\n","\n","# splits.json for LARa is a dict: {\"0\": {...}, \"1\": {...}, ...}\n","fold_ids = sorted(int(k) for k in splits_cfg.keys())\n","fold_cfgs = [splits_cfg[str(fid)] for fid in fold_ids]\n","\n","all_fold_rows = []\n","\n","for fold in fold_cfgs:\n","    k = int(fold[\"fold_id\"])\n","    test_subj = fold[\"test_subject\"]\n","    fold_tag = f\"fold_{k:02d}\"\n","    windows_dir = WINDOWS_ROOT / fold_tag\n","\n","    print(f\"\\n{'='*100}\")\n","    print(f\"Fold {k}: test_subject={test_subj} ({fold_tag})\")\n","    print(f\"{'='*100}\")\n","    print(f\"Windows directory: {windows_dir}\")\n","\n","    # Datasets\n","    train_ds = HARWindowDataset(\n","        windows_dir=windows_dir,\n","        split=\"train\",\n","        num_channels=NUM_CHANNELS,\n","        label_id_to_index=label_id_to_index,\n","    )\n","    test_ds = HARWindowDataset(\n","        windows_dir=windows_dir,\n","        split=\"test\",\n","        num_channels=NUM_CHANNELS,\n","        label_id_to_index=label_id_to_index,\n","    )\n","\n","    # Note: shuffle=False to preserve the temporal order of windows within each batch; context length = batch_size\n","    train_loader = DataLoader(\n","        train_ds,\n","        batch_size=TRAIN_BATCH,\n","        shuffle=False,\n","        drop_last=True,   # Keep a fixed context length during training\n","        num_workers=2,\n","        pin_memory=True,\n","        persistent_workers=False\n","    )\n","    test_loader  = DataLoader(\n","        test_ds,\n","        batch_size=EVAL_BATCH,\n","        shuffle=False,\n","        drop_last=False,  # Do not drop the last batch during testing\n","        num_workers=2,\n","        pin_memory=True,\n","        persistent_workers=False\n","    )\n","\n","    print(\n","        f\"Number of training windows: {len(train_ds)}; \"\n","        f\"number of test windows: {len(test_ds)}; \"\n","        f\"context length per batch = {TRAIN_BATCH} windows\"\n","    )\n","\n","    # Class weights (window-level)\n","    binc = np.bincount(train_ds.labels, minlength=NUM_CLASSES)\n","    w = (1.0 / (binc + 1e-12))\n","    w = w / w.sum() * NUM_CLASSES\n","    class_weights = torch.tensor(w, dtype=torch.float32, device=device)\n","    print(\"Class weights:\", np.round(w, 4))\n","\n","    # Model = Intra LSTM + Bi-Attention Inter (batch as the context sequence)\n","    model = DeepConvContext_BiAttention(\n","        num_channels=NUM_CHANNELS,\n","        num_classes=NUM_CLASSES,\n","        conv_channels=CONV_CHANNELS,\n","        hidden_intra=HIDDEN_UNITS,\n","        projection_dim=PROJECTION_DIM,\n","        attn_heads=ATTN_HEADS,\n","        num_attn_layers=ATTN_LAYERS,\n","        max_context_len=MAX_CONTEXT_WINS,\n","        dropout=DROPOUT_P,\n","        bidirectional=True   # Fixed to the Bi-Attention variant\n","    ).to(device)\n","\n","    criterion = nn.CrossEntropyLoss(weight=class_weights)\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n","\n","    # Training\n","    for epoch in range(1, EPOCHS + 1):\n","        tr_loss, tr_acc, tr_f1 = train_epoch(model, train_loader, criterion, optimizer)\n","        scheduler.step()\n","\n","        if epoch % 5 == 0 or epoch == 1:\n","            print(\n","                f\"[Fold {k}] Epoch {epoch:02d}/{EPOCHS} | \"\n","                f\"loss={tr_loss:.4f} acc={tr_acc:.4f} f1={tr_f1:.4f}\"\n","            )\n","\n","    # Testing (sample-level metrics)\n","    te_loss, te_acc, te_f1, te_mAP, cm = evaluate(model, test_loader, criterion)\n","    print(\n","        f\"\\nTest results (Fold {k}, subj {test_subj}): \"\n","        f\"Acc={te_acc:.4f}  Macro-F1={te_f1:.4f}  mAP@tIoU(5 thresholds)={te_mAP:.4f}\"\n","    )\n","\n","    # Save model\n","    ckpt_path = models_dir / f\"deepconvcontext_biattn_lara_fold{k}.pth\"\n","    torch.save(model.state_dict(), ckpt_path)\n","    print(f\"Saved checkpoint: {ckpt_path}\")\n","\n","    all_fold_rows.append({\n","        \"fold\": k,\n","        \"test_subject\": test_subj,\n","        \"train_windows\": int(len(train_ds)),\n","        \"test_windows\":  int(len(test_ds)),\n","        \"context_length_windows\": TRAIN_BATCH,\n","        \"test_accuracy\":   float(te_acc),\n","        \"test_f1_macro\":   float(te_f1),\n","        \"test_mAP\":        float(te_mAP),\n","        \"confusion_matrix\": cm.tolist()\n","    })\n","\n","    # Confusion matrix (sample-level)\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(\n","        cm,\n","        annot=True,\n","        fmt=\"d\",\n","        cmap=\"Blues\",\n","        xticklabels=STANDARD_NAMES,\n","        yticklabels=STANDARD_NAMES\n","    )\n","    plt.title(f\"LARa DeepConvContext Bi-Attention - Confusion Matrix - Fold {k} (Test: {test_subj})\")\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"True\")\n","    plt.tight_layout()\n","    fig_path = fig_dir / f\"lara_deepconvcontext_biattn_cm_fold{k}.png\"\n","    plt.savefig(fig_path, dpi=150)\n","    plt.close()\n","    print(f\"Saved confusion matrix: {fig_path}\")\n","\n","# ---------------------------\n","# 5) LOSO summary\n","# ---------------------------\n","if all_fold_rows:\n","    accs = np.array([r[\"test_accuracy\"] for r in all_fold_rows], dtype=float)\n","    f1s  = np.array([r[\"test_f1_macro\"] for r in all_fold_rows], dtype=float)\n","    mAPs = np.array([r[\"test_mAP\"] for r in all_fold_rows], dtype=float)\n","\n","    avg_acc = float(accs.mean())\n","    avg_f1  = float(f1s.mean())\n","    avg_mAP = float(mAPs.mean())\n","    std_acc = float(accs.std())\n","    std_f1  = float(f1s.std())\n","    std_mAP = float(mAPs.std())\n","else:\n","    avg_acc = avg_f1 = avg_mAP = 0.0\n","    std_acc = std_f1 = std_mAP = 0.0\n","\n","print(f\"\\n{'='*100}\")\n","print(\"LARa-MbientLab LOSO cross-validation results (sample-level evaluation, Bi-Attention variant, batch-as-context):\")\n","print(\n","    f\"Acc = {avg_acc:.4f} ± {std_acc:.4f} | \"\n","    f\"Macro-F1 = {avg_f1:.4f} ± {std_f1:.4f} | \"\n","    f\"mAP = {avg_mAP:.4f} ± {std_mAP:.4f}\"\n",")\n","\n","summary = {\n","    \"model\": \"DeepConvContext (Bi-Attention inter-window, batch-as-context, paper-aligned)\",\n","    \"dataset\": \"LARa-MbientLab\",\n","    \"validation\": \"LOSO\",\n","    \"num_folds\": len(all_fold_rows),\n","    \"seed\": SEED,\n","    \"class_order\": STANDARD_NAMES,\n","    \"class_label_ids\": class_label_ids,\n","    \"avg_accuracy\": avg_acc,\n","    \"std_accuracy\": std_acc,\n","    \"avg_f1_macro\": avg_f1,\n","    \"std_f1_macro\": std_f1,\n","    \"avg_mAP\": avg_mAP,\n","    \"std_mAP\": std_mAP,\n","    \"per_fold_results\": all_fold_rows\n","}\n","\n","results_path = logs_dir / \"deepconvcontext_biattn_lara_results_seed1.json\"\n","with open(results_path, \"w\") as f:\n","    json.dump(summary, f, indent=2)\n","\n","print(f\"\\nSaved results JSON: {results_path}\")\n","print(f\"Saved model checkpoints: {models_dir}/deepconvcontext_biattn_lara_fold*.pth\")\n","print(f\"Saved confusion matrices: {fig_dir}/lara_deepconvcontext_biattn_cm_fold*.png\")\n","print(f\"\\n{'='*100}\")\n","print(\"Step 10 completed (Bi-Attention, LARa MbientLab variant)\")\n","print(f\"{'='*100}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b77ZOikerT_7","executionInfo":{"status":"ok","timestamp":1763481462836,"user_tz":0,"elapsed":156457,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"731a226e-886d-4bc1-ec1e-c8100027602c"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Step 10: DeepConvContext (Bi-Attention, paper-aligned variant · LARa MbientLab)\n","====================================================================================================\n","Device in use: cuda\n","\n","Detected classes for LARa:\n","  num_classes = 6\n","  class order = ['walking', 'running', 'sitting', 'standing', 'upstairs', 'downstairs']\n","\n","Label ID mapping (raw_id -> index):\n","  1 -> 0 (walking)\n","  2 -> 1 (running)\n","  3 -> 2 (sitting)\n","  4 -> 3 (standing)\n","  5 -> 4 (upstairs)\n","  6 -> 5 (downstairs)\n","\n","Params: num_classes=6, num_channels=6, samples_per_window=150\n","Context length (windows) = train_batch = 100, epochs=30, lr=0.0001, wd=1e-06, step@10x0.9\n","\n","\n","====================================================================================================\n","Fold 0: test_subject=S07 (fold_00)\n","====================================================================================================\n","Windows directory: data/lara/mbientlab/proc/windows/fold_00\n","[HARWindowDataset] split=train | windows=4965, channels=6, samples=150\n","[HARWindowDataset] split=test | windows=766, channels=6, samples=150\n","Number of training windows: 4965; number of test windows: 766; context length per batch = 100 windows\n","Class weights: [0.2564 0.2452 0.3758 4.4293 0.0882 0.6052]\n","[Fold 0] Epoch 01/30 | loss=2.0541 acc=0.1796 f1=0.1408\n","[Fold 0] Epoch 05/30 | loss=1.8757 acc=0.2063 f1=0.1557\n","[Fold 0] Epoch 10/30 | loss=1.5375 acc=0.3261 f1=0.2462\n","[Fold 0] Epoch 15/30 | loss=1.5318 acc=0.2957 f1=0.2449\n","[Fold 0] Epoch 20/30 | loss=1.2432 acc=0.4304 f1=0.3917\n","[Fold 0] Epoch 25/30 | loss=1.2322 acc=0.4427 f1=0.4125\n","[Fold 0] Epoch 30/30 | loss=1.0991 acc=0.4800 f1=0.4693\n","\n","Test results (Fold 0, subj S07): Acc=0.4159  Macro-F1=0.3743  mAP@tIoU(5 thresholds)=0.2654\n","Saved checkpoint: models_lara_deepconvcontext_biattn/deepconvcontext_biattn_lara_fold0.pth\n","Saved confusion matrix: figures_lara_deepconvcontext_biattn/lara_deepconvcontext_biattn_cm_fold0.png\n","\n","====================================================================================================\n","Fold 1: test_subject=S08 (fold_01)\n","====================================================================================================\n","Windows directory: data/lara/mbientlab/proc/windows/fold_01\n","[HARWindowDataset] split=train | windows=5072, channels=6, samples=150\n","[HARWindowDataset] split=test | windows=659, channels=6, samples=150\n","Number of training windows: 5072; number of test windows: 659; context length per batch = 100 windows\n","Class weights: [0.3364 0.2998 0.5166 4.1093 0.1123 0.6255]\n","[Fold 1] Epoch 01/30 | loss=2.0468 acc=0.1622 f1=0.1354\n","[Fold 1] Epoch 05/30 | loss=1.8254 acc=0.2384 f1=0.1842\n","[Fold 1] Epoch 10/30 | loss=1.4239 acc=0.3662 f1=0.3489\n","[Fold 1] Epoch 15/30 | loss=1.3291 acc=0.3668 f1=0.3750\n","[Fold 1] Epoch 20/30 | loss=1.2297 acc=0.4070 f1=0.4280\n","[Fold 1] Epoch 25/30 | loss=1.1288 acc=0.4590 f1=0.4676\n","[Fold 1] Epoch 30/30 | loss=1.0246 acc=0.4818 f1=0.5014\n","\n","Test results (Fold 1, subj S08): Acc=0.4167  Macro-F1=0.3628  mAP@tIoU(5 thresholds)=0.0884\n","Saved checkpoint: models_lara_deepconvcontext_biattn/deepconvcontext_biattn_lara_fold1.pth\n","Saved confusion matrix: figures_lara_deepconvcontext_biattn/lara_deepconvcontext_biattn_cm_fold1.png\n","\n","====================================================================================================\n","Fold 2: test_subject=S09 (fold_02)\n","====================================================================================================\n","Windows directory: data/lara/mbientlab/proc/windows/fold_02\n","[HARWindowDataset] split=train | windows=4960, channels=6, samples=150\n","[HARWindowDataset] split=test | windows=771, channels=6, samples=150\n","Number of training windows: 4960; number of test windows: 771; context length per batch = 100 windows\n","Class weights: [0.3297 0.2756 0.464  4.2306 0.1139 0.5862]\n","[Fold 2] Epoch 01/30 | loss=2.0866 acc=0.1651 f1=0.1377\n","[Fold 2] Epoch 05/30 | loss=1.8338 acc=0.2241 f1=0.1857\n","[Fold 2] Epoch 10/30 | loss=1.5126 acc=0.3263 f1=0.2935\n","[Fold 2] Epoch 15/30 | loss=1.2933 acc=0.3873 f1=0.3612\n","[Fold 2] Epoch 20/30 | loss=1.1393 acc=0.4535 f1=0.4703\n","[Fold 2] Epoch 25/30 | loss=1.0446 acc=0.4694 f1=0.5123\n","[Fold 2] Epoch 30/30 | loss=0.9479 acc=0.5206 f1=0.5642\n","\n","Test results (Fold 2, subj S09): Acc=0.3575  Macro-F1=0.3411  mAP@tIoU(5 thresholds)=0.1814\n","Saved checkpoint: models_lara_deepconvcontext_biattn/deepconvcontext_biattn_lara_fold2.pth\n","Saved confusion matrix: figures_lara_deepconvcontext_biattn/lara_deepconvcontext_biattn_cm_fold2.png\n","\n","====================================================================================================\n","Fold 3: test_subject=S10 (fold_03)\n","====================================================================================================\n","Windows directory: data/lara/mbientlab/proc/windows/fold_03\n","[HARWindowDataset] split=train | windows=4858, channels=6, samples=150\n","[HARWindowDataset] split=test | windows=873, channels=6, samples=150\n","Number of training windows: 4858; number of test windows: 873; context length per batch = 100 windows\n","Class weights: [0.2933 0.2723 0.5284 4.2176 0.0997 0.5888]\n","[Fold 3] Epoch 01/30 | loss=1.9999 acc=0.1796 f1=0.1408\n","[Fold 3] Epoch 05/30 | loss=1.8128 acc=0.2233 f1=0.1765\n","[Fold 3] Epoch 10/30 | loss=1.5051 acc=0.3229 f1=0.2799\n","[Fold 3] Epoch 15/30 | loss=1.3519 acc=0.3690 f1=0.3687\n","[Fold 3] Epoch 20/30 | loss=1.2994 acc=0.3942 f1=0.3867\n","[Fold 3] Epoch 25/30 | loss=1.2179 acc=0.3954 f1=0.3978\n","[Fold 3] Epoch 30/30 | loss=1.1705 acc=0.4233 f1=0.4220\n","\n","Test results (Fold 3, subj S10): Acc=0.4153  Macro-F1=0.3774  mAP@tIoU(5 thresholds)=0.2126\n","Saved checkpoint: models_lara_deepconvcontext_biattn/deepconvcontext_biattn_lara_fold3.pth\n","Saved confusion matrix: figures_lara_deepconvcontext_biattn/lara_deepconvcontext_biattn_cm_fold3.png\n","\n","====================================================================================================\n","Fold 4: test_subject=S11 (fold_04)\n","====================================================================================================\n","Windows directory: data/lara/mbientlab/proc/windows/fold_04\n","[HARWindowDataset] split=train | windows=4985, channels=6, samples=150\n","[HARWindowDataset] split=test | windows=746, channels=6, samples=150\n","Number of training windows: 4985; number of test windows: 746; context length per batch = 100 windows\n","Class weights: [0.2996 0.2772 0.4201 4.3354 0.0963 0.5716]\n","[Fold 4] Epoch 01/30 | loss=2.0360 acc=0.1767 f1=0.1400\n","[Fold 4] Epoch 05/30 | loss=1.8685 acc=0.2465 f1=0.1868\n","[Fold 4] Epoch 10/30 | loss=1.5316 acc=0.3324 f1=0.2755\n","[Fold 4] Epoch 15/30 | loss=1.3600 acc=0.3822 f1=0.3420\n","[Fold 4] Epoch 20/30 | loss=1.2223 acc=0.4241 f1=0.4156\n","[Fold 4] Epoch 25/30 | loss=1.1568 acc=0.4447 f1=0.4414\n","[Fold 4] Epoch 30/30 | loss=1.0178 acc=0.4839 f1=0.5111\n","\n","Test results (Fold 4, subj S11): Acc=0.4471  Macro-F1=0.4002  mAP@tIoU(5 thresholds)=0.2227\n","Saved checkpoint: models_lara_deepconvcontext_biattn/deepconvcontext_biattn_lara_fold4.pth\n","Saved confusion matrix: figures_lara_deepconvcontext_biattn/lara_deepconvcontext_biattn_cm_fold4.png\n","\n","====================================================================================================\n","Fold 5: test_subject=S12 (fold_05)\n","====================================================================================================\n","Windows directory: data/lara/mbientlab/proc/windows/fold_05\n","[HARWindowDataset] split=train | windows=5442, channels=6, samples=150\n","[HARWindowDataset] split=test | windows=289, channels=6, samples=150\n","Number of training windows: 5442; number of test windows: 289; context length per batch = 100 windows\n","Class weights: [0.3008 0.2623 0.4288 4.3482 0.0954 0.5646]\n","[Fold 5] Epoch 01/30 | loss=2.0374 acc=0.1774 f1=0.1404\n","[Fold 5] Epoch 05/30 | loss=1.7868 acc=0.2528 f1=0.1965\n","[Fold 5] Epoch 10/30 | loss=1.4601 acc=0.3417 f1=0.3144\n","[Fold 5] Epoch 15/30 | loss=1.3000 acc=0.3720 f1=0.3690\n","[Fold 5] Epoch 20/30 | loss=1.1870 acc=0.4081 f1=0.4205\n","[Fold 5] Epoch 25/30 | loss=1.0936 acc=0.4428 f1=0.4400\n","[Fold 5] Epoch 30/30 | loss=0.9839 acc=0.4948 f1=0.5258\n","\n","Test results (Fold 5, subj S12): Acc=0.3586  Macro-F1=0.3162  mAP@tIoU(5 thresholds)=0.1830\n","Saved checkpoint: models_lara_deepconvcontext_biattn/deepconvcontext_biattn_lara_fold5.pth\n","Saved confusion matrix: figures_lara_deepconvcontext_biattn/lara_deepconvcontext_biattn_cm_fold5.png\n","\n","====================================================================================================\n","Fold 6: test_subject=S13 (fold_06)\n","====================================================================================================\n","Windows directory: data/lara/mbientlab/proc/windows/fold_06\n","[HARWindowDataset] split=train | windows=4853, channels=6, samples=150\n","[HARWindowDataset] split=test | windows=878, channels=6, samples=150\n","Number of training windows: 4853; number of test windows: 878; context length per batch = 100 windows\n","Class weights: [0.3327 0.331  0.5151 4.0808 0.114  0.6263]\n","[Fold 6] Epoch 01/30 | loss=2.0978 acc=0.1669 f1=0.1358\n","[Fold 6] Epoch 05/30 | loss=1.8809 acc=0.1956 f1=0.1575\n","[Fold 6] Epoch 10/30 | loss=1.4530 acc=0.3412 f1=0.3118\n","[Fold 6] Epoch 15/30 | loss=1.3250 acc=0.3665 f1=0.3539\n","[Fold 6] Epoch 20/30 | loss=1.1824 acc=0.4350 f1=0.4440\n","[Fold 6] Epoch 25/30 | loss=1.0457 acc=0.4817 f1=0.5090\n","[Fold 6] Epoch 30/30 | loss=1.0164 acc=0.4731 f1=0.4991\n","\n","Test results (Fold 6, subj S13): Acc=0.4016  Macro-F1=0.2975  mAP@tIoU(5 thresholds)=0.2082\n","Saved checkpoint: models_lara_deepconvcontext_biattn/deepconvcontext_biattn_lara_fold6.pth\n","Saved confusion matrix: figures_lara_deepconvcontext_biattn/lara_deepconvcontext_biattn_cm_fold6.png\n","\n","====================================================================================================\n","Fold 7: test_subject=S14 (fold_07)\n","====================================================================================================\n","Windows directory: data/lara/mbientlab/proc/windows/fold_07\n","[HARWindowDataset] split=train | windows=4982, channels=6, samples=150\n","[HARWindowDataset] split=test | windows=749, channels=6, samples=150\n","Number of training windows: 4982; number of test windows: 749; context length per batch = 100 windows\n","Class weights: [0.3197 0.2846 0.4718 4.1147 0.1061 0.7031]\n","[Fold 7] Epoch 01/30 | loss=2.0822 acc=0.1622 f1=0.1315\n","[Fold 7] Epoch 05/30 | loss=1.8563 acc=0.2365 f1=0.1779\n","[Fold 7] Epoch 10/30 | loss=1.4498 acc=0.3714 f1=0.3081\n","[Fold 7] Epoch 15/30 | loss=1.3377 acc=0.3939 f1=0.3644\n","[Fold 7] Epoch 20/30 | loss=1.2489 acc=0.3941 f1=0.3918\n","[Fold 7] Epoch 25/30 | loss=1.1651 acc=0.4386 f1=0.4366\n","[Fold 7] Epoch 30/30 | loss=1.0739 acc=0.4761 f1=0.4850\n","\n","Test results (Fold 7, subj S14): Acc=0.3373  Macro-F1=0.2694  mAP@tIoU(5 thresholds)=0.1363\n","Saved checkpoint: models_lara_deepconvcontext_biattn/deepconvcontext_biattn_lara_fold7.pth\n","Saved confusion matrix: figures_lara_deepconvcontext_biattn/lara_deepconvcontext_biattn_cm_fold7.png\n","\n","====================================================================================================\n","LARa-MbientLab LOSO cross-validation results (sample-level evaluation, Bi-Attention variant, batch-as-context):\n","Acc = 0.3938 ± 0.0356 | Macro-F1 = 0.3424 ± 0.0419 | mAP = 0.1872 ± 0.0511\n","\n","Saved results JSON: logs/deepconvcontext_biattn_lara_results_seed1.json\n","Saved model checkpoints: models_lara_deepconvcontext_biattn/deepconvcontext_biattn_lara_fold*.pth\n","Saved confusion matrices: figures_lara_deepconvcontext_biattn/lara_deepconvcontext_biattn_cm_fold*.png\n","\n","====================================================================================================\n","Step 10 completed (Bi-Attention, LARa MbientLab variant)\n","====================================================================================================\n","\n"]}]}]}