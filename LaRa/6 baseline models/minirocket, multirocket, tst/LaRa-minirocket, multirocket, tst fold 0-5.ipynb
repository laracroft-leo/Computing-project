{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyPiJMPW3tqJMT+f8GzUUiDk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install sktime"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P46zPJNNcV2M","executionInfo":{"status":"ok","timestamp":1763125068124,"user_tz":0,"elapsed":7766,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"36546b32-9ca3-4535-9f6a-af4b821f423f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sktime\n","  Downloading sktime-0.39.0-py3-none-any.whl.metadata (32 kB)\n","Requirement already satisfied: joblib<1.6,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from sktime) (1.5.2)\n","Requirement already satisfied: numpy<2.4,>=1.21 in /usr/local/lib/python3.12/dist-packages (from sktime) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from sktime) (25.0)\n","Requirement already satisfied: pandas<2.4.0,>=1.1 in /usr/local/lib/python3.12/dist-packages (from sktime) (2.2.2)\n","Collecting scikit-base<0.13.0,>=0.6.1 (from sktime)\n","  Downloading scikit_base-0.12.6-py3-none-any.whl.metadata (8.8 kB)\n","Requirement already satisfied: scikit-learn<1.8.0,>=0.24 in /usr/local/lib/python3.12/dist-packages (from sktime) (1.6.1)\n","Requirement already satisfied: scipy<2.0.0,>=1.2 in /usr/local/lib/python3.12/dist-packages (from sktime) (1.16.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4.0,>=1.1->sktime) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4.0,>=1.1->sktime) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4.0,>=1.1->sktime) (2025.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.8.0,>=0.24->sktime) (3.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<2.4.0,>=1.1->sktime) (1.17.0)\n","Downloading sktime-0.39.0-py3-none-any.whl (35.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_base-0.12.6-py3-none-any.whl (149 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: scikit-base, sktime\n","Successfully installed scikit-base-0.12.6 sktime-0.39.0\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P7LEHTPhXJoB","executionInfo":{"status":"ok","timestamp":1763124160578,"user_tz":0,"elapsed":4468,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"bee068e7-8c2e-4e06-ec34-1ea720242d3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["⚠️ Note: In Jupyter/Colab, PYTHONHASHSEED must be set before the kernel starts\n","   Suggestion: After setting environment variables, restart the runtime, then run the main code\n","\n","Configured random seeds: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","Generating SEEDS.yaml...\n","Generating requirements.txt...\n","Generating env.txt...\n","Generating environment.yml...\n","Collecting hardware information...\n","Collecting Git information...\n","Saving PyTorch build information...\n","Generating data checksums...\n","  data/ directory does not exist; skipping checksums\n","Computing environment hashes...\n","\n","============================================================\n","Step 0 complete - Reproducible environment configuration (top-conf/journal grade)\n","============================================================\n","Output directory: artifacts/env/\n","  ✓ SEEDS.yaml (seeds: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","  ✓ requirements.txt\n","  ✓ env.txt (with system summary)\n","  ✓ environment.yml\n","  ✓ hardware_log.json\n","  ✓ git_info.json (dirty=False)\n","  ✓ torch_build.txt\n","  ✓ ENV.SHA256 (covers all key files)\n","\n","Strict determinism configuration:\n","  - torch.use_deterministic_algorithms: True (warn_only=False)\n","  - cudnn.deterministic: True\n","  - cudnn.benchmark: False\n","  - TF32 disabled: False\n","  - Environment variables set:\n","    PYTHONHASHSEED: 0\n","    CUBLAS_WORKSPACE_CONFIG: :4096:8\n","    Thread control: OMP/MKL/OPENBLAS/NUMEXPR=1\n","============================================================\n"]}],"source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 0: Reproducible Environment (Colab/Jupyter adapted - top-conf/journal grade)\n","Generate a complete reproducible environment configuration\n","\"\"\"\n","\n","# ===== Set environment variables directly (Colab/Jupyter env) =====\n","import os\n","import sys\n","\n","os.environ[\"PYTHONHASHSEED\"] = \"0\"\n","os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n","os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n","os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n","os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n","os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n","\n","print(\"⚠️ Note: In Jupyter/Colab, PYTHONHASHSEED must be set before the kernel starts\")\n","print(\"   Suggestion: After setting environment variables, restart the runtime, then run the main code\\n\")\n","\n","# ===== Environment variables set; continue normal flow =====\n","import json\n","import hashlib\n","import subprocess\n","from pathlib import Path\n","from datetime import datetime, timezone\n","from contextlib import redirect_stdout\n","import io\n","\n","# Check Python version\n","assert sys.version_info >= (3, 10), f\"Require Python ≥ 3.10, current: {sys.version}\"\n","\n","# Create output directory\n","output_dir = Path(\"artifacts/env\")\n","output_dir.mkdir(parents=True, exist_ok=True)\n","\n","# 1. Multiple random seeds (0–9)\n","SEEDS = list(range(10))\n","print(f\"Configured random seeds: {SEEDS}\")\n","\n","# Import and configure\n","import random\n","import numpy as np\n","import torch\n","\n","# Initialize with the first seed\n","random.seed(SEEDS[0])\n","np.random.seed(SEEDS[0])\n","torch.manual_seed(SEEDS[0])\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEEDS[0])\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # Disable TF32\n","    torch.backends.cuda.matmul.allow_tf32 = False\n","    torch.backends.cudnn.allow_tf32 = False\n","\n","# Enable strict deterministic algorithms (not using warn_only)\n","torch.use_deterministic_algorithms(True)\n","\n","# Set matmul precision\n","if hasattr(torch, 'set_float32_matmul_precision'):\n","    torch.set_float32_matmul_precision(\"high\")\n","\n","# 2. Generate SEEDS.yaml (with fallback)\n","print(\"Generating SEEDS.yaml...\")\n","seeds_config = {\n","    \"seeds\": SEEDS,\n","    \"default_seed\": SEEDS[0],\n","    \"description\": \"Random seeds for python, numpy, torch, sklearn\"\n","}\n","try:\n","    import yaml\n","    with open(output_dir / \"SEEDS.yaml\", \"w\") as f:\n","        yaml.dump(seeds_config, f, default_flow_style=False)\n","except ImportError:\n","    # Fallback if PyYAML is not installed\n","    yaml_content = f\"\"\"seeds: {SEEDS}\n","default_seed: {SEEDS[0]}\n","description: Random seeds for python, numpy, torch, sklearn\n","\"\"\"\n","    with open(output_dir / \"SEEDS.yaml\", \"w\") as f:\n","        f.write(yaml_content)\n","\n","# 3. Generate requirements.txt (frozen versions)\n","print(\"Generating requirements.txt...\")\n","result = subprocess.run(\n","    [sys.executable, \"-m\", \"pip\", \"freeze\"],\n","    capture_output=True, text=True\n",")\n","requirements = result.stdout\n","with open(output_dir / \"requirements.txt\", \"w\") as f:\n","    f.write(requirements)\n","\n","# 4. Collect system info (for env.txt header)\n","import platform\n","system_info = []\n","system_info.append(\"=\"*60)\n","system_info.append(\"Environment Snapshot - System Overview\")\n","system_info.append(\"=\"*60)\n","system_info.append(f\"Time (UTC): {datetime.now(timezone.utc).isoformat()}\")\n","system_info.append(f\"Python: {sys.version}\")\n","system_info.append(f\"Platform: {platform.system()} {platform.release()} ({platform.machine()})\")\n","\n","try:\n","    import psutil\n","    system_info.append(f\"CPU: {psutil.cpu_count(logical=False)} cores / {psutil.cpu_count(logical=True)} threads\")\n","    system_info.append(f\"Memory: {round(psutil.virtual_memory().total / (1024**3), 2)} GB\")\n","except ImportError:\n","    pass\n","\n","system_info.append(f\"PyTorch: {torch.__version__}\")\n","if torch.cuda.is_available():\n","    system_info.append(f\"CUDA: {torch.version.cuda}\")\n","    system_info.append(f\"cuDNN: {torch.backends.cudnn.version()}\")\n","    try:\n","        out = subprocess.run(\n","            [\"nvidia-smi\", \"--query-gpu=driver_version\", \"--format=csv,noheader\"],\n","            capture_output=True, text=True\n","        )\n","        if out.returncode == 0 and out.stdout.strip():\n","            system_info.append(f\"NVIDIA driver: {out.stdout.strip().splitlines()[0]}\")\n","    except:\n","        pass\n","\n","system_info.append(\"\\nEnvironment variables:\")\n","for key in [\"PYTHONHASHSEED\", \"CUBLAS_WORKSPACE_CONFIG\", \"OMP_NUM_THREADS\",\n","            \"MKL_NUM_THREADS\", \"OPENBLAS_NUM_THREADS\", \"NUMEXPR_NUM_THREADS\"]:\n","    system_info.append(f\"  {key}={os.environ.get(key, 'N/A')}\")\n","\n","system_info.append(\"\\n\" + \"=\"*60)\n","system_info.append(\"Installed packages list\")\n","system_info.append(\"=\"*60 + \"\\n\")\n","\n","# 5. Generate env.txt (human-readable + system summary)\n","print(\"Generating env.txt...\")\n","result = subprocess.run(\n","    [sys.executable, \"-m\", \"pip\", \"list\"],\n","    capture_output=True, text=True\n",")\n","with open(output_dir / \"env.txt\", \"w\") as f:\n","    f.write(\"\\n\".join(system_info))\n","    f.write(result.stdout)\n","\n","# 6. Generate environment.yml\n","print(\"Generating environment.yml...\")\n","env_yml = f\"\"\"name: har_lara\n","channels:\n","  - defaults\n","  - conda-forge\n","dependencies:\n","  - python={sys.version_info.major}.{sys.version_info.minor}\n","  - pip\n","  - pip:\n","\"\"\"\n","for line in requirements.strip().split(\"\\n\"):\n","    if line and not line.startswith(\"#\"):\n","        env_yml += f\"      - {line}\\n\"\n","\n","with open(output_dir / \"environment.yml\", \"w\") as f:\n","    f.write(env_yml)\n","\n","# 7. Collect complete hardware information\n","print(\"Collecting hardware information...\")\n","hardware_info = {\n","    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n","    \"python_version\": sys.version,\n","    \"python_executable\": sys.executable,\n","    \"platform\": sys.platform,\n","    \"os\": platform.system(),\n","    \"os_release\": platform.release(),\n","    \"os_version\": platform.version(),\n","    \"machine\": platform.machine(),\n","    \"processor\": platform.processor(),\n","}\n","\n","try:\n","    import psutil\n","    hardware_info[\"cpu_count_physical\"] = psutil.cpu_count(logical=False)\n","    hardware_info[\"cpu_count_logical\"] = psutil.cpu_count(logical=True)\n","    hardware_info[\"memory_total_gb\"] = round(psutil.virtual_memory().total / (1024**3), 2)\n","except ImportError:\n","    pass\n","\n","hardware_info[\"torch_version\"] = torch.__version__\n","\n","if torch.cuda.is_available():\n","    hardware_info[\"gpu_available\"] = True\n","    hardware_info[\"gpu_count\"] = torch.cuda.device_count()\n","    hardware_info[\"gpu_names\"] = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n","    hardware_info[\"cuda_version\"] = torch.version.cuda\n","    hardware_info[\"cudnn_version\"] = torch.backends.cudnn.version()\n","\n","    gpu_details = []\n","    for i in range(torch.cuda.device_count()):\n","        props = torch.cuda.get_device_properties(i)\n","        gpu_details.append({\n","            \"id\": i,\n","            \"name\": props.name,\n","            \"compute_capability\": f\"{props.major}.{props.minor}\",\n","            \"total_memory_gb\": round(props.total_memory / (1024**3), 2),\n","            \"multi_processor_count\": props.multi_processor_count\n","        })\n","    hardware_info[\"gpu_details\"] = gpu_details\n","\n","    try:\n","        out = subprocess.run(\n","            [\"nvidia-smi\", \"--query-gpu=driver_version\", \"--format=csv,noheader\"],\n","            capture_output=True, text=True\n","        )\n","        if out.returncode == 0 and out.stdout.strip():\n","            hardware_info[\"nvidia_driver_version\"] = out.stdout.strip().splitlines()[0]\n","    except:\n","        pass\n","else:\n","    hardware_info[\"gpu_available\"] = False\n","\n","hardware_info[\"deterministic_config\"] = {\n","    \"cudnn_deterministic\": torch.backends.cudnn.deterministic,\n","    \"cudnn_benchmark\": torch.backends.cudnn.benchmark,\n","    \"use_deterministic_algorithms\": True,\n","    \"warn_only\": False,\n","    \"tf32_disabled\": not torch.backends.cuda.matmul.allow_tf32 if torch.cuda.is_available() else \"N/A\",\n","    \"float32_matmul_precision\": \"high\" if hasattr(torch, 'set_float32_matmul_precision') else \"N/A\",\n","    \"PYTHONHASHSEED\": os.environ.get(\"PYTHONHASHSEED\"),\n","    \"CUBLAS_WORKSPACE_CONFIG\": os.environ.get(\"CUBLAS_WORKSPACE_CONFIG\"),\n","    \"OMP_NUM_THREADS\": os.environ.get(\"OMP_NUM_THREADS\"),\n","    \"MKL_NUM_THREADS\": os.environ.get(\"MKL_NUM_THREADS\"),\n","    \"OPENBLAS_NUM_THREADS\": os.environ.get(\"OPENBLAS_NUM_THREADS\"),\n","    \"NUMEXPR_NUM_THREADS\": os.environ.get(\"NUMEXPR_NUM_THREADS\"),\n","}\n","\n","with open(output_dir / \"hardware_log.json\", \"w\") as f:\n","    json.dump(hardware_info, f, indent=2)\n","\n","# 8. Git commit + dirty flag\n","print(\"Collecting Git information...\")\n","git_info = {}\n","try:\n","    git_commit = subprocess.run(\n","        [\"git\", \"rev-parse\", \"HEAD\"],\n","        capture_output=True, text=True, check=True\n","    ).stdout.strip()\n","    git_info[\"commit\"] = git_commit\n","\n","    git_branch = subprocess.run(\n","        [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n","        capture_output=True, text=True, check=True\n","    ).stdout.strip()\n","    git_info[\"branch\"] = git_branch\n","\n","    dirty = subprocess.run(\n","        [\"git\", \"status\", \"--porcelain\"],\n","        capture_output=True, text=True\n","    ).stdout.strip()\n","    git_info[\"dirty\"] = bool(dirty)\n","except:\n","    git_info[\"commit\"] = \"N/A (not a git repo)\"\n","    git_info[\"dirty\"] = False\n","\n","with open(output_dir / \"git_info.json\", \"w\") as f:\n","    json.dump(git_info, f, indent=2)\n","\n","# 9. PyTorch build information\n","print(\"Saving PyTorch build information...\")\n","try:\n","    buf = io.StringIO()\n","    with redirect_stdout(buf):\n","        torch.__config__.show()\n","    (output_dir / \"torch_build.txt\").write_text(buf.getvalue(), encoding=\"utf-8\")\n","except:\n","    pass\n","\n","# 10. Data checksums (only original archives)\n","print(\"Generating data checksums...\")\n","data_dir = Path(\"data\")\n","if data_dir.exists():\n","    sha256sums = []\n","    archive_exts = {'.zip', '.tar', '.gz', '.tgz', '.bz2', '.xz', '.7z', '.rar'}\n","    for file_path in sorted(data_dir.rglob(\"*\")):\n","        if file_path.is_file() and file_path.suffix.lower() in archive_exts:\n","            sha256 = hashlib.sha256()\n","            with open(file_path, \"rb\") as f:\n","                for chunk in iter(lambda: f.read(65536), b\"\"):\n","                    sha256.update(chunk)\n","            rel_path = file_path.relative_to(data_dir)\n","            sha256sums.append(f\"{sha256.hexdigest()}  {rel_path}\")\n","\n","    if sha256sums:\n","        with open(output_dir / \"data_SHA256SUMS.txt\", \"w\") as f:\n","            f.write(\"\\n\".join(sha256sums))\n","        print(f\"  Generated checksums for {len(sha256sums)} archives\")\n","    else:\n","        print(\"  No archives in data/ directory; skipping checksums\")\n","else:\n","    print(\"  data/ directory does not exist; skipping checksums\")\n","\n","# 11. Compute environment hashes of all key files\n","print(\"Computing environment hashes...\")\n","env_files = [\n","    \"requirements.txt\",\n","    \"environment.yml\",\n","    \"env.txt\",\n","    \"SEEDS.yaml\",\n","    \"hardware_log.json\",\n","    \"git_info.json\"\n","]\n","sha256_lines = []\n","for filename in env_files:\n","    filepath = output_dir / filename\n","    if filepath.exists():\n","        sha256 = hashlib.sha256()\n","        with open(filepath, \"rb\") as f:\n","            sha256.update(f.read())\n","        sha256_lines.append(f\"{sha256.hexdigest()}  {filename}\")\n","\n","with open(output_dir / \"ENV.SHA256\", \"w\") as f:\n","    f.write(\"\\n\".join(sha256_lines))\n","\n","# Output summary\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 0 complete - Reproducible environment configuration (top-conf/journal grade)\")\n","print(\"=\"*60)\n","print(f\"Output directory: {output_dir}/\")\n","print(f\"  ✓ SEEDS.yaml (seeds: {SEEDS})\")\n","print(f\"  ✓ requirements.txt\")\n","print(f\"  ✓ env.txt (with system summary)\")\n","print(f\"  ✓ environment.yml\")\n","print(f\"  ✓ hardware_log.json\")\n","print(f\"  ✓ git_info.json (dirty={git_info.get('dirty', False)})\")\n","print(f\"  ✓ torch_build.txt\")\n","print(f\"  ✓ ENV.SHA256 (covers all key files)\")\n","if (output_dir / \"data_SHA256SUMS.txt\").exists():\n","    print(f\"  ✓ data_SHA256SUMS.txt (archives only)\")\n","\n","print(f\"\\nStrict determinism configuration:\")\n","print(f\"  - torch.use_deterministic_algorithms: True (warn_only=False)\")\n","print(f\"  - cudnn.deterministic: {torch.backends.cudnn.deterministic}\")\n","print(f\"  - cudnn.benchmark: {torch.backends.cudnn.benchmark}\")\n","if torch.cuda.is_available():\n","    print(f\"  - TF32 disabled: {not torch.backends.cuda.matmul.allow_tf32}\")\n","print(f\"  - Environment variables set:\")\n","print(f\"    PYTHONHASHSEED: {os.environ.get('PYTHONHASHSEED')}\")\n","print(f\"    CUBLAS_WORKSPACE_CONFIG: {os.environ.get('CUBLAS_WORKSPACE_CONFIG')}\")\n","print(f\"    Thread control: OMP/MKL/OPENBLAS/NUMEXPR=1\")\n","print(\"=\"*60)"]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Steps 1–2: Data Acquisition & Unpack Standardization (top-conf/journal grade)\n","Process the uploaded LARa MbientLab IMU archive\n","\"\"\"\n","\n","import os\n","import hashlib\n","import zipfile\n","import shutil\n","import json\n","import re\n","import numpy as np\n","from pathlib import Path\n","from datetime import datetime, timezone\n","import pandas as pd\n","\n","# ========== Helper functions ==========\n","def read_any_csv(path, nrows=None):\n","    \"\"\"CSV reader with auto delimiter detection\"\"\"\n","    try:\n","        return pd.read_csv(path, nrows=nrows, sep=None, engine=\"python\")\n","    except Exception:\n","        return pd.read_csv(path, nrows=nrows)\n","\n","def infer_sampling_rate(df):\n","    \"\"\"Infer sampling rate; auto-handle ns/μs/ms/s time units\"\"\"\n","    cols = [c.lower() for c in df.columns]\n","    time_cols = [c for c in df.columns if re.search(r\"(time|timestamp|epoch)\", c.lower())]\n","    if not time_cols:\n","        return None\n","\n","    c = time_cols[0]\n","    t = pd.to_numeric(df[c], errors=\"coerce\").dropna().to_numpy()\n","    if t.size < 3:\n","        return None\n","\n","    # Infer time unit by magnitude\n","    max_val = np.nanmax(np.abs(t[:1000])) if t.size else 0\n","    if max_val >= 1e12:      # nanoseconds\n","        scale = 1e-9\n","    elif max_val >= 1e9:     # nanoseconds\n","        scale = 1e-9\n","    elif max_val >= 1e6:     # microseconds\n","        scale = 1e-6\n","    elif max_val >= 1e3:     # milliseconds\n","        scale = 1e-3\n","    else:                    # seconds\n","        scale = 1.0\n","\n","    t_sec = t * scale\n","    dt = np.diff(t_sec)\n","    dt = dt[dt > 0]\n","    if dt.size == 0:\n","        return None\n","\n","    # Use median for robustness\n","    return float(np.round(1.0 / np.median(dt), 3))\n","\n","def infer_sensor_type(cols_lower, filename):\n","    \"\"\"Infer sensor type\"\"\"\n","    if 'label' in filename.lower() or 'activity' in filename.lower():\n","        return \"labels\"\n","\n","    sensors = []\n","    if any((\"acc\" in c) or (\"accelerom\" in c) for c in cols_lower):\n","        sensors.append(\"acc\")\n","    if any((\"gyro\" in c) or re.search(r\"\\bgyr\", c) for c in cols_lower):\n","        sensors.append(\"gyro\")\n","    if any((\"mag\" in c) or (\"magnetom\" in c) for c in cols_lower):\n","        sensors.append(\"mag\")\n","\n","    return \"+\".join(sensors) if sensors else \"unknown\"\n","\n","# LARa placement mapping (per official docs)\n","PLACEMENT_MAP = {\n","    \"L01\": \"lwrist\",      # Left wrist\n","    \"L02\": \"rwrist\",      # Right wrist\n","    \"L03\": \"chest\",       # Chest\n","    \"L04\": \"belt\",        # Belt\n","    \"L05\": \"lankle\",      # Left ankle\n","    \"L06\": \"pocket\",      # Pocket\n","    \"L07\": \"lforearm\",    # Left forearm\n","    \"L08\": \"lupperarm\",   # Left upper arm\n","}\n","\n","# ========== Step 1: Acquire & verify ==========\n","print(\"=\"*60)\n","print(\"Step 1: Data acquisition & verification\")\n","print(\"=\"*60)\n","\n","# Create directory structure\n","raw_dir = Path(\"data/lara/mbientlab/raw\")\n","raw_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Find uploaded zip files (prefer annotated versions)\n","uploaded_files = list(Path(\".\").glob(\"*annotated*MbientLab*.zip\"))\n","if not uploaded_files:\n","    uploaded_files = list(Path(\".\").glob(\"*MbientLab*.zip\"))\n","if not uploaded_files:\n","    uploaded_files = list(Path(\".\").glob(\"*.zip\"))\n","\n","if not uploaded_files:\n","    raise FileNotFoundError(\"No MbientLab data archive found; please upload a zip file first\")\n","\n","if len(uploaded_files) > 1:\n","    print(f\"Warning: found multiple candidate files: {[f.name for f in uploaded_files]}\")\n","    print(f\"Using the first: {uploaded_files[0].name}\")\n","\n","zip_file = uploaded_files[0]\n","print(f\"Found archive: {zip_file}\")\n","\n","# Move to raw data directory\n","target_zip = raw_dir / zip_file.name\n","if not target_zip.exists():\n","    shutil.copy2(zip_file, target_zip)\n","    print(f\"Copied to: {target_zip}\")\n","else:\n","    print(f\"File already exists: {target_zip}\")\n","\n","# Compute SHA256 checksum\n","print(\"Computing SHA256 checksum...\")\n","sha256_hash = hashlib.sha256()\n","with open(target_zip, \"rb\") as f:\n","    for chunk in iter(lambda: f.read(65536), b\"\"):\n","        sha256_hash.update(chunk)\n","\n","checksum = sha256_hash.hexdigest()\n","print(f\"SHA256: {checksum}\")\n","\n","# Save checksum\n","sha256_file = raw_dir / \"SHA256SUMS.txt\"\n","with open(sha256_file, \"w\") as f:\n","    f.write(f\"{checksum}  {target_zip.name}\\n\")\n","print(f\"Saved checksum: {sha256_file}\")\n","\n","# Record provenance (traceability)\n","provenance = {\n","    \"dataset\": \"LARa IMU-only / MbientLab\",\n","    \"origin\": \"manual-upload\",\n","    \"official_url\": \"https://sensor.informatik.uni-mannheim.de/#dataset_lara\",\n","    \"retrieved_at_utc\": datetime.now(timezone.utc).isoformat(),\n","    \"archive\": target_zip.name,\n","    \"sha256\": checksum\n","}\n","(raw_dir / \"PROVENANCE.json\").write_text(\n","    json.dumps(provenance, indent=2, ensure_ascii=False),\n","    encoding=\"utf-8\"\n",")\n","print(f\"Recorded provenance info: {raw_dir / 'PROVENANCE.json'}\")\n","\n","# Set raw archive to read-only\n","os.chmod(target_zip, 0o444)\n","print(f\"Set read-only permission: {target_zip}\")\n","\n","# ========== Step 2: Unpack & directory standardization ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 2: Unpack & directory standardization\")\n","print(\"=\"*60)\n","\n","# Extract to temp directory\n","temp_extract = raw_dir / \"temp_extract\"\n","temp_extract.mkdir(exist_ok=True)\n","\n","print(f\"Extracting {target_zip.name}...\")\n","with zipfile.ZipFile(target_zip, 'r') as zip_ref:\n","    zip_ref.extractall(temp_extract)\n","\n","# Scan extracted files and normalize\n","file_records = []\n","problems = []  # record files that failed to parse\n","\n","# Recursively scan all CSV/TSV files\n","for file_path in temp_extract.rglob(\"*\"):\n","    if not file_path.is_file():\n","        continue\n","\n","    # Process only data files\n","    if file_path.suffix.lower() not in ['.csv', '.tsv', '.txt']:\n","        continue\n","\n","    # Parse filename: LARa pattern L01_S07_R01.csv\n","    filename = file_path.stem\n","\n","    # Extract L01/L02/L03 (placement)\n","    placement_match = re.search(r'L(\\d+)', filename)\n","    placement_raw = f\"L{placement_match.group(1).zfill(2)}\" if placement_match else \"L00\"\n","    placement = PLACEMENT_MAP.get(placement_raw, placement_raw)\n","\n","    # Extract S07 (subject)\n","    subject_match = re.search(r'S(\\d+)', filename)\n","    subject_id = f\"S{subject_match.group(1).zfill(2)}\" if subject_match else \"S00\"\n","\n","    # Extract R01 (session)\n","    session_match = re.search(r'R(\\d+)', filename)\n","    session_id = f\"R{session_match.group(1).zfill(2)}\" if session_match else \"R01\"\n","\n","    # Detect parse failures (avoid LOSO leakage)\n","    if subject_id == \"S00\" or session_id == \"R01\":\n","        if not re.search(r'R01', filename):  # exclude real R01\n","            problems.append(str(file_path.relative_to(temp_extract)))\n","\n","    # Create standardized directory structure\n","    std_dir = raw_dir / subject_id / session_id / placement\n","    std_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # Standardized filename (lowercase, underscores)\n","    std_filename = file_path.name.lower().replace(' ', '_').replace('-', '_')\n","    std_path = std_dir / std_filename\n","\n","    # Copy to standardized location\n","    if not std_path.exists():\n","        shutil.copy2(file_path, std_path)\n","\n","    # Get file info\n","    file_size = file_path.stat().st_size\n","    num_rows = 0\n","    sampling_rate = None\n","    duration = None\n","    sensor_type = \"unknown\"\n","\n","    try:\n","        # Read sample\n","        df_sample = read_any_csv(file_path, nrows=2000)\n","        columns_lower = [c.lower() for c in df_sample.columns]\n","\n","        # Infer sensor type\n","        sensor_type = infer_sensor_type(columns_lower, filename)\n","\n","        # Infer sampling rate (skip for labels)\n","        if sensor_type != \"labels\":\n","            sampling_rate = infer_sampling_rate(df_sample)\n","\n","        # Count total rows (streaming to avoid loading big files)\n","        with open(file_path, \"rb\") as fh:\n","            num_rows = sum(1 for _ in fh) - 1  # minus header\n","\n","        # Compute duration\n","        if sampling_rate and num_rows > 0:\n","            duration = round(num_rows / sampling_rate, 2)\n","\n","    except Exception:\n","        pass  # silently skip files that cannot be parsed\n","\n","    # Record file info\n","    file_records.append({\n","        \"subject_id\": subject_id,\n","        \"session_id\": session_id,\n","        \"placement\": placement,\n","        \"placement_raw\": placement_raw,\n","        \"sensor_type\": sensor_type,\n","        \"original_path\": str(file_path.relative_to(temp_extract)),\n","        \"standardized_path\": str(std_path.relative_to(raw_dir)),\n","        \"filename\": std_filename,\n","        \"file_size_bytes\": file_size,\n","        \"num_rows\": num_rows,\n","        \"sampling_rate_hz\": sampling_rate,\n","        \"duration_sec\": duration,\n","    })\n","\n","print(f\"Processed {len(file_records)} files\")\n","\n","# Check parse failures\n","if problems:\n","    problems_file = raw_dir / \"PROBLEMS.log\"\n","    problems_file.write_text(\n","        \"The following files could not parse subject/session (would break LOSO):\\n\" +\n","        \"\\n\".join(problems) + \"\\n\",\n","        encoding=\"utf-8\"\n","    )\n","    raise RuntimeError(\n","        f\"Found {len(problems)} files with unparsed subject/session; \"\n","        f\"please check {problems_file} and fix\"\n","    )\n","\n","# Remove temp extraction directory\n","shutil.rmtree(temp_extract)\n","print(\"Removed temporary files\")\n","\n","# Generate file_index (Parquet preferred; fallback to CSV)\n","if file_records:\n","    file_index = pd.DataFrame(file_records)\n","\n","    # Sort\n","    file_index = file_index.sort_values(\n","        ['subject_id', 'session_id', 'placement', 'sensor_type']\n","    )\n","\n","    # Save index\n","    index_file = raw_dir / \"file_index.parquet\"\n","    try:\n","        file_index.to_parquet(index_file, index=False)\n","        saved_index = index_file\n","        print(f\"\\nGenerated file index: {saved_index}\")\n","    except Exception as e:\n","        print(f\"Warning: Parquet write failed ({e}); falling back to CSV\")\n","        index_file_csv = raw_dir / \"file_index.csv\"\n","        file_index.to_csv(index_file_csv, index=False)\n","        saved_index = index_file_csv\n","        print(f\"Generated file index: {saved_index}\")\n","\n","    # Show dataset statistics\n","    print(\"\\nDataset statistics:\")\n","    print(f\"  Number of subjects: {file_index['subject_id'].nunique()}\")\n","    print(f\"  Number of sessions: {file_index.groupby('subject_id')['session_id'].nunique().sum()}\")\n","    print(f\"  Placements: {sorted(file_index['placement'].unique().tolist())}\")\n","    print(f\"  Sensor types: {sorted(file_index['sensor_type'].unique().tolist())}\")\n","    print(f\"  Total files: {len(file_index)}\")\n","\n","    # Sampling rate stats\n","    sensor_files = file_index[file_index['sensor_type'] != 'labels']\n","    if not sensor_files.empty:\n","        rates = sensor_files['sampling_rate_hz'].dropna()\n","        if not rates.empty:\n","            print(f\"  Sampling rate range: {rates.min():.1f} - {rates.max():.1f} Hz\")\n","            print(f\"  Median sampling rate: {rates.median():.1f} Hz\")\n","\n","    # Preview first records\n","    print(\"\\nFile index preview:\")\n","    print(file_index.head(10).to_string())\n","else:\n","    print(\"Warning: No data files found\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Steps 1–2 complete (top-conf/journal grade)\")\n","print(\"=\"*60)\n","print(f\"Raw data: {raw_dir}/\")\n","print(f\"Checksum: {sha256_file}\")\n","print(f\"Provenance record: {raw_dir / 'PROVENANCE.json'}\")\n","print(f\"File index: {saved_index}\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yd_rRnUhXOwB","executionInfo":{"status":"ok","timestamp":1763124189993,"user_tz":0,"elapsed":29402,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"344bef53-eb21-4608-f417-b7a5f60e299b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 1: Data acquisition & verification\n","============================================================\n","Found archive: IMU data (annotated) _ MbientLab.zip\n","Copied to: data/lara/mbientlab/raw/IMU data (annotated) _ MbientLab.zip\n","Computing SHA256 checksum...\n","SHA256: 70968b6b8874375e96671af67e31c27ccb63793f31191f86e732d40f24ac3106\n","Saved checksum: data/lara/mbientlab/raw/SHA256SUMS.txt\n","Recorded provenance info: data/lara/mbientlab/raw/PROVENANCE.json\n","Set read-only permission: data/lara/mbientlab/raw/IMU data (annotated) _ MbientLab.zip\n","\n","============================================================\n","Step 2: Unpack & directory standardization\n","============================================================\n","Extracting IMU data (annotated) _ MbientLab.zip...\n","Processed 386 files\n","Removed temporary files\n","\n","Generated file index: data/lara/mbientlab/raw/file_index.parquet\n","\n","Dataset statistics:\n","  Number of subjects: 8\n","  Number of sessions: 193\n","  Placements: ['chest', 'lwrist', 'rwrist']\n","  Sensor types: ['acc+gyro', 'labels']\n","  Total files: 386\n","\n","File index preview:\n","    subject_id session_id placement placement_raw sensor_type                                                original_path                      standardized_path                filename  file_size_bytes  num_rows sampling_rate_hz duration_sec\n","265        S07        R01    lwrist           L01    acc+gyro         IMU data (annotated) _ MbientLab/S07/L01_S07_R01.csv         S07/R01/lwrist/l01_s07_r01.csv         l01_s07_r01.csv          7227132     11824             None         None\n","248        S07        R01    lwrist           L01      labels  IMU data (annotated) _ MbientLab/S07/L01_S07_R01_labels.csv  S07/R01/lwrist/l01_s07_r01_labels.csv  l01_s07_r01_labels.csv           485055     11824             None         None\n","273        S07        R02    lwrist           L01    acc+gyro         IMU data (annotated) _ MbientLab/S07/L01_S07_R02.csv         S07/R02/lwrist/l01_s07_r02.csv         l01_s07_r02.csv          7196022     11776             None         None\n","278        S07        R02    lwrist           L01      labels  IMU data (annotated) _ MbientLab/S07/L01_S07_R02_labels.csv  S07/R02/lwrist/l01_s07_r02_labels.csv  l01_s07_r02_labels.csv           483087     11776             None         None\n","260        S07        R03    rwrist           L02    acc+gyro         IMU data (annotated) _ MbientLab/S07/L02_S07_R03.csv         S07/R03/rwrist/l02_s07_r03.csv         l02_s07_r03.csv          7193816     11758             None         None\n","253        S07        R03    rwrist           L02      labels  IMU data (annotated) _ MbientLab/S07/L02_S07_R03_labels.csv  S07/R03/rwrist/l02_s07_r03_labels.csv  l02_s07_r03_labels.csv           482349     11758             None         None\n","269        S07        R05    rwrist           L02    acc+gyro         IMU data (annotated) _ MbientLab/S07/L02_S07_R05.csv         S07/R05/rwrist/l02_s07_r05.csv         l02_s07_r05.csv          7197459     11766             None         None\n","286        S07        R05    rwrist           L02      labels  IMU data (annotated) _ MbientLab/S07/L02_S07_R05_labels.csv  S07/R05/rwrist/l02_s07_r05_labels.csv  l02_s07_r05_labels.csv           482677     11766             None         None\n","258        S07        R06    rwrist           L02    acc+gyro         IMU data (annotated) _ MbientLab/S07/L02_S07_R06.csv         S07/R06/rwrist/l02_s07_r06.csv         l02_s07_r06.csv          7238567     11838             None         None\n","281        S07        R06    rwrist           L02      labels  IMU data (annotated) _ MbientLab/S07/L02_S07_R06_labels.csv  S07/R06/rwrist/l02_s07_r06_labels.csv  l02_s07_r06_labels.csv           485629     11838             None         None\n","\n","============================================================\n","Steps 1–2 complete (top-conf/journal grade)\n","============================================================\n","Raw data: data/lara/mbientlab/raw/\n","Checksum: data/lara/mbientlab/raw/SHA256SUMS.txt\n","Provenance record: data/lara/mbientlab/raw/PROVENANCE.json\n","File index: data/lara/mbientlab/raw/file_index.parquet\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 3: Metadata & Quality Audit (top-conf/journal grade - final)\n","Parse subjects, activity set, sampling rate, placement, session time; empty-window cleanup\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","from datetime import datetime, timezone\n","import json\n","import re\n","\n","# ========== Config ==========\n","MISSING_THRESHOLD = 0.05      # Missing-rate threshold 5%\n","GAP_THRESHOLD = 2.0           # Single-gap absolute threshold (seconds)\n","GAP_RATIO_THRESHOLD = 0.05    # Gap ratio threshold 5%\n","\n","print(\"=\"*60)\n","print(\"Step 3: Metadata & Quality Audit\")\n","print(\"=\"*60)\n","\n","# Load file index\n","raw_dir = Path(\"data/lara/mbientlab/raw\")\n","index_file = raw_dir / \"file_index.parquet\"\n","if not index_file.exists():\n","    index_file = raw_dir / \"file_index.csv\"\n","\n","print(f\"Loading file index: {index_file}\")\n","file_index = pd.read_parquet(index_file) if index_file.suffix == '.parquet' else pd.read_csv(index_file)\n","\n","# Initialize variables (avoid undefined in edge cases)\n","subject_agg = pd.DataFrame()\n","meta_subjects_file = None\n","meta_sessions_file = None\n","keep_sessions_file = None\n","\n","# ========== Helper functions ==========\n","def pick_scale(med_raw, sr_hint=None):\n","    \"\"\"Smartly pick time unit (s/ms/μs/ns → seconds)\"\"\"\n","    cands = [1.0, 1e-3, 1e-6, 1e-9]\n","\n","    if sr_hint and sr_hint > 0:\n","        target_dt = 1.0 / sr_hint\n","        return min(cands, key=lambda s: abs(med_raw * s - target_dt))\n","\n","    # Without hint: prefer median interval mapping into 5-400 Hz, bias toward ~50 Hz\n","    best, err = 1.0, float(\"inf\")\n","    for s in cands:\n","        dt = med_raw * s\n","        if dt <= 0:\n","            continue\n","        sr = 1.0 / dt\n","        score = 0 if 5 <= sr <= 400 else abs(sr - 50) * 10\n","        if score < err:\n","            best, err = s, score\n","    return best\n","\n","def extract_time_range_and_gaps(file_path, sampling_rate_hint=None, head_rows=20000, chunksize=200000):\n","    \"\"\"Read time column in chunks; extract range and gaps (incl. inter-chunk gaps, memory-friendly)\"\"\"\n","    try:\n","        # Infer time column & unit from a small sample\n","        df_head = pd.read_csv(file_path, nrows=head_rows, sep=None, engine=\"python\")\n","        time_cols = [c for c in df_head.columns if re.search(r\"(time|timestamp|epoch|ts)\", c, re.I)]\n","        if not time_cols:\n","            return None, None, 0.0, 0.0, 0.0\n","\n","        c = time_cols[0]\n","        s = pd.to_numeric(df_head[c], errors=\"coerce\").dropna().to_numpy()\n","\n","        # Numeric timestamp branch\n","        if s.size >= 3:\n","            diffs = np.diff(s)\n","            diffs = diffs[np.isfinite(diffs) & (diffs > 0)]\n","            if diffs.size > 0:\n","                med = float(np.median(diffs))\n","                scale = pick_scale(med, sampling_rate_hint)\n","                expected = (1.0 / sampling_rate_hint) if (sampling_rate_hint and sampling_rate_hint > 0) else (med * scale)\n","\n","                # OR logic: two independent thresholds\n","                rel_threshold = 10.0 * expected  # Relative threshold: 10× expected interval\n","                abs_threshold = GAP_THRESHOLD    # Absolute threshold: 2 s\n","\n","                first = None\n","                last = None\n","                prev = None\n","                gap_sec = 0.0\n","                max_gap = 0.0\n","\n","                for chunk in pd.read_csv(file_path, usecols=[c], sep=None, engine=\"python\", chunksize=chunksize):\n","                    v = pd.to_numeric(chunk[c], errors=\"coerce\").dropna().to_numpy()\n","                    if v.size == 0:\n","                        continue\n","\n","                    if first is None:\n","                        first = v[0]\n","\n","                    # Inter-chunk gaps (fix: use max as baseline)\n","                    if prev is not None:\n","                        delta = (v[0] - prev) * scale\n","                        cond_rel = delta > rel_threshold\n","                        cond_abs = delta > abs_threshold\n","\n","                        if cond_rel or cond_abs:\n","                            # If both trigger, use max (more lenient); if only one, use that one\n","                            if cond_rel and cond_abs:\n","                                base = max(rel_threshold, abs_threshold)\n","                            elif cond_rel:\n","                                base = rel_threshold\n","                            else:\n","                                base = abs_threshold\n","\n","                            gap_this = delta - base\n","                            gap_sec += gap_this\n","                            max_gap = max(max_gap, gap_this)\n","\n","                    # Intra-chunk gaps (fix: shape + baseline)\n","                    d = np.diff(v) * scale\n","                    mask_rel = d > rel_threshold\n","                    mask_abs = d > abs_threshold\n","                    mask = mask_rel | mask_abs\n","\n","                    if mask.any():\n","                        # Vectorized: choose the threshold triggered by each gap (use max if both)\n","                        both_triggered = mask_rel & mask_abs\n","                        thr_used = np.where(\n","                            both_triggered,\n","                            max(rel_threshold, abs_threshold),\n","                            np.where(mask_rel, rel_threshold, abs_threshold)\n","                        )\n","                        gaps = d[mask] - thr_used[mask]  # Fix: also index thr_used\n","                        gap_sec += float(gaps.sum())\n","                        max_gap = max(max_gap, float(gaps.max()))\n","\n","                    prev = v[-1]\n","                    last = v[-1]\n","\n","                if first is not None and last is not None:\n","                    start_sec = float(first * scale)\n","                    end_sec = float(last * scale)\n","                    total = end_sec - start_sec\n","                    ratio = float(gap_sec / total) if total > 0 else 0.0\n","                    return start_sec, end_sec, float(round(gap_sec, 2)), float(round(ratio, 4)), float(round(max_gap, 2))\n","\n","        # Fallback branch: datetime strings\n","        t_head = pd.to_datetime(df_head[c], utc=True, errors=\"coerce\").dropna()\n","        if t_head.size >= 3:\n","            med = float(t_head.diff().dt.total_seconds().dropna().median())\n","            if med > 0:\n","                expected = (1.0 / sampling_rate_hint) if (sampling_rate_hint and sampling_rate_hint > 0) else med\n","\n","                # OR logic\n","                rel_threshold = 10.0 * expected\n","                abs_threshold = GAP_THRESHOLD\n","\n","                first = None\n","                last = None\n","                prev = None\n","                gap_sec = 0.0\n","                max_gap = 0.0\n","\n","                for chunk in pd.read_csv(file_path, usecols=[c], sep=None, engine=\"python\", chunksize=chunksize):\n","                    tt = pd.to_datetime(chunk[c], utc=True, errors=\"coerce\").dropna()\n","                    if tt.empty:\n","                        continue\n","\n","                    if first is None:\n","                        first = tt.iloc[0]\n","\n","                    # Inter-chunk gaps (fix: use max as baseline)\n","                    if prev is not None:\n","                        delta = (tt.iloc[0] - prev).total_seconds()\n","                        cond_rel = delta > rel_threshold\n","                        cond_abs = delta > abs_threshold\n","\n","                        if cond_rel or cond_abs:\n","                            if cond_rel and cond_abs:\n","                                base = max(rel_threshold, abs_threshold)\n","                            elif cond_rel:\n","                                base = rel_threshold\n","                            else:\n","                                base = abs_threshold\n","\n","                            gap_this = delta - base\n","                            gap_sec += gap_this\n","                            max_gap = max(max_gap, gap_this)\n","\n","                    # Intra-chunk gaps (fix: shape + baseline)\n","                    d = tt.diff().dt.total_seconds().dropna()\n","                    mask_rel = d > rel_threshold\n","                    mask_abs = d > abs_threshold\n","                    mask = mask_rel | mask_abs\n","\n","                    if not mask.empty and mask.any():\n","                        both_triggered = mask_rel & mask_abs\n","                        thr_used = np.where(\n","                            both_triggered,\n","                            max(rel_threshold, abs_threshold),\n","                            np.where(mask_rel, rel_threshold, abs_threshold)\n","                        )\n","                        gaps = d[mask].values - thr_used[mask]\n","                        gap_sec += float(gaps.sum())\n","                        max_gap = max(max_gap, float(gaps.max()))\n","\n","                    prev = tt.iloc[-1]\n","                    last = tt.iloc[-1]\n","\n","                if first is not None and last is not None:\n","                    total = (last - first).total_seconds()\n","                    ratio = float(gap_sec / total) if total > 0 else 0.0\n","                    return first.timestamp(), last.timestamp(), float(round(gap_sec, 2)), float(round(ratio, 4)), float(round(max_gap, 2))\n","\n","        return None, None, 0.0, 0.0, 0.0\n","\n","    except Exception:\n","        return None, None, 0.0, 0.0, 0.0\n","\n","def safe_float(x, default=0.0):\n","    \"\"\"Safely cast to float, handling NaN/Inf\"\"\"\n","    try:\n","        if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))):\n","            return default\n","        return float(x)\n","    except:\n","        return default\n","\n","# ========== 1. Parse sensor data metadata ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Parse sensor data metadata\")\n","print(\"=\"*60)\n","\n","# Determine label files directly from filenames (more reliable)\n","label_files = file_index[\n","    file_index['filename'].str.contains('label', case=False, na=False)\n","].copy()\n","sensor_files = file_index[\n","    ~file_index['filename'].str.contains('label', case=False, na=False)\n","].copy()\n","\n","print(f\"Sensor files: {len(sensor_files)}\")\n","print(f\"Label files: {len(label_files)}\")\n","\n","# Extract time ranges for sensor files (receive 5 return values)\n","print(\"Extracting time spans and gap statistics (chunked)...\")\n","time_records = []\n","for idx, row in sensor_files.iterrows():\n","    file_path = raw_dir / row['standardized_path']\n","    start, end, gap_sec, gap_ratio, max_gap = extract_time_range_and_gaps(\n","        file_path,\n","        row['sampling_rate_hz']\n","    )\n","    time_records.append({\n","        'subject_id': row['subject_id'],\n","        'session_id': row['session_id'],\n","        'placement': row['placement'],\n","        'start_time': start,\n","        'end_time': end,\n","        'gap_seconds': gap_sec,\n","        'gap_ratio': gap_ratio,\n","        'max_gap_seconds': max_gap,\n","    })\n","\n","df_time_ranges = pd.DataFrame(time_records)\n","\n","# Aggregate time ranges by session (includes max_gap)\n","session_time_agg = df_time_ranges.groupby(['subject_id', 'session_id']).agg({\n","    'start_time': 'min',\n","    'end_time': 'max',\n","    'gap_seconds': 'sum',\n","    'max_gap_seconds': 'max',\n","}).reset_index()\n","\n","session_time_agg['session_duration_sec'] = (\n","    session_time_agg['end_time'] - session_time_agg['start_time']\n",")\n","session_time_agg['gap_ratio'] = (\n","    session_time_agg['gap_seconds'] / session_time_agg['session_duration_sec']\n",").fillna(0.0).infer_objects(copy=False)\n","\n","session_time_agg.rename(columns={\n","    'start_time': 'session_start_time',\n","    'end_time': 'session_end_time'\n","}, inplace=True)\n","\n","# Add ISO8601 (human-readable) times\n","def to_iso(x):\n","    try:\n","        if pd.notna(x):\n","            return datetime.fromtimestamp(float(x), tz=timezone.utc).isoformat()\n","    except:\n","        pass\n","    return None\n","\n","session_time_agg['session_start_utc'] = session_time_agg['session_start_time'].apply(to_iso)\n","session_time_agg['session_end_utc'] = session_time_agg['session_end_time'].apply(to_iso)\n","\n","print(f\"Extracted time spans for {len(session_time_agg)} sessions\")\n","\n","# ========== 2. Parse labels & activity statistics ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Parse labels & activity statistics\")\n","print(\"=\"*60)\n","\n","activity_stats = []\n","session_records = []\n","\n","for idx, label_row in label_files.iterrows():\n","    label_path = raw_dir / label_row['standardized_path']\n","\n","    if not label_path.exists():\n","        continue\n","\n","    try:\n","        # Read label file\n","        df_label = pd.read_csv(label_path, sep=None, engine='python')\n","\n","        # Find label column (LARa dataset uses 'Class')\n","        if 'Class' in df_label.columns:\n","            label_col = 'Class'\n","        elif 'class' in df_label.columns:\n","            label_col = 'class'\n","        else:\n","            label_cols = [c for c in df_label.columns if 'label' in c.lower() or 'activity' in c.lower()]\n","            if not label_cols:\n","                print(f\"  No label column ({df_label.columns.tolist()}): {label_path.name}\")\n","                continue\n","            label_col = label_cols[0]\n","\n","        # Count activity distribution\n","        activity_counts = df_label[label_col].value_counts()\n","        total_samples = len(df_label)\n","\n","        # Check missing\n","        missing_count = df_label[label_col].isna().sum()\n","        missing_rate = missing_count / total_samples if total_samples > 0 else 0\n","\n","        # Record session info\n","        session_info = {\n","            'subject_id': label_row['subject_id'],\n","            'session_id': label_row['session_id'],\n","            'placement': label_row['placement'],\n","            'total_samples': total_samples,\n","            'missing_samples': missing_count,\n","            'missing_rate': round(missing_rate, 4),\n","            'num_activities': len(activity_counts),\n","        }\n","\n","        # Add per-activity stats\n","        for activity, count in activity_counts.items():\n","            activity_stats.append({\n","                'subject_id': label_row['subject_id'],\n","                'session_id': label_row['session_id'],\n","                'placement': label_row['placement'],\n","                'activity': str(activity),\n","                'count': int(count),\n","                'percentage': round(count / total_samples * 100, 2)\n","            })\n","\n","        session_records.append(session_info)\n","\n","    except Exception as e:\n","        print(f\"  Warning: failed to parse {label_path.name}: {e}\")\n","        continue\n","\n","print(f\"Parsed {len(session_records)} sessions\")\n","\n","# ========== 2.1 Orphan session check ==========\n","print(\"\\nChecking orphan sessions...\")\n","sess_from_sensors = set(zip(sensor_files['subject_id'], sensor_files['session_id']))\n","sess_from_labels = set(zip(label_files['subject_id'], label_files['session_id']))\n","orphans = sess_from_sensors - sess_from_labels\n","\n","if orphans:\n","    orphan_file = raw_dir / \"QA_ISSUES.log\"\n","    with open(orphan_file, \"a\", encoding=\"utf-8\") as f:\n","        f.write(\"\\nSessions with sensors but no labels (orphan sessions):\\n\")\n","        for s, r in sorted(orphans):\n","            f.write(f\"  {s}-{r}\\n\")\n","    print(f\"⚠️  Found {len(orphans)} orphan sessions; logged to QA_ISSUES.log\")\n","\n","# ========== 3. Merge session metadata ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Merge session metadata\")\n","print(\"=\"*60)\n","\n","df_sessions = pd.DataFrame(session_records)\n","df_activities = pd.DataFrame(activity_stats)\n","\n","# Merge time info\n","if not df_sessions.empty and not session_time_agg.empty:\n","    df_sessions = df_sessions.merge(\n","        session_time_agg,\n","        on=['subject_id', 'session_id'],\n","        how='left'\n","    )\n","    print(f\"Merged time span info\")\n","\n","# ========== 4. Data quality checks & empty-window cleanup ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Data quality checks & empty-window cleanup\")\n","print(\"=\"*60)\n","\n","if not df_sessions.empty:\n","    # Generate keep flag\n","    df_sessions['keep'] = True\n","    df_sessions['reject_reason'] = ''\n","\n","    # Check missing-rate exceeds threshold\n","    high_missing_mask = df_sessions['missing_rate'] > MISSING_THRESHOLD\n","    if high_missing_mask.any():\n","        df_sessions.loc[high_missing_mask, 'keep'] = False\n","        df_sessions.loc[high_missing_mask, 'reject_reason'] = 'high_missing_rate'\n","        print(f\"⚠️  {high_missing_mask.sum()} sessions marked not kept due to high missing rate\")\n","\n","    # Check time-gap ratio exceeds threshold\n","    if 'gap_ratio' in df_sessions.columns:\n","        high_gap_mask = df_sessions['gap_ratio'] > GAP_RATIO_THRESHOLD\n","        if high_gap_mask.any():\n","            # Append reason if already rejected; otherwise mark alone\n","            for idx in df_sessions[high_gap_mask].index:\n","                if df_sessions.loc[idx, 'keep']:\n","                    df_sessions.loc[idx, 'keep'] = False\n","                    df_sessions.loc[idx, 'reject_reason'] = 'high_gap_ratio'\n","                else:\n","                    df_sessions.loc[idx, 'reject_reason'] += '+high_gap_ratio'\n","            print(f\"⚠️  {high_gap_mask.sum()} sessions marked not kept due to high gap ratio\")\n","\n","    # Summary\n","    keep_count = df_sessions['keep'].sum()\n","    reject_count = (~df_sessions['keep']).sum()\n","    print(f\"✓ QC result: keep {keep_count} sessions, reject {reject_count} sessions\")\n","\n","    # Save keep list\n","    keep_sessions_file = raw_dir / \"qa_keep_sessions.csv\"\n","    df_sessions[['subject_id', 'session_id', 'placement', 'keep', 'reject_reason',\n","                 'missing_rate', 'gap_ratio']].to_csv(keep_sessions_file, index=False)\n","    print(f\"✓ Saved: {keep_sessions_file}\")\n","\n","    # Log rejection details\n","    if reject_count > 0:\n","        rejected = df_sessions[~df_sessions['keep']]\n","        qa_issues = raw_dir / \"QA_ISSUES.log\"\n","        with open(qa_issues, \"a\") as f:\n","            f.write(f\"\\nSessions rejected by QC (total {reject_count}):\\n\\n\")\n","            f.write(rejected[['subject_id', 'session_id', 'placement', 'reject_reason',\n","                             'missing_rate', 'gap_ratio']].to_string(index=False))\n","        print(f\"  Details logged to: {qa_issues}\")\n","\n","# ========== 4.1 Generate file-level empty-window list ==========\n","print(\"\\nGenerating file-level empty-window list...\")\n","if not df_time_ranges.empty:\n","    empty_segments = df_time_ranges[\n","        df_time_ranges['gap_ratio'].notna() &\n","        (df_time_ranges['gap_ratio'] > GAP_RATIO_THRESHOLD)\n","    ].copy()\n","\n","    if not empty_segments.empty:\n","        empty_todo_file = raw_dir / \"EMPTY_SEGMENTS_TODO.csv\"\n","        empty_segments[['subject_id', 'session_id', 'placement',\n","                       'gap_seconds', 'gap_ratio', 'max_gap_seconds']].to_csv(empty_todo_file, index=False)\n","        print(f\"⚠️  Generated empty-segment list: {empty_todo_file} ({len(empty_segments)} files)\")\n","\n","# ========== 5. Generate subject-level metadata ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Generate subject-level metadata\")\n","print(\"=\"*60)\n","\n","if not df_sessions.empty:\n","    # Only count kept sessions\n","    df_keep = df_sessions[df_sessions['keep']]\n","\n","    if not df_keep.empty:\n","        # Aggregate by subject\n","        subject_agg = df_keep.groupby('subject_id').agg({\n","            'session_id': 'nunique',\n","            'total_samples': 'sum',\n","            'missing_samples': 'sum',\n","            'session_duration_sec': 'sum',\n","            'num_activities': 'sum',\n","        }).reset_index()\n","\n","        subject_agg.columns = ['subject_id', 'num_sessions', 'total_samples',\n","                               'total_missing', 'total_duration_sec', 'total_activities']\n","\n","        # Compute overall missing rate\n","        subject_agg['overall_missing_rate'] = (\n","            subject_agg['total_missing'] / subject_agg['total_samples']\n","        ).round(4)\n","\n","        # Add placement coverage\n","        placement_coverage = df_keep.groupby('subject_id')['placement'].apply(\n","            lambda x: ','.join(sorted(set(x)))\n","        ).reset_index()\n","        placement_coverage.columns = ['subject_id', 'placements']\n","\n","        subject_agg = subject_agg.merge(placement_coverage, on='subject_id')\n","\n","        # Save subject metadata\n","        meta_subjects_file = raw_dir / \"meta_subjects.csv\"\n","        subject_agg.to_csv(meta_subjects_file, index=False)\n","        print(f\"✓ Saved: {meta_subjects_file}\")\n","        print(f\"  Number of subjects: {len(subject_agg)}\")\n","\n","# ========== 6. Generate session-level metadata ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Generate session-level metadata\")\n","print(\"=\"*60)\n","\n","if not df_sessions.empty:\n","    # Add activity list\n","    if not df_activities.empty:\n","        activity_list = df_activities.groupby(['subject_id', 'session_id'])['activity'].apply(\n","            lambda x: ','.join(sorted(set(x)))\n","        ).reset_index()\n","        activity_list.columns = ['subject_id', 'session_id', 'activities']\n","\n","        df_sessions_full = df_sessions.merge(\n","            activity_list,\n","            on=['subject_id', 'session_id'],\n","            how='left'\n","        )\n","    else:\n","        df_sessions_full = df_sessions\n","\n","    # Save session metadata\n","    meta_sessions_file = raw_dir / \"meta_sessions.csv\"\n","    df_sessions_full.to_csv(meta_sessions_file, index=False)\n","    print(f\"✓ Saved: {meta_sessions_file}\")\n","    print(f\"  Number of sessions: {len(df_sessions_full)}\")\n","\n","# ========== 7. Generate quality audit report ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"7. Generate quality audit report\")\n","print(\"=\"*60)\n","\n","qa_report = []\n","qa_report.append(\"=\"*70)\n","qa_report.append(\"LARa MbientLab IMU Dataset - Quality Audit Report\")\n","qa_report.append(\"=\"*70)\n","qa_report.append(f\"Generated at: {datetime.now(timezone.utc).isoformat()}\")\n","qa_report.append(f\"Data path: {raw_dir}\")\n","qa_report.append(\"\")\n","\n","# Overall stats\n","qa_report.append(\"[1. Dataset overview]\")\n","qa_report.append(\"-\"*70)\n","if not subject_agg.empty:\n","    total_hours = safe_float(subject_agg['total_duration_sec'].sum() / 3600)\n","    qa_report.append(f\"Number of subjects: {len(subject_agg)}\")\n","    qa_report.append(f\"Total sessions: {subject_agg['num_sessions'].sum()}\")\n","    qa_report.append(f\"Total duration: {total_hours:.2f} hours\")\n","    qa_report.append(f\"Total samples: {subject_agg['total_samples'].sum():,}\")\n","qa_report.append(\"\")\n","\n","# Sampling rate stats\n","qa_report.append(\"[2. Sampling rate statistics]\")\n","qa_report.append(\"-\"*70)\n","if not sensor_files.empty:\n","    rates = sensor_files['sampling_rate_hz'].dropna()\n","    if not rates.empty:\n","        qa_report.append(f\"Sampling rate range: {rates.min():.2f} - {rates.max():.2f} Hz\")\n","        qa_report.append(f\"Median sampling rate: {rates.median():.2f} Hz\")\n","        qa_report.append(f\"Mode sampling rate: {rates.mode().values[0]:.2f} Hz\")\n","qa_report.append(\"\")\n","\n","# Placement coverage\n","qa_report.append(\"[3. Sensor placement coverage]\")\n","qa_report.append(\"-\"*70)\n","if not df_sessions.empty:\n","    df_keep = df_sessions[df_sessions['keep']]\n","    if not df_keep.empty:\n","        placement_dist = df_keep['placement'].value_counts()\n","        for placement, count in placement_dist.items():\n","            percentage = count / len(df_keep) * 100\n","            qa_report.append(f\"  {placement:15s}: {count:3d} sessions ({percentage:5.1f}%)\")\n","qa_report.append(\"\")\n","\n","# Activity distribution\n","qa_report.append(\"[4. Activity distribution]\")\n","qa_report.append(\"-\"*70)\n","if not df_activities.empty:\n","    activity_total = df_activities.groupby('activity').agg({\n","        'count': 'sum',\n","    }).sort_values('count', ascending=False)\n","\n","    total_count = activity_total['count'].sum()\n","    qa_report.append(f\"Number of activity classes: {len(activity_total)}\")\n","    qa_report.append(f\"Total samples: {total_count:,}\")\n","    qa_report.append(\"\")\n","    qa_report.append(\"Per-activity share:\")\n","    for activity, row in activity_total.iterrows():\n","        percentage = row['count'] / total_count * 100\n","        qa_report.append(f\"  {str(activity):30s}: {row['count']:8,} ({percentage:5.2f}%)\")\n","qa_report.append(\"\")\n","\n","# Data quality (incl. max_gap stats)\n","qa_report.append(\"[5. Data quality assessment]\")\n","qa_report.append(\"-\"*70)\n","if not df_sessions.empty:\n","    qa_report.append(f\"Missing-rate threshold: {MISSING_THRESHOLD*100}%\")\n","    qa_report.append(f\"Gap absolute threshold: {GAP_THRESHOLD} s\")\n","    qa_report.append(f\"Gap relative threshold: 10× expected interval\")\n","    qa_report.append(f\"Gap ratio threshold: {GAP_RATIO_THRESHOLD*100}%\")\n","\n","    avg_miss = safe_float(df_sessions['missing_rate'].mean())\n","    max_miss = safe_float(df_sessions['missing_rate'].max())\n","    med_miss = safe_float(df_sessions['missing_rate'].median())\n","\n","    qa_report.append(f\"Overall average missing rate: {avg_miss*100:.2f}%\")\n","    qa_report.append(f\"Max missing rate: {max_miss*100:.2f}%\")\n","    qa_report.append(f\"Median missing rate: {med_miss*100:.2f}%\")\n","\n","    if 'gap_ratio' in df_sessions.columns:\n","        avg_gap = safe_float(df_sessions['gap_ratio'].mean())\n","        max_gap_ratio = safe_float(df_sessions['gap_ratio'].max())\n","        qa_report.append(f\"Average gap ratio: {avg_gap*100:.2f}%\")\n","        qa_report.append(f\"Max gap ratio: {max_gap_ratio*100:.2f}%\")\n","\n","    if 'max_gap_seconds' in df_sessions.columns:\n","        max_single_gap = safe_float(df_sessions['max_gap_seconds'].max())\n","        qa_report.append(f\"Max single gap: {max_single_gap:.2f} s\")\n","\n","    keep_count = df_sessions['keep'].sum()\n","    total_count = len(df_sessions)\n","    pass_rate = keep_count / total_count * 100 if total_count > 0 else 0\n","    qa_report.append(f\"\")\n","    qa_report.append(f\"Sessions passing QC: {keep_count}/{total_count} ({pass_rate:.1f}%)\")\n","\n","if (raw_dir / \"EMPTY_SEGMENTS_TODO.csv\").exists():\n","    qa_report.append(\"\")\n","    qa_report.append(\"[Note] Empty/abnormal segments found; see: EMPTY_SEGMENTS_TODO.csv (exclude during later sliding-window segmentation)\")\n","\n","qa_report.append(\"\")\n","\n","# Per-subject details\n","qa_report.append(\"[6. Subject-level details]\")\n","qa_report.append(\"-\"*70)\n","if not subject_agg.empty:\n","    for _, subj in subject_agg.iterrows():\n","        qa_report.append(f\"Subject {subj['subject_id']}:\")\n","        qa_report.append(f\"  # sessions: {subj['num_sessions']}\")\n","        qa_report.append(f\"  Total duration: {subj['total_duration_sec']/60:.1f} minutes\")\n","        qa_report.append(f\"  Total samples: {subj['total_samples']:,}\")\n","        qa_report.append(f\"  Missing rate: {subj['overall_missing_rate']*100:.2f}%\")\n","        qa_report.append(f\"  Placements: {subj['placements']}\")\n","        qa_report.append(\"\")\n","\n","qa_report.append(\"=\"*70)\n","qa_report.append(\"End of report\")\n","qa_report.append(\"=\"*70)\n","\n","# Save QA report\n","qa_report_file = raw_dir / \"QA_REPORT.txt\"\n","with open(qa_report_file, \"w\", encoding=\"utf-8\") as f:\n","    f.write(\"\\n\".join(qa_report))\n","\n","print(f\"✓ Saved quality report: {qa_report_file}\")\n","\n","# Also print to console\n","print(\"\\n\" + \"\\n\".join(qa_report))\n","\n","# ========== 8. Generate summary JSON ==========\n","summary = {\n","    \"generated_at_utc\": datetime.now(timezone.utc).isoformat(),\n","    \"num_subjects\": int(len(subject_agg)) if not subject_agg.empty else 0,\n","    \"num_sessions_total\": len(df_sessions) if not df_sessions.empty else 0,\n","    \"num_sessions_keep\": int(df_sessions['keep'].sum()) if not df_sessions.empty else 0,\n","    \"total_duration_hours\": safe_float(subject_agg['total_duration_sec'].sum() / 3600) if not subject_agg.empty else 0.0,\n","    \"missing_threshold\": MISSING_THRESHOLD,\n","    \"gap_threshold_sec\": GAP_THRESHOLD,\n","    \"gap_ratio_threshold\": GAP_RATIO_THRESHOLD,\n","    \"avg_missing_rate\": safe_float(df_sessions['missing_rate'].mean()) if not df_sessions.empty else 0.0,\n","    \"avg_gap_ratio\": safe_float(df_sessions['gap_ratio'].mean()) if not df_sessions.empty and 'gap_ratio' in df_sessions.columns else 0.0,\n","    \"max_single_gap_seconds\": safe_float(df_sessions['max_gap_seconds'].max()) if not df_sessions.empty and 'max_gap_seconds' in df_sessions.columns else 0.0,\n","    \"num_activities\": int(len(activity_total)) if not df_activities.empty else 0,\n","    \"placements\": sorted(df_sessions[df_sessions['keep']]['placement'].unique().tolist()) if not df_sessions.empty and df_sessions['keep'].any() else [],\n","}\n","\n","summary_file = raw_dir / \"qa_summary.json\"\n","with open(summary_file, \"w\") as f:\n","    json.dump(summary, f, indent=2)\n","\n","print(f\"\\n✓ Saved summary: {summary_file}\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 3 complete - Metadata & Quality Audit (top-conf/journal grade)\")\n","print(\"=\"*60)\n","print(f\"Output files:\")\n","if meta_subjects_file:\n","    print(f\"  - {meta_subjects_file}\")\n","if meta_sessions_file:\n","    print(f\"  - {meta_sessions_file}\")\n","if keep_sessions_file:\n","    print(f\"  - {keep_sessions_file}\")\n","print(f\"  - {qa_report_file}\")\n","print(f\"  - {summary_file}\")\n","if (raw_dir / \"EMPTY_SEGMENTS_TODO.csv\").exists():\n","    print(f\"  - {raw_dir / 'EMPTY_SEGMENTS_TODO.csv'} (file-level empty-window list)\")\n","if (raw_dir / \"QA_ISSUES.log\").exists():\n","    print(f\"  - {raw_dir / 'QA_ISSUES.log'} (quality issue details)\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dMO1-CK7XP8V","executionInfo":{"status":"ok","timestamp":1763124315070,"user_tz":0,"elapsed":125070,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"b5cf7ece-0e9d-4564-f134-ca8a6fc1dbe4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 3: Metadata & Quality Audit\n","============================================================\n","Loading file index: data/lara/mbientlab/raw/file_index.parquet\n","\n","============================================================\n","1. Parse sensor data metadata\n","============================================================\n","Sensor files: 193\n","Label files: 193\n","Extracting time spans and gap statistics (chunked)...\n","Extracted time spans for 193 sessions\n","\n","============================================================\n","2. Parse labels & activity statistics\n","============================================================\n","Parsed 193 sessions\n","\n","Checking orphan sessions...\n","\n","============================================================\n","3. Merge session metadata\n","============================================================\n","Merged time span info\n","\n","============================================================\n","4. Data quality checks & empty-window cleanup\n","============================================================\n","✓ QC result: keep 193 sessions, reject 0 sessions\n","✓ Saved: data/lara/mbientlab/raw/qa_keep_sessions.csv\n","\n","Generating file-level empty-window list...\n","\n","============================================================\n","5. Generate subject-level metadata\n","============================================================\n","✓ Saved: data/lara/mbientlab/raw/meta_subjects.csv\n","  Number of subjects: 8\n","\n","============================================================\n","6. Generate session-level metadata\n","============================================================\n","✓ Saved: data/lara/mbientlab/raw/meta_sessions.csv\n","  Number of sessions: 193\n","\n","============================================================\n","7. Generate quality audit report\n","============================================================\n","✓ Saved quality report: data/lara/mbientlab/raw/QA_REPORT.txt\n","\n","======================================================================\n","LARa MbientLab IMU Dataset - Quality Audit Report\n","======================================================================\n","Generated at: 2025-11-14T12:45:17.383824+00:00\n","Data path: data/lara/mbientlab/raw\n","\n","[1. Dataset overview]\n","----------------------------------------------------------------------\n","Number of subjects: 8\n","Total sessions: 193\n","Total duration: 6.18 hours\n","Total samples: 2,224,452\n","\n","[2. Sampling rate statistics]\n","----------------------------------------------------------------------\n","\n","[3. Sensor placement coverage]\n","----------------------------------------------------------------------\n","  rwrist         :  96 sessions ( 49.7%)\n","  chest          :  85 sessions ( 44.0%)\n","  lwrist         :  12 sessions (  6.2%)\n","\n","[4. Activity distribution]\n","----------------------------------------------------------------------\n","Number of activity classes: 8\n","Total samples: 2,224,452\n","\n","Per-activity share:\n","  4                             : 1,198,354 (53.87%)\n","  0                             :  231,084 (10.39%)\n","  2                             :  197,087 ( 8.86%)\n","  1                             :  182,687 ( 8.21%)\n","  3                             :  150,685 ( 6.77%)\n","  5                             :  112,818 ( 5.07%)\n","  7                             :  107,298 ( 4.82%)\n","  6                             :   44,439 ( 2.00%)\n","\n","[5. Data quality assessment]\n","----------------------------------------------------------------------\n","Missing-rate threshold: 5.0%\n","Gap absolute threshold: 2.0 s\n","Gap relative threshold: 10× expected interval\n","Gap ratio threshold: 5.0%\n","Overall average missing rate: 0.00%\n","Max missing rate: 0.00%\n","Median missing rate: 0.00%\n","Average gap ratio: 0.00%\n","Max gap ratio: 0.00%\n","Max single gap: 0.00 s\n","\n","Sessions passing QC: 193/193 (100.0%)\n","\n","[6. Subject-level details]\n","----------------------------------------------------------------------\n","Subject S07:\n","  # sessions: 29\n","  Total duration: 57.1 minutes\n","  Total samples: 342,376\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","Subject S08:\n","  # sessions: 24\n","  Total duration: 47.4 minutes\n","  Total samples: 284,329\n","  Missing rate: 0.00%\n","  Placements: chest,rwrist\n","\n","Subject S09:\n","  # sessions: 29\n","  Total duration: 54.5 minutes\n","  Total samples: 326,680\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","Subject S10:\n","  # sessions: 23\n","  Total duration: 43.2 minutes\n","  Total samples: 259,019\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","Subject S11:\n","  # sessions: 14\n","  Total duration: 27.7 minutes\n","  Total samples: 165,980\n","  Missing rate: 0.00%\n","  Placements: lwrist,rwrist\n","\n","Subject S12:\n","  # sessions: 17\n","  Total duration: 30.5 minutes\n","  Total samples: 183,024\n","  Missing rate: 0.00%\n","  Placements: chest,rwrist\n","\n","Subject S13:\n","  # sessions: 29\n","  Total duration: 56.7 minutes\n","  Total samples: 340,084\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","Subject S14:\n","  # sessions: 28\n","  Total duration: 53.8 minutes\n","  Total samples: 322,960\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","======================================================================\n","End of report\n","======================================================================\n","\n","✓ Saved summary: data/lara/mbientlab/raw/qa_summary.json\n","\n","============================================================\n","Step 3 complete - Metadata & Quality Audit (top-conf/journal grade)\n","============================================================\n","Output files:\n","  - data/lara/mbientlab/raw/meta_subjects.csv\n","  - data/lara/mbientlab/raw/meta_sessions.csv\n","  - data/lara/mbientlab/raw/qa_keep_sessions.csv\n","  - data/lara/mbientlab/raw/QA_REPORT.txt\n","  - data/lara/mbientlab/raw/qa_summary.json\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 4: Channel & Placement Strategy Selection (top-conf/journal grade)\n","Select placement, raw channels, derived channels; generate config file\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import re\n","\n","print(\"=\"*60)\n","print(\"Step 4: Channel & Placement Strategy Selection\")\n","print(\"=\"*60)\n","\n","# ========== Placement → Prefix allowlist (eradicate cross-placement leakage) ==========\n","PREFIX_ALLOWLIST = {\n","    \"rwrist\": [\"RA_\"],\n","    \"lwrist\": [\"LA_\"],\n","    \"chest\":  [\"N_\"],\n","    # Extensible: \"rleg\": [\"RL_\"], \"lleg\": [\"LL_\"]\n","}\n","\n","REQ_SUFFIX = {\n","    \"ax\": \"AccelerometerX\", \"ay\": \"AccelerometerY\", \"az\": \"AccelerometerZ\",\n","    \"gx\": \"GyroscopeX\",     \"gy\": \"GyroscopeY\",     \"gz\": \"GyroscopeZ\",\n","}\n","\n","# Coverage threshold: required column presence ratio across files (1.0=100%, 0.95=95%)\n","MIN_COVERAGE = 1.0\n","\n","# Load metadata\n","raw_dir = Path(\"data/lara/mbientlab/raw\")\n","configs_dir = Path(\"configs\")\n","configs_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Load subject metadata\n","meta_subjects = pd.read_csv(raw_dir / \"meta_subjects.csv\")\n","print(f\"\\nLoaded subject metadata: {len(meta_subjects)} subjects\")\n","\n","# Load file index\n","index_file = raw_dir / \"file_index.parquet\"\n","if not index_file.exists():\n","    index_file = raw_dir / \"file_index.csv\"\n","file_index = pd.read_parquet(index_file) if index_file.suffix == '.parquet' else pd.read_csv(index_file)\n","\n","# Keep only sensor files (more robust: filter by sensor_type and filename)\n","if 'sensor_type' in file_index.columns:\n","    sensor_files = file_index[\n","        (file_index['sensor_type'].isin(['acc+gyro', 'acc', 'gyro'])) &\n","        ~file_index['filename'].str.contains('label', case=False, na=False)\n","    ].copy()\n","else:\n","    sensor_files = file_index[\n","        ~file_index['filename'].str.contains('label', case=False, na=False)\n","    ].copy()\n","\n","print(f\"Number of sensor files: {len(sensor_files)}\")\n","\n","# ========== 1. Analyze placement coverage ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Analyze placement coverage\")\n","print(\"=\"*60)\n","\n","# Count data volume per placement\n","placement_stats = sensor_files.groupby('placement').agg({\n","    'subject_id': 'nunique',\n","    'session_id': 'nunique',\n","    'file_size_bytes': 'sum',\n","    'num_rows': 'sum',\n","}).reset_index()\n","placement_stats.columns = ['placement', 'num_subjects', 'num_sessions', 'total_bytes', 'total_samples']\n","placement_stats = placement_stats.sort_values('total_samples', ascending=False)\n","\n","print(\"\\nPlacement statistics (sorted by sample count):\")\n","print(placement_stats.to_string(index=False))\n","\n","# Fix selection to right wrist (this round)\n","selected_placement = \"rwrist\"\n","print(f\"\\nFixed placement for this round: {selected_placement}\")\n","\n","# Check whether placement exists\n","if selected_placement not in placement_stats['placement'].values:\n","    raise ValueError(f\"Specified placement '{selected_placement}' does not exist in the data\")\n","\n","# Check which subjects have that placement\n","subjects_with_selected = sensor_files[sensor_files['placement'] == selected_placement]['subject_id'].unique()\n","print(f\"Subjects with {selected_placement} data: {len(subjects_with_selected)}/{len(meta_subjects)}\")\n","\n","# ========== 2. Allowlist validation & channel check ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Allowlist validation & channel check\")\n","print(\"=\"*60)\n","\n","# Read only from files of selected placement\n","placement_files = sensor_files[sensor_files['placement'] == selected_placement]\n","print(f\"Number of files for selected placement '{selected_placement}': {len(placement_files)}\")\n","\n","# Get allowlist prefixes\n","allowed_prefixes = PREFIX_ALLOWLIST.get(selected_placement, [])\n","assert allowed_prefixes, f\"Prefix allowlist for '{selected_placement}' not configured; please add it in PREFIX_ALLOWLIST\"\n","print(f\"\\nUsing placement→prefix allowlist: {selected_placement} → {allowed_prefixes}\")\n","\n","# Robust header-reading function\n","def read_cols(fp):\n","    \"\"\"Read column names (with fallback)\"\"\"\n","    try:\n","        return pd.read_csv(fp, nrows=5, sep=None, engine='python').columns.tolist()\n","    except Exception:\n","        return pd.read_csv(fp, nrows=5, sep=\",\").columns.tolist()\n","\n","# Read headers of all files\n","print(f\"\\nRead headers of all {len(placement_files)} files to check consistency...\")\n","all_columns_by_file = []\n","\n","for _, row in placement_files.iterrows():\n","    fp = raw_dir / row['standardized_path']\n","    cols = read_cols(fp)\n","    data_cols = [c for c in cols if not re.search(r'(time|timestamp|epoch|index|id|class|label)', c, re.I)]\n","    all_columns_by_file.append(data_cols)\n","\n","# Assert all files were read successfully\n","assert len(all_columns_by_file) == len(placement_files), \\\n","    f\"{len(placement_files)-len(all_columns_by_file)} '{selected_placement}' files failed header reading; fix or exclude these files first\"\n","\n","print(f\"✓ Successfully read {len(all_columns_by_file)} files\")\n","\n","# Show columns of the first file as a reference\n","if all_columns_by_file:\n","    print(f\"\\nData columns of the first file:\")\n","    for col in all_columns_by_file[0]:\n","        print(f\"  {col}\")\n","\n","# ========== 3. Build strict channel mapping (allowlist + consistency assertions) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Build strict channel mapping (allowlist + consistency assertions)\")\n","print(\"=\"*60)\n","\n","def extract_prefix(col):\n","    \"\"\"Extract column prefix\"\"\"\n","    m = re.match(r'^([A-Z]{1,}_)', col)\n","    return m.group(1) if m else None\n","\n","def build_mapping_from_allowlist(allowed_prefixes, all_cols_by_file, min_coverage=1.0):\n","    \"\"\"Compose column names from allowlist × suffix and check coverage\"\"\"\n","    mapping = {}\n","    missing_files = {}\n","\n","    for std, suf in REQ_SUFFIX.items():\n","        chosen = None\n","        for pfx in allowed_prefixes:\n","            cand = f\"{pfx}{suf}\"\n","            # Count in how many files this column exists\n","            present_files = [i for i, cols in enumerate(all_cols_by_file) if cand in cols]\n","            coverage = len(present_files) / len(all_cols_by_file)\n","\n","            if coverage >= min_coverage:\n","                chosen = cand\n","                if coverage < 1.0:\n","                    # Record indices of files missing this column (for later inspection)\n","                    missing_idx = [i for i in range(len(all_cols_by_file)) if i not in present_files]\n","                    missing_files[std] = missing_idx\n","                break\n","\n","        if not chosen:\n","            raise RuntimeError(\n","                f\"[Consistency assertion failed] {std}: Under prefixes {allowed_prefixes}, no '{suf}' meets {min_coverage*100:.0f}% coverage. \"\n","                f\"Check raw column names or change placement/prefix allowlist.\"\n","            )\n","\n","        mapping[std] = chosen\n","\n","    # Prefix consistency check: all mapped columns must come from allowlist\n","    used_prefixes = {extract_prefix(v) for v in mapping.values()}\n","    if not used_prefixes.issubset(set(allowed_prefixes)):\n","        raise RuntimeError(\n","            f\"[Consistency assertion failed] Final mapping prefixes {used_prefixes} are not all within allowlist {allowed_prefixes}\"\n","        )\n","\n","    return mapping, used_prefixes, missing_files\n","\n","# Build mapping\n","final_mapping, used_prefixes, missing_files = build_mapping_from_allowlist(\n","    allowed_prefixes, all_columns_by_file, MIN_COVERAGE\n",")\n","\n","print(\"\\nFinal channel mapping (standard_name <- original_column):\")\n","for std, orig in sorted(final_mapping.items()):\n","    print(f\"  {std} <- {orig}\")\n","\n","# Explicit hard assertions\n","assert len(used_prefixes) == 1, f\"A single prefix should be used; got {used_prefixes}\"\n","assert list(used_prefixes)[0] in set(PREFIX_ALLOWLIST[selected_placement]), \\\n","    f\"Source prefix {used_prefixes} not in allowlist {PREFIX_ALLOWLIST[selected_placement]} for {selected_placement}\"\n","\n","print(f\"\\n✓ Consistency assertions passed:\")\n","print(f\"  - Using a single prefix: {sorted(used_prefixes)}\")\n","print(f\"  - Prefix is in the allowlist: {PREFIX_ALLOWLIST[selected_placement]}\")\n","print(f\"  - Number of files checked: {len(all_columns_by_file)}\")\n","print(f\"  - Coverage requirement: {MIN_COVERAGE*100:.0f}%\")\n","\n","# If there are missing, print warnings\n","if missing_files:\n","    print(f\"\\n⚠️  The following channels are missing in some files (coverage threshold set to {MIN_COVERAGE*100:.0f}%):\")\n","    for std, idx_list in missing_files.items():\n","        print(f\"  {std}: missing in {len(idx_list)} files\")\n","\n","# ========== 4. Generate channel & placement config ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Generate channel & placement config\")\n","print(\"=\"*60)\n","\n","# Config content\n","config = {\n","    'dataset': 'LARa_MbientLab_IMU',\n","    'strategy': 'single_placement_baseline',\n","\n","    # Placement configuration\n","    'placements': {\n","        'selected': [selected_placement],\n","        'available': placement_stats['placement'].tolist(),\n","        'rationale': f'Fixed selection {selected_placement}, covering {len(subjects_with_selected)} subjects',\n","    },\n","\n","    # Raw channel configuration\n","    'channels': {\n","        'raw': ['ax', 'ay', 'az', 'gx', 'gy', 'gz'],\n","        'mapping': final_mapping,\n","        'prefix_allowlist': PREFIX_ALLOWLIST,\n","        'source_prefix': sorted(used_prefixes)[0],\n","        'min_coverage': MIN_COVERAGE,\n","        'description': {\n","            'ax': 'Accelerometer X axis (m/s² or g)',\n","            'ay': 'Accelerometer Y axis (m/s² or g)',\n","            'az': 'Accelerometer Z axis (m/s² or g)',\n","            'gx': 'Gyroscope X axis (rad/s or deg/s)',\n","            'gy': 'Gyroscope Y axis (rad/s or deg/s)',\n","            'gz': 'Gyroscope Z axis (rad/s or deg/s)',\n","        }\n","    },\n","\n","    # Derived channel configuration\n","    'derived_channels': {\n","        'acc_mag': {\n","            'formula': 'sqrt(ax^2 + ay^2 + az^2)',\n","            'description': 'Accelerometer vector magnitude',\n","        },\n","        'gyr_mag': {\n","            'formula': 'sqrt(gx^2 + gy^2 + gz^2)',\n","            'description': 'Gyroscope vector magnitude',\n","        }\n","    },\n","\n","    # Final channel order\n","    'final_channels': ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag'],\n","\n","    # Multi-placement fusion (reserved; currently disabled)\n","    'multi_placement_fusion': {\n","        'enabled': False,\n","        'strategy': None,\n","        'warning': 'If enabling multi-placement fusion, you must select the fusion strategy independently within each training fold to avoid cross-fold leakage',\n","    },\n","\n","    # Rigor notes\n","    'notes': [\n","        'Single-placement baseline: avoid cross-placement information leakage',\n","        'Channel mapping uses \"placement→prefix allowlist + consistency assertions\"; no cross-prefix voting',\n","        f'Consistency checked over all {len(all_columns_by_file)} {selected_placement} files',\n","        f'Coverage requirement: {MIN_COVERAGE*100:.0f}% (tunable tolerance)',\n","        'Derived channels are computed at feature-extraction stage to preserve raw data integrity',\n","        'Any multi-placement fusion must be chosen & validated within each LOSO fold',\n","    ]\n","}\n","\n","# Save config\n","config_file = configs_dir / \"channels.yaml\"\n","with open(config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","print(f\"✓ Saved config: {config_file}\")\n","\n","# ========== 5. Validate config (random multi-file sampling) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Validate config\")\n","print(\"=\"*60)\n","\n","# Verify coverage of selected placement across all sensor files\n","files_with_placement = sensor_files[sensor_files['placement'] == selected_placement]\n","\n","print(f\"\\nValidate placement '{selected_placement}':\")\n","print(f\"  Files: {len(files_with_placement)}\")\n","print(f\"  Subjects: {files_with_placement['subject_id'].nunique()}\")\n","print(f\"  Sessions: {files_with_placement['session_id'].nunique()}\")\n","\n","# Validate channel mapping: randomly sample multiple files\n","verify_sample_size = min(5, len(files_with_placement))\n","verify_df = files_with_placement.sample(n=verify_sample_size, random_state=0)\n","\n","print(f\"\\nValidate channel mapping (random sample of {verify_sample_size} files):\")\n","for idx, sample_file in verify_df.iterrows():\n","    sample_path = raw_dir / sample_file['standardized_path']\n","    try:\n","        df_verify = pd.read_csv(sample_path, nrows=100, sep=None, engine='python')\n","\n","        print(f\"\\nFile: {sample_file['filename']}\")\n","        all_found = True\n","        for std_name in ['ax', 'ay', 'az', 'gx', 'gy', 'gz']:\n","            if std_name in final_mapping:\n","                orig_name = final_mapping[std_name]\n","                if orig_name in df_verify.columns:\n","                    sample_val = df_verify[orig_name].iloc[0]\n","                    print(f\"  ✓ {std_name} <- {orig_name} (sample value: {sample_val:.4f})\")\n","                else:\n","                    print(f\"  ✗ {std_name} <- {orig_name} (column not found)\")\n","                    all_found = False\n","            else:\n","                print(f\"  ✗ {std_name} (not mapped)\")\n","                all_found = False\n","\n","        if not all_found:\n","            print(f\"  ⚠️  This file failed validation\")\n","\n","    except Exception as e:\n","        print(f\"\\nFile: {sample_file['filename']}\")\n","        print(f\"  ✗ Error during validation: {e}\")\n","\n","# Compute derived-channel examples on the first successfully validated file\n","for idx, sample_file in verify_df.iterrows():\n","    sample_path = raw_dir / sample_file['standardized_path']\n","    try:\n","        df_verify = pd.read_csv(sample_path, nrows=100, sep=None, engine='python')\n","        if all(final_mapping[ch] in df_verify.columns for ch in ['ax', 'ay', 'az', 'gx', 'gy', 'gz']):\n","            acc_mag = np.sqrt(\n","                df_verify[final_mapping['ax']].values**2 +\n","                df_verify[final_mapping['ay']].values**2 +\n","                df_verify[final_mapping['az']].values**2\n","            )\n","            gyr_mag = np.sqrt(\n","                df_verify[final_mapping['gx']].values**2 +\n","                df_verify[final_mapping['gy']].values**2 +\n","                df_verify[final_mapping['gz']].values**2\n","            )\n","\n","            print(f\"\\nDerived-channel example values (file: {sample_file['filename']}):\")\n","            print(f\"  acc_mag: min={acc_mag.min():.4f}, max={acc_mag.max():.4f}, mean={acc_mag.mean():.4f}\")\n","            print(f\"  gyr_mag: min={gyr_mag.min():.4f}, max={gyr_mag.max():.4f}, mean={gyr_mag.mean():.4f}\")\n","            break\n","    except:\n","        continue\n","\n","# ========== 6. Fuse check (reload config for verification) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Fuse check (reload config for verification)\")\n","print(\"=\"*60)\n","\n","with open(config_file, \"r\", encoding=\"utf-8\") as f:\n","    cfg = yaml.safe_load(f)\n","\n","# Extract prefixes of all mapped columns\n","srcs = list(cfg[\"channels\"][\"mapping\"].values())\n","pfxs = {re.match(r'^([A-Za-z]+_)', s).group(1) for s in srcs if re.match(r'^([A-Za-z]+_)', s)}\n","\n","# Assertion: all channels use the same prefix\n","assert len(pfxs) == 1, f\"ax..gz not using a single prefix: {pfxs}\"\n","\n","# Assertion: prefix in allowlist\n","sel = cfg[\"placements\"][\"selected\"][0]\n","allow = set(cfg[\"channels\"][\"prefix_allowlist\"][sel])\n","assert list(pfxs)[0] in allow, f\"Prefix {pfxs} not in {sel} allowlist {allow}\"\n","\n","print(f\"✓ Config fuse check passed:\")\n","print(f\"  - Reloaded config: {config_file}\")\n","print(f\"  - All channels use a single prefix: {pfxs}\")\n","print(f\"  - Prefix is in {sel} allowlist: {allow}\")\n","\n","# ========== 7. Summary ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 4 complete - Channels & Placement Strategy\")\n","print(\"=\"*60)\n","print(f\"\\nConfig summary:\")\n","print(f\"  Strategy: single-placement baseline\")\n","print(f\"  Fixed placement: {config['placements']['selected']}\")\n","print(f\"  Raw channels: {config['channels']['raw']}\")\n","print(f\"  Derived channels: {list(config['derived_channels'].keys())}\")\n","print(f\"  Final number of channels: {len(config['final_channels'])}\")\n","print(f\"  Prefix used: {sorted(used_prefixes)}\")\n","print(f\"  Coverage requirement: {MIN_COVERAGE*100:.0f}%\")\n","print(f\"\\nConfig file: {config_file}\")\n","print(f\"\\nRigor guarantees:\")\n","print(f\"  1. ✓ Use placement→prefix allowlist (hard-coded)\")\n","print(f\"  2. ✓ Consistency assertions across all files ({len(all_columns_by_file)} files)\")\n","print(f\"  3. ✓ No cross-prefix voting; avoid mis-selection\")\n","print(f\"  4. ✓ Error out if column names don't match allowlist\")\n","print(f\"  5. ✓ Explicit assertions: single prefix + within allowlist\")\n","print(f\"  6. ✓ Abort if header reading fails\")\n","print(f\"  7. ✓ Randomly sample {verify_sample_size} files to validate mapping\")\n","print(f\"  8. ✓ Fuse check: reload config and verify prefix\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PgsK4MlgXREx","executionInfo":{"status":"ok","timestamp":1763124315202,"user_tz":0,"elapsed":133,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"966a5710-f36c-4725-d5a8-e08e4f997738"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 4: Channel & Placement Strategy Selection\n","============================================================\n","\n","Loaded subject metadata: 8 subjects\n","Number of sensor files: 193\n","\n","============================================================\n","1. Analyze placement coverage\n","============================================================\n","\n","Placement statistics (sorted by sample count):\n","placement  num_subjects  num_sessions  total_bytes  total_samples\n","   rwrist             8            14    685467785        1120045\n","    chest             7            14    595662725         972496\n","   lwrist             6             2     80626579         131911\n","\n","Fixed placement for this round: rwrist\n","Subjects with rwrist data: 8/8\n","\n","============================================================\n","2. Allowlist validation & channel check\n","============================================================\n","Number of files for selected placement 'rwrist': 96\n","\n","Using placement→prefix allowlist: rwrist → ['RA_']\n","\n","Read headers of all 96 files to check consistency...\n","✓ Successfully read 96 files\n","\n","Data columns of the first file:\n","  LA_AccelerometerX\n","  LA_AccelerometerY\n","  LA_AccelerometerZ\n","  LA_GyroscopeX\n","  LA_GyroscopeY\n","  LA_GyroscopeZ\n","  LL_AccelerometerX\n","  LL_AccelerometerY\n","  LL_AccelerometerZ\n","  LL_GyroscopeX\n","  LL_GyroscopeY\n","  LL_GyroscopeZ\n","  N_AccelerometerX\n","  N_AccelerometerY\n","  N_AccelerometerZ\n","  N_GyroscopeX\n","  N_GyroscopeY\n","  N_GyroscopeZ\n","  RA_AccelerometerX\n","  RA_AccelerometerY\n","  RA_AccelerometerZ\n","  RA_GyroscopeX\n","  RA_GyroscopeY\n","  RA_GyroscopeZ\n","  RL_AccelerometerX\n","  RL_AccelerometerY\n","  RL_AccelerometerZ\n","  RL_GyroscopeX\n","  RL_GyroscopeY\n","  RL_GyroscopeZ\n","\n","============================================================\n","3. Build strict channel mapping (allowlist + consistency assertions)\n","============================================================\n","\n","Final channel mapping (standard_name <- original_column):\n","  ax <- RA_AccelerometerX\n","  ay <- RA_AccelerometerY\n","  az <- RA_AccelerometerZ\n","  gx <- RA_GyroscopeX\n","  gy <- RA_GyroscopeY\n","  gz <- RA_GyroscopeZ\n","\n","✓ Consistency assertions passed:\n","  - Using a single prefix: ['RA_']\n","  - Prefix is in the allowlist: ['RA_']\n","  - Number of files checked: 96\n","  - Coverage requirement: 100%\n","\n","============================================================\n","4. Generate channel & placement config\n","============================================================\n","✓ Saved config: configs/channels.yaml\n","\n","============================================================\n","5. Validate config\n","============================================================\n","\n","Validate placement 'rwrist':\n","  Files: 96\n","  Subjects: 8\n","  Sessions: 14\n","\n","Validate channel mapping (random sample of 5 files):\n","\n","File: l02_s09_r05.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.3215)\n","  ✓ ay <- RA_AccelerometerY (sample value: -0.9254)\n","  ✓ az <- RA_AccelerometerZ (sample value: 0.2765)\n","  ✓ gx <- RA_GyroscopeX (sample value: -1.1439)\n","  ✓ gy <- RA_GyroscopeY (sample value: -1.3566)\n","  ✓ gz <- RA_GyroscopeZ (sample value: -0.7625)\n","\n","File: l02_s14_r03.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.2813)\n","  ✓ ay <- RA_AccelerometerY (sample value: -0.9499)\n","  ✓ az <- RA_AccelerometerZ (sample value: 0.1393)\n","  ✓ gx <- RA_GyroscopeX (sample value: 1.2430)\n","  ✓ gy <- RA_GyroscopeY (sample value: 12.4323)\n","  ✓ gz <- RA_GyroscopeZ (sample value: 4.8893)\n","\n","File: l02_s07_r06.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.1674)\n","  ✓ ay <- RA_AccelerometerY (sample value: -0.9645)\n","  ✓ az <- RA_AccelerometerZ (sample value: -0.0895)\n","  ✓ gx <- RA_GyroscopeX (sample value: -1.0865)\n","  ✓ gy <- RA_GyroscopeY (sample value: 9.2328)\n","  ✓ gz <- RA_GyroscopeZ (sample value: 55.7498)\n","\n","File: l02_s11_r06.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.5316)\n","  ✓ ay <- RA_AccelerometerY (sample value: -0.8636)\n","  ✓ az <- RA_AccelerometerZ (sample value: 0.1911)\n","  ✓ gx <- RA_GyroscopeX (sample value: 8.0598)\n","  ✓ gy <- RA_GyroscopeY (sample value: 32.2562)\n","  ✓ gz <- RA_GyroscopeZ (sample value: -1.3813)\n","\n","File: l02_s12_r15.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.9424)\n","  ✓ ay <- RA_AccelerometerY (sample value: -1.0324)\n","  ✓ az <- RA_AccelerometerZ (sample value: 0.9479)\n","  ✓ gx <- RA_GyroscopeX (sample value: 59.7400)\n","  ✓ gy <- RA_GyroscopeY (sample value: 145.9488)\n","  ✓ gz <- RA_GyroscopeZ (sample value: -34.0791)\n","\n","Derived-channel example values (file: l02_s09_r05.csv):\n","  acc_mag: min=0.9377, max=1.0473, mean=1.0126\n","  gyr_mag: min=0.4164, max=9.8511, mean=3.7085\n","\n","============================================================\n","6. Fuse check (reload config for verification)\n","============================================================\n","✓ Config fuse check passed:\n","  - Reloaded config: configs/channels.yaml\n","  - All channels use a single prefix: {'RA_'}\n","  - Prefix is in rwrist allowlist: {'RA_'}\n","\n","============================================================\n","Step 4 complete - Channels & Placement Strategy\n","============================================================\n","\n","Config summary:\n","  Strategy: single-placement baseline\n","  Fixed placement: ['rwrist']\n","  Raw channels: ['ax', 'ay', 'az', 'gx', 'gy', 'gz']\n","  Derived channels: ['acc_mag', 'gyr_mag']\n","  Final number of channels: 8\n","  Prefix used: ['RA_']\n","  Coverage requirement: 100%\n","\n","Config file: configs/channels.yaml\n","\n","Rigor guarantees:\n","  1. ✓ Use placement→prefix allowlist (hard-coded)\n","  2. ✓ Consistency assertions across all files (96 files)\n","  3. ✓ No cross-prefix voting; avoid mis-selection\n","  4. ✓ Error out if column names don't match allowlist\n","  5. ✓ Explicit assertions: single prefix + within allowlist\n","  6. ✓ Abort if header reading fails\n","  7. ✓ Randomly sample 5 files to validate mapping\n","  8. ✓ Fuse check: reload config and verify prefix\n","============================================================\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","\n","# ========== Manually set FOLD_ID (if the environment variable is not set) ==========\n","if \"FOLD_ID\" not in os.environ:\n","    print(\"⚠️ Environment variable FOLD_ID is not set; please specify it manually:\")\n","    print(\"Hint: If your LOSO has N folds, FOLD_ID should be 0 to N-1\")\n","    fold_input = input(\"Enter FOLD_ID (press Enter to default to 0): \").strip()\n","    os.environ[\"FOLD_ID\"] = fold_input if fold_input else \"0\"\n","    print(f\"✓ FOLD_ID set to {os.environ['FOLD_ID']}\")\n","\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","\"\"\"\n","Step 5: Timeline Unification & Resampling (top-conf/journal grade - flawless)\n","Unify to 50 Hz; linear interpolation/forward-fill; align start/end\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import re\n","import os\n","import json\n","from datetime import datetime, timezone\n","\n","# ========== Config ==========\n","TARGET_FREQ_HZ = 50.0           # Target sampling rate\n","MAX_INTERP_GAP_MS = 20.0        # Maximum interpolation gap (milliseconds)\n","MAX_INTERP_RATIO = 0.15         # Gap coverage threshold 15% (constant; applied globally)\n","\n","print(\"=\"*60)\n","print(\"Step 5: Timeline Unification & Resampling\")\n","print(\"=\"*60)\n","\n","# Load config and metadata\n","raw_dir = Path(\"data/lara/mbientlab/raw\")\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","proc_dir.mkdir(parents=True, exist_ok=True)\n","\n","configs_dir = Path(\"configs\")\n","with open(configs_dir / \"channels.yaml\", 'r', encoding='utf-8') as f:\n","    channel_config = yaml.safe_load(f)\n","\n","selected_placement = channel_config['placements']['selected'][0]\n","channel_mapping = channel_config['channels']['mapping']\n","print(f\"\\nTarget sampling rate: {TARGET_FREQ_HZ} Hz\")\n","print(f\"Selected placement: {selected_placement}\")\n","\n","# Load QC results (all kept sessions)\n","qa_keep = pd.read_csv(raw_dir / \"qa_keep_sessions.csv\")\n","keep_sessions = qa_keep[qa_keep['keep'] == True].copy()\n","keep_sessions = keep_sessions[keep_sessions['placement'] == selected_placement].copy()\n","\n","# Load train-fold markers (for statistics)\n","splits_path = configs_dir / \"splits.json\"\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","\n","train_subjects = set()\n","if splits_path.exists() and FOLD_ID >= 0:\n","    with open(splits_path, \"r\") as f:\n","        splits = json.load(f)\n","    train_subjects = set(splits[str(FOLD_ID)][\"train_subjects\"])\n","    print(f\"\\nTrain-fold markers: FOLD_ID={FOLD_ID}\")\n","    print(f\"  Train subjects: {len(train_subjects)}\")\n","    print(f\"  Total sessions: {len(keep_sessions)}\")\n","    print(f\"  Train sessions: {keep_sessions['subject_id'].isin(train_subjects).sum()}\")\n","    print(f\"  Test sessions: {(~keep_sessions['subject_id'].isin(train_subjects)).sum()}\")\n","else:\n","    print(f\"\\nTrain-fold markers not enabled (only logging; no pruning)\")\n","    print(f\"  Total sessions: {len(keep_sessions)}\")\n","\n","# Fix 1: Prune switch (enabled only when training fold exists)\n","APPLY_PRUNE = len(train_subjects) > 0\n","keep_sessions['is_train'] = (\n","    keep_sessions['subject_id'].isin(train_subjects) if len(train_subjects) > 0 else False\n",")\n","\n","print(f\"  Prune switch: {'ON' if APPLY_PRUNE else 'OFF'}\")\n","\n","# Load file index\n","index_file = raw_dir / \"file_index.parquet\"\n","if not index_file.exists():\n","    index_file = raw_dir / \"file_index.csv\"\n","file_index = pd.read_parquet(index_file) if index_file.suffix == '.parquet' else pd.read_csv(index_file)\n","\n","# ========== Helper functions ==========\n","def detect_time_column(df):\n","    \"\"\"Detect time column (avoid false positive matches on 'ts' substring)\"\"\"\n","    time_cols = [c for c in df.columns\n","                 if re.search(r'(^|_)(time|timestamp|epoch|ts)($|_)', c, re.I)]\n","    return time_cols[0] if time_cols else None\n","\n","def parse_time_to_seconds(time_series):\n","    \"\"\"Convert time to seconds (correctly infer Unix timestamp units)\"\"\"\n","    numeric = pd.to_numeric(time_series, errors='coerce')\n","    if numeric.notna().sum() > len(time_series) * 0.9:\n","        vals = numeric.dropna().values\n","        max_val = np.abs(vals[:1000]).max() if len(vals) else 0\n","\n","        # Infer by 2025 Unix timestamp magnitude\n","        if max_val > 1e17:      # nanoseconds\n","            return numeric * 1e-9\n","        elif max_val > 1e14:    # microseconds\n","            return numeric * 1e-6\n","        elif max_val > 1e11:    # milliseconds\n","            return numeric * 1e-3\n","        else:                   # seconds\n","            return numeric\n","\n","    dt = pd.to_datetime(time_series, utc=True, errors='coerce')\n","    if dt.notna().sum() > len(time_series) * 0.9:\n","        epoch = pd.Timestamp(\"1970-01-01\", tz='UTC')\n","        return (dt - epoch).dt.total_seconds()\n","\n","    return None\n","\n","def resample_sensor_data(df, time_col, data_cols, target_freq_hz=50.0, max_gap_ms=20.0):\n","    \"\"\"Resample sensor data (return cleaned time for labels)\"\"\"\n","    time_sec = parse_time_to_seconds(df[time_col])\n","    if time_sec is None:\n","        raise ValueError(\"Unable to parse time column\")\n","\n","    valid_mask = time_sec.notna() & df[data_cols].notna().all(axis=1)\n","    time_clean = time_sec[valid_mask].values\n","    data_clean = df.loc[valid_mask, data_cols].values\n","\n","    if len(time_clean) < 2:\n","        return None, 0.0, 0, 0.0, 0.0, None\n","\n","    # De-duplicate + sort\n","    unique_idx = np.unique(time_clean, return_index=True)[1]\n","    time_clean = time_clean[unique_idx]\n","    data_clean = data_clean[unique_idx]\n","\n","    order = np.argsort(time_clean)\n","    time_clean = time_clean[order]\n","    data_clean = data_clean[order]\n","\n","    # Original frequency\n","    dt_orig = np.median(np.diff(time_clean))\n","    orig_freq_hz = 1.0 / dt_orig if dt_orig > 0 else 0.0\n","\n","    # Build target timeline with integer number of samples\n","    dt = 1.0 / target_freq_hz\n","    t_start = time_clean[0]\n","    t_end = time_clean[-1]\n","    n_samples = int(np.round((t_end - t_start) / dt))\n","    target_time = t_start + np.arange(n_samples + 1) * dt\n","\n","    # Linear interpolation\n","    resampled_data = np.zeros((len(target_time), len(data_cols)))\n","    for i in range(len(data_cols)):\n","        resampled_data[:, i] = np.interp(target_time, time_clean, data_clean[:, i])\n","\n","    # Large-gap detection (account for jitter)\n","    max_gap_sec = max(max_gap_ms / 1000.0, 1.25 * dt)\n","    time_diffs = np.diff(time_clean)\n","    gap_mask = time_diffs > max_gap_sec\n","\n","    is_in_gap = np.zeros(len(target_time), dtype=int)\n","    is_forced_nan = np.zeros(len(target_time), dtype=int)\n","    actual_interp_count = 0\n","    total_gap_time = 0.0\n","\n","    if gap_mask.any():\n","        for i in range(len(time_clean) - 1):\n","            if gap_mask[i]:\n","                t_gap_start = time_clean[i]\n","                t_gap_end = time_clean[i + 1]\n","                gap_duration = t_gap_end - t_gap_start\n","                total_gap_time += gap_duration\n","\n","                idxs = np.where((target_time > t_gap_start) & (target_time < t_gap_end))[0]\n","\n","                if idxs.size > 0:\n","                    is_in_gap[idxs] = 1\n","                    actual_interp_count += 1\n","\n","                    if idxs.size > 1:\n","                        forced_nan_idxs = idxs[1:]\n","                        is_forced_nan[forced_nan_idxs] = 1\n","                        resampled_data[forced_nan_idxs, :] = np.nan\n","\n","    # Gap coverage\n","    gap_points = int(is_in_gap.sum())\n","    interp_ratio = gap_points / len(target_time) if len(target_time) > 0 else 0.0\n","\n","    # Gap time fraction\n","    total_duration = t_end - t_start\n","    gap_time_fraction = total_gap_time / total_duration if total_duration > 0 else 0.0\n","\n","    resampled_df = pd.DataFrame(resampled_data, columns=data_cols)\n","    resampled_df.insert(0, 'time_sec', target_time)\n","    resampled_df['is_in_gap'] = is_in_gap\n","    resampled_df['is_forced_nan'] = is_forced_nan\n","\n","    return resampled_df, interp_ratio, gap_points, gap_time_fraction, orig_freq_hz, time_clean\n","\n","def resample_labels(df_label, label_col, target_time, sensor_time_original=None, label_time_col=None):\n","    \"\"\"Resample labels (boundary NaN + sorting)\"\"\"\n","    if label_time_col is not None:\n","        time_sec = parse_time_to_seconds(df_label[label_time_col])\n","        if time_sec is None:\n","            raise ValueError(\"Unable to parse label time column\")\n","\n","        valid_mask = time_sec.notna() & df_label[label_col].notna()\n","        time_clean = time_sec[valid_mask].values\n","        labels_clean = df_label.loc[valid_mask, label_col].values\n","    else:\n","        if sensor_time_original is None:\n","            raise ValueError(\"Labels have no time column and no sensor time provided\")\n","\n","        min_len = min(len(df_label), len(sensor_time_original))\n","        if abs(len(df_label) - len(sensor_time_original)) > min_len * 0.01:\n","            raise ValueError(f\"Label rows ({len(df_label)}) differ too much from sensor rows ({len(sensor_time_original)})\")\n","\n","        time_clean = sensor_time_original[:min_len]\n","        labels_clean = df_label[label_col].iloc[:min_len].values\n","\n","        valid_mask = pd.notna(labels_clean)\n","        time_clean = time_clean[valid_mask]\n","        labels_clean = labels_clean[valid_mask]\n","\n","    if len(time_clean) == 0:\n","        return np.full(len(target_time), np.nan)\n","\n","    # Explicit sorting\n","    order = np.argsort(time_clean)\n","    time_clean = time_clean[order]\n","    labels_clean = labels_clean[order]\n","\n","    idx = np.searchsorted(time_clean, target_time, side='right') - 1\n","    idx = np.clip(idx, 0, len(time_clean) - 1)\n","\n","    labels = labels_clean[idx].copy()\n","\n","    # Fix: cast integers to float to allow NaN\n","    if labels.dtype.kind in ['i', 'u']:  # integer or unsigned integer\n","        labels = labels.astype('float64')\n","\n","    # Boundary NaNs\n","    mask_before = target_time < time_clean[0]\n","    mask_after = target_time > time_clean[-1]\n","    labels[mask_before | mask_after] = np.nan\n","\n","    return labels\n","\n","# ========== 1. Process all sessions ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Resampling\")\n","print(\"=\"*60)\n","\n","resampled_records = []\n","interp_stats = []\n","issues = []\n","\n","for idx, session in keep_sessions.iterrows():\n","    subject_id = session['subject_id']\n","    session_id = session['session_id']\n","    placement = session['placement']\n","    is_train = session['is_train']\n","\n","    print(f\"\\nProcessing {subject_id}/{session_id}/{placement} {'[TRAIN]' if is_train else '[TEST]'}...\")\n","\n","    sensor_file = file_index[\n","        (file_index['subject_id'] == subject_id) &\n","        (file_index['session_id'] == session_id) &\n","        (file_index['placement'] == placement) &\n","        (~file_index['filename'].str.contains('label', case=False, na=False))\n","    ]\n","\n","    label_file = file_index[\n","        (file_index['subject_id'] == subject_id) &\n","        (file_index['session_id'] == session_id) &\n","        (file_index['placement'] == placement) &\n","        (file_index['filename'].str.contains('label', case=False, na=False))\n","    ]\n","\n","    if sensor_file.empty or label_file.empty:\n","        print(f\"  Skip: missing files\")\n","        continue\n","\n","    sensor_path = raw_dir / sensor_file.iloc[0]['standardized_path']\n","    label_path = raw_dir / label_file.iloc[0]['standardized_path']\n","\n","    try:\n","        df_sensor = pd.read_csv(sensor_path, sep=None, engine='python')\n","        time_col = detect_time_column(df_sensor)\n","        if not time_col:\n","            print(f\"  Skip: no time column\")\n","            continue\n","\n","        data_cols = [channel_mapping[std] for std in ['ax', 'ay', 'az', 'gx', 'gy', 'gz']]\n","        missing_cols = [c for c in data_cols if c not in df_sensor.columns]\n","        if missing_cols:\n","            print(f\"  Skip: missing columns {missing_cols}\")\n","            continue\n","\n","        print(f\"  Resampling sensors ({len(df_sensor)} rows)...\")\n","        result = resample_sensor_data(\n","            df_sensor, time_col, data_cols, TARGET_FREQ_HZ, MAX_INTERP_GAP_MS\n","        )\n","\n","        if result[0] is None:\n","            print(f\"  Skip: resampling failed\")\n","            continue\n","\n","        # Fix 2: receive cleaned time for labels\n","        resampled_sensor, interp_ratio, gap_points, gap_time_frac, orig_freq, sensor_time_clean = result\n","\n","        valid_samples = resampled_sensor[data_cols].notna().all(axis=1).sum()\n","        nan_samples = len(resampled_sensor) - valid_samples\n","        forced_nan_points = int(resampled_sensor['is_forced_nan'].sum())\n","\n","        print(f\"  → {len(resampled_sensor)} rows, gap coverage: {interp_ratio*100:.2f}%, NaN: {nan_samples}\")\n","\n","        # Fix 1: prune based on switch\n","        if interp_ratio > MAX_INTERP_RATIO:\n","            msg = f\"Gap coverage too high ({interp_ratio*100:.1f}%)\"\n","            print(f\"  ⚠️  {msg}\")\n","            issues.append({\n","                'subject_id': subject_id,\n","                'session_id': session_id,\n","                'placement': placement,\n","                'is_train': is_train,\n","                'issue': 'high_gap_coverage',\n","                'gap_coverage': round(interp_ratio, 4),\n","            })\n","            if APPLY_PRUNE:\n","                continue\n","\n","        interp_stats.append({\n","            'subject_id': subject_id,\n","            'session_id': session_id,\n","            'placement': placement,\n","            'is_train': is_train,\n","            'original_samples': len(df_sensor),\n","            'original_freq_hz': round(orig_freq, 2),\n","            'resampled_samples': len(resampled_sensor),\n","            'valid_samples': valid_samples,\n","            'nan_samples': nan_samples,\n","            'gap_points': gap_points,\n","            'gap_coverage': round(interp_ratio, 4),\n","            'gap_time_fraction': round(gap_time_frac, 4),\n","            'forced_nan_points': forced_nan_points,\n","        })\n","\n","        df_label = pd.read_csv(label_path, sep=None, engine='python')\n","\n","        label_col = None\n","        for col_candidate in ['Class', 'class', 'label', 'Label', 'activity', 'Activity']:\n","            if col_candidate in df_label.columns:\n","                label_col = col_candidate\n","                break\n","\n","        if not label_col:\n","            for col in df_label.columns:\n","                if any(kw in col.lower() for kw in ['label', 'activity', 'class', 'action']):\n","                    label_col = col\n","                    break\n","\n","        if not label_col:\n","            print(f\"  Skip: no label column\")\n","            issues.append({\n","                'subject_id': subject_id,\n","                'session_id': session_id,\n","                'placement': placement,\n","                'is_train': is_train,\n","                'issue': 'no_label_column',\n","            })\n","            continue\n","\n","        label_time_col = detect_time_column(df_label)\n","        target_time = resampled_sensor['time_sec'].values\n","\n","        print(f\"  Resampling labels...\")\n","        try:\n","            if label_time_col:\n","                resampled_labels = resample_labels(\n","                    df_label, label_col, target_time,\n","                    label_time_col=label_time_col\n","                )\n","            else:\n","                # Fix 2: use cleaned sensor time (consistent basis)\n","                resampled_labels = resample_labels(\n","                    df_label, label_col, target_time,\n","                    sensor_time_original=sensor_time_clean\n","                )\n","\n","            resampled_sensor['label'] = resampled_labels\n","\n","        except Exception as e:\n","            print(f\"  Skip: label resampling failed - {e}\")\n","            issues.append({\n","                'subject_id': subject_id,\n","                'session_id': session_id,\n","                'placement': placement,\n","                'is_train': is_train,\n","                'issue': 'label_resample_error',\n","                'error': str(e),  # Fix 3: include error details\n","            })\n","            continue\n","\n","        resampled_sensor.rename(columns={\n","            channel_mapping['ax']: 'ax',\n","            channel_mapping['ay']: 'ay',\n","            channel_mapping['az']: 'az',\n","            channel_mapping['gx']: 'gx',\n","            channel_mapping['gy']: 'gy',\n","            channel_mapping['gz']: 'gz',\n","        }, inplace=True)\n","\n","        resampled_sensor.insert(0, 'subject_id', subject_id)\n","        resampled_sensor.insert(1, 'session_id', session_id)\n","        resampled_sensor.insert(2, 'placement', placement)\n","\n","        resampled_records.append(resampled_sensor)\n","        print(f\"  ✓ Done\")\n","\n","    except Exception as e:\n","        print(f\"  ✗ Error: {e}\")\n","        issues.append({\n","            'subject_id': subject_id,\n","            'session_id': session_id,\n","            'placement': placement,\n","            'is_train': is_train,\n","            'issue': 'processing_error',\n","            'error': str(e),  # Fix 3: include error details\n","        })\n","\n","print(f\"\\nSuccessfully processed: {len(resampled_records)} sessions\")\n","print(f\"Skipped/failed: {len(issues)} sessions\")\n","\n","# ========== 2. Combine & save ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Combine & Save\")\n","print(\"=\"*60)\n","\n","if resampled_records:\n","    df_all = pd.concat(resampled_records, ignore_index=True)\n","\n","    # Optimization: cast dtypes (reduce size)\n","    for c in ['ax', 'ay', 'az', 'gx', 'gy', 'gz']:\n","        df_all[c] = df_all[c].astype('float32')\n","    df_all['time_sec'] = df_all['time_sec'].astype('float64')  # Keep high precision for time\n","\n","    output_file = proc_dir / \"resampled.parquet\"\n","\n","    if output_file.exists():\n","        import shutil\n","        if output_file.is_dir():\n","            shutil.rmtree(output_file)\n","        else:\n","            output_file.unlink()\n","        print(f\"Removed old data: {output_file}\")\n","\n","    df_all.to_parquet(\n","        output_file,\n","        index=False,\n","        partition_cols=['subject_id', 'placement'],\n","        engine='pyarrow'\n","    )\n","    print(f\"✓ Saved: {output_file}\")\n","    print(f\"  Total rows: {len(df_all):,}\")\n","    print(f\"  # subjects: {df_all['subject_id'].nunique()}\")\n","    print(f\"  # sessions: {df_all.groupby(['subject_id', 'session_id']).ngroups}\")\n","\n","    valid_mask = df_all[['ax', 'ay', 'az', 'gx', 'gy', 'gz']].notna().all(axis=1)\n","    print(f\"  Valid samples: {valid_mask.sum():,} ({valid_mask.sum()/len(df_all)*100:.1f}%)\")\n","    print(f\"  Samples with NaN: {(~valid_mask).sum():,}\")\n","\n","    print(\"\\nData preview:\")\n","    print(df_all.head(10).to_string())\n","\n","    print(\"\\nNumeric column stats (valid samples):\")\n","    numeric_cols = ['ax', 'ay', 'az', 'gx', 'gy', 'gz']\n","    print(df_all.loc[valid_mask, numeric_cols].describe().round(4))\n","else:\n","    print(\"Warning: No data to save\")\n","\n","# ========== 3. Save statistics ==========\n","if interp_stats:\n","    df_interp = pd.DataFrame(interp_stats)\n","    interp_file = proc_dir / \"resample_stats.csv\"\n","    df_interp.to_csv(interp_file, index=False)\n","    print(f\"\\n✓ Saved stats: {interp_file}\")\n","\n","    if 'is_train' in df_interp.columns and df_interp['is_train'].any():\n","        train_stats = df_interp[df_interp['is_train']]\n","        print(f\"\\nGap statistics (train fold):\")\n","        print(f\"  Mean gap coverage: {train_stats['gap_coverage'].mean()*100:.2f}%\")\n","        print(f\"  Max gap coverage: {train_stats['gap_coverage'].max()*100:.2f}%\")\n","        print(f\"  Mean gap time fraction: {train_stats['gap_time_fraction'].mean()*100:.2f}%\")\n","\n","        print(f\"\\nGap statistics (overall):\")\n","        print(f\"  Mean gap coverage: {df_interp['gap_coverage'].mean()*100:.2f}%\")\n","        print(f\"  Max gap coverage: {df_interp['gap_coverage'].max()*100:.2f}%\")\n","    else:\n","        print(f\"\\nGap statistics:\")\n","        print(f\"  Mean gap coverage: {df_interp['gap_coverage'].mean()*100:.2f}%\")\n","        print(f\"  Max gap coverage: {df_interp['gap_coverage'].max()*100:.2f}%\")\n","\n","if issues:\n","    df_issues = pd.DataFrame(issues)\n","    issues_file = proc_dir / \"resample_issues.csv\"\n","    df_issues.to_csv(issues_file, index=False)\n","    print(f\"\\n⚠️  Saved issue records: {issues_file} ({len(issues)} items)\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 5 complete - Flawless version\")\n","print(\"=\"*60)\n","print(f\"\\nFinal fixes:\")\n","print(f\"  1. ✓ Prune switch (enabled only when training fold is set)\")\n","print(f\"  2. ✓ Label time harmonized (reuse cleaned time)\")\n","print(f\"  3. ✓ Complete error information (\\\"error\\\" field)\")\n","print(f\"  4. ✓ Comment fix (constant threshold 0.15)\")\n","print(f\"  5. ✓ Type optimization (float32/float64)\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gYRms4dCXSXe","executionInfo":{"status":"ok","timestamp":1763130375152,"user_tz":0,"elapsed":51203,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"5df25ceb-8735-4d6b-b211-38b946aee98c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["⚠️ Environment variable FOLD_ID is not set; please specify it manually:\n","Hint: If your LOSO has N folds, FOLD_ID should be 0 to N-1\n","Enter FOLD_ID (press Enter to default to 0): 5\n","✓ FOLD_ID set to 5\n","============================================================\n","Step 5: Timeline Unification & Resampling\n","============================================================\n","\n","Target sampling rate: 50.0 Hz\n","Selected placement: rwrist\n","\n","Train-fold markers: FOLD_ID=5\n","  Train subjects: 7\n","  Total sessions: 96\n","  Train sessions: 90\n","  Test sessions: 6\n","  Prune switch: ON\n","\n","============================================================\n","1. Resampling\n","============================================================\n","\n","Processing S07/R03/rwrist [TRAIN]...\n","  Resampling sensors (11758 rows)...\n","  → 5879 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R05/rwrist [TRAIN]...\n","  Resampling sensors (11766 rows)...\n","  → 5883 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R06/rwrist [TRAIN]...\n","  Resampling sensors (11838 rows)...\n","  → 5919 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R07/rwrist [TRAIN]...\n","  Resampling sensors (11795 rows)...\n","  → 5898 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R08/rwrist [TRAIN]...\n","  Resampling sensors (11804 rows)...\n","  → 5902 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R09/rwrist [TRAIN]...\n","  Resampling sensors (11779 rows)...\n","  → 5890 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R10/rwrist [TRAIN]...\n","  Resampling sensors (11777 rows)...\n","  → 5889 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R11/rwrist [TRAIN]...\n","  Resampling sensors (11775 rows)...\n","  → 5888 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R12/rwrist [TRAIN]...\n","  Resampling sensors (11814 rows)...\n","  → 5908 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R13/rwrist [TRAIN]...\n","  Resampling sensors (11842 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R14/rwrist [TRAIN]...\n","  Resampling sensors (11806 rows)...\n","  → 5903 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R15/rwrist [TRAIN]...\n","  Resampling sensors (11774 rows)...\n","  → 5888 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R16/rwrist [TRAIN]...\n","  Resampling sensors (11863 rows)...\n","  → 5932 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R03/rwrist [TRAIN]...\n","  Resampling sensors (11868 rows)...\n","  → 5935 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R04/rwrist [TRAIN]...\n","  Resampling sensors (11843 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R06/rwrist [TRAIN]...\n","  Resampling sensors (11864 rows)...\n","  → 5932 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R07/rwrist [TRAIN]...\n","  Resampling sensors (11901 rows)...\n","  → 5951 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R08/rwrist [TRAIN]...\n","  Resampling sensors (11856 rows)...\n","  → 5929 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R10/rwrist [TRAIN]...\n","  Resampling sensors (11926 rows)...\n","  → 5963 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R11/rwrist [TRAIN]...\n","  Resampling sensors (11903 rows)...\n","  → 5952 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R12/rwrist [TRAIN]...\n","  Resampling sensors (11924 rows)...\n","  → 5963 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R13/rwrist [TRAIN]...\n","  Resampling sensors (11925 rows)...\n","  → 5963 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R15/rwrist [TRAIN]...\n","  Resampling sensors (11270 rows)...\n","  → 5636 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R16/rwrist [TRAIN]...\n","  Resampling sensors (11540 rows)...\n","  → 5770 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R03/rwrist [TRAIN]...\n","  Resampling sensors (11866 rows)...\n","  → 5934 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R04/rwrist [TRAIN]...\n","  Resampling sensors (11907 rows)...\n","  → 5954 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R05/rwrist [TRAIN]...\n","  Resampling sensors (11895 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R06/rwrist [TRAIN]...\n","  Resampling sensors (11916 rows)...\n","  → 5959 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R07/rwrist [TRAIN]...\n","  Resampling sensors (11860 rows)...\n","  → 5931 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R08/rwrist [TRAIN]...\n","  Resampling sensors (11909 rows)...\n","  → 5955 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R09/rwrist [TRAIN]...\n","  Resampling sensors (11906 rows)...\n","  → 5953 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R10/rwrist [TRAIN]...\n","  Resampling sensors (11886 rows)...\n","  → 5944 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R11/rwrist [TRAIN]...\n","  Resampling sensors (11880 rows)...\n","  → 5941 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R12/rwrist [TRAIN]...\n","  Resampling sensors (11866 rows)...\n","  → 5933 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R13/rwrist [TRAIN]...\n","  Resampling sensors (11904 rows)...\n","  → 5953 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R14/rwrist [TRAIN]...\n","  Resampling sensors (4035 rows)...\n","  → 2018 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R15/rwrist [TRAIN]...\n","  Resampling sensors (11895 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R16/rwrist [TRAIN]...\n","  Resampling sensors (11895 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R03/rwrist [TRAIN]...\n","  Resampling sensors (11795 rows)...\n","  → 5898 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R04/rwrist [TRAIN]...\n","  Resampling sensors (11806 rows)...\n","  → 5903 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R05/rwrist [TRAIN]...\n","  Resampling sensors (11815 rows)...\n","  → 5908 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R06/rwrist [TRAIN]...\n","  Resampling sensors (11767 rows)...\n","  → 5884 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R07/rwrist [TRAIN]...\n","  Resampling sensors (11869 rows)...\n","  → 5935 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R08/rwrist [TRAIN]...\n","  Resampling sensors (11794 rows)...\n","  → 5898 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R09/rwrist [TRAIN]...\n","  Resampling sensors (11791 rows)...\n","  → 5896 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R10/rwrist [TRAIN]...\n","  Resampling sensors (11863 rows)...\n","  → 5932 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R11/rwrist [TRAIN]...\n","  Resampling sensors (11822 rows)...\n","  → 5911 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R12/rwrist [TRAIN]...\n","  Resampling sensors (11845 rows)...\n","  → 5923 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R13/rwrist [TRAIN]...\n","  Resampling sensors (11792 rows)...\n","  → 5897 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R14/rwrist [TRAIN]...\n","  Resampling sensors (11798 rows)...\n","  → 5900 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R15/rwrist [TRAIN]...\n","  Resampling sensors (11803 rows)...\n","  → 5902 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R16/rwrist [TRAIN]...\n","  Resampling sensors (11751 rows)...\n","  → 5876 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R03/rwrist [TRAIN]...\n","  Resampling sensors (11858 rows)...\n","  → 5929 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R04/rwrist [TRAIN]...\n","  Resampling sensors (11809 rows)...\n","  → 5905 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R05/rwrist [TRAIN]...\n","  Resampling sensors (11821 rows)...\n","  → 5911 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R06/rwrist [TRAIN]...\n","  Resampling sensors (11843 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R07/rwrist [TRAIN]...\n","  Resampling sensors (11897 rows)...\n","  → 5949 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R08/rwrist [TRAIN]...\n","  Resampling sensors (11889 rows)...\n","  → 5945 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R09/rwrist [TRAIN]...\n","  Resampling sensors (11906 rows)...\n","  → 5953 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R10/rwrist [TRAIN]...\n","  Resampling sensors (11880 rows)...\n","  → 5940 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R11/rwrist [TRAIN]...\n","  Resampling sensors (11859 rows)...\n","  → 5930 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R12/rwrist [TRAIN]...\n","  Resampling sensors (11885 rows)...\n","  → 5943 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R13/rwrist [TRAIN]...\n","  Resampling sensors (11841 rows)...\n","  → 5921 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R15/rwrist [TRAIN]...\n","  Resampling sensors (11879 rows)...\n","  → 5940 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R11/rwrist [TEST]...\n","  Resampling sensors (11894 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R12/rwrist [TEST]...\n","  Resampling sensors (2817 rows)...\n","  → 1409 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R13/rwrist [TEST]...\n","  Resampling sensors (11905 rows)...\n","  → 5953 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R14/rwrist [TEST]...\n","  Resampling sensors (11489 rows)...\n","  → 5745 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R15/rwrist [TEST]...\n","  Resampling sensors (11927 rows)...\n","  → 5964 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R16/rwrist [TEST]...\n","  Resampling sensors (11843 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R03/rwrist [TRAIN]...\n","  Resampling sensors (11845 rows)...\n","  → 5923 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R04/rwrist [TRAIN]...\n","  Resampling sensors (11878 rows)...\n","  → 5940 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R05/rwrist [TRAIN]...\n","  Resampling sensors (11910 rows)...\n","  → 5956 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R06/rwrist [TRAIN]...\n","  Resampling sensors (11902 rows)...\n","  → 5951 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R07/rwrist [TRAIN]...\n","  Resampling sensors (11903 rows)...\n","  → 5952 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R08/rwrist [TRAIN]...\n","  Resampling sensors (11896 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R09/rwrist [TRAIN]...\n","  Resampling sensors (11910 rows)...\n","  → 5955 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R10/rwrist [TRAIN]...\n","  Resampling sensors (11918 rows)...\n","  → 5959 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R11/rwrist [TRAIN]...\n","  Resampling sensors (11861 rows)...\n","  → 5931 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R12/rwrist [TRAIN]...\n","  Resampling sensors (11912 rows)...\n","  → 5956 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R13/rwrist [TRAIN]...\n","  Resampling sensors (11894 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R14/rwrist [TRAIN]...\n","  Resampling sensors (11884 rows)...\n","  → 5943 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R15/rwrist [TRAIN]...\n","  Resampling sensors (11892 rows)...\n","  → 5946 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R16/rwrist [TRAIN]...\n","  Resampling sensors (11870 rows)...\n","  → 5935 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R03/rwrist [TRAIN]...\n","  Resampling sensors (11849 rows)...\n","  → 5925 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R04/rwrist [TRAIN]...\n","  Resampling sensors (11760 rows)...\n","  → 5881 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R06/rwrist [TRAIN]...\n","  Resampling sensors (11856 rows)...\n","  → 5928 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R07/rwrist [TRAIN]...\n","  Resampling sensors (11863 rows)...\n","  → 5932 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R08/rwrist [TRAIN]...\n","  Resampling sensors (11921 rows)...\n","  → 5961 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R09/rwrist [TRAIN]...\n","  Resampling sensors (11871 rows)...\n","  → 5936 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R10/rwrist [TRAIN]...\n","  Resampling sensors (11911 rows)...\n","  → 5956 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R11/rwrist [TRAIN]...\n","  Resampling sensors (11843 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R12/rwrist [TRAIN]...\n","  Resampling sensors (11787 rows)...\n","  → 5894 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R14/rwrist [TRAIN]...\n","  Resampling sensors (11851 rows)...\n","  → 5926 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R15/rwrist [TRAIN]...\n","  Resampling sensors (11823 rows)...\n","  → 5912 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R16/rwrist [TRAIN]...\n","  Resampling sensors (11851 rows)...\n","  → 5926 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Successfully processed: 96 sessions\n","Skipped/failed: 0 sessions\n","\n","============================================================\n","2. Combine & Save\n","============================================================\n","Removed old data: data/lara/mbientlab/proc/resampled.parquet\n","✓ Saved: data/lara/mbientlab/proc/resampled.parquet\n","  Total rows: 560,070\n","  # subjects: 8\n","  # sessions: 96\n","  Valid samples: 560,070 (100.0%)\n","  Samples with NaN: 0\n","\n","Data preview:\n","  subject_id session_id placement      time_sec        ax        ay        az         gx         gy         gz  is_in_gap  is_forced_nan  label\n","0        S07        R03    rwrist  1.564739e+09 -0.281438 -0.924541  0.325248   1.414270  24.960403  -9.089332          0              0    6.0\n","1        S07        R03    rwrist  1.564739e+09 -0.298464 -0.935255  0.334803   3.294115  27.049623 -12.293148          0              0    6.0\n","2        S07        R03    rwrist  1.564739e+09 -0.263180 -1.002837  0.456556   2.767826  14.701869 -19.894005          0              0    6.0\n","3        S07        R03    rwrist  1.564739e+09 -0.174738 -1.060583  0.545962   5.506332   5.940687 -22.905399          0              0    6.0\n","4        S07        R03    rwrist  1.564739e+09 -0.125629 -1.083030  0.620053  16.071184  10.412006 -25.808487          0              0    6.0\n","5        S07        R03    rwrist  1.564739e+09 -0.148535 -1.093232  0.639439  25.785658  20.021729 -29.624060          0              0    6.0\n","6        S07        R03    rwrist  1.564739e+09 -0.210257 -1.095191  0.675926  40.849041  25.070871 -35.659191          0              0    6.0\n","7        S07        R03    rwrist  1.564739e+09 -0.222465 -1.095854  0.701550  45.951775  22.461361 -37.891891          0              0    6.0\n","8        S07        R03    rwrist  1.564739e+09 -0.214872 -1.125343  0.729511  57.344635  10.825806 -42.137077          0              0    6.0\n","9        S07        R03    rwrist  1.564739e+09 -0.218806 -1.212497  0.748468  68.151352   9.044179 -45.126099          0              0    6.0\n","\n","Numeric column stats (valid samples):\n","                ax           ay           az           gx           gy  \\\n","count  560070.0000  560070.0000  560070.0000  560070.0000  560070.0000   \n","mean       -0.6569      -0.1522       0.3342      -1.0943       0.6134   \n","std         0.4212       0.5260       0.4697      77.1153      72.2905   \n","min       -11.1498      -9.2003     -27.0872   -4185.1846   -1426.9553   \n","25%        -0.9339      -0.5590       0.1043     -19.9812     -22.0506   \n","50%        -0.7474      -0.0536       0.3487       0.1252      -0.3429   \n","75%        -0.4270       0.2280       0.6074      20.5185      21.7400   \n","max        32.7536      11.5694      14.6606    1110.7982    1697.6891   \n","\n","                gz  \n","count  560070.0000  \n","mean        0.5667  \n","std        72.9628  \n","min     -3227.1677  \n","25%       -19.0527  \n","50%         0.5039  \n","75%        22.0198  \n","max       736.3111  \n","\n","✓ Saved stats: data/lara/mbientlab/proc/resample_stats.csv\n","\n","Gap statistics (train fold):\n","  Mean gap coverage: 0.00%\n","  Max gap coverage: 0.00%\n","  Mean gap time fraction: 0.00%\n","\n","Gap statistics (overall):\n","  Mean gap coverage: 0.00%\n","  Max gap coverage: 0.00%\n","\n","============================================================\n","Step 5 complete - Flawless version\n","============================================================\n","\n","Final fixes:\n","  1. ✓ Prune switch (enabled only when training fold is set)\n","  2. ✓ Label time harmonized (reuse cleaned time)\n","  3. ✓ Complete error information (\"error\" field)\n","  4. ✓ Comment fix (constant threshold 0.15)\n","  5. ✓ Type optimization (float32/float64)\n","============================================================\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","\n","# ========== Manually set FOLD_ID (if the environment variable is not set) ==========\n","if \"FOLD_ID\" not in os.environ:\n","    print(\"⚠️ Environment variable FOLD_ID is not set; please specify it manually:\")\n","    print(\"Hint: if your LOSO has N folds, FOLD_ID should be 0 to N-1\")\n","    fold_input = input(\"Please enter FOLD_ID (press Enter to default to 0): \").strip()\n","    os.environ[\"FOLD_ID\"] = fold_input if fold_input else \"0\"\n","    print(f\"✓ FOLD_ID has been set to {os.environ['FOLD_ID']}\")\n","\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","\"\"\"\n","Step 6: Sensor Preprocessing (top-conf/journal grade - final fixed version)\n","Accelerometer high-pass to remove gravity; gyroscope denoising; adaptive ±Nσ clipping (target 1%)\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import json\n","import os\n","from scipy import signal\n","\n","# ========== Config ==========\n","# Accelerometer high-pass (remove gravity)\n","ACC_HPF_CUTOFF_HZ = 0.3      # Cutoff frequency\n","ACC_HPF_ORDER = 2            # Filter order\n","\n","# Gyroscope low-pass (denoise)\n","GYR_LPF_CUTOFF_HZ = 20.0     # Cutoff frequency\n","GYR_LPF_ORDER = 2            # Filter order\n","\n","# Fix: adaptive clipping threshold (auto-tuned to target clipping rate)\n","TARGET_CLIP_RATE = 0.01      # Target clipping rate 1% (sum of both tails on the train fold)\n","\n","# Sampling rate (from Step 5)\n","SAMPLING_RATE_HZ = 50.0\n","\n","# Unit conversions\n","DEG2RAD = np.pi / 180.0\n","G_TO_MS2 = 9.80665\n","\n","print(\"=\"*60)\n","print(\"Step 6: Sensor Preprocessing\")\n","print(\"=\"*60)\n","\n","# Load data\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","\n","print(f\"\\nLoading resampled data: {proc_dir / 'resampled.parquet'}\")\n","df = pd.read_parquet(proc_dir / \"resampled.parquet\")\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Number of subjects: {df['subject_id'].nunique()}\")\n","print(f\"Number of sessions: {df.groupby(['subject_id', 'session_id'], observed=True).ngroups}\")\n","\n","# ========== 0. Unit normalization ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"0. Unit normalization\")\n","print(\"=\"*60)\n","\n","acc_channels = ['ax', 'ay', 'az']\n","print(f\"\\nAccelerometer unit conversion: g → m/s²\")\n","for ch in acc_channels:\n","    if ch in df.columns:\n","        mask = df[ch].notna()\n","        df.loc[mask, ch] = df.loc[mask, ch] * G_TO_MS2\n","\n","print(f\"✓ Conversion factor: {G_TO_MS2:.5f}\")\n","\n","gyr_channels = ['gx', 'gy', 'gz']\n","print(f\"\\nGyroscope unit conversion: deg/s → rad/s\")\n","for ch in gyr_channels:\n","    if ch in df.columns:\n","        mask = df[ch].notna()\n","        df.loc[mask, ch] = df.loc[mask, ch] * DEG2RAD\n","\n","print(f\"✓ Conversion factor: π/180 = {DEG2RAD:.6f}\")\n","\n","# ========== Helper functions ==========\n","def design_highpass_filter(cutoff_hz, fs_hz, order=2):\n","    \"\"\"Design a high-pass Butterworth filter\"\"\"\n","    nyq = 0.5 * fs_hz\n","    normal_cutoff = cutoff_hz / nyq\n","    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n","    return b, a\n","\n","def design_lowpass_filter(cutoff_hz, fs_hz, order=2):\n","    \"\"\"Design a low-pass Butterworth filter\"\"\"\n","    nyq = 0.5 * fs_hz\n","    normal_cutoff = cutoff_hz / nyq\n","    b, a = signal.butter(order, normal_cutoff, btype='low', analog=False)\n","    return b, a\n","\n","def filtfilt_nan_safe(x, b, a):\n","    \"\"\"Zero-phase filtering tolerant to NaN (filter each contiguous non-NaN run)\"\"\"\n","    y = x.copy()\n","    good = np.isfinite(x)\n","\n","    if not good.any():\n","        return x\n","\n","    idx = np.where(good)[0]\n","    cuts = np.where(np.diff(idx) > 1)[0] + 1\n","    runs = np.split(idx, cuts)\n","\n","    padlen = 3 * (max(len(a), len(b)) - 1)\n","\n","    for run in runs:\n","        seg = x[run]\n","\n","        if len(seg) > padlen:\n","            y[run] = signal.filtfilt(b, a, seg, method=\"pad\")\n","        else:\n","            tmp = signal.lfilter(b, a, seg)\n","            y[run] = signal.lfilter(b, a, tmp[::-1])[::-1]\n","\n","    return y\n","\n","def apply_filter_by_session(df, channels, b, a):\n","    \"\"\"Apply zero-phase filtering grouped by session (polish: add placement grouping + sorting)\"\"\"\n","    filtered_data = []\n","\n","    # Polish 1: include placement grouping; sort by time_sec\n","    for (subj, sess, plc), group in df.groupby(['subject_id', 'session_id', 'placement'], observed=True):\n","        group = group.sort_values('time_sec').copy()\n","\n","        for ch in channels:\n","            if ch not in group.columns:\n","                continue\n","\n","            data = group[ch].values\n","            filtered = filtfilt_nan_safe(data, b, a)\n","            group[ch] = filtered\n","\n","        filtered_data.append(group)\n","\n","    return pd.concat(filtered_data, ignore_index=True)\n","\n","def compute_clip_thresholds_target(df, channels, target_rate=0.01, use_robust=True):\n","    \"\"\"Fix: adapt thresholds to a target clipping rate (Scheme A)\n","\n","    Args:\n","        target_rate: target total clipping rate for both tails (e.g., 0.01 = 1%)\n","        use_robust: if True, use Median±MAD; otherwise Mean±Std\n","    \"\"\"\n","    eps = 1e-6\n","    thresholds = {}\n","\n","    for ch in channels:\n","        if ch not in df.columns:\n","            continue\n","\n","        x = df[ch].dropna().values\n","        if x.size == 0:\n","            continue\n","\n","        if use_robust:\n","            # Robust estimate: Median ± k·(1.4826·MAD)\n","            median = np.median(x)\n","            mad = np.median(np.abs(x - median))\n","            robust_std = max(1.4826 * mad, eps)\n","\n","            # deviations already absolute (two-sided combined); use 1 - target_rate\n","            deviations = np.abs(x - median) / robust_std\n","            k = np.quantile(deviations, 1 - target_rate)\n","\n","            lower = median - k * robust_std\n","            upper = median + k * robust_std\n","\n","            thresholds[ch] = {\n","                'center': float(median),\n","                'scale': float(robust_std),\n","                'k': float(k),\n","                'lower': float(lower),\n","                'upper': float(upper),\n","                'method': f'Median±k·MAD (k={k:.3f}, both tails total {target_rate*100:.1f}%)',\n","            }\n","        else:\n","            # Conventional estimate: Mean ± k·Std\n","            mean = np.mean(x)\n","            std = max(np.std(x), eps)\n","\n","            deviations = np.abs(x - mean) / std\n","            k = np.quantile(deviations, 1 - target_rate)\n","\n","            lower = mean - k * std\n","            upper = mean + k * std\n","\n","            thresholds[ch] = {\n","                'center': float(mean),\n","                'scale': float(std),\n","                'k': float(k),\n","                'lower': float(lower),\n","                'upper': float(upper),\n","                'method': f'Mean±k·Std (k={k:.3f}, both tails total {target_rate*100:.1f}%)',\n","            }\n","\n","    return thresholds\n","\n","def apply_clip(df, channels, thresholds):\n","    \"\"\"Apply clipping and compute actual clipping rate\"\"\"\n","    df_clipped = df.copy()\n","    clip_stats = {}\n","\n","    for ch in channels:\n","        if ch not in df_clipped.columns or ch not in thresholds:\n","            continue\n","\n","        lower = thresholds[ch]['lower']\n","        upper = thresholds[ch]['upper']\n","\n","        mask = df_clipped[ch].notna()\n","        total = mask.sum()\n","\n","        if total > 0:\n","            outliers = ((df_clipped.loc[mask, ch] < lower) | (df_clipped.loc[mask, ch] > upper)).sum()\n","            clip_rate = outliers / total\n","            clip_stats[ch] = {\n","                'outliers': int(outliers),\n","                'total': int(total),\n","                'rate': float(clip_rate),\n","            }\n","\n","        df_clipped.loc[mask, ch] = df_clipped.loc[mask, ch].clip(lower, upper)\n","\n","    return df_clipped, clip_stats\n","\n","# ========== 1. Design filters ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Design filters\")\n","print(\"=\"*60)\n","\n","print(f\"\\nAccelerometer high-pass filter:\")\n","print(f\"  Cutoff frequency: {ACC_HPF_CUTOFF_HZ} Hz\")\n","print(f\"  Order: {ACC_HPF_ORDER}\")\n","acc_b, acc_a = design_highpass_filter(ACC_HPF_CUTOFF_HZ, SAMPLING_RATE_HZ, ACC_HPF_ORDER)\n","\n","print(f\"\\nGyroscope low-pass filter:\")\n","print(f\"  Cutoff frequency: {GYR_LPF_CUTOFF_HZ} Hz\")\n","print(f\"  Order: {GYR_LPF_ORDER}\")\n","gyr_b, gyr_a = design_lowpass_filter(GYR_LPF_CUTOFF_HZ, SAMPLING_RATE_HZ, GYR_LPF_ORDER)\n","\n","# ========== 2. Apply filters (by session + placement) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Apply filters (by session + placement, zero-phase)\")\n","print(\"=\"*60)\n","\n","print(\"\\nApplying accelerometer high-pass (remove gravity)...\")\n","acc_channels = ['ax', 'ay', 'az']\n","df_filtered = apply_filter_by_session(df, acc_channels, acc_b, acc_a)\n","print(\"✓ Done\")\n","\n","print(\"\\nApplying gyroscope low-pass (denoise)...\")\n","df_filtered = apply_filter_by_session(df_filtered, gyr_channels, gyr_b, gyr_a)\n","print(\"✓ Done\")\n","\n","# ========== 3. Compute clipping thresholds (adaptive to target rate) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Compute adaptive clipping thresholds (fix: target clipping rate)\")\n","print(\"=\"*60)\n","\n","splits_path = configs_dir / \"splits.json\"\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","\n","if splits_path.exists() and FOLD_ID >= 0:\n","    with open(splits_path, \"r\") as f:\n","        splits = json.load(f)\n","    train_subjects = set(splits[str(FOLD_ID)][\"train_subjects\"])\n","    df_for_stats = df_filtered[df_filtered[\"subject_id\"].isin(train_subjects)]\n","    print(f\"Estimate clipping thresholds on training fold: FOLD_ID={FOLD_ID}\")\n","    print(f\"  Number of train subjects: {len(train_subjects)}\")\n","    print(f\"  Target clip rate: {TARGET_CLIP_RATE*100:.1f}%\")\n","else:\n","    df_for_stats = df_filtered\n","    print(\"Estimate clipping thresholds on all data\")\n","    print(f\"  Target clip rate: {TARGET_CLIP_RATE*100:.1f}%\")\n","\n","all_channels = acc_channels + gyr_channels\n","clip_thresholds = compute_clip_thresholds_target(\n","    df_for_stats, all_channels, TARGET_CLIP_RATE, use_robust=True\n",")\n","\n","print(f\"\\nClipping thresholds (adaptive robust estimation):\")\n","for ch, thresh in clip_thresholds.items():\n","    print(f\"  {ch}:\")\n","    print(f\"    center: {thresh['center']:.4f}\")\n","    print(f\"    scale: {thresh['scale']:.4f}\")\n","    print(f\"    k: {thresh['k']:.3f}\")\n","    print(f\"    range: [{thresh['lower']:.4f}, {thresh['upper']:.4f}]\")\n","\n","# ========== 4. Apply clipping ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Apply adaptive clipping\")\n","print(\"=\"*60)\n","\n","df_clipped, clip_stats = apply_clip(df_filtered, all_channels, clip_thresholds)\n","\n","print(\"\\nActual clipping statistics:\")\n","for ch, stats in clip_stats.items():\n","    print(f\"  {ch}: {stats['outliers']:,} / {stats['total']:,} ({stats['rate']*100:.2f}%)\")\n","\n","# ========== 5. Cast to float32 to save memory ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Data type optimization\")\n","print(\"=\"*60)\n","\n","numeric_cols = ['ax', 'ay', 'az', 'gx', 'gy', 'gz']\n","for col in numeric_cols:\n","    if col in df_clipped.columns:\n","        df_clipped[col] = df_clipped[col].astype('float32')\n","\n","print(f\"✓ Sensor columns cast to float32\")\n","print(f\"✓ time_sec kept as float64\")\n","\n","# ========== 6. Save results ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Save results\")\n","print(\"=\"*60)\n","\n","output_file = proc_dir / \"filtered.parquet\"\n","\n","if output_file.exists():\n","    import shutil\n","    if output_file.is_dir():\n","        shutil.rmtree(output_file)\n","    else:\n","        output_file.unlink()\n","    print(f\"Removed old data: {output_file}\")\n","\n","df_clipped.to_parquet(\n","    output_file,\n","    index=False,\n","    partition_cols=['subject_id', 'placement'],\n","    engine='pyarrow'\n",")\n","print(f\"✓ Saved: {output_file}\")\n","print(f\"  Data shape: {df_clipped.shape}\")\n","\n","print(\"\\nData preview:\")\n","print(df_clipped.head(10).to_string())\n","\n","print(\"\\nPost-filter numeric column stats:\")\n","valid_mask = df_clipped[['ax', 'ay', 'az', 'gx', 'gy', 'gz']].notna().all(axis=1)\n","print(df_clipped.loc[valid_mask, ['ax', 'ay', 'az', 'gx', 'gy', 'gz']].describe().round(4))\n","\n","# ========== 7. Save filter configuration ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"7. Save filter configuration\")\n","print(\"=\"*60)\n","\n","filter_config = {\n","    'sampling_rate_hz': SAMPLING_RATE_HZ,\n","\n","    'units': {\n","        'accelerometer': 'm/s² (converted from g)',\n","        'gyroscope': 'rad/s (converted from deg/s)',\n","        'conversion': {\n","            'accelerometer_g_to_ms2': G_TO_MS2,\n","            'gyroscope_deg_to_rad': DEG2RAD,\n","        }\n","    },\n","\n","    'dtypes': {\n","        'sensor_channels': 'float32',\n","        'time_sec': 'float64',\n","    },\n","\n","    'accelerometer': {\n","        'filter_type': 'highpass',\n","        'purpose': 'detrend (remove gravity)',\n","        'method': 'Butterworth',\n","        'cutoff_hz': ACC_HPF_CUTOFF_HZ,\n","        'order': ACC_HPF_ORDER,\n","        'coefficients': {\n","            'b': acc_b.tolist(),\n","            'a': acc_a.tolist(),\n","        },\n","        'zero_phase': True,\n","    },\n","\n","    'gyroscope': {\n","        'filter_type': 'lowpass',\n","        'purpose': 'denoise',\n","        'method': 'Butterworth',\n","        'cutoff_hz': GYR_LPF_CUTOFF_HZ,\n","        'order': GYR_LPF_ORDER,\n","        'coefficients': {\n","            'b': gyr_b.tolist(),\n","            'a': gyr_a.tolist(),\n","        },\n","        'zero_phase': True,\n","    },\n","\n","    'clipping': {\n","        'method': 'Adaptive robust estimation (Median±k·MAD, Scheme A)',\n","        'target_clip_rate': TARGET_CLIP_RATE,\n","        'estimated_on': 'train_fold' if FOLD_ID >= 0 else 'all_data',\n","        'fold_id': FOLD_ID if FOLD_ID >= 0 else None,\n","        'thresholds': clip_thresholds,\n","        'actual_clip_stats': clip_stats,  # Polish 2: record actual clipping rate\n","        'rationale': f'Auto-adjust k so the training-fold clipping rate reaches the target {TARGET_CLIP_RATE*100:.1f}%',\n","    },\n","\n","    'notes': [\n","        'All filters use filtfilt for zero phase',\n","        'Filtering is grouped by session + placement, sorted by time_sec; avoid crossing session boundaries',\n","        'filtfilt_nan_safe filters each contiguous non-NaN run separately',\n","        'Accelerometer converted from g to m/s² (×9.80665)',\n","        'Gyroscope converted from deg/s to rad/s (×π/180)',\n","        f'Adaptive clipping thresholds: determine k on the training fold so clipping ≈ {TARGET_CLIP_RATE*100:.1f}%, then apply consistently to all data',\n","        'NaNs remain unchanged',\n","        'Sensor columns are float32; time_sec is float64',\n","    ]\n","}\n","\n","filter_config_file = configs_dir / \"filter.yaml\"\n","with open(filter_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(filter_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","print(f\"✓ Saved filter configuration: {filter_config_file}\")\n","\n","filter_config_json = configs_dir / \"filter.json\"\n","with open(filter_config_json, 'w', encoding='utf-8') as f:\n","    json.dump(filter_config, f, indent=2)\n","\n","print(f\"✓ Saved filter configuration: {filter_config_json}\")\n","\n","# ========== 8. Summary ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 6 complete - Sensor preprocessing (final fixed version)\")\n","print(\"=\"*60)\n","print(f\"\\nConfig:\")\n","print(f\"  Units: Acc g→m/s², Gyro deg/s→rad/s\")\n","print(f\"  Accelerometer: high-pass {ACC_HPF_CUTOFF_HZ} Hz (remove gravity)\")\n","print(f\"  Gyroscope: low-pass {GYR_LPF_CUTOFF_HZ} Hz (denoise)\")\n","print(f\"  Clipping: adaptive ±k·MAD (target {TARGET_CLIP_RATE*100:.1f}%)\")\n","if FOLD_ID >= 0:\n","    print(f\"  Clipping thresholds: based on training fold (FOLD_ID={FOLD_ID})\")\n","print(f\"\\nResults:\")\n","print(f\"  Output file: {output_file}\")\n","print(f\"  Config file: {filter_config_file}\")\n","print(f\"  Data shape: {df_clipped.shape}\")\n","print(\"\\nFinal fixes:\")\n","print(f\"  ✓ Adaptive clipping thresholds (Scheme A)\")\n","print(f\"  ✓ Target clipping rate {TARGET_CLIP_RATE*100:.1f}%, auto-solve k\")\n","print(f\"  ✓ Polish 1: group by placement + sort by time_sec\")\n","print(f\"  ✓ Polish 2: write actual clipping rate into config\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HaLjjP0VXTwE","executionInfo":{"status":"ok","timestamp":1763130377384,"user_tz":0,"elapsed":2216,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"86d86ffc-fd2e-485b-e9d5-74db120dd517"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 6: Sensor Preprocessing\n","============================================================\n","\n","Loading resampled data: data/lara/mbientlab/proc/resampled.parquet\n","Data shape: (560070, 13)\n","Number of subjects: 8\n","Number of sessions: 96\n","\n","============================================================\n","0. Unit normalization\n","============================================================\n","\n","Accelerometer unit conversion: g → m/s²\n","✓ Conversion factor: 9.80665\n","\n","Gyroscope unit conversion: deg/s → rad/s\n","✓ Conversion factor: π/180 = 0.017453\n","\n","============================================================\n","1. Design filters\n","============================================================\n","\n","Accelerometer high-pass filter:\n","  Cutoff frequency: 0.3 Hz\n","  Order: 2\n","\n","Gyroscope low-pass filter:\n","  Cutoff frequency: 20.0 Hz\n","  Order: 2\n","\n","============================================================\n","2. Apply filters (by session + placement, zero-phase)\n","============================================================\n","\n","Applying accelerometer high-pass (remove gravity)...\n","✓ Done\n","\n","Applying gyroscope low-pass (denoise)...\n","✓ Done\n","\n","============================================================\n","3. Compute adaptive clipping thresholds (fix: target clipping rate)\n","============================================================\n","Estimate clipping thresholds on training fold: FOLD_ID=5\n","  Number of train subjects: 7\n","  Target clip rate: 1.0%\n","\n","Clipping thresholds (adaptive robust estimation):\n","  ax:\n","    center: 0.0163\n","    scale: 1.3417\n","    k: 6.731\n","    range: [-9.0143, 9.0469]\n","  ay:\n","    center: 0.0125\n","    scale: 1.3670\n","    k: 5.845\n","    range: [-7.9782, 8.0032]\n","  az:\n","    center: 0.0015\n","    scale: 1.3317\n","    k: 5.973\n","    range: [-7.9532, 7.9562]\n","  gx:\n","    center: 0.0036\n","    scale: 0.5196\n","    k: 6.514\n","    range: [-3.3814, 3.3886]\n","  gy:\n","    center: -0.0067\n","    scale: 0.5551\n","    k: 8.546\n","    range: [-4.7503, 4.7368]\n","  gz:\n","    center: 0.0088\n","    scale: 0.5231\n","    k: 7.488\n","    range: [-3.9086, 3.9261]\n","\n","============================================================\n","4. Apply adaptive clipping\n","============================================================\n","\n","Actual clipping statistics:\n","  ax: 5,721 / 560,070 (1.02%)\n","  ay: 5,629 / 560,070 (1.01%)\n","  az: 5,625 / 560,070 (1.00%)\n","  gx: 5,609 / 560,070 (1.00%)\n","  gy: 5,647 / 560,070 (1.01%)\n","  gz: 5,853 / 560,070 (1.05%)\n","\n","============================================================\n","5. Data type optimization\n","============================================================\n","✓ Sensor columns cast to float32\n","✓ time_sec kept as float64\n","\n","============================================================\n","6. Save results\n","============================================================\n","Removed old data: data/lara/mbientlab/proc/filtered.parquet\n","✓ Saved: data/lara/mbientlab/proc/filtered.parquet\n","  Data shape: (560070, 13)\n","\n","Data preview:\n","  session_id      time_sec        ax        ay        az        gx        gy        gz  is_in_gap  is_forced_nan  label subject_id placement\n","0        R03  1.564739e+09  0.891079 -2.660401  0.238257  0.025291  0.435949 -0.158735          0              0    6.0        S07    rwrist\n","1        R03  1.564739e+09  0.715069 -2.840439  0.239453  0.059089  0.468073 -0.219117          0              0    6.0        S07    rwrist\n","2        R03  1.564739e+09  1.051161 -3.582292  1.341616  0.042692  0.260707 -0.339956          0              0    6.0        S07    rwrist\n","3        R03  1.564739e+09  1.907651 -4.231893  2.127376  0.107476  0.100950 -0.407764          0              0    6.0        S07    rwrist\n","4        R03  1.564739e+09  2.377473 -4.539616  2.763897  0.263063  0.184827 -0.442223          0              0    6.0        S07    rwrist\n","5        R03  1.564739e+09  2.140075 -4.731589  2.865023  0.472918  0.345074 -0.525765          0              0    6.0        S07    rwrist\n","6        R03  1.564739e+09  1.521002 -4.847136  3.135042  0.686333  0.442941 -0.613507          0              0    6.0        S07    rwrist\n","7        R03  1.564739e+09  1.386402 -4.954428  3.299844  0.829848  0.386559 -0.669225          0              0    6.0        S07    rwrist\n","8        R03  1.564739e+09  1.444855 -5.348888  3.488989  0.973541  0.191755 -0.729564          0              0    6.0        S07    rwrist\n","9        R03  1.564739e+09  1.389077 -6.313359  3.591348  1.214971  0.159118 -0.790919          0              0    6.0        S07    rwrist\n","\n","Post-filter numeric column stats:\n","                ax           ay           az           gx           gy  \\\n","count  560070.0000  560070.0000  560070.0000  560070.0000  560070.0000   \n","mean       -0.0088       0.0070       0.0014      -0.0023       0.0101   \n","std         2.2719       2.1457       2.0867       0.8858       1.1594   \n","min        -9.0143      -7.9782      -7.9532      -3.3814      -4.7503   \n","25%        -0.9098      -0.9143      -0.9075      -0.3482      -0.3829   \n","50%         0.0152       0.0141       0.0019       0.0021      -0.0064   \n","75%         0.9153       0.9493       0.9042       0.3572       0.3771   \n","max         9.0469       8.0032       7.9562       3.3886       4.7368   \n","\n","                gz  \n","count  560070.0000  \n","mean        0.0234  \n","std         0.9858  \n","min        -3.9086  \n","25%        -0.3321  \n","50%         0.0088  \n","75%         0.3837  \n","max         3.9261  \n","\n","============================================================\n","7. Save filter configuration\n","============================================================\n","✓ Saved filter configuration: configs/filter.yaml\n","✓ Saved filter configuration: configs/filter.json\n","\n","============================================================\n","Step 6 complete - Sensor preprocessing (final fixed version)\n","============================================================\n","\n","Config:\n","  Units: Acc g→m/s², Gyro deg/s→rad/s\n","  Accelerometer: high-pass 0.3 Hz (remove gravity)\n","  Gyroscope: low-pass 20.0 Hz (denoise)\n","  Clipping: adaptive ±k·MAD (target 1.0%)\n","  Clipping thresholds: based on training fold (FOLD_ID=5)\n","\n","Results:\n","  Output file: data/lara/mbientlab/proc/filtered.parquet\n","  Config file: configs/filter.yaml\n","  Data shape: (560070, 13)\n","\n","Final fixes:\n","  ✓ Adaptive clipping thresholds (Scheme A)\n","  ✓ Target clipping rate 1.0%, auto-solve k\n","  ✓ Polish 1: group by placement + sort by time_sec\n","  ✓ Polish 2: write actual clipping rate into config\n","============================================================\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","\n","# ========== Manually set FOLD_ID (if the environment variable is not set) ==========\n","if \"FOLD_ID\" not in os.environ:\n","    print(\"⚠️ Environment variable FOLD_ID is not set; please specify it manually:\")\n","    print(\"Hint: If your LOSO has N folds, FOLD_ID should be 0 to N-1\")\n","    fold_input = input(\"Please enter FOLD_ID (press Enter to default to 0): \").strip()\n","    os.environ[\"FOLD_ID\"] = fold_input if fold_input else \"0\"\n","    print(f\"✓ FOLD_ID has been set to {os.environ['FOLD_ID']}\")\n","\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","\"\"\"\n","Step 7: Coordinate/Magnitude Normalization (top-conf/journal grade)\n","Compute magnitude channels; z-score standardization (train-only statistics)\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import json\n","import os\n","import pickle\n","\n","# ========== Config ==========\n","EPSILON = 1e-8  # Prevent division by zero\n","\n","print(\"=\"*60)\n","print(\"Step 7: Coordinate/Magnitude Normalization\")\n","print(\"=\"*60)\n","\n","# Load data\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","\n","print(f\"\\nLoading filtered data: {proc_dir / 'filtered.parquet'}\")\n","df = pd.read_parquet(proc_dir / \"filtered.parquet\")\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Number of subjects: {df['subject_id'].nunique()}\")\n","print(f\"Number of sessions: {df.groupby(['subject_id', 'session_id'], observed=True).ngroups}\")\n","\n","# ========== 1. Compute derived channels (magnitude) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Compute derived channels (magnitude)\")\n","print(\"=\"*60)\n","\n","# Accelerometer magnitude\n","print(\"\\nComputing acc_mag = sqrt(ax² + ay² + az²)...\")\n","df['acc_mag'] = np.sqrt(\n","    df['ax'].values**2 +\n","    df['ay'].values**2 +\n","    df['az'].values**2\n",").astype('float32')\n","\n","# Gyroscope magnitude\n","print(\"Computing gyr_mag = sqrt(gx² + gy² + gz²)...\")\n","df['gyr_mag'] = np.sqrt(\n","    df['gx'].values**2 +\n","    df['gy'].values**2 +\n","    df['gz'].values**2\n",").astype('float32')\n","\n","print(f\"✓ Added derived channels: acc_mag, gyr_mag\")\n","\n","# Show derived-channel stats\n","print(\"\\nDerived channel statistics (post-filter):\")\n","for col in ['acc_mag', 'gyr_mag']:\n","    valid_data = df[col].dropna()\n","    if len(valid_data) > 0:\n","        print(f\"  {col}:\")\n","        print(f\"    Mean: {valid_data.mean():.4f}\")\n","        print(f\"    Std: {valid_data.std():.4f}\")\n","        print(f\"    Range: [{valid_data.min():.4f}, {valid_data.max():.4f}]\")\n","\n","# ========== 2. Determine training set ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Determine training set\")\n","print(\"=\"*60)\n","\n","# Check whether to standardize per fold\n","splits_path = configs_dir / \"splits.json\"\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","\n","if splits_path.exists() and FOLD_ID >= 0:\n","    with open(splits_path, \"r\") as f:\n","        splits = json.load(f)\n","    train_subjects = set(splits[str(FOLD_ID)][\"train_subjects\"])\n","    test_subjects = set(splits[str(FOLD_ID)][\"test_subjects\"])\n","\n","    train_mask = df[\"subject_id\"].isin(train_subjects)\n","    df_train = df[train_mask]\n","\n","    print(f\"Compute statistics on training fold: FOLD_ID={FOLD_ID}\")\n","    print(f\"  Number of train subjects: {len(train_subjects)}\")\n","    print(f\"  Number of test subjects: {len(test_subjects)}\")\n","    print(f\"  Train samples: {len(df_train):,}\")\n","    print(f\"  Total samples: {len(df):,}\")\n","else:\n","    df_train = df\n","    train_subjects = set(df['subject_id'].unique())\n","    test_subjects = set()\n","    FOLD_ID = -1\n","    print(\"Compute statistics on all data (no fold split)\")\n","    print(f\"  Samples: {len(df):,}\")\n","\n","# ========== 3. Compute z-score parameters (train-only) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Compute z-score parameters (train-only)\")\n","print(\"=\"*60)\n","\n","# Channels to standardize\n","channels_to_normalize = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","# Compute mean and std (train-only valid data)\n","scaler_params = {}\n","\n","print(\"\\nz-score parameters (train set):\")\n","for ch in channels_to_normalize:\n","    if ch not in df_train.columns:\n","        continue\n","\n","    # Use non-NaN values only\n","    valid_data = df_train[ch].dropna().values\n","\n","    if len(valid_data) > 0:\n","        mean = float(np.mean(valid_data))\n","        std = float(np.std(valid_data))\n","\n","        # Guard against zero std\n","        if std < EPSILON:\n","            std = 1.0\n","\n","        scaler_params[ch] = {\n","            'mean': mean,\n","            'std': std,\n","        }\n","\n","        print(f\"  {ch}:\")\n","        print(f\"    Mean: {mean:.6f}\")\n","        print(f\"    Std: {std:.6f}\")\n","\n","# ========== 4. Apply z-score standardization ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Apply z-score standardization\")\n","print(\"=\"*60)\n","\n","df_normalized = df.copy()\n","\n","for ch in channels_to_normalize:\n","    if ch not in scaler_params:\n","        continue\n","\n","    mean = scaler_params[ch]['mean']\n","    std = scaler_params[ch]['std']\n","\n","    # Standardize non-NaN values only; cast to float32 to avoid warnings\n","    mask = df_normalized[ch].notna()\n","    normalized_values = ((df_normalized.loc[mask, ch] - mean) / (std + EPSILON)).astype('float32')\n","    df_normalized.loc[mask, ch] = normalized_values\n","\n","print(f\"✓ Standardized {len(scaler_params)} channels\")\n","\n","# Show post-standardization stats\n","print(\"\\nPost-standardization stats (train set):\")\n","for ch in channels_to_normalize:\n","    if ch not in scaler_params:\n","        continue\n","\n","    if FOLD_ID >= 0:\n","        valid_data = df_normalized.loc[train_mask, ch].dropna()\n","    else:\n","        valid_data = df_normalized[ch].dropna()\n","\n","    if len(valid_data) > 0:\n","        print(f\"  {ch}:\")\n","        print(f\"    Mean: {valid_data.mean():.6f} (should be near 0)\")\n","        print(f\"    Std: {valid_data.std():.6f} (should be near 1)\")\n","\n","# ========== 5. Save results ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Save results\")\n","print(\"=\"*60)\n","\n","# Save normalized data\n","output_file = proc_dir / \"normalized.parquet\"\n","\n","# Delete existing directory (avoid duplicate appends)\n","if output_file.exists():\n","    import shutil\n","    shutil.rmtree(output_file)\n","    print(f\"Removed old data: {output_file}\")\n","\n","df_normalized.to_parquet(\n","    output_file,\n","    index=False,\n","    partition_cols=['subject_id', 'placement'],\n","    engine='pyarrow'\n",")\n","print(f\"✓ Saved: {output_file}\")\n","print(f\"  Data shape: {df_normalized.shape}\")\n","\n","# Show data preview\n","print(\"\\nData preview:\")\n","display_cols = ['subject_id', 'session_id', 'ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag', 'label']\n","available_cols = [c for c in display_cols if c in df_normalized.columns]\n","print(df_normalized[available_cols].head(10).to_string())\n","\n","# Post-standardization numeric stats (overall)\n","print(\"\\nPost-standardization numeric column stats (overall):\")\n","numeric_cols = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","valid_mask = df_normalized[numeric_cols].notna().all(axis=1)\n","print(df_normalized.loc[valid_mask, numeric_cols].describe().round(4))\n","\n","# ========== 6. Save scaler parameters ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Save scaler parameters\")\n","print(\"=\"*60)\n","\n","scaler_info = {\n","    'fold_id': FOLD_ID,\n","    'epsilon': EPSILON,\n","    'train_subjects': sorted(list(train_subjects)),\n","    'test_subjects': sorted(list(test_subjects)) if test_subjects else None,\n","    'channels': channels_to_normalize,\n","    'params': scaler_params,\n","    'notes': [\n","        'z-score standardization: (x - mean) / (std + ε)',\n","        'Mean and std computed from the training set only',\n","        'If std < ε, set std = 1.0 to avoid divide-by-zero',\n","        'NaN values are excluded from stats and remain NaN after normalization',\n","    ]\n","}\n","\n","# Save as pickle (per-fold supported)\n","if FOLD_ID >= 0:\n","    scaler_file = proc_dir / f\"standardization_fold{FOLD_ID}.pkl\"\n","else:\n","    scaler_file = proc_dir / \"standardization.pkl\"\n","\n","with open(scaler_file, 'wb') as f:\n","    pickle.dump(scaler_info, f)\n","\n","print(f\"✓ Saved scaler: {scaler_file}\")\n","\n","# Also save as JSON (human-readable)\n","if FOLD_ID >= 0:\n","    scaler_json = proc_dir / f\"standardization_fold{FOLD_ID}.json\"\n","else:\n","    scaler_json = proc_dir / \"standardization.json\"\n","\n","with open(scaler_json, 'w') as f:\n","    json.dump(scaler_info, f, indent=2)\n","\n","print(f\"✓ Saved scaler: {scaler_json}\")\n","\n","# ========== 7. Validate standardization ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"7. Validate standardization\")\n","print(\"=\"*60)\n","\n","# Check whether train set is ~0 mean and ~1 std\n","if FOLD_ID >= 0:\n","    print(\"\\nTrain set validation:\")\n","    for ch in channels_to_normalize[:3]:  # check first 3 channels only\n","        if ch in scaler_params:\n","            valid_data = df_normalized.loc[train_mask, ch].dropna()\n","            if len(valid_data) > 0:\n","                mean_check = valid_data.mean()\n","                std_check = valid_data.std()\n","                print(f\"  {ch}: mean={mean_check:.6f}, std={std_check:.6f}\")\n","\n","    # Check that test set uses train-set statistics (should not be 0/1)\n","    print(\"\\nTest set validation (should use train-set statistics; not 0/1):\")\n","    test_mask = df[\"subject_id\"].isin(test_subjects)\n","    for ch in channels_to_normalize[:3]:\n","        if ch in scaler_params:\n","            valid_data = df_normalized.loc[test_mask, ch].dropna()\n","            if len(valid_data) > 0:\n","                mean_check = valid_data.mean()\n","                std_check = valid_data.std()\n","                print(f\"  {ch}: mean={mean_check:.6f}, std={std_check:.6f}\")\n","\n","# ========== 8. Summary ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 7 complete - Coordinate/Magnitude Normalization\")\n","print(\"=\"*60)\n","print(f\"\\nConfig:\")\n","print(f\"  Method: z-score standardization\")\n","print(f\"  ε (avoid divide-by-zero): {EPSILON}\")\n","print(f\"  Standardized channels: {len(scaler_params)}\")\n","if FOLD_ID >= 0:\n","    print(f\"  Train fold: FOLD_ID={FOLD_ID}\")\n","    print(f\"  Train subjects: {len(train_subjects)}\")\n","    print(f\"  Test subjects: {len(test_subjects)}\")\n","print(f\"\\nResults:\")\n","print(f\"  Output data: {output_file}\")\n","print(f\"  Scaler (pkl): {scaler_file}\")\n","print(f\"  Scaler (json): {scaler_json}\")\n","print(f\"  Data shape: {df_normalized.shape}\")\n","print(f\"  New columns: acc_mag, gyr_mag\")\n","print(\"\\nRigor guarantees:\")\n","print(\"  1. ✓ Mean/std computed from training set only\")\n","print(\"  2. ✓ Test set standardized using training-set statistics\")\n","print(\"  3. ✓ ε={} prevents divide-by-zero\".format(EPSILON))\n","print(\"  4. ✓ NaNs remain unchanged\")\n","print(\"  5. ✓ Scaler saved independently per fold\")\n","print(\"  6. ✓ Derived channels acc_mag, gyr_mag\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0_J8cREgXVWi","executionInfo":{"status":"ok","timestamp":1763130378092,"user_tz":0,"elapsed":694,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"1063f0da-70c5-4060-ee6a-3016bcf472d3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 7: Coordinate/Magnitude Normalization\n","============================================================\n","\n","Loading filtered data: data/lara/mbientlab/proc/filtered.parquet\n","Data shape: (560070, 13)\n","Number of subjects: 8\n","Number of sessions: 96\n","\n","============================================================\n","1. Compute derived channels (magnitude)\n","============================================================\n","\n","Computing acc_mag = sqrt(ax² + ay² + az²)...\n","Computing gyr_mag = sqrt(gx² + gy² + gz²)...\n","✓ Added derived channels: acc_mag, gyr_mag\n","\n","Derived channel statistics (post-filter):\n","  acc_mag:\n","    Mean: 2.9145\n","    Std: 2.3731\n","    Range: [0.0073, 14.4637]\n","  gyr_mag:\n","    Mean: 1.3117\n","    Std: 1.1757\n","    Range: [0.0015, 7.0330]\n","\n","============================================================\n","2. Determine training set\n","============================================================\n","Compute statistics on training fold: FOLD_ID=5\n","  Number of train subjects: 7\n","  Number of test subjects: 1\n","  Train samples: 529,129\n","  Total samples: 560,070\n","\n","============================================================\n","3. Compute z-score parameters (train-only)\n","============================================================\n","\n","z-score parameters (train set):\n","  ax:\n","    Mean: -0.007802\n","    Std: 2.263945\n","  ay:\n","    Mean: 0.006754\n","    Std: 2.134056\n","  az:\n","    Mean: 0.000973\n","    Std: 2.080673\n","  gx:\n","    Mean: 0.001587\n","    Std: 0.880861\n","  gy:\n","    Mean: 0.010848\n","    Std: 1.153428\n","  gz:\n","    Mean: 0.022977\n","    Std: 0.977118\n","  acc_mag:\n","    Mean: 2.896521\n","    Std: 2.370467\n","  gyr_mag:\n","    Mean: 1.300143\n","    Std: 1.171046\n","\n","============================================================\n","4. Apply z-score standardization\n","============================================================\n","✓ Standardized 8 channels\n","\n","Post-standardization stats (train set):\n","  ax:\n","    Mean: -0.000000 (should be near 0)\n","    Std: 0.999769 (should be near 1)\n","  ay:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999805 (should be near 1)\n","  az:\n","    Mean: -0.000000 (should be near 0)\n","    Std: 0.999830 (should be near 1)\n","  gx:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999789 (should be near 1)\n","  gy:\n","    Mean: -0.000000 (should be near 0)\n","    Std: 0.999701 (should be near 1)\n","  gz:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999771 (should be near 1)\n","  acc_mag:\n","    Mean: -0.000000 (should be near 0)\n","    Std: 0.999921 (should be near 1)\n","  gyr_mag:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999919 (should be near 1)\n","\n","============================================================\n","5. Save results\n","============================================================\n","Removed old data: data/lara/mbientlab/proc/normalized.parquet\n","✓ Saved: data/lara/mbientlab/proc/normalized.parquet\n","  Data shape: (560070, 15)\n","\n","Data preview:\n","  subject_id session_id        ax        ay        az        gx        gy        gz   acc_mag   gyr_mag  label\n","0        S07        R03  0.397042 -1.249806  0.114042  0.026910  0.368554 -0.185967 -0.034069 -0.713470    6.0\n","1        S07        R03  0.319297 -1.334170  0.114617  0.065279  0.396406 -0.247763  0.017851 -0.666032    6.0\n","2        S07        R03  0.467751 -1.681796  0.644332  0.046664  0.216623 -0.371432  0.451622 -0.742591    6.0\n","3        S07        R03  0.846069 -1.986193  1.021979  0.120211  0.078117 -0.440827  0.932191 -0.739970    6.0\n","4        S07        R03  1.053593 -2.130389  1.327900  0.296840  0.150836 -0.476093  1.234279 -0.643360    6.0\n","5        S07        R03  0.948732 -2.220346  1.376502  0.535079  0.289768 -0.561592  1.280099 -0.438308    6.0\n","6        S07        R03  0.675283 -2.274491  1.506277  0.777359  0.374617 -0.651388  1.296422 -0.237869    6.0\n","7        S07        R03  0.615830 -2.324767  1.585483  0.940285  0.325734 -0.708411  1.356506 -0.141883    6.0\n","8        S07        R03  0.641649 -2.509607  1.676389  1.103412  0.156842 -0.770164  1.540243 -0.058539    6.0\n","9        S07        R03  0.617011 -2.961550  1.725584  1.377496  0.128547 -0.832956  1.897713  0.135170    6.0\n","\n","Post-standardization numeric column stats (overall):\n","                ax           ay           az           gx           gy  \\\n","count  560070.0000  560070.0000  560070.0000  560070.0000  560070.0000   \n","mean       -0.0004       0.0001       0.0002      -0.0044      -0.0007   \n","std         1.0035       1.0054       1.0029       1.0055       1.0052   \n","min        -3.9782      -3.7417      -3.8229      -3.8406      -4.1278   \n","25%        -0.3984      -0.4316      -0.4366      -0.3971      -0.3414   \n","50%         0.0102       0.0034       0.0004       0.0006      -0.0149   \n","75%         0.4078       0.4417       0.4341       0.4037       0.3176   \n","max         3.9995       3.7471       3.8234       3.8452       4.0973   \n","\n","                gz      acc_mag      gyr_mag  \n","count  560070.0000  560070.0000  560070.0000  \n","mean        0.0004       0.0076       0.0099  \n","std         1.0089       1.0011       1.0040  \n","min        -4.0236      -1.2189      -1.1090  \n","25%        -0.3634      -0.7097      -0.7217  \n","50%        -0.0145      -0.2881      -0.3162  \n","75%         0.3691       0.4208       0.4218  \n","max         3.9946       4.8797       4.8955  \n","\n","============================================================\n","6. Save scaler parameters\n","============================================================\n","✓ Saved scaler: data/lara/mbientlab/proc/standardization_fold5.pkl\n","✓ Saved scaler: data/lara/mbientlab/proc/standardization_fold5.json\n","\n","============================================================\n","7. Validate standardization\n","============================================================\n","\n","Train set validation:\n","  ax: mean=-0.000000, std=0.999769\n","  ay: mean=0.000000, std=0.999805\n","  az: mean=-0.000000, std=0.999830\n","\n","Test set validation (should use train-set statistics; not 0/1):\n","  ax: mean=-0.007991, std=1.066771\n","  ay: mean=0.001945, std=1.097978\n","  az: mean=0.003483, std=1.055684\n","\n","============================================================\n","Step 7 complete - Coordinate/Magnitude Normalization\n","============================================================\n","\n","Config:\n","  Method: z-score standardization\n","  ε (avoid divide-by-zero): 1e-08\n","  Standardized channels: 8\n","  Train fold: FOLD_ID=5\n","  Train subjects: 7\n","  Test subjects: 1\n","\n","Results:\n","  Output data: data/lara/mbientlab/proc/normalized.parquet\n","  Scaler (pkl): data/lara/mbientlab/proc/standardization_fold5.pkl\n","  Scaler (json): data/lara/mbientlab/proc/standardization_fold5.json\n","  Data shape: (560070, 15)\n","  New columns: acc_mag, gyr_mag\n","\n","Rigor guarantees:\n","  1. ✓ Mean/std computed from training set only\n","  2. ✓ Test set standardized using training-set statistics\n","  3. ✓ ε=1e-08 prevents divide-by-zero\n","  4. ✓ NaNs remain unchanged\n","  5. ✓ Scaler saved independently per fold\n","  6. ✓ Derived channels acc_mag, gyr_mag\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\n","\"\"\"\n","Step 8: Label Alignment & Cleaning (top-conf/journal grade - revised)\n","Clean NULL/transition, unify to a standard label set, and record mappings\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import json\n","from collections import Counter\n","\n","# ========== Config ==========\n","\n","# Label cleaning strategy\n","NULL_STRATEGY = \"remove\"  # \"remove\" or \"merge_to_transition\"\n","TRANSITION_STRATEGY = \"merge_to_nearest\"  # \"remove\" or \"merge_to_nearest\"\n","\n","# Unmapped label threshold (abort if exceeded)\n","UNMAPPED_THRESHOLD = 0.01  # 1%\n","\n","print(\"=\"*60)\n","print(\"Step 8: Label Alignment & Cleaning\")\n","print(\"=\"*60)\n","\n","# Create directories\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","reports_dir = Path(\"reports\")\n","reports_dir.mkdir(parents=True, exist_ok=True)\n","\n","print(f\"\\nLoading normalized data: {proc_dir / 'normalized.parquet'}\")\n","df = pd.read_parquet(proc_dir / \"normalized.parquet\")\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Number of subjects: {df['subject_id'].nunique()}\")\n","\n","# ========== 1. Analyze original label distribution ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Analyze original label distribution\")\n","print(\"=\"*60)\n","\n","# Count all labels\n","label_counts = df['label'].value_counts(dropna=False)\n","total_samples = len(df)\n","null_count = df['label'].isna().sum()\n","\n","print(f\"\\nOriginal label stats:\")\n","print(f\"  Total samples: {total_samples:,}\")\n","print(f\"  NULL samples: {null_count:,} ({null_count/total_samples*100:.2f}%)\")\n","print(f\"  Number of label classes: {df['label'].nunique(dropna=True)}\")\n","\n","print(f\"\\nLabel distribution (top 20):\")\n","for label, count in label_counts.head(20).items():\n","    pct = count / total_samples * 100\n","    print(f\"  {str(label):30s}: {count:8,} ({pct:5.2f}%)\")\n","\n","# ========== 2. Define label mapping rules ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Define label mapping rules\")\n","print(\"=\"*60)\n","\n","# Map LARa dataset labels to a cross-dataset unified label superset\n","# Covers LARa / RealWorld / SHL\n","LABEL_MAPPING = {\n","    # Basic activities (shared by RealWorld + LARa)\n","    1: {\"original\": \"walking\", \"mapped\": \"walking\", \"category\": \"locomotion\"},\n","    2: {\"original\": \"running\", \"mapped\": \"running\", \"category\": \"locomotion\"},\n","    3: {\"original\": \"shuffling\", \"mapped\": \"walking\", \"category\": \"locomotion\"},  # merge into walking\n","    4: {\"original\": \"stairs (ascending)\", \"mapped\": \"upstairs\", \"category\": \"locomotion\"},\n","    5: {\"original\": \"stairs (descending)\", \"mapped\": \"downstairs\", \"category\": \"locomotion\"},\n","    6: {\"original\": \"standing\", \"mapped\": \"standing\", \"category\": \"static\"},\n","    7: {\"original\": \"sitting\", \"mapped\": \"sitting\", \"category\": \"static\"},\n","    8: {\"original\": \"lying\", \"mapped\": \"lying\", \"category\": \"static\"},\n","\n","    # Transport (specific to LARa; not in RealWorld)\n","    13: {\"original\": \"cycling (sit)\", \"mapped\": \"cycling\", \"category\": \"transport\"},\n","    14: {\"original\": \"cycling (stand)\", \"mapped\": \"cycling\", \"category\": \"transport\"},\n","    130: {\"original\": \"cycling\", \"mapped\": \"cycling\", \"category\": \"transport\"},\n","\n","    17: {\"original\": \"car\", \"mapped\": \"car\", \"category\": \"transport\"},\n","    18: {\"original\": \"bus\", \"mapped\": \"bus\", \"category\": \"transport\"},\n","    19: {\"original\": \"train\", \"mapped\": \"train\", \"category\": \"transport\"},\n","    20: {\"original\": \"subway\", \"mapped\": \"subway\", \"category\": \"transport\"},\n","\n","    # Transition label\n","    0: {\"original\": \"transition\", \"mapped\": \"transition\", \"category\": \"transition\"},\n","}\n","\n","# Cross-dataset unified label superset (LARa + RealWorld + SHL)\n","UNIFIED_LABELS = {\n","    \"walking\": 1,\n","    \"running\": 2,\n","    \"sitting\": 3,\n","    \"standing\": 4,\n","    \"upstairs\": 5,\n","    \"downstairs\": 6,\n","    \"lying\": 7,\n","    \"cycling\": 8,\n","    \"car\": 9,\n","    \"bus\": 10,\n","    \"train\": 11,\n","    \"subway\": 12,\n","    \"transition\": 0,  # kept or cleaned\n","}\n","\n","print(f\"\\nDefined mapping rules: {len(LABEL_MAPPING)} original labels\")\n","print(f\"Unified label set: {len(UNIFIED_LABELS)} labels (cross-dataset superset)\")\n","\n","print(f\"\\nMapping examples:\")\n","for orig_id, info in list(LABEL_MAPPING.items())[:10]:\n","    print(f\"  {orig_id} ({info['original']}) -> {info['mapped']}\")\n","\n","# ========== 3. Audit assertion: check unmapped labels ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Audit assertion: check unmapped labels\")\n","print(\"=\"*60)\n","\n","# Find all original label IDs (excluding NULL)\n","orig_ids = set(df['label'].dropna().astype(int).unique())\n","covered_ids = set(LABEL_MAPPING.keys())\n","unmapped_ids = sorted(orig_ids - covered_ids)\n","\n","if unmapped_ids:\n","    # Count samples for unmapped labels\n","    unmapped_counts = []\n","    for uid in unmapped_ids:\n","        count = (df['label'] == uid).sum()\n","        pct = count / total_samples\n","        unmapped_counts.append({\n","            'original_label_id': uid,\n","            'sample_count': count,\n","            'percentage': round(pct * 100, 4),\n","        })\n","\n","    df_unmapped = pd.DataFrame(unmapped_counts)\n","    total_unmapped = df_unmapped['sample_count'].sum()\n","    unmapped_ratio = total_unmapped / total_samples\n","\n","    # Save list of unmapped labels\n","    unmapped_file = reports_dir / \"unmapped_labels.csv\"\n","    df_unmapped.to_csv(unmapped_file, index=False)\n","\n","    print(f\"\\n⚠️ Found unmapped labels: {len(unmapped_ids)}\")\n","    print(f\"  Unmapped sample count: {total_unmapped:,} ({unmapped_ratio*100:.2f}%)\")\n","    print(f\"  Details saved to: {unmapped_file}\")\n","    print(f\"\\nList of unmapped labels:\")\n","    print(df_unmapped.to_string(index=False))\n","\n","    # Abort if threshold exceeded\n","    if unmapped_ratio > UNMAPPED_THRESHOLD:\n","        raise RuntimeError(\n","            f\"Unmapped label ratio {unmapped_ratio*100:.2f}% exceeds threshold {UNMAPPED_THRESHOLD*100}%. \"\n","            f\"Please check {unmapped_file} and extend LABEL_MAPPING.\"\n","        )\n","    else:\n","        print(f\"\\n✓ Unmapped label ratio does not exceed threshold {UNMAPPED_THRESHOLD*100}%; continuing (will mark as NULL)\")\n","else:\n","    print(f\"\\n✓ All original labels are covered\")\n","\n","# ========== 4. Apply label mapping ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Apply label mapping\")\n","print(\"=\"*60)\n","\n","df_mapped = df.copy()\n","\n","# Keep a copy of original labels (nullable integer)\n","df_mapped['label_original'] = df_mapped['label'].astype('Int32')\n","\n","# Apply mapping\n","def map_label(label):\n","    \"\"\"Map a single label\"\"\"\n","    if pd.isna(label):\n","        return np.nan\n","\n","    label = int(label)\n","    if label in LABEL_MAPPING:\n","        mapped_name = LABEL_MAPPING[label]['mapped']\n","        return UNIFIED_LABELS[mapped_name]\n","    else:\n","        # Unknown labels marked as NaN\n","        return np.nan\n","\n","df_mapped['label'] = df_mapped['label_original'].apply(map_label)\n","\n","# Stats after mapping\n","mapped_label_counts = df_mapped['label'].value_counts(dropna=False)\n","null_after_mapping = df_mapped['label'].isna().sum()\n","\n","print(f\"\\nPost-mapping label stats:\")\n","print(f\"  NULL samples: {null_after_mapping:,} ({null_after_mapping/total_samples*100:.2f}%)\")\n","print(f\"  Number of valid label classes: {df_mapped['label'].nunique(dropna=True)}\")\n","\n","print(f\"\\nPost-mapping distribution:\")\n","for label, count in mapped_label_counts.head(15).items():\n","    pct = count / total_samples * 100\n","    # find label name\n","    label_name = \"NULL\"\n","    if not pd.isna(label):\n","        label_name = [k for k, v in UNIFIED_LABELS.items() if v == int(label)][0]\n","    print(f\"  {label_name:15s} ({str(label):2s}): {count:8,} ({pct:5.2f}%)\")\n","\n","# ========== 5. Clean NULL and transition labels (true nearest neighbor) ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Clean NULL and transition labels (true nearest neighbor)\")\n","print(\"=\"*60)\n","\n","df_cleaned = df_mapped.copy()\n","\n","# Handle NULL labels\n","if NULL_STRATEGY == \"remove\":\n","    null_mask = df_cleaned['label'].isna()\n","    removed_null = null_mask.sum()\n","    df_cleaned = df_cleaned[~null_mask].copy()\n","    print(f\"\\nNULL handling: removed {removed_null:,} samples\")\n","elif NULL_STRATEGY == \"merge_to_transition\":\n","    null_mask = df_cleaned['label'].isna()\n","    df_cleaned.loc[null_mask, 'label'] = UNIFIED_LABELS['transition']\n","    print(f\"\\nNULL handling: merged into transition ({null_mask.sum():,} samples)\")\n","\n","# Detect time column\n","time_col = None\n","for candidate in ['time_sec', 'timestamp', 'timestamp_ms', 'time', 'epoch_ms']:\n","    if candidate in df_cleaned.columns:\n","        time_col = candidate\n","        break\n","\n","if time_col:\n","    print(f\"\\nDetected time column: {time_col}\")\n","else:\n","    print(f\"\\nNo time column detected; will process by index order\")\n","\n","# Handle transition label (true nearest neighbor)\n","transition_value = UNIFIED_LABELS['transition']\n","if TRANSITION_STRATEGY == \"remove\":\n","    trans_mask = df_cleaned['label'] == transition_value\n","    removed_trans = trans_mask.sum()\n","    df_cleaned = df_cleaned[~trans_mask].copy()\n","    print(f\"Transition handling: removed {removed_trans:,} samples\")\n","\n","elif TRANSITION_STRATEGY == \"merge_to_nearest\":\n","    trans_mask = df_cleaned['label'] == transition_value\n","    trans_count = trans_mask.sum()\n","\n","    if trans_count > 0:\n","        print(f\"Transition handling: merge {trans_count:,} samples using nearest-neighbor interpolation\")\n","\n","        # Sort by time (ensure nearest-neighbor semantics)\n","        if time_col:\n","            df_cleaned = df_cleaned.sort_values(\n","                ['subject_id', 'session_id', 'placement', time_col],\n","                kind='stable'\n","            ).copy()\n","            print(f\"  ✓ Sorted by [{time_col}]\")\n","        else:\n","            df_cleaned = df_cleaned.sort_index(kind='stable').copy()\n","            print(f\"  ⚠️ Sorted by index (no time column)\")\n","\n","        # True nearest-neighbor merge\n","        merged_count = 0\n","        for (subj, sess, plc), group in df_cleaned.groupby(\n","            ['subject_id', 'session_id', 'placement'], observed=True\n","        ):\n","            idx = group.index\n","            labels = df_cleaned.loc[idx, 'label'].copy()\n","\n","            # Replace transition with NaN\n","            labels_with_nan = labels.replace(transition_value, np.nan).astype('float')\n","\n","            if labels_with_nan.isna().any():\n","                # Use nearest interpolation (true nearest neighbor)\n","                labels_filled = labels_with_nan.interpolate(\n","                    method='nearest',\n","                    limit_direction='both'\n","                )\n","\n","                # Count successfully merged items\n","                was_trans = (labels == transition_value)\n","                now_filled = labels_filled.notna()\n","                merged_this_group = (was_trans & now_filled).sum()\n","                merged_count += merged_this_group\n","\n","                # Update labels (round then cast to int)\n","                df_cleaned.loc[idx, 'label'] = labels_filled.round()\n","\n","        print(f\"  ✓ Successfully merged {merged_count:,} transition samples to nearest labels\")\n","\n","        # Remove transitions that could not be merged (entire segments are transition)\n","        remaining_trans = (df_cleaned['label'] == transition_value).sum()\n","        if remaining_trans > 0:\n","            df_cleaned = df_cleaned[df_cleaned['label'] != transition_value].copy()\n","            print(f\"  ✓ Removed remaining {remaining_trans:,} transition samples that could not be merged\")\n","\n","# Remove remaining NaNs\n","final_nan = df_cleaned['label'].isna().sum()\n","if final_nan > 0:\n","    df_cleaned = df_cleaned[df_cleaned['label'].notna()].copy()\n","    print(f\"\\nRemoved final residual NaN samples: {final_nan:,}\")\n","\n","# Cast to int32\n","df_cleaned['label'] = df_cleaned['label'].astype('int32')\n","\n","# Reset index\n","df_cleaned = df_cleaned.reset_index(drop=True)\n","\n","print(f\"\\nData after cleaning:\")\n","print(f\"  Samples: {len(df_cleaned):,}\")\n","print(f\"  Number of label classes: {df_cleaned['label'].nunique()}\")\n","print(f\"  Retention rate: {len(df_cleaned)/total_samples*100:.2f}%\")\n","\n","# ========== 6. Audit assertion: verify final label set ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Audit assertion: verify final label set\")\n","print(\"=\"*60)\n","\n","# Determine allowed label set\n","allowed_labels = set(UNIFIED_LABELS.values())\n","if TRANSITION_STRATEGY == \"remove\":\n","    allowed_labels.discard(UNIFIED_LABELS['transition'])\n","\n","# Check actual label set\n","actual_labels = set(df_cleaned['label'].unique())\n","unexpected = sorted(actual_labels - allowed_labels)\n","\n","if unexpected:\n","    raise RuntimeError(\n","        f\"Illegal labels found after cleaning: {unexpected}\\n\"\n","        f\"Allowed labels: {sorted(allowed_labels)}\"\n","    )\n","else:\n","    print(f\"✓ Final label set validation passed\")\n","    print(f\"  Allowed labels: {sorted(allowed_labels)}\")\n","    print(f\"  Actual labels: {sorted(actual_labels)}\")\n","\n","# ========== 7. Final label distribution ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"7. Final label distribution\")\n","print(\"=\"*60)\n","\n","final_label_counts = df_cleaned['label'].value_counts()\n","\n","print(f\"\\nFinal label distribution:\")\n","for label_id, count in final_label_counts.items():\n","    pct = count / len(df_cleaned) * 100\n","    label_name = [k for k, v in UNIFIED_LABELS.items() if v == int(label_id)][0]\n","    print(f\"  {label_name:15s} ({int(label_id):2d}): {count:8,} ({pct:5.2f}%)\")\n","\n","# By-category statistics\n","category_stats = {}\n","for label_id, count in final_label_counts.items():\n","    label_name = [k for k, v in UNIFIED_LABELS.items() if v == int(label_id)][0]\n","    # Find category\n","    category = None\n","    for orig_id, info in LABEL_MAPPING.items():\n","        if info['mapped'] == label_name:\n","            category = info['category']\n","            break\n","\n","    if category:\n","        category_stats[category] = category_stats.get(category, 0) + count\n","\n","print(f\"\\nBy-category statistics:\")\n","for category, count in sorted(category_stats.items()):\n","    pct = count / len(df_cleaned) * 100\n","    print(f\"  {category:15s}: {count:8,} ({pct:5.2f}%)\")\n","\n","# ========== 8. Save results ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"8. Save results\")\n","print(\"=\"*60)\n","\n","# Save cleaned data (using directory layout)\n","output_dir = proc_dir / \"labeled\"\n","if output_dir.exists():\n","    import shutil\n","    shutil.rmtree(output_dir)\n","\n","df_cleaned.to_parquet(\n","    output_dir,\n","    index=False,\n","    partition_cols=['subject_id', 'placement'],\n","    engine='pyarrow'\n",")\n","\n","print(f\"✓ Saved: {output_dir}/\")\n","print(f\"  Data shape: {df_cleaned.shape}\")\n","print(f\"  Partitions: subject_id / placement\")\n","\n","# ========== 9. Save label mapping config (rich) ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"9. Save label mapping config (rich)\")\n","print(\"=\"*60)\n","\n","# Build labels_map with more info\n","labels_map_data = []\n","for label_name, label_id in sorted(UNIFIED_LABELS.items(), key=lambda x: x[1]):\n","    if label_name == \"transition\" and TRANSITION_STRATEGY == \"remove\":\n","        continue  # exclude removed transition\n","\n","    # Find original label IDs and names\n","    original_ids = []\n","    original_names = []\n","    category = None\n","\n","    for orig_id, info in LABEL_MAPPING.items():\n","        if info['mapped'] == label_name:\n","            original_ids.append(str(orig_id))\n","            original_names.append(info['original'])\n","            if category is None:\n","                category = info['category']\n","\n","    # Actual sample count\n","    sample_count = final_label_counts.get(label_id, 0)\n","\n","    labels_map_data.append({\n","        'label_id': label_id,\n","        'label_name': label_name,\n","        'category': category or 'unknown',\n","        'sample_count': int(sample_count),\n","        'percentage': round(sample_count / len(df_cleaned) * 100, 2) if len(df_cleaned) > 0 else 0.0,\n","        'original_label_ids': ','.join(original_ids) if original_ids else '',\n","        'original_label_names': '; '.join(original_names) if original_names else '',\n","        'source_dataset': 'LARa-MbientLab',\n","        'description': f\"{label_name} activity\",\n","    })\n","\n","df_labels_map = pd.DataFrame(labels_map_data)\n","labels_map_file = proc_dir / \"labels_map.csv\"\n","df_labels_map.to_csv(labels_map_file, index=False)\n","\n","print(f\"✓ Saved label mapping: {labels_map_file}\")\n","print(f\"\\nLabel mapping table:\")\n","print(df_labels_map.to_string(index=False))\n","\n","# Save detailed configuration\n","label_config = {\n","    'dataset': 'LARa-MbientLab',\n","    'label_system': 'Cross-dataset unified label superset (covers LARa/RealWorld/SHL)',\n","    'unified_labels': UNIFIED_LABELS,\n","    'label_mapping': LABEL_MAPPING,\n","    'cleaning_strategy': {\n","        'null_strategy': NULL_STRATEGY,\n","        'transition_strategy': TRANSITION_STRATEGY,\n","        'transition_method': 'nearest-neighbor interpolation (true nearest neighbor)' if TRANSITION_STRATEGY == 'merge_to_nearest' else 'remove',\n","        'time_sorted': time_col is not None,\n","        'time_column': time_col,\n","        'unmapped_threshold': UNMAPPED_THRESHOLD,\n","    },\n","    'statistics': {\n","        'original_samples': int(total_samples),\n","        'cleaned_samples': int(len(df_cleaned)),\n","        'removed_samples': int(total_samples - len(df_cleaned)),\n","        'removal_rate': float((total_samples - len(df_cleaned)) / total_samples),\n","        'original_label_count': int(df['label'].nunique(dropna=True)),\n","        'final_label_count': int(df_cleaned['label'].nunique()),\n","        'unmapped_label_count': len(unmapped_ids) if unmapped_ids else 0,\n","    },\n","    'label_distribution': {\n","        label_name: int(final_label_counts.get(label_id, 0))\n","        for label_name, label_id in UNIFIED_LABELS.items()\n","        if label_name != 'transition' or TRANSITION_STRATEGY != 'remove'\n","    },\n","    'notes': [\n","        'Label mapping based on cross-dataset unified label superset (LARa + RealWorld + SHL)',\n","        f'NULL label strategy: {NULL_STRATEGY}',\n","        f'Transition label strategy: {TRANSITION_STRATEGY} (true nearest-neighbor interpolation)',\n","        'Unmapped original labels are automatically marked as NULL',\n","        f'Unmapped label threshold: {UNMAPPED_THRESHOLD*100}%',\n","        f'Sorted by time column: {time_col if time_col else \"No (by index)\"}',\n","        'Mapping table saved at proc/labels_map.csv',\n","        'label_original column uses nullable integer Int32',\n","        'Includes audit assertions to ensure label set integrity',\n","    ]\n","}\n","\n","label_config_file = configs_dir / \"labels.yaml\"\n","with open(label_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(label_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","print(f\"✓ Saved config: {label_config_file}\")\n","\n","label_config_json = configs_dir / \"labels.json\"\n","with open(label_config_json, 'w', encoding='utf-8') as f:\n","    json.dump(label_config, f, indent=2)\n","\n","print(f\"✓ Saved config: {label_config_json}\")\n","\n","# ========== 10. Summary ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 8 complete - Label alignment & cleaning (top-tier revised)\")\n","print(\"=\"*60)\n","\n","print(f\"\\nConfig:\")\n","print(f\"  Label system: cross-dataset unified superset (LARa/RealWorld/SHL)\")\n","print(f\"  NULL strategy: {NULL_STRATEGY}\")\n","print(f\"  Transition strategy: {TRANSITION_STRATEGY} (true nearest neighbor)\")\n","print(f\"  Unmapped threshold: {UNMAPPED_THRESHOLD*100}%\")\n","print(f\"  Time column: {time_col if time_col else 'No (by index)'}\")\n","\n","print(f\"\\nResults:\")\n","print(f\"  Original samples: {total_samples:,}\")\n","print(f\"  Cleaned samples: {len(df_cleaned):,}\")\n","print(f\"  Removed samples: {total_samples - len(df_cleaned):,}\")\n","print(f\"  Retention rate: {len(df_cleaned)/total_samples*100:.2f}%\")\n","\n","print(f\"\\nLabel stats:\")\n","print(f\"  Original label classes: {df['label'].nunique(dropna=True)}\")\n","print(f\"  Final label classes: {df_cleaned['label'].nunique()}\")\n","print(f\"  Unmapped labels: {len(unmapped_ids) if unmapped_ids else 0}\")\n","\n","print(f\"\\nOutputs:\")\n","print(f\"  Data: {output_dir}/\")\n","print(f\"  Mapping table: {labels_map_file}\")\n","print(f\"  Config: {label_config_file}\")\n","if unmapped_ids:\n","    print(f\"  Unmapped list: {reports_dir / 'unmapped_labels.csv'}\")\n","\n","print(\"\\nKey fixes (top-tier):\")\n","print(\"  1. ✓ True nearest-neighbor merge (interpolate method='nearest')\")\n","print(\"  2. ✓ Sort by time before processing (correct semantics)\")\n","print(\"  3. ✓ Record unmapped labels to reports/unmapped_labels.csv\")\n","print(\"  4. ✓ label_original uses nullable Int32\")\n","print(\"  5. ✓ Removed irrelevant MAJORITY_VOTE_THRESHOLD\")\n","print(\"  6. ✓ Audit assertions (fail-fast)\")\n","print(\"  7. ✓ labels_map.csv includes original names and source\")\n","print(\"  8. ✓ Label system described as cross-dataset superset\")\n","print(\"  9. ✓ Output directory changed to labeled/\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YCdSDbgRXXKl","executionInfo":{"status":"ok","timestamp":1763130379706,"user_tz":0,"elapsed":1580,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"94519759-6970-4682-eff3-8c645baf54f5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 8: Label Alignment & Cleaning\n","============================================================\n","\n","Loading normalized data: data/lara/mbientlab/proc/normalized.parquet\n","Data shape: (560070, 15)\n","Number of subjects: 8\n","\n","============================================================\n","1. Analyze original label distribution\n","============================================================\n","\n","Original label stats:\n","  Total samples: 560,070\n","  NULL samples: 28 (0.00%)\n","  Number of label classes: 8\n","\n","Label distribution (top 20):\n","  4.0                           :  215,988 (38.56%)\n","  2.0                           :   86,132 (15.38%)\n","  0.0                           :   73,180 (13.07%)\n","  7.0                           :   48,927 ( 8.74%)\n","  5.0                           :   43,966 ( 7.85%)\n","  1.0                           :   42,039 ( 7.51%)\n","  3.0                           :   38,224 ( 6.82%)\n","  6.0                           :   11,586 ( 2.07%)\n","  nan                           :       28 ( 0.00%)\n","\n","============================================================\n","2. Define label mapping rules\n","============================================================\n","\n","Defined mapping rules: 16 original labels\n","Unified label set: 13 labels (cross-dataset superset)\n","\n","Mapping examples:\n","  1 (walking) -> walking\n","  2 (running) -> running\n","  3 (shuffling) -> walking\n","  4 (stairs (ascending)) -> upstairs\n","  5 (stairs (descending)) -> downstairs\n","  6 (standing) -> standing\n","  7 (sitting) -> sitting\n","  8 (lying) -> lying\n","  13 (cycling (sit)) -> cycling\n","  14 (cycling (stand)) -> cycling\n","\n","============================================================\n","3. Audit assertion: check unmapped labels\n","============================================================\n","\n","✓ All original labels are covered\n","\n","============================================================\n","4. Apply label mapping\n","============================================================\n","\n","Post-mapping label stats:\n","  NULL samples: 28 (0.00%)\n","  Number of valid label classes: 7\n","\n","Post-mapping distribution:\n","  upstairs        (5.0):  215,988 (38.56%)\n","  running         (2.0):   86,132 (15.38%)\n","  walking         (1.0):   80,263 (14.33%)\n","  transition      (0.0):   73,180 (13.07%)\n","  sitting         (3.0):   48,927 ( 8.74%)\n","  downstairs      (6.0):   43,966 ( 7.85%)\n","  standing        (4.0):   11,586 ( 2.07%)\n","  NULL            (nan):       28 ( 0.00%)\n","\n","============================================================\n","5. Clean NULL and transition labels (true nearest neighbor)\n","============================================================\n","\n","NULL handling: removed 28 samples\n","\n","Detected time column: time_sec\n","Transition handling: merge 73,180 samples using nearest-neighbor interpolation\n","  ✓ Sorted by [time_sec]\n","  ✓ Successfully merged 69,642 transition samples to nearest labels\n","\n","Removed final residual NaN samples: 3,538\n","\n","Data after cleaning:\n","  Samples: 556,504\n","  Number of label classes: 6\n","  Retention rate: 99.36%\n","\n","============================================================\n","6. Audit assertion: verify final label set\n","============================================================\n","✓ Final label set validation passed\n","  Allowed labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n","  Actual labels: [np.int32(1), np.int32(2), np.int32(3), np.int32(4), np.int32(5), np.int32(6)]\n","\n","============================================================\n","7. Final label distribution\n","============================================================\n","\n","Final label distribution:\n","  upstairs        ( 5):  255,038 (45.83%)\n","  walking         ( 1):   96,957 (17.42%)\n","  running         ( 2):   89,302 (16.05%)\n","  sitting         ( 3):   51,931 ( 9.33%)\n","  downstairs      ( 6):   50,391 ( 9.05%)\n","  standing        ( 4):   12,885 ( 2.32%)\n","\n","By-category statistics:\n","  locomotion     :  491,688 (88.35%)\n","  static         :   64,816 (11.65%)\n","\n","============================================================\n","8. Save results\n","============================================================\n","✓ Saved: data/lara/mbientlab/proc/labeled/\n","  Data shape: (556504, 16)\n","  Partitions: subject_id / placement\n","\n","============================================================\n","9. Save label mapping config (rich)\n","============================================================\n","✓ Saved label mapping: data/lara/mbientlab/proc/labels_map.csv\n","\n","Label mapping table:\n"," label_id label_name   category  sample_count  percentage original_label_ids                    original_label_names source_dataset         description\n","        0 transition transition             0        0.00                  0                              transition LARa-MbientLab transition activity\n","        1    walking locomotion         96957       17.42                1,3                      walking; shuffling LARa-MbientLab    walking activity\n","        2    running locomotion         89302       16.05                  2                                 running LARa-MbientLab    running activity\n","        3    sitting     static         51931        9.33                  7                                 sitting LARa-MbientLab    sitting activity\n","        4   standing     static         12885        2.32                  6                                standing LARa-MbientLab   standing activity\n","        5   upstairs locomotion        255038       45.83                  4                      stairs (ascending) LARa-MbientLab   upstairs activity\n","        6 downstairs locomotion         50391        9.05                  5                     stairs (descending) LARa-MbientLab downstairs activity\n","        7      lying     static             0        0.00                  8                                   lying LARa-MbientLab      lying activity\n","        8    cycling  transport             0        0.00          13,14,130 cycling (sit); cycling (stand); cycling LARa-MbientLab    cycling activity\n","        9        car  transport             0        0.00                 17                                     car LARa-MbientLab        car activity\n","       10        bus  transport             0        0.00                 18                                     bus LARa-MbientLab        bus activity\n","       11      train  transport             0        0.00                 19                                   train LARa-MbientLab      train activity\n","       12     subway  transport             0        0.00                 20                                  subway LARa-MbientLab     subway activity\n","✓ Saved config: configs/labels.yaml\n","✓ Saved config: configs/labels.json\n","\n","============================================================\n","Step 8 complete - Label alignment & cleaning (top-tier revised)\n","============================================================\n","\n","Config:\n","  Label system: cross-dataset unified superset (LARa/RealWorld/SHL)\n","  NULL strategy: remove\n","  Transition strategy: merge_to_nearest (true nearest neighbor)\n","  Unmapped threshold: 1.0%\n","  Time column: time_sec\n","\n","Results:\n","  Original samples: 560,070\n","  Cleaned samples: 556,504\n","  Removed samples: 3,566\n","  Retention rate: 99.36%\n","\n","Label stats:\n","  Original label classes: 8\n","  Final label classes: 6\n","  Unmapped labels: 0\n","\n","Outputs:\n","  Data: data/lara/mbientlab/proc/labeled/\n","  Mapping table: data/lara/mbientlab/proc/labels_map.csv\n","  Config: configs/labels.yaml\n","\n","Key fixes (top-tier):\n","  1. ✓ True nearest-neighbor merge (interpolate method='nearest')\n","  2. ✓ Sort by time before processing (correct semantics)\n","  3. ✓ Record unmapped labels to reports/unmapped_labels.csv\n","  4. ✓ label_original uses nullable Int32\n","  5. ✓ Removed irrelevant MAJORITY_VOTE_THRESHOLD\n","  6. ✓ Audit assertions (fail-fast)\n","  7. ✓ labels_map.csv includes original names and source\n","  8. ✓ Label system described as cross-dataset superset\n","  9. ✓ Output directory changed to labeled/\n","============================================================\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","\n","# ========== Manually set FOLD_ID (if the environment variable is not set) ==========\n","if \"FOLD_ID\" not in os.environ:\n","    print(\"⚠️ Environment variable FOLD_ID is not set; please specify it manually:\")\n","    print(\"Hint: if your LOSO has N folds, FOLD_ID should be 0 to N-1\")\n","    fold_input = input(\"Please enter FOLD_ID (press Enter to default to 0): \").strip()\n","    os.environ[\"FOLD_ID\"] = fold_input if fold_input else \"0\"\n","    print(f\"✓ FOLD_ID has been set to {os.environ['FOLD_ID']}\")\n","\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","\n","\"\"\"\n","Step 9: Sliding-window Slicing (top-conf/journal grade - revised)\n","Slice with fixed window length/step; assign window label by majority label\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import json\n","import os\n","from collections import Counter\n","\n","# ========== Config ==========\n","\n","# Sliding-window parameters\n","SAMPLING_RATE_HZ = 50.0\n","WINDOW_SIZE_SEC = 3.0\n","OVERLAP_RATIO = 0.5\n","\n","# Compute sample counts\n","WINDOW_SIZE = int(WINDOW_SIZE_SEC * SAMPLING_RATE_HZ)  # 150 samples\n","STEP_SIZE = int(WINDOW_SIZE * (1 - OVERLAP_RATIO))  # 75 samples\n","\n","# Majority label threshold\n","DOMINANT_THRESHOLD = 0.8\n","\n","# Feature columns (8 channels)\n","FEATURE_COLS = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","print(\"=\"*60)\n","print(\"Step 9: Sliding-window slicing\")\n","print(\"=\"*60)\n","\n","# Create output directory (persist per fold)\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","\n","# Create subdirectory based on FOLD_ID\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","fold_tag = f\"fold_{FOLD_ID:02d}\" if FOLD_ID >= 0 else \"all\"\n","windows_dir = proc_dir / \"windows\" / fold_tag\n","windows_dir.mkdir(parents=True, exist_ok=True)\n","\n","if FOLD_ID >= 0:\n","    print(f\"\\nUsing train fold: FOLD_ID={FOLD_ID}\")\n","    print(f\"Output directory: {windows_dir}\")\n","else:\n","    print(f\"\\nFOLD_ID not specified; using all data\")\n","    print(f\"Output directory: {windows_dir}\")\n","\n","print(f\"\\nSliding-window parameters:\")\n","print(f\"  Window length: {WINDOW_SIZE_SEC} s = {WINDOW_SIZE} samples @ {SAMPLING_RATE_HZ} Hz\")\n","print(f\"  Step size: {STEP_SIZE} samples (overlap {OVERLAP_RATIO*100:.0f}%)\")\n","print(f\"  Dominant label threshold: {DOMINANT_THRESHOLD*100:.0f}%\")\n","\n","# ========== 1. Load data ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Load cleaned data\")\n","print(\"=\"*60)\n","\n","labeled_dir = proc_dir / \"labeled\"\n","print(f\"Loading data: {labeled_dir}/\")\n","df = pd.read_parquet(labeled_dir)\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Number of subjects: {df['subject_id'].nunique()}\")\n","print(f\"Number of label classes: {df['label'].nunique()}\")\n","\n","# Check required columns\n","required_cols = ['subject_id', 'session_id', 'placement', 'label'] + FEATURE_COLS\n","missing_cols = [c for c in required_cols if c not in df.columns]\n","if missing_cols:\n","    raise ValueError(f\"Missing required columns: {missing_cols}\")\n","\n","print(f\"\\nFeature columns: {FEATURE_COLS}\")\n","\n","# Detect time column\n","time_col = 'time_sec' if 'time_sec' in df.columns else None\n","if time_col:\n","    print(f\"Time column: {time_col}\")\n","else:\n","    print(\"No time column detected; will sort by index\")\n","\n","# ========== 2. Load train/test split ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Load train/test split\")\n","print(\"=\"*60)\n","\n","splits_path = configs_dir / \"splits.json\"\n","\n","if splits_path.exists() and FOLD_ID >= 0:\n","    with open(splits_path, \"r\") as f:\n","        splits = json.load(f)\n","\n","    train_subjects = set(splits[str(FOLD_ID)][\"train_subjects\"])\n","    test_subjects = set(splits[str(FOLD_ID)][\"test_subjects\"])\n","\n","    print(f\"Train subjects: {len(train_subjects)}\")\n","    print(f\"Test subjects: {len(test_subjects)}\")\n","\n","    # Split data\n","    df_train = df[df['subject_id'].isin(train_subjects)].copy()\n","    df_test = df[df['subject_id'].isin(test_subjects)].copy()\n","\n","    print(f\"\\nTrain set: {len(df_train):,} samples\")\n","    print(f\"Test set: {len(df_test):,} samples\")\n","else:\n","    print(\"No train-fold config found; using all data\")\n","    df_train = df.copy()\n","    df_test = pd.DataFrame()\n","    train_subjects = set(df['subject_id'].unique())\n","    test_subjects = set()\n","\n","# ========== 3. Define sliding-window function (with time continuity check) ==========\n","\n","def sliding_window_extract(df_subset, window_size, step_size, dominant_threshold, time_col=None):\n","    \"\"\"\n","    Perform sliding-window slicing grouped by session.\n","\n","    Returns:\n","        windows_list: list of window feature arrays\n","        metadata_list: list of window metadata dicts\n","    \"\"\"\n","    windows_list = []\n","    metadata_list = []\n","    window_id = 0\n","\n","    # Group by session\n","    for (subj, sess, plc), group in df_subset.groupby(\n","        ['subject_id', 'session_id', 'placement'], observed=True\n","    ):\n","        # Sort by time column (preferred), otherwise by index\n","        if time_col and time_col in group.columns:\n","            group = group.sort_values(time_col, kind='stable').copy()\n","        else:\n","            group = group.sort_index(kind='stable').copy()\n","\n","        # Extract features and labels\n","        features = group[FEATURE_COLS].values\n","        labels = group['label'].values\n","\n","        # Extract timestamps (if any)\n","        if time_col and time_col in group.columns:\n","            timestamps = group[time_col].values\n","        else:\n","            timestamps = None\n","\n","        # Sliding-window slicing\n","        n_samples = len(group)\n","        for start_idx in range(0, n_samples - window_size + 1, step_size):\n","            end_idx = start_idx + window_size\n","\n","            # Extract window\n","            window_features = features[start_idx:end_idx]\n","            window_labels = labels[start_idx:end_idx]\n","\n","            # Check NaNs\n","            if np.isnan(window_features).any():\n","                continue\n","\n","            # Time continuity check (if timestamps exist)\n","            if timestamps is not None:\n","                expected_duration = (window_size - 1) / SAMPLING_RATE_HZ\n","                actual_duration = timestamps[end_idx - 1] - timestamps[start_idx]\n","                # Allow 10% jitter\n","                if abs(actual_duration - expected_duration) > 0.1 * expected_duration:\n","                    continue\n","\n","            # Compute dominant label\n","            label_counts = Counter(window_labels)\n","            dominant_label, dominant_count = label_counts.most_common(1)[0]\n","            dominant_ratio = dominant_count / window_size\n","\n","            # Keep only windows that meet the threshold\n","            if dominant_ratio < dominant_threshold:\n","                continue\n","\n","            # Extract time range\n","            if timestamps is not None:\n","                time_start = timestamps[start_idx]\n","                time_end = timestamps[end_idx - 1]\n","                time_range = f\"{time_start:.3f}-{time_end:.3f}\"\n","            else:\n","                time_range = f\"{start_idx}-{end_idx-1}\"\n","\n","            # Save window\n","            windows_list.append(window_features)\n","\n","            # Save metadata\n","            metadata_list.append({\n","                'window_id': window_id,\n","                'subject_id': subj,\n","                'session_id': sess,\n","                'placement': plc,\n","                'label': int(dominant_label),\n","                'label_purity': round(dominant_ratio, 4),\n","                'time_range': time_range,\n","                'start_idx': start_idx,\n","                'end_idx': end_idx,\n","            })\n","\n","            window_id += 1\n","\n","    return windows_list, metadata_list\n","\n","# ========== 4. Extract training-set windows ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Extract training-set windows\")\n","print(\"=\"*60)\n","\n","# Initialize variables (avoid undefined)\n","train_windows = []\n","df_train_meta = pd.DataFrame()\n","train_label_counts = pd.Series(dtype=int)\n","\n","if not df_train.empty:\n","    print(f\"Processing train set ({len(df_train):,} samples)...\")\n","\n","    train_windows, train_metadata = sliding_window_extract(\n","        df_train, WINDOW_SIZE, STEP_SIZE, DOMINANT_THRESHOLD, time_col\n","    )\n","\n","    print(f\"✓ Extracted training windows: {len(train_windows):,}\")\n","\n","    if train_windows:\n","        # To numpy array\n","        X_train = np.array(train_windows, dtype='float32')  # shape: (n_windows, window_size, n_features)\n","        df_train_meta = pd.DataFrame(train_metadata)\n","\n","        print(f\"  X_train shape: {X_train.shape}\")\n","        print(f\"  Feature dimensions: {X_train.shape[2]} channels × {X_train.shape[1]} timesteps\")\n","\n","        # Label distribution\n","        train_label_counts = df_train_meta['label'].value_counts().sort_index()\n","        print(f\"\\nTrain-set label distribution:\")\n","        for label, count in train_label_counts.items():\n","            pct = count / len(df_train_meta) * 100\n","            print(f\"  Label {label}: {count:6,} windows ({pct:5.2f}%)\")\n","\n","        # Label purity stats\n","        avg_purity = df_train_meta['label_purity'].mean()\n","        min_purity = df_train_meta['label_purity'].min()\n","        print(f\"\\nTrain-set label purity:\")\n","        print(f\"  Mean: {avg_purity*100:.2f}%\")\n","        print(f\"  Min: {min_purity*100:.2f}%\")\n","\n","        # Save train set\n","        print(f\"\\nSaving train set...\")\n","\n","        # Save features (numpy)\n","        X_train_npy_file = windows_dir / \"X_train.npy\"\n","        np.save(X_train_npy_file, X_train)\n","        print(f\"  ✓ {X_train_npy_file} (feature tensor)\")\n","\n","        # Save metadata (Parquet; filename aligned to X_train.parquet)\n","        X_train_meta_file = windows_dir / \"X_train.parquet\"\n","        df_train_meta[['window_id', 'subject_id', 'session_id', 'placement',\n","                       'label', 'label_purity', 'time_range', 'start_idx', 'end_idx']].to_parquet(\n","            X_train_meta_file, index=False\n","        )\n","        print(f\"  ✓ {X_train_meta_file} (metadata)\")\n","\n","        # Save label vector\n","        y_train = df_train_meta['label'].values.astype('int32')\n","        y_train_file = windows_dir / \"y_train.npy\"\n","        np.save(y_train_file, y_train)\n","        print(f\"  ✓ {y_train_file}\")\n","\n","        # Export label distribution snapshot (for audit)\n","        train_label_counts.to_csv(windows_dir / \"train_label_counts.csv\", header=['count'])\n","        print(f\"  ✓ train_label_counts.csv\")\n","    else:\n","        print(\"⚠️ No training windows extracted\")\n","else:\n","    print(\"Train set is empty; skipping\")\n","\n","# ========== 5. Extract test-set windows ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Extract test-set windows\")\n","print(\"=\"*60)\n","\n","# Initialize variables (avoid undefined)\n","test_windows = []\n","df_test_meta = pd.DataFrame()\n","test_label_counts = pd.Series(dtype=int)\n","\n","if not df_test.empty:\n","    print(f\"Processing test set ({len(df_test):,} samples)...\")\n","\n","    test_windows, test_metadata = sliding_window_extract(\n","        df_test, WINDOW_SIZE, STEP_SIZE, DOMINANT_THRESHOLD, time_col\n","    )\n","\n","    print(f\"✓ Extracted test windows: {len(test_windows):,}\")\n","\n","    if test_windows:\n","        # To numpy array\n","        X_test = np.array(test_windows, dtype='float32')\n","        df_test_meta = pd.DataFrame(test_metadata)\n","\n","        print(f\"  X_test shape: {X_test.shape}\")\n","\n","        # Label distribution\n","        test_label_counts = df_test_meta['label'].value_counts().sort_index()\n","        print(f\"\\nTest-set label distribution:\")\n","        for label, count in test_label_counts.items():\n","            pct = count / len(df_test_meta) * 100\n","            print(f\"  Label {label}: {count:6,} windows ({pct:5.2f}%)\")\n","\n","        # Label purity stats\n","        avg_purity = df_test_meta['label_purity'].mean()\n","        min_purity = df_test_meta['label_purity'].min()\n","        print(f\"\\nTest-set label purity:\")\n","        print(f\"  Mean: {avg_purity*100:.2f}%\")\n","        print(f\"  Min: {min_purity*100:.2f}%\")\n","\n","        # Save test set\n","        print(f\"\\nSaving test set...\")\n","\n","        # Save features (numpy)\n","        X_test_npy_file = windows_dir / \"X_test.npy\"\n","        np.save(X_test_npy_file, X_test)\n","        print(f\"  ✓ {X_test_npy_file} (feature tensor)\")\n","\n","        # Save metadata (Parquet; filename aligned to X_test.parquet)\n","        X_test_meta_file = windows_dir / \"X_test.parquet\"\n","        df_test_meta[['window_id', 'subject_id', 'session_id', 'placement',\n","                      'label', 'label_purity', 'time_range', 'start_idx', 'end_idx']].to_parquet(\n","            X_test_meta_file, index=False\n","        )\n","        print(f\"  ✓ {X_test_meta_file} (metadata)\")\n","\n","        # Save label vector\n","        y_test = df_test_meta['label'].values.astype('int32')\n","        y_test_file = windows_dir / \"y_test.npy\"\n","        np.save(y_test_file, y_test)\n","        print(f\"  ✓ {y_test_file}\")\n","\n","        # Export label distribution snapshot (for audit)\n","        test_label_counts.to_csv(windows_dir / \"test_label_counts.csv\", header=['count'])\n","        print(f\"  ✓ test_label_counts.csv\")\n","    else:\n","        print(\"⚠️ No test windows extracted\")\n","else:\n","    print(\"Test set is empty; skipping\")\n","\n","# ========== 6. Save window configuration ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Save window configuration\")\n","print(\"=\"*60)\n","\n","# Build config\n","window_config = {\n","    'window_parameters': {\n","        'sampling_rate_hz': SAMPLING_RATE_HZ,\n","        'window_size_sec': WINDOW_SIZE_SEC,\n","        'window_size_samples': WINDOW_SIZE,\n","        'overlap_ratio': OVERLAP_RATIO,\n","        'step_size_samples': STEP_SIZE,\n","        'dominant_threshold': DOMINANT_THRESHOLD,\n","    },\n","    'features': {\n","        'channels': FEATURE_COLS,\n","        'n_channels': len(FEATURE_COLS),\n","        'description': '8-channel IMU features (ax,ay,az,gx,gy,gz,acc_mag,gyr_mag)',\n","    },\n","    'dataset_split': {\n","        'fold_id': FOLD_ID if FOLD_ID >= 0 else None,\n","        'fold_tag': fold_tag,\n","        'train_subjects': sorted(list(train_subjects)),\n","        'test_subjects': sorted(list(test_subjects)) if test_subjects else None,\n","    },\n","    'statistics': {},\n","    'notes': [\n","        f'Window parameters: {WINDOW_SIZE_SEC}s @ {SAMPLING_RATE_HZ}Hz = {WINDOW_SIZE} samples',\n","        f'Step size: {STEP_SIZE} samples (overlap {OVERLAP_RATIO*100:.0f}%)',\n","        f'Dominant label threshold: {DOMINANT_THRESHOLD*100:.0f}% (discard windows below threshold)',\n","        'Features: 8 channels (3-axis accelerometer + 3-axis gyroscope + 2 magnitudes)',\n","        'Data formats: X_*.npy (float32 tensor), X_*.parquet (metadata), y_*.npy (int32)',\n","        'Metadata includes: window_id/time_range/label/label_purity, etc.',\n","        'Slice per session to ensure temporal continuity',\n","        f'Order by {time_col if time_col else \"index\"}',\n","        'Discard windows containing NaN',\n","        'Time continuity check (allow 10% jitter)',\n","        f'Persist by fold: windows/{fold_tag}/ (avoid overwrite)',\n","    ]\n","}\n","\n","# Add train stats\n","if train_windows:\n","    window_config['statistics']['train'] = {\n","        'n_windows': len(train_windows),\n","        'n_subjects': int(df_train_meta['subject_id'].nunique()),\n","        'n_sessions': int(df_train_meta.groupby(['subject_id', 'session_id']).ngroups),\n","        'label_distribution': {int(k): int(v) for k, v in train_label_counts.items()},\n","        'avg_label_purity': round(float(df_train_meta['label_purity'].mean()), 4),\n","        'min_label_purity': round(float(df_train_meta['label_purity'].min()), 4),\n","    }\n","\n","# Add test stats\n","if test_windows:\n","    window_config['statistics']['test'] = {\n","        'n_windows': len(test_windows),\n","        'n_subjects': int(df_test_meta['subject_id'].nunique()),\n","        'n_sessions': int(df_test_meta.groupby(['subject_id', 'session_id']).ngroups),\n","        'label_distribution': {int(k): int(v) for k, v in test_label_counts.items()},\n","        'avg_label_purity': round(float(df_test_meta['label_purity'].mean()), 4),\n","        'min_label_purity': round(float(df_test_meta['label_purity'].min()), 4),\n","    }\n","\n","# Save config\n","window_config_file = configs_dir / \"windows.yaml\"\n","with open(window_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(window_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","print(f\"✓ Saved config: {window_config_file}\")\n","\n","window_config_json = configs_dir / \"windows.json\"\n","with open(window_config_json, 'w', encoding='utf-8') as f:\n","    json.dump(window_config, f, indent=2)\n","\n","print(f\"✓ Saved config: {window_config_json}\")\n","\n","# ========== 7. Summary ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 9 complete - Sliding-window slicing (revised)\")\n","print(\"=\"*60)\n","\n","print(f\"\\nWindow parameters:\")\n","print(f\"  Window length: {WINDOW_SIZE_SEC} s = {WINDOW_SIZE} samples\")\n","print(f\"  Step size: {STEP_SIZE} samples (overlap {OVERLAP_RATIO*100:.0f}%)\")\n","print(f\"  Dominant threshold: {DOMINANT_THRESHOLD*100:.0f}%\")\n","print(f\"  Feature dimension: {len(FEATURE_COLS)} channels\")\n","print(f\"  Sort order: {time_col if time_col else 'index'}\")\n","\n","if train_windows:\n","    print(f\"\\nTrain set:\")\n","    print(f\"  # windows: {len(train_windows):,}\")\n","    print(f\"  Shape: {X_train.shape}\")\n","    print(f\"  Subjects: {df_train_meta['subject_id'].nunique()}\")\n","    print(f\"  Sessions: {df_train_meta.groupby(['subject_id', 'session_id']).ngroups}\")\n","    print(f\"  Average purity: {df_train_meta['label_purity'].mean()*100:.2f}%\")\n","\n","if test_windows:\n","    print(f\"\\nTest set:\")\n","    print(f\"  # windows: {len(test_windows):,}\")\n","    print(f\"  Shape: {X_test.shape}\")\n","    print(f\"  Subjects: {df_test_meta['subject_id'].nunique()}\")\n","    print(f\"  Sessions: {df_test_meta.groupby(['subject_id', 'session_id']).ngroups}\")\n","    print(f\"  Average purity: {df_test_meta['label_purity'].mean()*100:.2f}%\")\n","\n","print(f\"\\nOutputs:\")\n","print(f\"  Directory: {windows_dir}/\")\n","if train_windows:\n","    print(f\"  Train: X_train.npy (tensor), X_train.parquet (metadata), y_train.npy\")\n","    print(f\"         train_label_counts.csv\")\n","if test_windows:\n","    print(f\"  Test:  X_test.npy (tensor), X_test.parquet (metadata), y_test.npy\")\n","    print(f\"         test_label_counts.csv\")\n","print(f\"  Config: {window_config_file}\")\n","\n","print(\"\\nKey fixes:\")\n","print(\"  1. ✓ Sort by time column (time_sec preferred, otherwise index)\")\n","print(\"  2. ✓ Metadata filenames aligned to X_train/test.parquet\")\n","print(\"  3. ✓ Safe variable initialization (avoid NameError)\")\n","print(\"  4. ✓ Export label-distribution snapshot CSV (for audit)\")\n","print(\"  5. ✓ Time continuity check (allow 10% jitter)\")\n","print(\"  6. ✓ Slice by session (avoid cross-session windows)\")\n","print(\"  7. ✓ Dominant label proportion ≥ 80%\")\n","print(\"  8. ✓ Discard windows containing NaN\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QvnwjEDfXYdj","executionInfo":{"status":"ok","timestamp":1763130380364,"user_tz":0,"elapsed":640,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"c9cde80f-de39-450d-f3db-a2e7ba45e5b9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 9: Sliding-window slicing\n","============================================================\n","\n","Using train fold: FOLD_ID=5\n","Output directory: data/lara/mbientlab/proc/windows/fold_05\n","\n","Sliding-window parameters:\n","  Window length: 3.0 s = 150 samples @ 50.0 Hz\n","  Step size: 75 samples (overlap 50%)\n","  Dominant label threshold: 80%\n","\n","============================================================\n","1. Load cleaned data\n","============================================================\n","Loading data: data/lara/mbientlab/proc/labeled/\n","Data shape: (556504, 16)\n","Number of subjects: 8\n","Number of label classes: 6\n","\n","Feature columns: ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","Time column: time_sec\n","\n","============================================================\n","2. Load train/test split\n","============================================================\n","Train subjects: 7\n","Test subjects: 1\n","\n","Train set: 525,581 samples\n","Test set: 30,923 samples\n","\n","============================================================\n","4. Extract training-set windows\n","============================================================\n","Processing train set (525,581 samples)...\n","✓ Extracted training windows: 5,442\n","  X_train shape: (5442, 150, 8)\n","  Feature dimensions: 8 channels × 150 timesteps\n","\n","Train-set label distribution:\n","  Label 1:    824 windows (15.14%)\n","  Label 2:    945 windows (17.36%)\n","  Label 3:    578 windows (10.62%)\n","  Label 4:     57 windows ( 1.05%)\n","  Label 5:  2,599 windows (47.76%)\n","  Label 6:    439 windows ( 8.07%)\n","\n","Train-set label purity:\n","  Mean: 98.49%\n","  Min: 80.00%\n","\n","Saving train set...\n","  ✓ data/lara/mbientlab/proc/windows/fold_05/X_train.npy (feature tensor)\n","  ✓ data/lara/mbientlab/proc/windows/fold_05/X_train.parquet (metadata)\n","  ✓ data/lara/mbientlab/proc/windows/fold_05/y_train.npy\n","  ✓ train_label_counts.csv\n","\n","============================================================\n","5. Extract test-set windows\n","============================================================\n","Processing test set (30,923 samples)...\n","✓ Extracted test windows: 289\n","  X_test shape: (289, 150, 8)\n","\n","Test-set label distribution:\n","  Label 1:     80 windows (27.68%)\n","  Label 2:     49 windows (16.96%)\n","  Label 3:     25 windows ( 8.65%)\n","  Label 4:      9 windows ( 3.11%)\n","  Label 5:    106 windows (36.68%)\n","  Label 6:     20 windows ( 6.92%)\n","\n","Test-set label purity:\n","  Mean: 98.00%\n","  Min: 80.00%\n","\n","Saving test set...\n","  ✓ data/lara/mbientlab/proc/windows/fold_05/X_test.npy (feature tensor)\n","  ✓ data/lara/mbientlab/proc/windows/fold_05/X_test.parquet (metadata)\n","  ✓ data/lara/mbientlab/proc/windows/fold_05/y_test.npy\n","  ✓ test_label_counts.csv\n","\n","============================================================\n","6. Save window configuration\n","============================================================\n","✓ Saved config: configs/windows.yaml\n","✓ Saved config: configs/windows.json\n","\n","============================================================\n","Step 9 complete - Sliding-window slicing (revised)\n","============================================================\n","\n","Window parameters:\n","  Window length: 3.0 s = 150 samples\n","  Step size: 75 samples (overlap 50%)\n","  Dominant threshold: 80%\n","  Feature dimension: 8 channels\n","  Sort order: time_sec\n","\n","Train set:\n","  # windows: 5,442\n","  Shape: (5442, 150, 8)\n","  Subjects: 7\n","  Sessions: 90\n","  Average purity: 98.49%\n","\n","Test set:\n","  # windows: 289\n","  Shape: (289, 150, 8)\n","  Subjects: 1\n","  Sessions: 6\n","  Average purity: 98.00%\n","\n","Outputs:\n","  Directory: data/lara/mbientlab/proc/windows/fold_05/\n","  Train: X_train.npy (tensor), X_train.parquet (metadata), y_train.npy\n","         train_label_counts.csv\n","  Test:  X_test.npy (tensor), X_test.parquet (metadata), y_test.npy\n","         test_label_counts.csv\n","  Config: configs/windows.yaml\n","\n","Key fixes:\n","  1. ✓ Sort by time column (time_sec preferred, otherwise index)\n","  2. ✓ Metadata filenames aligned to X_train/test.parquet\n","  3. ✓ Safe variable initialization (avoid NameError)\n","  4. ✓ Export label-distribution snapshot CSV (for audit)\n","  5. ✓ Time continuity check (allow 10% jitter)\n","  6. ✓ Slice by session (avoid cross-session windows)\n","  7. ✓ Dominant label proportion ≥ 80%\n","  8. ✓ Discard windows containing NaN\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\n","\"\"\"\n","Step 10: LOSO Split (top-conf/journal grade)\n","Leave-One-Subject-Out: 1 subject for test per fold, the rest for training\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import json\n","import yaml\n","from collections import defaultdict\n","\n","print(\"=\"*60)\n","print(\"Step 10: LOSO split\")\n","print(\"=\"*60)\n","\n","# Path configuration\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","configs_dir.mkdir(parents=True, exist_ok=True)\n","\n","# ========== 1. Load data and get subject list ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Load data and get subject list\")\n","print(\"=\"*60)\n","\n","labeled_dir = proc_dir / \"labeled\"\n","print(f\"Loading data: {labeled_dir}/\")\n","\n","df = pd.read_parquet(labeled_dir)\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Total samples: {len(df):,}\")\n","\n","# Extract all subjects\n","all_subjects = sorted(df['subject_id'].unique().tolist())\n","n_subjects = len(all_subjects)\n","\n","print(f\"\\nSubject list:\")\n","print(f\"  Total: {n_subjects} subjects\")\n","print(f\"  IDs: {all_subjects}\")\n","\n","# Sample count per subject\n","subject_sample_counts = df['subject_id'].value_counts().sort_index()\n","print(f\"\\nSample count per subject:\")\n","for subj in all_subjects:\n","    count = subject_sample_counts.get(subj, 0)\n","    pct = count / len(df) * 100\n","    print(f\"  {subj}: {count:8,} samples ({pct:5.2f}%)\")\n","\n","# ========== 2. Generate LOSO split ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Generate LOSO split\")\n","print(\"=\"*60)\n","\n","print(f\"\\nLOSO strategy: Leave-One-Subject-Out\")\n","print(f\"  #folds = #subjects = {n_subjects}\")\n","print(f\"  Per fold: 1 subject for test, {n_subjects-1} subjects for train\")\n","\n","# Create split dict\n","splits = {}\n","\n","for fold_id, test_subject in enumerate(all_subjects):\n","    # Test set: current subject\n","    test_subjects = [test_subject]\n","\n","    # Train set: all other subjects\n","    train_subjects = [s for s in all_subjects if s != test_subject]\n","\n","    # Save split\n","    splits[str(fold_id)] = {\n","        \"fold_id\": fold_id,\n","        \"test_subject\": test_subject,\n","        \"test_subjects\": test_subjects,  # list for compatibility\n","        \"train_subjects\": train_subjects,\n","        \"n_train\": len(train_subjects),\n","        \"n_test\": len(test_subjects),\n","    }\n","\n","    print(f\"  Fold {fold_id}: test {test_subject}, train {len(train_subjects)} subjects\")\n","\n","print(f\"\\n✓ Generated {len(splits)} LOSO folds\")\n","\n","# ========== 3. Validate split integrity ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Validate split integrity\")\n","print(\"=\"*60)\n","\n","# Check 1: each subject appears exactly once in the test set\n","test_subject_appearances = defaultdict(int)\n","for fold_id, fold_info in splits.items():\n","    for subj in fold_info['test_subjects']:\n","        test_subject_appearances[subj] += 1\n","\n","print(f\"\\nCheck 1: times each subject appears as test\")\n","all_once = True\n","for subj in all_subjects:\n","    count = test_subject_appearances[subj]\n","    status = \"✓\" if count == 1 else \"✗\"\n","    print(f\"  {status} {subj}: {count} time(s)\")\n","    if count != 1:\n","        all_once = False\n","\n","if all_once:\n","    print(f\"  ✓ All subjects appear exactly once\")\n","else:\n","    raise RuntimeError(\"Split validation failed: subject test appearances not equal to 1\")\n","\n","# Check 2: train and test sets are disjoint\n","print(f\"\\nCheck 2: train and test sets are disjoint\")\n","all_disjoint = True\n","for fold_id, fold_info in splits.items():\n","    train_set = set(fold_info['train_subjects'])\n","    test_set = set(fold_info['test_subjects'])\n","    overlap = train_set & test_set\n","\n","    if overlap:\n","        print(f\"  ✗ Fold {fold_id}: overlap exists {overlap}\")\n","        all_disjoint = False\n","\n","if all_disjoint:\n","    print(f\"  ✓ Train/test sets are completely disjoint for all folds\")\n","else:\n","    raise RuntimeError(\"Split validation failed: train and test sets have overlap\")\n","\n","# Check 3: all subjects covered\n","print(f\"\\nCheck 3: all subjects covered\")\n","covered_subjects = set()\n","for fold_id, fold_info in splits.items():\n","    covered_subjects.update(fold_info['train_subjects'])\n","    covered_subjects.update(fold_info['test_subjects'])\n","\n","missing = set(all_subjects) - covered_subjects\n","extra = covered_subjects - set(all_subjects)\n","\n","if not missing and not extra:\n","    print(f\"  ✓ All subjects are covered; no missing or extra subjects\")\n","else:\n","    if missing:\n","        print(f\"  ✗ Missing subjects: {missing}\")\n","    if extra:\n","        print(f\"  ✗ Extra subjects: {extra}\")\n","    raise RuntimeError(\"Split validation failed: subject coverage incomplete\")\n","\n","# Check 4: sample count stats\n","print(f\"\\nCheck 4: per-fold sample counts\")\n","fold_sample_stats = []\n","for fold_id, fold_info in splits.items():\n","    train_subjects = fold_info['train_subjects']\n","    test_subjects = fold_info['test_subjects']\n","\n","    n_train_samples = df[df['subject_id'].isin(train_subjects)].shape[0]\n","    n_test_samples = df[df['subject_id'].isin(test_subjects)].shape[0]\n","\n","    fold_sample_stats.append({\n","        'fold_id': int(fold_id),\n","        'test_subject': fold_info['test_subject'],\n","        'n_train_samples': n_train_samples,\n","        'n_test_samples': n_test_samples,\n","        'train_ratio': round(n_train_samples / len(df), 4),\n","        'test_ratio': round(n_test_samples / len(df), 4),\n","    })\n","\n","df_fold_stats = pd.DataFrame(fold_sample_stats)\n","\n","print(f\"\\nPer-fold sample distribution:\")\n","print(df_fold_stats.to_string(index=False))\n","\n","# Summary\n","print(f\"\\nSample distribution summary:\")\n","print(f\"  Train sample count: {df_fold_stats['n_train_samples'].min():,} ~ {df_fold_stats['n_train_samples'].max():,}\")\n","print(f\"  Test sample count: {df_fold_stats['n_test_samples'].min():,} ~ {df_fold_stats['n_test_samples'].max():,}\")\n","print(f\"  Average train ratio: {df_fold_stats['train_ratio'].mean()*100:.2f}%\")\n","print(f\"  Average test ratio: {df_fold_stats['test_ratio'].mean()*100:.2f}%\")\n","\n","print(f\"\\n✓ All validations passed\")\n","\n","# ========== 4. Save split configuration ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Save split configuration\")\n","print(\"=\"*60)\n","\n","# Save splits.json\n","splits_file = configs_dir / \"splits.json\"\n","with open(splits_file, 'w', encoding='utf-8') as f:\n","    json.dump(splits, f, indent=2)\n","\n","print(f\"✓ Saved: {splits_file}\")\n","\n","# Save detailed config (with metadata)\n","loso_config = {\n","    'strategy': 'LOSO (Leave-One-Subject-Out)',\n","    'description': 'One subject for test in each fold; remaining subjects for training',\n","    'n_folds': n_subjects,\n","    'n_subjects': n_subjects,\n","    'all_subjects': all_subjects,\n","    'fold_statistics': {\n","        'train_samples_min': int(df_fold_stats['n_train_samples'].min()),\n","        'train_samples_max': int(df_fold_stats['n_train_samples'].max()),\n","        'train_samples_mean': int(df_fold_stats['n_train_samples'].mean()),\n","        'test_samples_min': int(df_fold_stats['n_test_samples'].min()),\n","        'test_samples_max': int(df_fold_stats['n_test_samples'].max()),\n","        'test_samples_mean': int(df_fold_stats['n_test_samples'].mean()),\n","        'avg_train_ratio': round(float(df_fold_stats['train_ratio'].mean()), 4),\n","        'avg_test_ratio': round(float(df_fold_stats['test_ratio'].mean()), 4),\n","    },\n","    'validation': {\n","        'no_subject_overlap': True,\n","        'all_subjects_covered': True,\n","        'each_subject_tested_once': True,\n","    },\n","    'anti_leakage_principles': [\n","        'Train and test sets are completely separated by subject',\n","        'Window slicing is performed after splitting to ensure no cross-fold leakage',\n","        'Statistics (mean/std) are computed from the training fold only',\n","        'Feature engineering is performed independently within each fold',\n","        'Hyperparameter tuning uses training-fold data only (nested CV optional)',\n","        'Final model evaluation is strictly based on the corresponding fold’s test set',\n","        'When aggregating results across folds, use metrics from independent test sets',\n","    ],\n","    'notes': [\n","        f'LOSO split: {n_subjects} folds; 1 subject per fold for test',\n","        'Ensure each subject appears exactly once in the test set',\n","        'Train/test sets are mutually exclusive with no subject overlap',\n","        'Suitable for small-sample settings with large inter-subject variability',\n","        'Report mean and standard deviation across all folds',\n","    ]\n","}\n","\n","loso_config_file = configs_dir / \"loso.yaml\"\n","with open(loso_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(loso_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","print(f\"✓ Saved: {loso_config_file}\")\n","\n","loso_config_json = configs_dir / \"loso.json\"\n","with open(loso_config_json, 'w', encoding='utf-8') as f:\n","    json.dump(loso_config, f, indent=2)\n","\n","print(f\"✓ Saved: {loso_config_json}\")\n","\n","# Save per-fold sample stats\n","fold_stats_file = configs_dir / \"loso_fold_stats.csv\"\n","df_fold_stats.to_csv(fold_stats_file, index=False)\n","print(f\"✓ Saved: {fold_stats_file}\")\n","\n","# ========== 5. Generate usage example ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Generate usage example\")\n","print(\"=\"*60)\n","\n","example_code = '''\n","# ========== LOSO Usage Example ==========\n","\n","import json\n","from pathlib import Path\n","\n","# 1. Load splits\n","with open(\"configs/splits.json\", \"r\") as f:\n","    splits = json.load(f)\n","\n","# 2. Iterate over folds\n","for fold_id in range(len(splits)):\n","    print(f\"\\\\n========== Fold {fold_id} ==========\")\n","\n","    # Get current fold split\n","    fold = splits[str(fold_id)]\n","    train_subjects = fold[\"train_subjects\"]\n","    test_subject = fold[\"test_subject\"]\n","\n","    print(f\"Train: {len(train_subjects)} subjects\")\n","    print(f\"Test: {test_subject}\")\n","\n","    # 3. Set environment variable (used by later steps)\n","    import os\n","    os.environ[\"FOLD_ID\"] = str(fold_id)\n","\n","    # 4. Run training pipeline\n","    # - Step 6: per-fold clipping (statistics from train only)\n","    # - Step 7: per-fold standardization (statistics from train only)\n","    # - Step 9: per-fold windowing\n","    # - Train model (training windows only)\n","    # - Evaluate model (test windows only)\n","\n","    # 5. Save results of current fold\n","    # results[fold_id] = {\"accuracy\": acc, \"f1\": f1, ...}\n","\n","# 6. Aggregate results across folds\n","# mean_acc = np.mean([r[\"accuracy\"] for r in results.values()])\n","# std_acc = np.std([r[\"accuracy\"] for r in results.values()])\n","# print(f\"Mean accuracy: {mean_acc:.4f} ± {std_acc:.4f}\")\n","\n","# ========== Anti-leakage Checklist ==========\n","# ✓ Train/test separated by subject\n","# ✓ Statistics (mean/std) computed from training set only\n","# ✓ Feature scaling uses parameters from training set\n","# ✓ Windowing performed after splitting\n","# ✓ Hyperparameter tuning uses training data only\n","# ✓ Test set used strictly for final evaluation\n","'''\n","\n","example_file = configs_dir / \"loso_usage_example.py\"\n","with open(example_file, 'w', encoding='utf-8') as f:\n","    f.write(example_code)\n","\n","print(f\"✓ Generated usage example: {example_file}\")\n","\n","print(\"\\nHow to use:\")\n","print(\"  1. export FOLD_ID=0  # set current fold\")\n","print(\"  2. Run steps 6–9 (they will use the corresponding fold automatically)\")\n","print(\"  3. Train the model and evaluate\")\n","print(\"  4. Repeat steps 1–3 for all folds\")\n","print(\"  5. Aggregate results (mean ± std)\")\n","\n","# ========== 6. Split visualization info ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Split visualization info\")\n","print(\"=\"*60)\n","\n","print(f\"\\nLOSO split matrix (first 5 folds):\")\n","print(f\"{'Fold':<6} {'TestSubject':<12} {'#TrainSubs':<12} {'#TestSamples':<12} {'#TrainSamples':<12}\")\n","print(\"-\" * 60)\n","\n","for i in range(min(5, len(splits))):\n","    fold = splits[str(i)]\n","    stats = df_fold_stats[df_fold_stats['fold_id'] == i].iloc[0]\n","    print(f\"{i:<6} {fold['test_subject']:<12} {fold['n_train']:<12} \"\n","          f\"{stats['n_test_samples']:<12} {stats['n_train_samples']:<12}\")\n","\n","if len(splits) > 5:\n","    print(f\"... (total {len(splits)} folds)\")\n","\n","# ========== 7. Summary ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 10 complete - LOSO split\")\n","print(\"=\"*60)\n","\n","print(f\"\\nSplit strategy:\")\n","print(f\"  Method: LOSO (Leave-One-Subject-Out)\")\n","print(f\"  #folds: {n_subjects}\")\n","print(f\"  #subjects: {n_subjects}\")\n","print(f\"  Train per fold: {n_subjects-1} subjects\")\n","print(f\"  Test per fold: 1 subject\")\n","\n","print(f\"\\nData distribution:\")\n","print(f\"  Total samples: {len(df):,}\")\n","print(f\"  Train ratio (avg): {df_fold_stats['train_ratio'].mean()*100:.2f}%\")\n","print(f\"  Test ratio (avg): {df_fold_stats['test_ratio'].mean()*100:.2f}%\")\n","\n","print(f\"\\nValidation results:\")\n","print(f\"  ✓ No subject overlap\")\n","print(f\"  ✓ All subjects covered\")\n","print(f\"  ✓ Each subject tested exactly once\")\n","print(f\"  ✓ Train/test sets are disjoint\")\n","\n","print(f\"\\nOutput files:\")\n","print(f\"  Main config: {splits_file}\")\n","print(f\"  Detailed config: {loso_config_file}\")\n","print(f\"  Fold stats: {fold_stats_file}\")\n","print(f\"  Usage example: {example_file}\")\n","\n","print(\"\\nAnti-leakage principles:\")\n","print(\"  1. ✓ Fully separated by subject\")\n","print(\"  2. ✓ Statistics computed from training fold only\")\n","print(\"  3. ✓ Feature engineering is fold-internal\")\n","print(\"  4. ✓ Window slicing performed after splitting\")\n","print(\"  5. ✓ Hyperparameter tuning limited to training data\")\n","print(\"  6. ✓ Test set used strictly for independent evaluation\")\n","print(\"  7. ✓ Cross-fold aggregation uses independent metrics\")\n","\n","print(\"\\nNext steps:\")\n","print(\"  - Set export FOLD_ID=<fold_id>\")\n","print(\"  - Re-run steps 6–9 (per-fold processing)\")\n","print(\"  - Train and evaluate models\")\n","print(\"  - Iterate all folds and aggregate results\")\n","\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F9ASPTqdXaFb","executionInfo":{"status":"ok","timestamp":1763130380761,"user_tz":0,"elapsed":374,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"65386e44-e24a-4c64-d15f-44b85042553b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 10: LOSO split\n","============================================================\n","\n","============================================================\n","1. Load data and get subject list\n","============================================================\n","Loading data: data/lara/mbientlab/proc/labeled/\n","Data shape: (556504, 16)\n","Total samples: 556,504\n","\n","Subject list:\n","  Total: 8 subjects\n","  IDs: ['S07', 'S08', 'S09', 'S10', 'S11', 'S12', 'S13', 'S14']\n","\n","Sample count per subject:\n","  S07:   76,522 samples (13.75%)\n","  S08:   64,857 samples (11.65%)\n","  S09:   77,701 samples (13.96%)\n","  S10:   82,659 samples (14.85%)\n","  S11:   70,410 samples (12.65%)\n","  S12:   30,923 samples ( 5.56%)\n","  S13:   82,335 samples (14.80%)\n","  S14:   71,097 samples (12.78%)\n","\n","============================================================\n","2. Generate LOSO split\n","============================================================\n","\n","LOSO strategy: Leave-One-Subject-Out\n","  #folds = #subjects = 8\n","  Per fold: 1 subject for test, 7 subjects for train\n","  Fold 0: test S07, train 7 subjects\n","  Fold 1: test S08, train 7 subjects\n","  Fold 2: test S09, train 7 subjects\n","  Fold 3: test S10, train 7 subjects\n","  Fold 4: test S11, train 7 subjects\n","  Fold 5: test S12, train 7 subjects\n","  Fold 6: test S13, train 7 subjects\n","  Fold 7: test S14, train 7 subjects\n","\n","✓ Generated 8 LOSO folds\n","\n","============================================================\n","3. Validate split integrity\n","============================================================\n","\n","Check 1: times each subject appears as test\n","  ✓ S07: 1 time(s)\n","  ✓ S08: 1 time(s)\n","  ✓ S09: 1 time(s)\n","  ✓ S10: 1 time(s)\n","  ✓ S11: 1 time(s)\n","  ✓ S12: 1 time(s)\n","  ✓ S13: 1 time(s)\n","  ✓ S14: 1 time(s)\n","  ✓ All subjects appear exactly once\n","\n","Check 2: train and test sets are disjoint\n","  ✓ Train/test sets are completely disjoint for all folds\n","\n","Check 3: all subjects covered\n","  ✓ All subjects are covered; no missing or extra subjects\n","\n","Check 4: per-fold sample counts\n","\n","Per-fold sample distribution:\n"," fold_id test_subject  n_train_samples  n_test_samples  train_ratio  test_ratio\n","       0          S07           479982           76522       0.8625      0.1375\n","       1          S08           491647           64857       0.8835      0.1165\n","       2          S09           478803           77701       0.8604      0.1396\n","       3          S10           473845           82659       0.8515      0.1485\n","       4          S11           486094           70410       0.8735      0.1265\n","       5          S12           525581           30923       0.9444      0.0556\n","       6          S13           474169           82335       0.8520      0.1480\n","       7          S14           485407           71097       0.8722      0.1278\n","\n","Sample distribution summary:\n","  Train sample count: 473,845 ~ 525,581\n","  Test sample count: 30,923 ~ 82,659\n","  Average train ratio: 87.50%\n","  Average test ratio: 12.50%\n","\n","✓ All validations passed\n","\n","============================================================\n","4. Save split configuration\n","============================================================\n","✓ Saved: configs/splits.json\n","✓ Saved: configs/loso.yaml\n","✓ Saved: configs/loso.json\n","✓ Saved: configs/loso_fold_stats.csv\n","\n","============================================================\n","5. Generate usage example\n","============================================================\n","✓ Generated usage example: configs/loso_usage_example.py\n","\n","How to use:\n","  1. export FOLD_ID=0  # set current fold\n","  2. Run steps 6–9 (they will use the corresponding fold automatically)\n","  3. Train the model and evaluate\n","  4. Repeat steps 1–3 for all folds\n","  5. Aggregate results (mean ± std)\n","\n","============================================================\n","6. Split visualization info\n","============================================================\n","\n","LOSO split matrix (first 5 folds):\n","Fold   TestSubject  #TrainSubs   #TestSamples #TrainSamples\n","------------------------------------------------------------\n","0      S07          7            76522        479982      \n","1      S08          7            64857        491647      \n","2      S09          7            77701        478803      \n","3      S10          7            82659        473845      \n","4      S11          7            70410        486094      \n","... (total 8 folds)\n","\n","============================================================\n","Step 10 complete - LOSO split\n","============================================================\n","\n","Split strategy:\n","  Method: LOSO (Leave-One-Subject-Out)\n","  #folds: 8\n","  #subjects: 8\n","  Train per fold: 7 subjects\n","  Test per fold: 1 subject\n","\n","Data distribution:\n","  Total samples: 556,504\n","  Train ratio (avg): 87.50%\n","  Test ratio (avg): 12.50%\n","\n","Validation results:\n","  ✓ No subject overlap\n","  ✓ All subjects covered\n","  ✓ Each subject tested exactly once\n","  ✓ Train/test sets are disjoint\n","\n","Output files:\n","  Main config: configs/splits.json\n","  Detailed config: configs/loso.yaml\n","  Fold stats: configs/loso_fold_stats.csv\n","  Usage example: configs/loso_usage_example.py\n","\n","Anti-leakage principles:\n","  1. ✓ Fully separated by subject\n","  2. ✓ Statistics computed from training fold only\n","  3. ✓ Feature engineering is fold-internal\n","  4. ✓ Window slicing performed after splitting\n","  5. ✓ Hyperparameter tuning limited to training data\n","  6. ✓ Test set used strictly for independent evaluation\n","  7. ✓ Cross-fold aggregation uses independent metrics\n","\n","Next steps:\n","  - Set export FOLD_ID=<fold_id>\n","  - Re-run steps 6–9 (per-fold processing)\n","  - Train and evaluate models\n","  - Iterate all folds and aggregate results\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 11: Feature Extraction / Input Preparation (top-tier conference level)\n","Generate three formats in a single pass: MiniRocket, MultiRocket, Tensor\n","\"\"\"\n","\n","import os\n","import sys\n","import numpy as np\n","from pathlib import Path\n","import json\n","import hashlib\n","import platform\n","import warnings\n","warnings.filterwarnings('ignore', category=UserWarning, message='.*NumPy.*')\n","warnings.filterwarnings('ignore', message='.*Failed to initialize NumPy.*')\n","\n","# ========== Dependency check ==========\n","def check_package(package_name, import_name=None):\n","    \"\"\"Check whether a Python package is installed\"\"\"\n","    if import_name is None:\n","        import_name = package_name\n","    try:\n","        __import__(import_name)\n","        return True\n","    except ImportError:\n","        return False\n","\n","# ========== Manually set FOLD_ID ==========\n","if \"FOLD_ID\" not in os.environ:\n","    print(\"⚠️ Environment variable FOLD_ID is not set; please specify it manually:\")\n","    fold_input = input(\"Please enter FOLD_ID (press Enter to use 0 by default): \").strip()\n","    os.environ[\"FOLD_ID\"] = fold_input if fold_input else \"0\"\n","    print(f\"✓ FOLD_ID set to {os.environ['FOLD_ID']}\")\n","\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","SEED = int(os.environ.get(\"SEED\", \"0\"))\n","\n","print(\"=\"*60)\n","print(\"Step 11: Feature extraction / input preparation (generate all formats in one pass)\")\n","print(\"=\"*60)\n","\n","# ========== Check dependencies ==========\n","print(\"\\nChecking dependencies...\")\n","has_sktime = check_package(\"sktime\")\n","has_torch = check_package(\"torch\")\n","\n","if not has_sktime:\n","    print(\"⚠️ sktime is not installed; MiniRocket/MultiRocket will be skipped\")\n","else:\n","    print(\"✓ sktime is installed\")\n","\n","if not has_torch:\n","    print(\"⚠️ torch is not installed; the Tensor representation will be skipped\")\n","else:\n","    print(\"✓ torch is installed\")\n","\n","if not has_sktime and not has_torch:\n","    raise RuntimeError(\"Neither sktime nor torch is installed; cannot proceed.\")\n","\n","# ========== Path configuration ==========\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","fold_tag = f\"fold_{FOLD_ID:02d}\" if FOLD_ID >= 0 else \"all\"\n","windows_dir = proc_dir / \"windows\" / fold_tag\n","\n","print(f\"\\nFold identifier: {fold_tag}\")\n","print(f\"Random seed: {SEED}\")\n","print(f\"Loading windowed data from: {windows_dir}/\")\n","\n","# ========== 1. Load windowed data ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Load windowed data\")\n","print(\"=\"*60)\n","\n","X_train_file = windows_dir / \"X_train.npy\"\n","X_test_file = windows_dir / \"X_test.npy\"\n","y_train_file = windows_dir / \"y_train.npy\"\n","y_test_file = windows_dir / \"y_test.npy\"\n","\n","for f in [X_train_file, X_test_file, y_train_file, y_test_file]:\n","    if not f.exists():\n","        raise FileNotFoundError(f\"File not found: {f}\")\n","\n","X_train = np.load(X_train_file)\n","X_test = np.load(X_test_file)\n","y_train = np.load(y_train_file)\n","y_test = np.load(y_test_file)\n","\n","print(f\"\\nTraining set:\")\n","print(f\"  X_train: {X_train.shape}, dtype={X_train.dtype}\")\n","print(f\"  y_train: {y_train.shape}, dtype={y_train.dtype}\")\n","print(f\"\\nTest set:\")\n","print(f\"  X_test: {X_test.shape}, dtype={X_test.dtype}\")\n","print(f\"  y_test: {y_test.shape}, dtype={y_test.dtype}\")\n","\n","n_train, window_size, n_channels = X_train.shape\n","n_test = X_test.shape[0]\n","n_classes = len(np.unique(y_train))\n","\n","print(f\"\\nData summary:\")\n","print(f\"  Number of training windows: {n_train:,}\")\n","print(f\"  Number of test windows: {n_test:,}\")\n","print(f\"  Window length: {window_size} time steps\")\n","print(f\"  Number of channels: {n_channels}\")\n","print(f\"  Number of classes: {n_classes} (estimated from training set only)\")\n","\n","# ========== 2. Collect version information ==========\n","versions = {\n","    \"python\": platform.python_version(),\n","    \"numpy\": np.__version__,\n","}\n","\n","if has_sktime:\n","    import sktime\n","    versions[\"sktime\"] = sktime.__version__\n","if has_torch:\n","    import torch\n","    versions[\"torch\"] = torch.__version__\n","\n","print(f\"\\nEnvironment versions:\")\n","print(f\"  Python: {versions['python']}\")\n","print(f\"  NumPy: {versions['numpy']}\")\n","if \"sktime\" in versions:\n","    print(f\"  sktime: {versions['sktime']}\")\n","if \"torch\" in versions:\n","    print(f\"  PyTorch: {versions['torch']}\")\n","\n","# ========== 3. Process all representations ==========\n","results = {}\n","\n","# ========== 3.1 MiniRocket ==========\n","if has_sktime:\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"2. MiniRocketMultivariate\")\n","    print(\"=\"*60)\n","\n","    from sktime.transformations.panel.rocket import MiniRocketMultivariate\n","\n","    print(f\"Initialising transformer (seed={SEED}, n_jobs={os.cpu_count()})...\")\n","    transformer = MiniRocketMultivariate(random_state=SEED, n_jobs=os.cpu_count())\n","\n","    print(f\"Data layout: (N, L, C) -> (N, C, L), float32 -> float64\")\n","    X_train_sktime = X_train.transpose(0, 2, 1).astype('float64')\n","    X_test_sktime = X_test.transpose(0, 2, 1).astype('float64')\n","\n","    print(\"Fitting and transforming...\")\n","    features_train = transformer.fit_transform(X_train_sktime)\n","    features_test = transformer.transform(X_test_sktime)\n","\n","    if hasattr(features_train, 'values'):\n","        features_train = features_train.values\n","    if hasattr(features_test, 'values'):\n","        features_test = features_test.values\n","\n","    print(f\"✓ Feature shape: {features_train.shape}\")\n","\n","    output_dir = proc_dir / \"features\" / fold_tag\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","\n","    train_file = output_dir / \"minirocket_train.npz\"\n","    test_file = output_dir / \"minirocket_test.npz\"\n","\n","    np.savez_compressed(train_file, X=features_train.astype('float32'), y=y_train)\n","    np.savez_compressed(test_file, X=features_test.astype('float32'), y=y_test)\n","\n","    results[\"minirocket\"] = {\n","        \"feature_dim\": int(features_train.shape[1]),\n","        \"train_file\": str(train_file),\n","        \"test_file\": str(test_file),\n","        \"train_sha256\": hashlib.sha256(open(train_file, 'rb').read()).hexdigest(),\n","        \"test_sha256\": hashlib.sha256(open(test_file, 'rb').read()).hexdigest(),\n","    }\n","\n","    print(f\"✓ Saved: {train_file}\")\n","    print(f\"✓ Saved: {test_file}\")\n","\n","# ========== 3.2 MultiRocket ==========\n","if has_sktime:\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"3. MultiRocketMultivariate\")\n","    print(\"=\"*60)\n","\n","    from sktime.transformations.panel.rocket import MultiRocketMultivariate\n","\n","    print(f\"Initialising transformer (seed={SEED}, n_jobs={os.cpu_count()})...\")\n","    transformer = MultiRocketMultivariate(random_state=SEED, n_jobs=os.cpu_count())\n","\n","    print(f\"Data layout: (N, L, C) -> (N, C, L), float32 -> float64\")\n","    X_train_sktime = X_train.transpose(0, 2, 1).astype('float64')\n","    X_test_sktime = X_test.transpose(0, 2, 1).astype('float64')\n","\n","    print(\"Fitting and transforming (this may take some time)...\")\n","    features_train = transformer.fit_transform(X_train_sktime)\n","    features_test = transformer.transform(X_test_sktime)\n","\n","    if hasattr(features_train, 'values'):\n","        features_train = features_train.values\n","    if hasattr(features_test, 'values'):\n","        features_test = features_test.values\n","\n","    print(f\"✓ Feature shape: {features_train.shape}\")\n","\n","    output_dir = proc_dir / \"features\" / fold_tag\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","\n","    train_file = output_dir / \"multirocket_train.npz\"\n","    test_file = output_dir / \"multirocket_test.npz\"\n","\n","    np.savez_compressed(train_file, X=features_train.astype('float32'), y=y_train)\n","    np.savez_compressed(test_file, X=features_test.astype('float32'), y=y_test)\n","\n","    results[\"multirocket\"] = {\n","        \"feature_dim\": int(features_train.shape[1]),\n","        \"train_file\": str(train_file),\n","        \"test_file\": str(test_file),\n","        \"train_sha256\": hashlib.sha256(open(train_file, 'rb').read()).hexdigest(),\n","        \"test_sha256\": hashlib.sha256(open(test_file, 'rb').read()).hexdigest(),\n","    }\n","\n","    print(f\"✓ Saved: {train_file}\")\n","    print(f\"✓ Saved: {test_file}\")\n","\n","# ========== 3.3 Tensor ==========\n","if has_torch:\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"4. Tensor (N, C, L)\")\n","    print(\"=\"*60)\n","\n","    import torch\n","\n","    print(\"Converting to PyTorch tensors (N, C, L)...\")\n","    X_train_tensor = torch.tensor(X_train.transpose(0, 2, 1), dtype=torch.float32)\n","    X_test_tensor = torch.tensor(X_test.transpose(0, 2, 1), dtype=torch.float32)\n","    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n","    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n","\n","    print(f\"✓ Tensor shape: {tuple(X_train_tensor.shape)}\")\n","\n","    output_dir = proc_dir / \"tensors\" / fold_tag\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","\n","    train_file = output_dir / \"train.pt\"\n","    test_file = output_dir / \"test.pt\"\n","\n","    torch.save({'X': X_train_tensor, 'y': y_train_tensor}, train_file)\n","    torch.save({'X': X_test_tensor, 'y': y_test_tensor}, test_file)\n","\n","    results[\"tensor\"] = {\n","        \"tensor_shape\": list(X_train_tensor.shape),\n","        \"train_file\": str(train_file),\n","        \"test_file\": str(test_file),\n","        \"train_sha256\": hashlib.sha256(open(train_file, 'rb').read()).hexdigest(),\n","        \"test_sha256\": hashlib.sha256(open(test_file, 'rb').read()).hexdigest(),\n","    }\n","\n","    print(f\"✓ Saved: {train_file}\")\n","    print(f\"✓ Saved: {test_file}\")\n","\n","# ========== 4. Save unified configuration ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Save configuration\")\n","print(\"=\"*60)\n","\n","config = {\n","    \"fold_id\": FOLD_ID,\n","    \"fold_tag\": fold_tag,\n","    \"seed\": SEED,\n","    \"versions\": versions,\n","    \"input_shape\": {\n","        \"train\": list(X_train.shape),\n","        \"test\": list(X_test.shape)\n","    },\n","    \"n_classes\": n_classes,\n","    \"methods\": results,\n","    \"notes\": [\n","        \"Generate three feature/input formats in a single pass\",\n","        \"MiniRocket: multivariate variant with fixed-dimensional features\",\n","        \"MultiRocket: multivariate variant with high-dimensional features\",\n","        \"Tensor: (N, C, L) layout, suitable for TST/Transformer models\",\n","        f\"Random seed: {SEED}\",\n","        \"Number of classes estimated from the training set only\",\n","        \"All files include SHA-256 fingerprints\",\n","    ]\n","}\n","\n","configs_dir = Path(\"configs\")\n","configs_dir.mkdir(parents=True, exist_ok=True)\n","\n","config_file = configs_dir / f\"features_all_fold_{FOLD_ID:02d}.json\"\n","with open(config_file, \"w\") as f:\n","    json.dump(config, f, indent=2)\n","\n","print(f\"✓ Configuration saved: {config_file}\")\n","\n","# ========== 5. Final summary ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 11 completed - all feature representations have been generated\")\n","print(\"=\"*60)\n","\n","print(f\"\\nFold identifier: {fold_tag}\")\n","print(f\"Random seed: {SEED}\")\n","\n","print(f\"\\nInput:\")\n","print(f\"  Training windows: {n_train:,} × ({window_size}, {n_channels})\")\n","print(f\"  Test windows: {n_test:,} × ({window_size}, {n_channels})\")\n","\n","print(f\"\\nGenerated representations:\")\n","for method, info in results.items():\n","    if method in [\"minirocket\", \"multirocket\"]:\n","        print(f\"  {method.upper()}: feature dimension {info['feature_dim']}\")\n","    else:\n","        print(f\"  {method.upper()}: tensor shape {info['tensor_shape']}\")\n","\n","print(f\"\\nOutput files:\")\n","for method, info in results.items():\n","    print(f\"  {method}:\")\n","    print(f\"    {info['train_file']}\")\n","    print(f\"    {info['test_file']}\")\n","\n","print(f\"\\nConfiguration: {config_file}\")\n","\n","print(\"\\nKey properties:\")\n","print(\"  ✓ Generate all three formats in a single pass\")\n","print(\"  ✓ Multivariate Rocket variants\")\n","print(\"  ✓ Tensor representation (N, C, L)\")\n","print(\"  ✓ Parallel acceleration\")\n","print(\"  ✓ SHA-256 fingerprints\")\n","print(\"  ✓ Version tracking\")\n","\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J56kBbxHfkpA","executionInfo":{"status":"ok","timestamp":1763130491580,"user_tz":0,"elapsed":110821,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"d5a17251-f88d-4940-8ef6-67004c2d7517"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 11: Feature extraction / input preparation (generate all formats in one pass)\n","============================================================\n","\n","Checking dependencies...\n","✓ sktime is installed\n","✓ torch is installed\n","\n","Fold identifier: fold_05\n","Random seed: 0\n","Loading windowed data from: data/lara/mbientlab/proc/windows/fold_05/\n","\n","============================================================\n","1. Load windowed data\n","============================================================\n","\n","Training set:\n","  X_train: (5442, 150, 8), dtype=float32\n","  y_train: (5442,), dtype=int32\n","\n","Test set:\n","  X_test: (289, 150, 8), dtype=float32\n","  y_test: (289,), dtype=int32\n","\n","Data summary:\n","  Number of training windows: 5,442\n","  Number of test windows: 289\n","  Window length: 150 time steps\n","  Number of channels: 8\n","  Number of classes: 6 (estimated from training set only)\n","\n","Environment versions:\n","  Python: 3.12.12\n","  NumPy: 2.0.2\n","  sktime: 0.39.0\n","  PyTorch: 2.8.0+cu126\n","\n","============================================================\n","2. MiniRocketMultivariate\n","============================================================\n","Initialising transformer (seed=0, n_jobs=12)...\n","Data layout: (N, L, C) -> (N, C, L), float32 -> float64\n","Fitting and transforming...\n","✓ Feature shape: (5442, 9996)\n","✓ Saved: data/lara/mbientlab/proc/features/fold_05/minirocket_train.npz\n","✓ Saved: data/lara/mbientlab/proc/features/fold_05/minirocket_test.npz\n","\n","============================================================\n","3. MultiRocketMultivariate\n","============================================================\n","Initialising transformer (seed=0, n_jobs=12)...\n","Data layout: (N, L, C) -> (N, C, L), float32 -> float64\n","Fitting and transforming (this may take some time)...\n","✓ Feature shape: (5442, 49728)\n","✓ Saved: data/lara/mbientlab/proc/features/fold_05/multirocket_train.npz\n","✓ Saved: data/lara/mbientlab/proc/features/fold_05/multirocket_test.npz\n","\n","============================================================\n","4. Tensor (N, C, L)\n","============================================================\n","Converting to PyTorch tensors (N, C, L)...\n","✓ Tensor shape: (5442, 8, 150)\n","✓ Saved: data/lara/mbientlab/proc/tensors/fold_05/train.pt\n","✓ Saved: data/lara/mbientlab/proc/tensors/fold_05/test.pt\n","\n","============================================================\n","5. Save configuration\n","============================================================\n","✓ Configuration saved: configs/features_all_fold_05.json\n","\n","============================================================\n","Step 11 completed - all feature representations have been generated\n","============================================================\n","\n","Fold identifier: fold_05\n","Random seed: 0\n","\n","Input:\n","  Training windows: 5,442 × (150, 8)\n","  Test windows: 289 × (150, 8)\n","\n","Generated representations:\n","  MINIROCKET: feature dimension 9996\n","  MULTIROCKET: feature dimension 49728\n","  TENSOR: tensor shape [5442, 8, 150]\n","\n","Output files:\n","  minirocket:\n","    data/lara/mbientlab/proc/features/fold_05/minirocket_train.npz\n","    data/lara/mbientlab/proc/features/fold_05/minirocket_test.npz\n","  multirocket:\n","    data/lara/mbientlab/proc/features/fold_05/multirocket_train.npz\n","    data/lara/mbientlab/proc/features/fold_05/multirocket_test.npz\n","  tensor:\n","    data/lara/mbientlab/proc/tensors/fold_05/train.pt\n","    data/lara/mbientlab/proc/tensors/fold_05/test.pt\n","\n","Configuration: configs/features_all_fold_05.json\n","\n","Key properties:\n","  ✓ Generate all three formats in a single pass\n","  ✓ Multivariate Rocket variants\n","  ✓ Tensor representation (N, C, L)\n","  ✓ Parallel acceleration\n","  ✓ SHA-256 fingerprints\n","  ✓ Version tracking\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 12: Verification of Feature / Input Normalization (top-tier conference level)\n","Validation objective: ensure MiniRocket/MultiRocket features are not re-normalized;\n","Tensor inputs were already normalized in Step 7.\n","\"\"\"\n","\n","import os\n","import numpy as np\n","from pathlib import Path\n","import json\n","\n","# ========== Helper functions ==========\n","def assert_finite(name, arr):\n","    \"\"\"Check whether the array contains any NaN/Inf\"\"\"\n","    if not np.isfinite(arr).all():\n","        raise ValueError(f\"{name} contains NaN/Inf; please trace back to Step 7/11 for diagnosis\")\n","\n","def fraction_near_zscore(X, m=0.02, s=0.05):\n","    \"\"\"Check whether features were incorrectly z-score normalized\"\"\"\n","    col_mean = X.mean(axis=0)\n","    col_std = X.std(axis=0)\n","    return float(np.mean((np.abs(col_mean) <= m) & (np.abs(col_std - 1) <= s)))\n","\n","# ========== Manually set FOLD_ID ==========\n","if \"FOLD_ID\" not in os.environ:\n","    print(\"⚠️ Environment variable FOLD_ID is not set; please specify it manually:\")\n","    fold_input = input(\"Please enter FOLD_ID (press Enter to use 0 by default): \").strip()\n","    os.environ[\"FOLD_ID\"] = fold_input if fold_input else \"0\"\n","    print(f\"✓ FOLD_ID has been set to {os.environ['FOLD_ID']}\")\n","\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","\n","print(\"=\"*60)\n","print(\"Step 12: Verification of feature / input normalization\")\n","print(\"=\"*60)\n","\n","# ========== Path configuration ==========\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","fold_tag = f\"fold_{FOLD_ID:02d}\" if FOLD_ID >= 0 else \"all\"\n","\n","print(f\"\\nFold identifier: {fold_tag}\")\n","\n","# ========== 1. Check and verify features ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Check feature / input files\")\n","print(\"=\"*60)\n","\n","results = {}\n","\n","# ========== 1.1 Check MiniRocket ==========\n","minirocket_dir = proc_dir / \"features\" / fold_tag\n","minirocket_train = minirocket_dir / \"minirocket_train.npz\"\n","minirocket_test = minirocket_dir / \"minirocket_test.npz\"\n","\n","if minirocket_train.exists() and minirocket_test.exists():\n","    print(\"\\n✓ MiniRocket features:\")\n","\n","    # Safe loading\n","    data_train = np.load(minirocket_train, allow_pickle=False)\n","    data_test = np.load(minirocket_test, allow_pickle=False)\n","\n","    X_train = data_train['X']\n","    X_test = data_test['X']\n","    y_train_mr = data_train['y']\n","    y_test_mr = data_test['y']\n","\n","    # Hard check 1: feature dimensionality must match\n","    assert X_train.shape[1] == X_test.shape[1], \"MiniRocket feature dimensionality mismatch between train and test\"\n","\n","    # Hard check 2: X/y sample counts must align\n","    assert X_train.shape[0] == y_train_mr.shape[0], \"MiniRocket training set X/y sample counts are inconsistent\"\n","    assert X_test.shape[0] == y_test_mr.shape[0], \"MiniRocket test set X/y sample counts are inconsistent\"\n","\n","    # NaN/Inf check\n","    assert_finite(\"MiniRocket_train\", X_train)\n","    assert_finite(\"MiniRocket_test\", X_test)\n","\n","    print(f\"  Train: {X_train.shape}, dtype={X_train.dtype}\")\n","    print(f\"  Test: {X_test.shape}, dtype={X_test.dtype}\")\n","    print(f\"  Summary (train): mean={X_train.mean():.4f}, std={X_train.std():.4f}\")\n","\n","    # Check for mistaken re-normalization\n","    frac = fraction_near_zscore(X_train)\n","    if frac > 0.8:\n","        print(f\"  ⚠️ Approximately {frac:.0%} of feature columns appear close to N(0,1); \"\n","              f\"please verify that no unintended second normalization has been applied\")\n","\n","    results[\"minirocket\"] = {\n","        \"status\": \"pass_through\",\n","        \"train_shape\": list(X_train.shape),\n","        \"test_shape\": list(X_test.shape),\n","        \"train_mean\": float(X_train.mean()),\n","        \"train_std\": float(X_train.std()),\n","        \"frac_columns_near_zscore\": frac,\n","        \"note\": \"MiniRocket features are not re-normalized and are passed through as-is\"\n","    }\n","else:\n","    print(\"\\n⚠️ MiniRocket feature files not found; skipping\")\n","\n","# ========== 1.2 Check MultiRocket ==========\n","multirocket_train = minirocket_dir / \"multirocket_train.npz\"\n","multirocket_test = minirocket_dir / \"multirocket_test.npz\"\n","\n","if multirocket_train.exists() and multirocket_test.exists():\n","    print(\"\\n✓ MultiRocket features:\")\n","\n","    # Safe loading\n","    data_train = np.load(multirocket_train, allow_pickle=False)\n","    data_test = np.load(multirocket_test, allow_pickle=False)\n","\n","    X_train = data_train['X']\n","    X_test = data_test['X']\n","    y_train_mr = data_train['y']\n","    y_test_mr = data_test['y']\n","\n","    # Hard check 1: feature dimensionality must match\n","    assert X_train.shape[1] == X_test.shape[1], \"MultiRocket feature dimensionality mismatch between train and test\"\n","\n","    # Hard check 2: X/y sample counts must align\n","    assert X_train.shape[0] == y_train_mr.shape[0], \"MultiRocket training set X/y sample counts are inconsistent\"\n","    assert X_test.shape[0] == y_test_mr.shape[0], \"MultiRocket test set X/y sample counts are inconsistent\"\n","\n","    # NaN/Inf check\n","    assert_finite(\"MultiRocket_train\", X_train)\n","    assert_finite(\"MultiRocket_test\", X_test)\n","\n","    print(f\"  Train: {X_train.shape}, dtype={X_train.dtype}\")\n","    print(f\"  Test: {X_test.shape}, dtype={X_test.dtype}\")\n","    print(f\"  Summary (train): mean={X_train.mean():.4f}, std={X_train.std():.4f}\")\n","\n","    # Check for mistaken re-normalization\n","    frac = fraction_near_zscore(X_train)\n","    if frac > 0.8:\n","        print(f\"  ⚠️ Approximately {frac:.0%} of feature columns appear close to N(0,1); \"\n","              f\"please verify that no unintended second normalization has been applied\")\n","\n","    results[\"multirocket\"] = {\n","        \"status\": \"pass_through\",\n","        \"train_shape\": list(X_train.shape),\n","        \"test_shape\": list(X_test.shape),\n","        \"train_mean\": float(X_train.mean()),\n","        \"train_std\": float(X_train.std()),\n","        \"frac_columns_near_zscore\": frac,\n","        \"note\": \"MultiRocket features are not re-normalized and are passed through as-is\"\n","    }\n","else:\n","    print(\"\\n⚠️ MultiRocket feature files not found; skipping\")\n","\n","# ========== 1.3 Check Tensor ==========\n","tensor_dir = proc_dir / \"tensors\" / fold_tag\n","tensor_train = tensor_dir / \"train.pt\"\n","tensor_test = tensor_dir / \"test.pt\"\n","\n","if tensor_train.exists() and tensor_test.exists():\n","    try:\n","        import torch\n","        print(\"\\n✓ Tensor inputs:\")\n","\n","        # Pin to CPU to avoid environment-specific differences\n","        data_train = torch.load(tensor_train, map_location=\"cpu\")\n","        data_test = torch.load(tensor_test, map_location=\"cpu\")\n","\n","        X_train = data_train['X']\n","        X_test = data_test['X']\n","        y_train_ts = data_train['y']\n","        y_test_ts = data_test['y']\n","\n","        # Hard check 1: n_channels (C) must match\n","        assert X_train.shape[1] == X_test.shape[1], \"Tensor n_channels (C) mismatch between train and test\"\n","\n","        # Hard check 2: window_size / timepoints (L) must match\n","        assert X_train.shape[2] == X_test.shape[2], \"Tensor window_size / timepoints (L) mismatch between train and test\"\n","\n","        # Hard check 3: X/y sample counts must align\n","        assert X_train.shape[0] == y_train_ts.shape[0], \"Tensor training set X/y sample counts are inconsistent\"\n","        assert X_test.shape[0] == y_test_ts.shape[0], \"Tensor test set X/y sample counts are inconsistent\"\n","\n","        # NaN/Inf check\n","        if not torch.isfinite(X_train).all():\n","            raise ValueError(\"Tensor_train contains NaN/Inf; please trace back to Step 7/11 for diagnosis\")\n","        if not torch.isfinite(X_test).all():\n","            raise ValueError(\"Tensor_test contains NaN/Inf; please trace back to Step 7/11 for diagnosis\")\n","\n","        print(f\"  Train: {tuple(X_train.shape)} (N,C,L), dtype={X_train.dtype}\")\n","        print(f\"  Test: {tuple(X_test.shape)} (N,C,L), dtype={X_test.dtype}\")\n","\n","        # Global statistics (note: use .item())\n","        mean_train = X_train.float().mean().item()\n","        std_train = X_train.float().std(unbiased=False).item()\n","        print(f\"  Summary (train): mean={mean_train:.4f}, std={std_train:.4f}\")\n","\n","        # Channel-wise verification (recommended check)\n","        ch_mean = X_train.float().mean(dim=(0, 2))  # shape [C]\n","        ch_std = X_train.float().std(dim=(0, 2), unbiased=False)  # shape [C]\n","\n","        eps_mean, eps_std = 0.02, 0.05\n","        ok_mean = bool((ch_mean.abs() <= eps_mean).all())\n","        ok_std = bool((ch_std.sub(1).abs() <= eps_std).all())\n","\n","        max_mean_dev = ch_mean.abs().max().item()\n","        max_std_dev = (ch_std - 1).abs().max().item()\n","\n","        print(f\"  Channel-level verification: mean<=±{eps_mean}? {ok_mean} \"\n","              f\"(max deviation={max_mean_dev:.4f})\")\n","        print(f\"               std≈1±{eps_std}? {ok_std} \"\n","              f\"(max deviation={max_std_dev:.4f})\")\n","\n","        results[\"tensor\"] = {\n","            \"status\": \"already_normalized\",\n","            \"train_shape\": list(X_train.shape),\n","            \"test_shape\": list(X_test.shape),\n","            \"train_mean\": mean_train,\n","            \"train_std\": std_train,\n","            \"per_channel_mean_max_abs\": max_mean_dev,\n","            \"per_channel_std_max_dev\": max_std_dev,\n","            \"channel_mean_ok\": ok_mean,\n","            \"channel_std_ok\": ok_std,\n","            \"note\": \"Tensor inputs were already channel-wise z-score normalized in Step 7; no additional processing required\"\n","        }\n","    except ImportError:\n","        print(\"\\n⚠️ PyTorch is not installed; cannot validate Tensor inputs\")\n","else:\n","    print(\"\\n⚠️ Tensor inputs not found; skipping\")\n","\n","# ========== 2. Cross-check metadata from Step 11 ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Cross-check metadata from Step 11\")\n","print(\"=\"*60)\n","\n","step11_meta = {}\n","meta_path = proc_dir / \"features\" / fold_tag / \"features_meta.json\"\n","\n","if meta_path.exists():\n","    with open(meta_path, \"r\") as f:\n","        meta_11 = json.load(f)\n","\n","    step11_meta = {\n","        \"method\": meta_11.get(\"method\"),\n","        \"transformer_class\": meta_11.get(\"transformer_class\"),\n","        \"feature_dim\": meta_11.get(\"feature_dim\"),\n","        \"tensor_format\": meta_11.get(\"tensor_format\"),\n","        \"versions\": meta_11.get(\"versions\"),\n","        \"train_sha256\": meta_11.get(\"train_sha256\"),\n","        \"test_sha256\": meta_11.get(\"test_sha256\"),\n","    }\n","\n","    print(\"✓ Metadata from Step 11 successfully loaded:\")\n","    print(f\"  Method: {step11_meta.get('method')}\")\n","    print(f\"  Transformer: {step11_meta.get('transformer_class')}\")\n","    print(f\"  Feature dimension: {step11_meta.get('feature_dim')}\")\n","    print(f\"  Versions: {step11_meta.get('versions')}\")\n","else:\n","    print(\"⚠️ Step 11 metadata not found (possibly generated by an older pipeline version)\")\n","\n","# ========== 3. Confirmation of normalization strategy ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Confirmation of normalization strategy\")\n","print(\"=\"*60)\n","\n","print(\"\\nNormalization principles:\")\n","print(\"  ✓ MiniRocket/MultiRocket: features are extracted from Step 7–normalized windows; no second normalization\")\n","print(\"  ✓ Tensor: already channel-wise z-score normalized in Step 7; no further preprocessing\")\n","print(\"  ✓ All inputs preserve the normalization established in Step 7\")\n","\n","# ========== 4. Save confirmation configuration ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Save confirmation configuration\")\n","print(\"=\"*60)\n","\n","config = {\n","    \"fold_id\": FOLD_ID,\n","    \"fold_tag\": fold_tag,\n","    \"strategy\": \"no_additional_normalization\",\n","    \"results\": results,\n","    \"step11_meta\": step11_meta,\n","    \"normalization_policy\": {\n","        \"minirocket\": \"no second normalization; pass through as-is\",\n","        \"multirocket\": \"no second normalization; pass through as-is\",\n","        \"tensor\": \"already normalized in Step 7; no additional processing\"\n","    },\n","    \"validation\": {\n","        \"shape_consistency\": \"✓ checked\",\n","        \"xy_alignment\": \"✓ checked\",\n","        \"nan_inf_check\": \"✓ checked\",\n","        \"mistaken_normalization\": \"✓ checked\",\n","        \"channel_level_zscore\": \"✓ verified (Tensor)\"\n","    },\n","    \"notes\": [\n","        \"This step is a purely confirmatory no-op and does not modify any data\",\n","        \"MiniRocket/MultiRocket features are derived from the Step 7–normalized windows and are not normalized again\",\n","        \"Tensor inputs were already channel-wise z-score normalized in Step 7\",\n","        \"All pipelines preserve the normalization established in Step 7\",\n","        \"Rocket features are intrinsically scale-robust and do not require additional normalization\",\n","        \"Hard checks performed: dimensional consistency, X/y alignment, NaN/Inf detection, mistaken normalization diagnostics\",\n","        \"Metadata from Step 11 has been cross-checked\",\n","    ]\n","}\n","\n","configs_dir = Path(\"configs\")\n","configs_dir.mkdir(parents=True, exist_ok=True)\n","\n","config_file = configs_dir / f\"feature_norm_fold_{FOLD_ID:02d}.json\"\n","with open(config_file, \"w\") as f:\n","    json.dump(config, f, indent=2)\n","\n","print(f\"✓ Confirmation configuration saved: {config_file}\")\n","\n","# ========== 5. Summary ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 12 completed - normalization strategy verified\")\n","print(\"=\"*60)\n","\n","print(f\"\\nFold identifier: {fold_tag}\")\n","\n","print(f\"\\nVerification summary:\")\n","for method, info in results.items():\n","    print(f\"  {method.upper()}: {info['status']}\")\n","    print(f\"    Shape: {info['train_shape']} (train)\")\n","    print(f\"    Mean / std: {info['train_mean']:.4f} / {info['train_std']:.4f}\")\n","    if \"frac_columns_near_zscore\" in info:\n","        print(f\"    Mistaken normalization check: {info['frac_columns_near_zscore']:.1%} of columns close to z-score\")\n","    if \"channel_mean_ok\" in info:\n","        print(f\"    Channel-level verification: mean_ok={info['channel_mean_ok']}, \"\n","              f\"std_ok={info['channel_std_ok']}\")\n","\n","print(f\"\\nKey principles:\")\n","print(\"  ✓ MiniRocket/MultiRocket: no second normalization\")\n","print(\"  ✓ Tensor: maintain Step 7 channel-wise z-score\")\n","print(\"  ✓ All data passed through to the training stage as-is\")\n","\n","print(f\"\\nHard checks:\")\n","print(\"  ✓ Dimensional consistency\")\n","print(\"  ✓ X/y sample alignment\")\n","print(\"  ✓ NaN/Inf checks\")\n","print(\"  ✓ Mistaken normalization diagnostics\")\n","print(\"  ✓ Channel-wise z-score verification\")\n","print(\"  ✓ Cross-checking of Step 11 metadata\")\n","\n","print(f\"\\nConfiguration file: {config_file}\")\n","\n","print(\"\\nNext steps:\")\n","print(\"  - MiniRocket/MultiRocket features can be used directly for Ridge/XGBoost training\")\n","print(\"  - Tensor inputs can be used directly for TST/Transformer training\")\n","print(\"  - All inputs have undergone the required normalization\")\n","\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_34EJdLweavj","executionInfo":{"status":"ok","timestamp":1763130503413,"user_tz":0,"elapsed":11850,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"d66dca9a-f141-4850-fe1c-d3b8541ae911"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 12: Verification of feature / input normalization\n","============================================================\n","\n","Fold identifier: fold_05\n","\n","============================================================\n","1. Check feature / input files\n","============================================================\n","\n","✓ MiniRocket features:\n","  Train: (5442, 9996), dtype=float32\n","  Test: (289, 9996), dtype=float32\n","  Summary (train): mean=0.5005, std=0.3070\n","\n","✓ MultiRocket features:\n","  Train: (5442, 49728), dtype=float32\n","  Test: (289, 49728), dtype=float32\n","  Summary (train): mean=21.7258, std=31.7934\n","\n","✓ Tensor inputs:\n","  Train: (5442, 8, 150) (N,C,L), dtype=torch.float32\n","  Test: (289, 8, 150) (N,C,L), dtype=torch.float32\n","  Summary (train): mean=-0.0105, std=0.9675\n","  Channel-level verification: mean<=±0.02? False (max deviation=0.0408)\n","               std≈1±0.05? False (max deviation=0.0556)\n","\n","============================================================\n","2. Cross-check metadata from Step 11\n","============================================================\n","⚠️ Step 11 metadata not found (possibly generated by an older pipeline version)\n","\n","============================================================\n","3. Confirmation of normalization strategy\n","============================================================\n","\n","Normalization principles:\n","  ✓ MiniRocket/MultiRocket: features are extracted from Step 7–normalized windows; no second normalization\n","  ✓ Tensor: already channel-wise z-score normalized in Step 7; no further preprocessing\n","  ✓ All inputs preserve the normalization established in Step 7\n","\n","============================================================\n","4. Save confirmation configuration\n","============================================================\n","✓ Confirmation configuration saved: configs/feature_norm_fold_05.json\n","\n","============================================================\n","Step 12 completed - normalization strategy verified\n","============================================================\n","\n","Fold identifier: fold_05\n","\n","Verification summary:\n","  MINIROCKET: pass_through\n","    Shape: [5442, 9996] (train)\n","    Mean / std: 0.5005 / 0.3070\n","    Mistaken normalization check: 0.0% of columns close to z-score\n","  MULTIROCKET: pass_through\n","    Shape: [5442, 49728] (train)\n","    Mean / std: 21.7258 / 31.7934\n","    Mistaken normalization check: 0.0% of columns close to z-score\n","  TENSOR: already_normalized\n","    Shape: [5442, 8, 150] (train)\n","    Mean / std: -0.0105 / 0.9675\n","    Channel-level verification: mean_ok=False, std_ok=False\n","\n","Key principles:\n","  ✓ MiniRocket/MultiRocket: no second normalization\n","  ✓ Tensor: maintain Step 7 channel-wise z-score\n","  ✓ All data passed through to the training stage as-is\n","\n","Hard checks:\n","  ✓ Dimensional consistency\n","  ✓ X/y sample alignment\n","  ✓ NaN/Inf checks\n","  ✓ Mistaken normalization diagnostics\n","  ✓ Channel-wise z-score verification\n","  ✓ Cross-checking of Step 11 metadata\n","\n","Configuration file: configs/feature_norm_fold_05.json\n","\n","Next steps:\n","  - MiniRocket/MultiRocket features can be used directly for Ridge/XGBoost training\n","  - Tensor inputs can be used directly for TST/Transformer training\n","  - All inputs have undergone the required normalization\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 13: Model training (top-tier conference/journal level - Ridge coarse-to-fine search)\n","Ridge: two-stage coarse→fine with sample_weight(balanced)\n","TST: class-imbalance weighting + early stopping\n","\"\"\"\n","\n","import os\n","import sys\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import json\n","import pickle\n","import time\n","import random\n","from sklearn.linear_model import RidgeClassifier\n","from sklearn.model_selection import GridSearchCV, GroupKFold, GroupShuffleSplit\n","from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score\n","from sklearn.utils.class_weight import compute_sample_weight\n","\n","# ========== Manually set FOLD_ID ==========\n","if \"FOLD_ID\" not in os.environ:\n","    print(\"⚠️ Environment variable FOLD_ID is not set; please specify it manually:\")\n","    fold_input = input(\"Please enter FOLD_ID (press Enter to use 0 by default): \").strip()\n","    os.environ[\"FOLD_ID\"] = fold_input if fold_input else \"0\"\n","    print(f\"✓ FOLD_ID has been set to {os.environ['FOLD_ID']}\")\n","\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","SEED = int(os.environ.get(\"SEED\", \"0\"))\n","\n","print(\"=\"*60)\n","print(\"Step 13: Model training (Ridge coarse-to-fine search version)\")\n","print(\"=\"*60)\n","\n","# ========== Path configuration ==========\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","fold_tag = f\"fold_{FOLD_ID:02d}\" if FOLD_ID >= 0 else \"all\"\n","models_dir = Path(\"models\") / fold_tag\n","models_dir.mkdir(parents=True, exist_ok=True)\n","\n","tuning_dir = Path(\"tuning\") / fold_tag\n","tuning_dir.mkdir(parents=True, exist_ok=True)\n","\n","print(f\"\\nFold identifier: {fold_tag}\")\n","print(f\"Random seed: {SEED}\")\n","\n","# ========== Load subject information ==========\n","print(\"\\nLoading subject grouping information...\")\n","meta_file = proc_dir / \"windows\" / fold_tag / \"X_train.parquet\"\n","\n","if not meta_file.exists():\n","    raise FileNotFoundError(\n","        f\"Metadata file not found: {meta_file}\\n\"\n","        \"GroupKFold requires subject information\"\n","    )\n","\n","df_meta = pd.read_parquet(meta_file)\n","train_subjects = df_meta['subject_id'].values\n","unique_subjects = np.unique(train_subjects)\n","print(f\"✓ Training subjects: {len(unique_subjects)}\")\n","\n","# Adaptive number of folds for GroupKFold\n","n_groups = len(unique_subjects)\n","n_splits = min(5, n_groups)\n","if n_splits < 2:\n","    raise ValueError(f\"Only {n_groups} subjects; cannot perform GroupKFold with ≥2 splits\")\n","print(f\"  GroupKFold number of splits: {n_splits}\")\n","\n","# ========== Ridge coarse-to-fine search function ==========\n","def run_ridge_search(X, y, groups, n_splits, tag, tuning_dir):\n","    \"\"\"\n","    Two-stage Ridge search: coarse→fine, with sample_weight='balanced'.\n","    Automatically expands the search range when the optimum lies on the boundary.\n","    \"\"\"\n","    # Compute sample weights\n","    sample_weights = compute_sample_weight('balanced', y)\n","\n","    # Coarse grid\n","    alpha_coarse = np.logspace(-5, 5, 11) if tag == \"multirocket\" else np.logspace(-4, 4, 9)\n","    cv = GroupKFold(n_splits=n_splits)\n","\n","    gs_coarse = GridSearchCV(\n","        RidgeClassifier(),\n","        {'alpha': alpha_coarse.tolist()},\n","        cv=cv, scoring='f1_macro',\n","        n_jobs=-1, verbose=0, return_train_score=True\n","    )\n","\n","    print(f\"  Coarse grid: alpha={alpha_coarse.min():.1e}~{alpha_coarse.max():.1e} ({len(alpha_coarse)} points)\")\n","    gs_coarse.fit(X, y, groups=groups, sample_weight=sample_weights)\n","\n","    pd.DataFrame(gs_coarse.cv_results_).to_csv(\n","        tuning_dir / f\"{tag}_cv_coarse.csv\", index=False\n","    )\n","\n","    a_best = gs_coarse.best_params_['alpha']\n","    print(f\"  Best (coarse): alpha={a_best:.2e}, CV_F1={gs_coarse.best_score_:.4f}\")\n","\n","    # If the optimum hits the boundary, expand the range; otherwise refine around the optimum\n","    if a_best == alpha_coarse.min():\n","        a_fine = np.logspace(np.log10(alpha_coarse.min()) - 2, np.log10(a_best) + 1, 9)\n","        print(f\"  Hit lower boundary, expanding range down to {a_fine.min():.1e}\")\n","    elif a_best == alpha_coarse.max():\n","        a_fine = np.logspace(np.log10(a_best) - 1, np.log10(alpha_coarse.max()) + 2, 9)\n","        print(f\"  Hit upper boundary, expanding range up to {a_fine.max():.1e}\")\n","    else:\n","        a_fine = a_best * np.array([0.03, 0.1, 0.3, 1, 3, 10, 30])\n","        print(f\"  Refining around the current optimum\")\n","\n","    # Fine grid\n","    a_fine = np.unique(a_fine)\n","    gs_fine = GridSearchCV(\n","        RidgeClassifier(),\n","        {'alpha': a_fine.tolist()},\n","        cv=cv, scoring='f1_macro',\n","        n_jobs=-1, verbose=0, return_train_score=True\n","    )\n","\n","    print(f\"  Fine grid: {len(a_fine)} points\")\n","    gs_fine.fit(X, y, groups=groups, sample_weight=sample_weights)\n","\n","    pd.DataFrame(gs_fine.cv_results_).to_csv(\n","        tuning_dir / f\"{tag}_cv_fine.csv\", index=False\n","    )\n","\n","    print(f\"  Best (fine): alpha={gs_fine.best_params_['alpha']:.2e}, CV_F1={gs_fine.best_score_:.4f}\")\n","\n","    return gs_fine\n","\n","all_results = {}\n","\n","# ========== 1. MiniRocket + Ridge ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. MiniRocket + RidgeClassifier\")\n","print(\"=\"*60)\n","\n","features_dir = proc_dir / \"features\" / fold_tag\n","minirocket_train = features_dir / \"minirocket_train.npz\"\n","minirocket_test = features_dir / \"minirocket_test.npz\"\n","\n","if minirocket_train.exists() and minirocket_test.exists():\n","    print(\"\\nLoading MiniRocket features...\")\n","    data_train = np.load(minirocket_train, allow_pickle=False)\n","    data_test = np.load(minirocket_test, allow_pickle=False)\n","\n","    X_train = data_train['X']\n","    y_train = data_train['y']\n","    X_test = data_test['X']\n","    y_test = data_test['y']\n","\n","    print(f\"  Train: {X_train.shape}\")\n","    print(f\"  Test: {X_test.shape}\")\n","\n","    assert X_train.shape[0] == len(train_subjects), \"Number of samples is inconsistent with subject array\"\n","\n","    print(f\"\\nHyperparameter search (coarse→fine, sample_weight=balanced)...\")\n","    start_time = time.time()\n","    grid_search = run_ridge_search(X_train, y_train, train_subjects, n_splits, \"minirocket\", tuning_dir)\n","    search_time = time.time() - start_time\n","\n","    print(f\"✓ Search finished: {search_time:.2f}s\")\n","\n","    # Evaluation\n","    best_clf = grid_search.best_estimator_\n","    y_train_pred = best_clf.predict(X_train)\n","    y_test_pred = best_clf.predict(X_test)\n","\n","    train_acc = accuracy_score(y_train, y_train_pred)\n","    train_f1 = f1_score(y_train, y_train_pred, average='macro', zero_division=0)\n","    train_bal_acc = balanced_accuracy_score(y_train, y_train_pred)\n","\n","    test_acc = accuracy_score(y_test, y_test_pred)\n","    test_f1 = f1_score(y_test, y_test_pred, average='macro', zero_division=0)\n","    test_bal_acc = balanced_accuracy_score(y_test, y_test_pred)\n","\n","    print(f\"\\nResults:\")\n","    print(f\"  Train: Acc={train_acc:.4f}, F1={train_f1:.4f}, BalAcc={train_bal_acc:.4f}\")\n","    print(f\"  Test:  Acc={test_acc:.4f}, F1={test_f1:.4f}, BalAcc={test_bal_acc:.4f}\")\n","\n","    # Save\n","    np.savez_compressed(\n","        models_dir / \"minirocket_test_preds.npz\",\n","        y_true=y_test, y_pred=y_test_pred,\n","        logits=best_clf.decision_function(X_test)\n","    )\n","\n","    with open(models_dir / \"minirocket_ridge.pkl\", 'wb') as f:\n","        pickle.dump(best_clf, f)\n","\n","    with open(tuning_dir / \"minirocket_best_params.json\", 'w') as f:\n","        json.dump(grid_search.best_params_, f, indent=2)\n","\n","    all_results['minirocket'] = {\n","        'best_params': grid_search.best_params_,\n","        'cv_f1_macro': float(grid_search.best_score_),\n","        'train_acc': float(train_acc),\n","        'train_f1': float(train_f1),\n","        'train_bal_acc': float(train_bal_acc),\n","        'test_acc': float(test_acc),\n","        'test_f1': float(test_f1),\n","        'test_bal_acc': float(test_bal_acc),\n","        'search_time': float(search_time),\n","    }\n","\n","    print(f\"✓ Model and predictions saved\")\n","else:\n","    print(\"⚠️ MiniRocket features not found; skipping\")\n","\n","# ========== 2. MultiRocket + Ridge ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. MultiRocket + RidgeClassifier\")\n","print(\"=\"*60)\n","\n","multirocket_train = features_dir / \"multirocket_train.npz\"\n","multirocket_test = features_dir / \"multirocket_test.npz\"\n","\n","if multirocket_train.exists() and multirocket_test.exists():\n","    print(\"\\nLoading MultiRocket features...\")\n","    data_train = np.load(multirocket_train, allow_pickle=False)\n","    data_test = np.load(multirocket_test, allow_pickle=False)\n","\n","    X_train = data_train['X']\n","    y_train = data_train['y']\n","    X_test = data_test['X']\n","    y_test = data_test['y']\n","\n","    print(f\"  Train: {X_train.shape}\")\n","    print(f\"  Test:  {X_test.shape}\")\n","\n","    assert X_train.shape[0] == len(train_subjects), \"Number of samples is inconsistent with subject array\"\n","\n","    print(f\"\\nHyperparameter search (coarse→fine, sample_weight=balanced)...\")\n","    start_time = time.time()\n","    grid_search = run_ridge_search(X_train, y_train, train_subjects, n_splits, \"multirocket\", tuning_dir)\n","    search_time = time.time() - start_time\n","\n","    print(f\"✓ Search finished: {search_time:.2f}s\")\n","\n","    # Evaluation\n","    best_clf = grid_search.best_estimator_\n","    y_train_pred = best_clf.predict(X_train)\n","    y_test_pred = best_clf.predict(X_test)\n","\n","    train_acc = accuracy_score(y_train, y_train_pred)\n","    train_f1 = f1_score(y_train, y_train_pred, average='macro', zero_division=0)\n","    train_bal_acc = balanced_accuracy_score(y_train, y_train_pred)\n","\n","    test_acc = accuracy_score(y_test, y_test_pred)\n","    test_f1 = f1_score(y_test, y_test_pred, average='macro', zero_division=0)\n","    test_bal_acc = balanced_accuracy_score(y_test, y_test_pred)\n","\n","    print(f\"\\nResults:\")\n","    print(f\"  Train: Acc={train_acc:.4f}, F1={train_f1:.4f}, BalAcc={train_bal_acc:.4f}\")\n","    print(f\"  Test:  Acc={test_acc:.4f}, F1={test_f1:.4f}, BalAcc={test_bal_acc:.4f}\")\n","\n","    # Save\n","    np.savez_compressed(\n","        models_dir / \"multirocket_test_preds.npz\",\n","        y_true=y_test, y_pred=y_test_pred,\n","        logits=best_clf.decision_function(X_test)\n","    )\n","\n","    with open(models_dir / \"multirocket_ridge.pkl\", 'wb') as f:\n","        pickle.dump(best_clf, f)\n","\n","    with open(tuning_dir / \"multirocket_best_params.json\", 'w') as f:\n","        json.dump(grid_search.best_params_, f, indent=2)\n","\n","    all_results['multirocket'] = {\n","        'best_params': grid_search.best_params_,\n","        'cv_f1_macro': float(grid_search.best_score_),\n","        'train_acc': float(train_acc),\n","        'train_f1': float(train_f1),\n","        'train_bal_acc': float(train_bal_acc),\n","        'test_acc': float(test_acc),\n","        'test_f1': float(test_f1),\n","        'test_bal_acc': float(test_bal_acc),\n","        'search_time': float(search_time),\n","    }\n","\n","    print(f\"✓ Model and predictions saved\")\n","else:\n","    print(\"⚠️ MultiRocket features not found; skipping\")\n","\n","# ========== 3. Tensor + TST ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Tensor + TST\")\n","print(\"=\"*60)\n","\n","tensor_dir = proc_dir / \"tensors\" / fold_tag\n","tensor_train = tensor_dir / \"train.pt\"\n","tensor_test = tensor_dir / \"test.pt\"\n","\n","if tensor_train.exists() and tensor_test.exists():\n","    try:\n","        import torch\n","        import torch.nn as nn\n","        from torch.utils.data import TensorDataset, DataLoader\n","\n","        # Set random seeds\n","        random.seed(SEED)\n","        np.random.seed(SEED)\n","        torch.manual_seed(SEED)\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed_all(SEED)\n","            torch.backends.cudnn.deterministic = True\n","            torch.backends.cudnn.benchmark = False\n","\n","        print(\"\\nLoading Tensor data...\")\n","        data_train = torch.load(tensor_train, map_location=\"cpu\")\n","        data_test = torch.load(tensor_test, map_location=\"cpu\")\n","\n","        X_train_full = data_train['X']\n","        y_train_full = data_train['y']\n","        X_test = data_test['X']\n","        y_test = data_test['y']\n","\n","        print(f\"  Train (full): {tuple(X_train_full.shape)}\")\n","        print(f\"  Test:         {tuple(X_test.shape)}\")\n","\n","        assert X_train_full.shape[0] == len(train_subjects), \"Number of samples is inconsistent with subject array\"\n","\n","        # Convert labels to torch.long and enforce zero-based indexing\n","        def ensure_long_zero_based(y_t):\n","            if y_t.dtype != torch.long:\n","                y_t = y_t.long()\n","            uniq = torch.unique(y_t, sorted=True)\n","            expected = torch.arange(len(uniq), dtype=torch.long)\n","            if not torch.equal(uniq, expected):\n","                y_new = torch.zeros_like(y_t)\n","                for new_idx, old_val in enumerate(uniq):\n","                    y_new[y_t == old_val] = new_idx\n","                return y_new\n","            return y_t\n","\n","        y_train_full = ensure_long_zero_based(y_train_full)\n","        y_test = ensure_long_zero_based(y_test)\n","\n","        # Split into train/validation sets\n","        print(\"\\nSplitting into train/validation sets (10% subjects for val)...\")\n","        dummy = np.zeros(len(train_subjects))\n","        splitter = GroupShuffleSplit(n_splits=1, test_size=0.1, random_state=SEED)\n","        train_idx, val_idx = next(splitter.split(dummy, dummy, groups=train_subjects))\n","\n","        train_idx_t = torch.as_tensor(train_idx, dtype=torch.long)\n","        val_idx_t = torch.as_tensor(val_idx, dtype=torch.long)\n","\n","        X_train = X_train_full.index_select(0, train_idx_t)\n","        y_train = y_train_full.index_select(0, train_idx_t)\n","        X_val = X_train_full.index_select(0, val_idx_t)\n","        y_val = y_train_full.index_select(0, val_idx_t)\n","\n","        print(f\"  Train: {tuple(X_train.shape)}\")\n","        print(f\"  Val:   {tuple(X_val.shape)}\")\n","\n","        # Crop sequence length to an integer multiple of the patch length\n","        PATCH_LEN = 15\n","        def crop_to_multiple(x, m):\n","            L_eff = (x.shape[2] // m) * m\n","            return x[:, :, :L_eff].contiguous(), L_eff\n","\n","        X_train, seq_len = crop_to_multiple(X_train, PATCH_LEN)\n","        X_val, _ = crop_to_multiple(X_val, PATCH_LEN)\n","        X_test, _ = crop_to_multiple(X_test, PATCH_LEN)\n","\n","        print(f\"  Effective seq_len after cropping: {seq_len}\")\n","\n","        n_channels = X_train.shape[1]\n","        n_classes = int(torch.unique(y_train_full).numel())\n","\n","        # TST model\n","        class SimpleTST(nn.Module):\n","            def __init__(self, n_channels, seq_len, n_classes, patch_len=15,\n","                         d_model=128, n_heads=8, n_layers=2, dropout=0.1):\n","                super().__init__()\n","                self.patch_len = patch_len\n","                self.n_patches = seq_len // patch_len\n","\n","                self.patch_embed = nn.Linear(n_channels * patch_len, d_model)\n","                self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches, d_model))\n","\n","                encoder_layer = nn.TransformerEncoderLayer(\n","                    d_model=d_model, nhead=n_heads, dim_feedforward=d_model*4,\n","                    dropout=dropout, batch_first=True\n","                )\n","                self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n","\n","                self.fc = nn.Linear(d_model, n_classes)\n","                self.dropout = nn.Dropout(dropout)\n","\n","            def forward(self, x):\n","                N, C, L = x.shape\n","                x = x.unfold(2, self.patch_len, self.patch_len)\n","                x = x.permute(0, 2, 1, 3).contiguous()\n","                x = x.view(N, self.n_patches, -1)\n","\n","                x = self.patch_embed(x)\n","                x = x + self.pos_embed\n","                x = self.transformer(x)\n","                x = x.mean(dim=1)\n","                x = self.dropout(x)\n","                x = self.fc(x)\n","                return x\n","\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        print(f\"\\nUsing device: {device}\")\n","\n","        model = SimpleTST(\n","            n_channels=n_channels, seq_len=seq_len, n_classes=n_classes,\n","            patch_len=PATCH_LEN, d_model=128, n_heads=8, n_layers=2, dropout=0.1\n","        ).to(device)\n","\n","        print(f\"Number of model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n","\n","        # Class-imbalance weighting\n","        counts = torch.bincount(y_train)\n","        weights = (1.0 / counts.float())\n","        weights = weights / weights.mean()\n","\n","        criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n","        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n","\n","        # DataLoader\n","        NUM_WORKERS = min(4, os.cpu_count() or 2)\n","        PIN = torch.cuda.is_available()\n","\n","        train_loader = DataLoader(\n","            TensorDataset(X_train, y_train), batch_size=128, shuffle=True,\n","            num_workers=NUM_WORKERS, pin_memory=PIN\n","        )\n","        val_loader = DataLoader(\n","            TensorDataset(X_val, y_val), batch_size=128, shuffle=False,\n","            num_workers=NUM_WORKERS, pin_memory=PIN\n","        )\n","        test_loader = DataLoader(\n","            TensorDataset(X_test, y_test), batch_size=128, shuffle=False,\n","            num_workers=NUM_WORKERS, pin_memory=PIN\n","        )\n","\n","        # Training\n","        epochs = 50\n","        best_val_f1 = -1.0\n","        best_epoch = 0\n","        patience = 10\n","        patience_counter = 0\n","\n","        print(f\"\\nTraining (epochs={epochs}, patience={patience})...\")\n","        start_time = time.time()\n","\n","        for epoch in range(epochs):\n","            model.train()\n","            train_loss = 0.0\n","            for X_batch, y_batch in train_loader:\n","                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","                optimizer.zero_grad()\n","                outputs = model(X_batch)\n","                loss = criterion(outputs, y_batch)\n","                loss.backward()\n","                optimizer.step()\n","                train_loss += loss.item()\n","\n","            # Validation\n","            model.eval()\n","            val_preds, val_labels = [], []\n","            with torch.no_grad():\n","                for X_batch, y_batch in val_loader:\n","                    X_batch = X_batch.to(device)\n","                    outputs = model(X_batch)\n","                    _, predicted = torch.max(outputs, 1)\n","                    val_preds.extend(predicted.cpu().tolist())\n","                    val_labels.extend(y_batch.tolist())\n","\n","            val_f1 = f1_score(val_labels, val_preds, average='macro', zero_division=0)\n","\n","            if val_f1 > best_val_f1:\n","                best_val_f1 = val_f1\n","                best_epoch = epoch + 1\n","                patience_counter = 0\n","                torch.save(model.state_dict(), models_dir / \"tst_best.pt\")\n","            else:\n","                patience_counter += 1\n","\n","            if (epoch + 1) % 10 == 0:\n","                print(f\"  Epoch {epoch+1}: Loss={train_loss/len(train_loader):.4f}, ValF1={val_f1:.4f}\")\n","\n","            if patience_counter >= patience:\n","                print(f\"  Early stopping at epoch {epoch+1}\")\n","                break\n","\n","        train_time = time.time() - start_time\n","\n","        # Load best model\n","        model.load_state_dict(torch.load(models_dir / \"tst_best.pt\"))\n","        model.eval()\n","\n","        # Training set evaluation\n","        train_full_loader = DataLoader(\n","            TensorDataset(X_train_full, y_train_full), batch_size=128, shuffle=False,\n","            num_workers=NUM_WORKERS, pin_memory=PIN\n","        )\n","\n","        train_preds = []\n","        with torch.no_grad():\n","            for X_batch, _ in train_full_loader:\n","                X_batch = X_batch.to(device)\n","                outputs = model(X_batch)\n","                _, predicted = torch.max(outputs, 1)\n","                train_preds.extend(predicted.cpu().tolist())\n","\n","        y_train_full_np = np.array(y_train_full.cpu().tolist())\n","        train_preds = np.array(train_preds)\n","        train_acc = accuracy_score(y_train_full_np, train_preds)\n","        train_f1 = f1_score(y_train_full_np, train_preds, average='macro', zero_division=0)\n","        train_bal_acc = balanced_accuracy_score(y_train_full_np, train_preds)\n","\n","        # Test set evaluation\n","        y_test_np = np.array(y_test.cpu().tolist())\n","        test_preds = []\n","        test_logits_list = []\n","\n","        with torch.no_grad():\n","            for X_batch, _ in test_loader:\n","                X_batch = X_batch.to(device)\n","                outputs = model(X_batch)\n","                _, predicted = torch.max(outputs, 1)\n","                test_preds.extend(predicted.cpu().tolist())\n","                test_logits_list.append(np.array(outputs.cpu().tolist()))\n","\n","        test_preds = np.array(test_preds)\n","        test_logits = np.concatenate(test_logits_list, axis=0)\n","        test_acc = accuracy_score(y_test_np, test_preds)\n","        test_f1 = f1_score(y_test_np, test_preds, average='macro', zero_division=0)\n","        test_bal_acc = balanced_accuracy_score(y_test_np, test_preds)\n","\n","        print(f\"\\n✓ Training finished: {train_time:.2f}s\")\n","        print(f\"\\nResults:\")\n","        print(f\"  Train: Acc={train_acc:.4f}, F1={train_f1:.4f}, BalAcc={train_bal_acc:.4f}\")\n","        print(f\"  Val:   BestF1={best_val_f1:.4f} (Epoch {best_epoch})\")\n","        print(f\"  Test:  Acc={test_acc:.4f}, F1={test_f1:.4f}, BalAcc={test_bal_acc:.4f}\")\n","\n","        # Save\n","        np.savez_compressed(\n","            models_dir / \"tst_test_preds.npz\",\n","            y_true=y_test_np, y_pred=test_preds, logits=test_logits\n","        )\n","\n","        all_results['tst'] = {\n","            'best_val_f1': float(best_val_f1),\n","            'best_epoch': int(best_epoch),\n","            'train_acc': float(train_acc),\n","            'train_f1': float(train_f1),\n","            'train_bal_acc': float(train_bal_acc),\n","            'test_acc': float(test_acc),\n","            'test_f1': float(test_f1),\n","            'test_bal_acc': float(test_bal_acc),\n","            'train_time': float(train_time),\n","        }\n","\n","        print(f\"✓ Model and predictions saved\")\n","\n","    except ImportError:\n","        print(\"⚠️ PyTorch is not installed; skipping TST\")\n","else:\n","    print(\"⚠️ Tensor data not found; skipping TST\")\n","\n","# ========== Save summary of results ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Save summary of results\")\n","print(\"=\"*60)\n","\n","summary = {\n","    'fold_id': FOLD_ID,\n","    'fold_tag': fold_tag,\n","    'seed': SEED,\n","    'results': all_results,\n","    'improvements': [\n","        'Ridge: two-stage coarse→fine search with sample_weight=balanced',\n","        'Automatically expand the search range when the optimum is at the boundary (to avoid false convergence)',\n","        'MiniRocket: 1e-4~1e4 (9 points) → refined to a 7-point local search',\n","        'MultiRocket: 1e-5~1e5 (11 points) → refined to a 7-point local search',\n","        'TST: class-imbalance weighting + early stopping',\n","    ],\n","}\n","\n","summary_file = models_dir / \"training_summary.json\"\n","with open(summary_file, 'w') as f:\n","    json.dump(summary, f, indent=2)\n","\n","print(f\"✓ Summary saved: {summary_file}\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 13 completed\")\n","print(\"=\"*60)\n","\n","print(f\"\\nFold identifier: {fold_tag}\")\n","print(f\"\\nSummary of results:\")\n","for method, results in all_results.items():\n","    print(f\"\\n{method.upper()}:\")\n","    print(f\"  Test F1:      {results['test_f1']:.4f}\")\n","    print(f\"  Test BalAcc:  {results['test_bal_acc']:.4f}\")\n","\n","print(f\"\\nOutputs:\")\n","print(f\"  models/{fold_tag}/\")\n","print(f\"  tuning/{fold_tag}/\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vDz1lIX8geh3","executionInfo":{"status":"ok","timestamp":1763130902748,"user_tz":0,"elapsed":399322,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"50a426e6-7697-47c1-bb30-98ff8b092d9d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 13: Model training (Ridge coarse-to-fine search version)\n","============================================================\n","\n","Fold identifier: fold_05\n","Random seed: 0\n","\n","Loading subject grouping information...\n","✓ Training subjects: 7\n","  GroupKFold number of splits: 5\n","\n","============================================================\n","1. MiniRocket + RidgeClassifier\n","============================================================\n","\n","Loading MiniRocket features...\n","  Train: (5442, 9996)\n","  Test: (289, 9996)\n","\n","Hyperparameter search (coarse→fine, sample_weight=balanced)...\n","  Coarse grid: alpha=1.0e-04~1.0e+04 (9 points)\n","  Best (coarse): alpha=1.00e+01, CV_F1=0.5321\n","  Refining around the current optimum\n","  Fine grid: 7 points\n","  Best (fine): alpha=1.00e+01, CV_F1=0.5321\n","✓ Search finished: 57.40s\n","\n","Results:\n","  Train: Acc=0.9530, F1=0.9575, BalAcc=0.9818\n","  Test:  Acc=0.4567, F1=0.3696, BalAcc=0.3580\n","✓ Model and predictions saved\n","\n","============================================================\n","2. MultiRocket + RidgeClassifier\n","============================================================\n","\n","Loading MultiRocket features...\n","  Train: (5442, 49728)\n","  Test:  (289, 49728)\n","\n","Hyperparameter search (coarse→fine, sample_weight=balanced)...\n","  Coarse grid: alpha=1.0e-05~1.0e+05 (11 points)\n","  Best (coarse): alpha=1.00e+05, CV_F1=0.4914\n","  Hit upper boundary, expanding range up to 1.0e+07\n","  Fine grid: 9 points\n","  Best (fine): alpha=1.78e+06, CV_F1=0.5298\n","✓ Search finished: 318.83s\n","\n","Results:\n","  Train: Acc=0.9269, F1=0.9303, BalAcc=0.9675\n","  Test:  Acc=0.4498, F1=0.3911, BalAcc=0.3769\n","✓ Model and predictions saved\n","\n","============================================================\n","3. Tensor + TST\n","============================================================\n","\n","Loading Tensor data...\n","  Train (full): (5442, 8, 150)\n","  Test:         (289, 8, 150)\n","\n","Splitting into train/validation sets (10% subjects for val)...\n","  Train: (4693, 8, 150)\n","  Val:   (749, 8, 150)\n","  Effective seq_len after cropping: 150\n","\n","Using device: cuda\n","Number of model parameters: 414,086\n","\n","Training (epochs=50, patience=10)...\n","  Epoch 10: Loss=0.5898, ValF1=0.5045\n","  Early stopping at epoch 17\n","\n","✓ Training finished: 11.99s\n","\n","Results:\n","  Train: Acc=0.6321, F1=0.6659, BalAcc=0.7286\n","  Val:   BestF1=0.5142 (Epoch 7)\n","  Test:  Acc=0.3564, F1=0.3246, BalAcc=0.3043\n","✓ Model and predictions saved\n","\n","============================================================\n","4. Save summary of results\n","============================================================\n","✓ Summary saved: models/fold_05/training_summary.json\n","\n","============================================================\n","Step 13 completed\n","============================================================\n","\n","Fold identifier: fold_05\n","\n","Summary of results:\n","\n","MINIROCKET:\n","  Test F1:      0.3696\n","  Test BalAcc:  0.3580\n","\n","MULTIROCKET:\n","  Test F1:      0.3911\n","  Test BalAcc:  0.3769\n","\n","TST:\n","  Test F1:      0.3246\n","  Test BalAcc:  0.3043\n","\n","Outputs:\n","  models/fold_05/\n","  tuning/fold_05/\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 16: Evaluation summary (top-tier conference/journal level)\n","Metrics: Macro-F1, Balanced Accuracy, Per-class F1, Confusion Matrix\n","Confidence intervals: BCa bootstrap (n=1000)\n","Reporting: cross-fold mean ± standard deviation ± 95% CI, subject-level F1 mean\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import json\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import (\n","    accuracy_score, f1_score, balanced_accuracy_score,\n","    confusion_matrix, classification_report\n",")\n","from scipy import stats\n","import warnings\n","warnings.filterwarnings('ignore', message='y_pred contains classes not in y_true')\n","\n","print(\"=\"*60)\n","print(\"Step 16: Evaluation summary\")\n","print(\"=\"*60)\n","\n","# ========== Path configuration ==========\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","models_base = Path(\"models\")\n","metrics_dir = Path(\"metrics\")\n","metrics_dir.mkdir(parents=True, exist_ok=True)\n","\n","# ========== BCa Bootstrap function ==========\n","def bca_bootstrap_ci(y_true, y_pred, metric_func, n_bootstrap=1000, confidence=0.95):\n","    \"\"\"\n","    BCa (Bias-Corrected and Accelerated) bootstrap confidence interval\n","\n","    Parameters:\n","        y_true: ground-truth labels\n","        y_pred: predicted labels\n","        metric_func: metric function (e.g., lambda y_t, y_p: f1_score(y_t, y_p, average='macro'))\n","        n_bootstrap: number of bootstrap resamples\n","        confidence: confidence level\n","\n","    Returns:\n","        (lower, upper): lower and upper bounds of the confidence interval\n","    \"\"\"\n","    n = len(y_true)\n","\n","    # Original statistic\n","    theta_hat = metric_func(y_true, y_pred)\n","\n","    # Bootstrap resampling\n","    theta_bootstrap = []\n","    for _ in range(n_bootstrap):\n","        idx = np.random.choice(n, size=n, replace=True)\n","        try:\n","            theta_b = metric_func(y_true[idx], y_pred[idx])\n","            theta_bootstrap.append(theta_b)\n","        except:\n","            continue\n","\n","    theta_bootstrap = np.array(theta_bootstrap)\n","\n","    # Bias-correction factor z0\n","    p0 = np.mean(theta_bootstrap < theta_hat)\n","    if p0 == 0:\n","        z0 = -3.0\n","    elif p0 == 1:\n","        z0 = 3.0\n","    else:\n","        z0 = stats.norm.ppf(p0)\n","\n","    # Acceleration factor a (jackknife)\n","    theta_jack = []\n","    for i in range(n):\n","        idx = np.concatenate([np.arange(i), np.arange(i + 1, n)])\n","        try:\n","            theta_j = metric_func(y_true[idx], y_pred[idx])\n","            theta_jack.append(theta_j)\n","        except:\n","            continue\n","\n","    theta_jack = np.array(theta_jack)\n","    theta_jack_mean = np.mean(theta_jack)\n","\n","    num = np.sum((theta_jack_mean - theta_jack) ** 3)\n","    den = 6 * (np.sum((theta_jack_mean - theta_jack) ** 2) ** (3 / 2))\n","    a = num / den if den != 0 else 0\n","\n","    # Adjusted percentiles\n","    alpha = 1 - confidence\n","    z_alpha = stats.norm.ppf(alpha / 2)\n","    z_1_alpha = stats.norm.ppf(1 - alpha / 2)\n","\n","    p_lower = stats.norm.cdf(z0 + (z0 + z_alpha) / (1 - a * (z0 + z_alpha)))\n","    p_upper = stats.norm.cdf(z0 + (z0 + z_1_alpha) / (1 - a * (z0 + z_1_alpha)))\n","\n","    p_lower = np.clip(p_lower, 0, 1)\n","    p_upper = np.clip(p_upper, 0, 1)\n","\n","    # Confidence interval\n","    lower = np.percentile(theta_bootstrap, p_lower * 100)\n","    upper = np.percentile(theta_bootstrap, p_upper * 100)\n","\n","    return float(lower), float(upper)\n","\n","# ========== Discover all folds ==========\n","print(\"\\nSearching for all trained folds...\")\n","all_folds = sorted([d.name for d in models_base.iterdir() if d.is_dir() and d.name.startswith(\"fold_\")])\n","print(f\"Found {len(all_folds)} folds: {all_folds}\")\n","\n","if not all_folds:\n","    raise FileNotFoundError(\"No trained folds found; please run Step 13 first\")\n","\n","# ========== Load label mapping ==========\n","labels_map_file = proc_dir / \"labels_map.csv\"\n","if labels_map_file.exists():\n","    labels_map = pd.read_csv(labels_map_file)\n","    label_names = labels_map.set_index('label_id')['label_name'].to_dict()\n","else:\n","    label_names = None\n","    print(\"⚠️ Label mapping file not found; numeric labels will be used\")\n","\n","# ========== Process each fold ==========\n","methods = ['minirocket', 'multirocket', 'tst']\n","all_results = {method: [] for method in methods}\n","\n","for fold_tag in all_folds:\n","    print(f\"\\n{'='*60}\")\n","    print(f\"Processing {fold_tag}\")\n","    print(f\"{'='*60}\")\n","\n","    models_dir = models_base / fold_tag\n","\n","    # Extract fold_id\n","    fold_id = int(fold_tag.split('_')[1])\n","\n","    # Read test-set metadata (for subject-level analysis)\n","    meta_file = proc_dir / \"windows\" / fold_tag / \"X_test.parquet\"\n","    if meta_file.exists():\n","        df_meta = pd.read_parquet(meta_file)\n","        test_subjects = df_meta['subject_id'].values\n","    else:\n","        test_subjects = None\n","        print(\"⚠️ Test-set metadata not found\")\n","\n","    for method in methods:\n","        pred_file = models_dir / f\"{method}_test_preds.npz\"\n","\n","        if not pred_file.exists():\n","            print(f\"  ⚠️ {method}: prediction file not found, skipping\")\n","            continue\n","\n","        print(f\"\\n  Processing {method}...\")\n","\n","        # Load predictions\n","        data = np.load(pred_file, allow_pickle=False)\n","        y_true = data['y_true']\n","        y_pred = data['y_pred']\n","\n","        print(f\"    Number of samples: {len(y_true)}\")\n","\n","        # Basic metrics\n","        acc = accuracy_score(y_true, y_pred)\n","        macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n","        bal_acc = balanced_accuracy_score(y_true, y_pred)\n","\n","        # Per-class F1\n","        per_class_f1 = f1_score(y_true, y_pred, average=None, zero_division=0)\n","\n","        # Confusion matrix\n","        cm = confusion_matrix(y_true, y_pred)\n","\n","        print(f\"    Acc: {acc:.4f}\")\n","        print(f\"    Macro-F1: {macro_f1:.4f}\")\n","        print(f\"    Balanced Acc: {bal_acc:.4f}\")\n","\n","        # BCa Bootstrap 95% CI\n","        print(f\"    Computing BCa 95% CI (n=1000)...\")\n","\n","        macro_f1_ci = bca_bootstrap_ci(\n","            y_true, y_pred,\n","            lambda yt, yp: f1_score(yt, yp, average='macro', zero_division=0),\n","            n_bootstrap=1000\n","        )\n","\n","        bal_acc_ci = bca_bootstrap_ci(\n","            y_true, y_pred,\n","            lambda yt, yp: balanced_accuracy_score(yt, yp),\n","            n_bootstrap=1000\n","        )\n","\n","        print(f\"    Macro-F1 95% CI: [{macro_f1_ci[0]:.4f}, {macro_f1_ci[1]:.4f}]\")\n","        print(f\"    Balanced Acc 95% CI: [{bal_acc_ci[0]:.4f}, {bal_acc_ci[1]:.4f}]\")\n","\n","        # Subject-level Macro-F1\n","        subject_f1_list = []\n","        if test_subjects is not None:\n","            unique_subjects = np.unique(test_subjects)\n","            for subj in unique_subjects:\n","                mask = test_subjects == subj\n","                if mask.sum() > 0:\n","                    subj_f1 = f1_score(\n","                        y_true[mask], y_pred[mask],\n","                        average='macro', zero_division=0\n","                    )\n","                    subject_f1_list.append(subj_f1)\n","\n","            if subject_f1_list:\n","                subject_f1_mean = np.mean(subject_f1_list)\n","                subject_f1_std = np.std(subject_f1_list)\n","                print(f\"    Subject-level F1: {subject_f1_mean:.4f}±{subject_f1_std:.4f} (n={len(subject_f1_list)})\")\n","\n","        # Store per-fold results\n","        fold_results = {\n","            'fold_id': fold_id,\n","            'fold_tag': fold_tag,\n","            'method': method,\n","            'n_samples': int(len(y_true)),\n","            'n_subjects': len(np.unique(test_subjects)) if test_subjects is not None else None,\n","            'accuracy': float(acc),\n","            'macro_f1': float(macro_f1),\n","            'macro_f1_ci_lower': float(macro_f1_ci[0]),\n","            'macro_f1_ci_upper': float(macro_f1_ci[1]),\n","            'balanced_accuracy': float(bal_acc),\n","            'balanced_accuracy_ci_lower': float(bal_acc_ci[0]),\n","            'balanced_accuracy_ci_upper': float(bal_acc_ci[1]),\n","            'per_class_f1': per_class_f1.tolist(),\n","            'confusion_matrix': cm.tolist(),\n","            'subject_level_f1_mean': float(np.mean(subject_f1_list)) if subject_f1_list else None,\n","            'subject_level_f1_std': float(np.std(subject_f1_list)) if subject_f1_list else None,\n","            'subject_level_f1_list': [float(x) for x in subject_f1_list] if subject_f1_list else None,\n","        }\n","\n","        # Save per-fold metrics JSON\n","        fold_metrics_file = metrics_dir / f\"{fold_tag}_{method}.json\"\n","        with open(fold_metrics_file, 'w') as f:\n","            json.dump(fold_results, f, indent=2)\n","        print(f\"    ✓ Saved: {fold_metrics_file}\")\n","\n","        # Plot confusion matrix\n","        plt.figure(figsize=(10, 8))\n","\n","        # Row-normalized confusion matrix\n","        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","        # Label names\n","        unique_labels = np.unique(np.concatenate([y_true, y_pred]))\n","        if label_names:\n","            tick_labels = [label_names.get(int(l), str(l)) for l in unique_labels]\n","        else:\n","            tick_labels = [str(l) for l in unique_labels]\n","\n","        sns.heatmap(\n","            cm_norm, annot=True, fmt='.2f', cmap='Blues',\n","            xticklabels=tick_labels, yticklabels=tick_labels,\n","            cbar_kws={'label': 'Normalized Count'}\n","        )\n","        plt.title(f'{method.upper()} - {fold_tag}\\nMacro-F1: {macro_f1:.4f}', fontsize=14)\n","        plt.ylabel('True Label', fontsize=12)\n","        plt.xlabel('Predicted Label', fontsize=12)\n","        plt.tight_layout()\n","\n","        cm_file = metrics_dir / f\"confusion_{fold_tag}_{method}.png\"\n","        plt.savefig(cm_file, dpi=150, bbox_inches='tight')\n","        plt.close()\n","        print(f\"    ✓ Saved: {cm_file}\")\n","\n","        # Add to overall list\n","        all_results[method].append(fold_results)\n","\n","# ========== Aggregate across all folds ==========\n","print(f\"\\n{'='*60}\")\n","print(\"Aggregate across all folds\")\n","print(f\"{'='*60}\")\n","\n","summary = {}\n","\n","for method in methods:\n","    if not all_results[method]:\n","        print(f\"\\n⚠️ {method.upper()}: no data, skipping\")\n","        continue\n","\n","    print(f\"\\n{method.upper()}:\")\n","    print(\"-\" * 60)\n","\n","    results = all_results[method]\n","    n_folds = len(results)\n","\n","    # Extract per-fold metrics\n","    macro_f1_list = [r['macro_f1'] for r in results]\n","    bal_acc_list = [r['balanced_accuracy'] for r in results]\n","\n","    # Cross-fold statistics\n","    macro_f1_mean = np.mean(macro_f1_list)\n","    macro_f1_std = np.std(macro_f1_list, ddof=1)\n","\n","    bal_acc_mean = np.mean(bal_acc_list)\n","    bal_acc_std = np.std(bal_acc_list, ddof=1)\n","\n","    # Average CI width across folds\n","    ci_lower_list = [r['macro_f1_ci_lower'] for r in results]\n","    ci_upper_list = [r['macro_f1_ci_upper'] for r in results]\n","    ci_width_mean = np.mean([u - l for l, u in zip(ci_lower_list, ci_upper_list)])\n","\n","    # Subject-level F1 aggregation\n","    subject_f1_means = [r['subject_level_f1_mean'] for r in results if r['subject_level_f1_mean'] is not None]\n","    if subject_f1_means:\n","        subject_f1_overall = np.mean(subject_f1_means)\n","        subject_f1_overall_std = np.std(subject_f1_means, ddof=1)\n","    else:\n","        subject_f1_overall = None\n","        subject_f1_overall_std = None\n","\n","    print(f\"  Number of folds: {n_folds}\")\n","    print(f\"  Macro-F1: {macro_f1_mean:.4f} ± {macro_f1_std:.4f}\")\n","    print(f\"  Balanced Acc: {bal_acc_mean:.4f} ± {bal_acc_std:.4f}\")\n","    print(f\"  Mean BCa CI width (Macro-F1): {ci_width_mean:.4f}\")\n","\n","    if subject_f1_overall is not None:\n","        print(f\"  Subject-level F1 (cross-fold mean): {subject_f1_overall:.4f} ± {subject_f1_overall_std:.4f}\")\n","\n","    # Per-class F1 aggregation\n","    per_class_f1_all = [r['per_class_f1'] for r in results]\n","    n_classes = len(per_class_f1_all[0])\n","    per_class_f1_mean = np.mean(per_class_f1_all, axis=0)\n","    per_class_f1_std = np.std(per_class_f1_all, axis=0, ddof=1)\n","\n","    print(f\"\\n  Per-class F1 (cross-fold mean ± std):\")\n","    for cls_idx in range(n_classes):\n","        cls_name = label_names.get(cls_idx, str(cls_idx)) if label_names else str(cls_idx)\n","        print(f\"    {cls_name:15s}: {per_class_f1_mean[cls_idx]:.4f} ± {per_class_f1_std[cls_idx]:.4f}\")\n","\n","    # Save aggregated results\n","    summary[method] = {\n","        'n_folds': n_folds,\n","        'macro_f1_mean': float(macro_f1_mean),\n","        'macro_f1_std': float(macro_f1_std),\n","        'macro_f1_ci_width_mean': float(ci_width_mean),\n","        'balanced_accuracy_mean': float(bal_acc_mean),\n","        'balanced_accuracy_std': float(bal_acc_std),\n","        'subject_level_f1_mean': float(subject_f1_overall) if subject_f1_overall is not None else None,\n","        'subject_level_f1_std': float(subject_f1_overall_std) if subject_f1_overall is not None else None,\n","        'per_class_f1_mean': per_class_f1_mean.tolist(),\n","        'per_class_f1_std': per_class_f1_std.tolist(),\n","        'fold_results': results,\n","    }\n","\n","# Save aggregated JSON\n","summary_file = metrics_dir / \"summary_all_folds.json\"\n","with open(summary_file, 'w') as f:\n","    json.dump(summary, f, indent=2)\n","\n","print(f\"\\n{'='*60}\")\n","print(f\"✓ Aggregated summary saved: {summary_file}\")\n","print(f\"{'='*60}\")\n","\n","# ========== Build comparison table ==========\n","print(f\"\\n{'='*60}\")\n","print(\"Method comparison table\")\n","print(f\"{'='*60}\\n\")\n","\n","comparison_data = []\n","for method in methods:\n","    if method in summary:\n","        s = summary[method]\n","        comparison_data.append({\n","            'Method': method.upper(),\n","            'Macro-F1': f\"{s['macro_f1_mean']:.4f}±{s['macro_f1_std']:.4f}\",\n","            'Balanced Acc': f\"{s['balanced_accuracy_mean']:.4f}±{s['balanced_accuracy_std']:.4f}\",\n","            'Subject-F1': f\"{s['subject_level_f1_mean']:.4f}±{s['subject_level_f1_std']:.4f}\" if s['subject_level_f1_mean'] else 'N/A',\n","            'N Folds': s['n_folds'],\n","        })\n","\n","df_comparison = pd.DataFrame(comparison_data)\n","print(df_comparison.to_string(index=False))\n","\n","# Save comparison table\n","comparison_file = metrics_dir / \"comparison_table.csv\"\n","df_comparison.to_csv(comparison_file, index=False)\n","print(f\"\\n✓ Comparison table saved: {comparison_file}\")\n","\n","# ========== Generate cross-fold boxplots ==========\n","print(f\"\\n{'='*60}\")\n","print(\"Generating cross-fold boxplots\")\n","print(f\"{'='*60}\")\n","\n","plot_data = []\n","for method in methods:\n","    if method in summary:\n","        for result in summary[method]['fold_results']:\n","            plot_data.append({\n","                'Method': method.upper(),\n","                'Fold': result['fold_tag'],\n","                'Macro-F1': result['macro_f1'],\n","                'Balanced Acc': result['balanced_accuracy'],\n","            })\n","\n","df_plot = pd.DataFrame(plot_data)\n","\n","if not df_plot.empty:\n","    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","    # Macro-F1\n","    df_plot_pivot = df_plot.pivot(index='Fold', columns='Method', values='Macro-F1')\n","    df_plot_pivot.plot(kind='box', ax=axes[0])\n","    axes[0].set_title('Macro-F1 across Folds', fontsize=14)\n","    axes[0].set_ylabel('Macro-F1', fontsize=12)\n","    axes[0].grid(True, alpha=0.3)\n","\n","    # Balanced Accuracy\n","    df_plot_pivot2 = df_plot.pivot(index='Fold', columns='Method', values='Balanced Acc')\n","    df_plot_pivot2.plot(kind='box', ax=axes[1])\n","    axes[1].set_title('Balanced Accuracy across Folds', fontsize=14)\n","    axes[1].set_ylabel('Balanced Accuracy', fontsize=12)\n","    axes[1].grid(True, alpha=0.3)\n","\n","    plt.tight_layout()\n","    boxplot_file = metrics_dir / \"boxplot_across_folds.png\"\n","    plt.savefig(boxplot_file, dpi=150, bbox_inches='tight')\n","    plt.close()\n","    print(f\"✓ Saved: {boxplot_file}\")\n","\n","print(f\"\\n{'='*60}\")\n","print(\"Step 16 completed\")\n","print(f\"{'='*60}\")\n","print(f\"\\nMain outputs:\")\n","print(f\"  - metrics/fold_*_*.json (per-fold detailed results)\")\n","print(f\"  - metrics/confusion_*.png (confusion matrices)\")\n","print(f\"  - metrics/summary_all_folds.json (aggregated results)\")\n","print(f\"  - metrics/comparison_table.csv (comparison table)\")\n","print(f\"  - metrics/boxplot_across_folds.png (boxplots across folds)\")\n","print(f\"\\nManuscript-friendly reporting format:\")\n","for method in methods:\n","    if method in summary:\n","        s = summary[method]\n","        f1 = s['macro_f1_mean']\n","        f1_std = s['macro_f1_std']\n","        subj_f1 = s['subject_level_f1_mean']\n","        subj_f1_std = s['subject_level_f1_std']\n","        print(f\"  {method.upper()}: Macro-F1 = {f1:.2%} ± {f1_std:.2%}\", end='')\n","        if subj_f1:\n","            print(f\", Subject-level F1 = {subj_f1:.2%} ± {subj_f1_std:.2%}\")\n","        else:\n","            print()\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FdkFDh4HikUL","executionInfo":{"status":"ok","timestamp":1763130988842,"user_tz":0,"elapsed":86085,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"2fad0ce8-b861-4ec6-cd45-2b9f107da0a5"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 16: Evaluation summary\n","============================================================\n","\n","Searching for all trained folds...\n","Found 6 folds: ['fold_00', 'fold_01', 'fold_02', 'fold_03', 'fold_04', 'fold_05']\n","\n","============================================================\n","Processing fold_00\n","============================================================\n","\n","  Processing minirocket...\n","    Number of samples: 766\n","    Acc: 0.5796\n","    Macro-F1: 0.5381\n","    Balanced Acc: 0.5432\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.5049, 0.5687]\n","    Balanced Acc 95% CI: [0.5015, 0.5787]\n","    Subject-level F1: 0.5381±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_00_minirocket.json\n","    ✓ Saved: metrics/confusion_fold_00_minirocket.png\n","\n","  Processing multirocket...\n","    Number of samples: 766\n","    Acc: 0.5574\n","    Macro-F1: 0.5040\n","    Balanced Acc: 0.5068\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.4749, 0.5322]\n","    Balanced Acc 95% CI: [0.4701, 0.5400]\n","    Subject-level F1: 0.5040±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_00_multirocket.json\n","    ✓ Saved: metrics/confusion_fold_00_multirocket.png\n","\n","  Processing tst...\n","    Number of samples: 766\n","    Acc: 0.4896\n","    Macro-F1: 0.4384\n","    Balanced Acc: 0.4281\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.3991, 0.4786]\n","    Balanced Acc 95% CI: [0.3768, 0.4703]\n","    Subject-level F1: 0.4384±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_00_tst.json\n","    ✓ Saved: metrics/confusion_fold_00_tst.png\n","\n","============================================================\n","Processing fold_01\n","============================================================\n","\n","  Processing minirocket...\n","    Number of samples: 659\n","    Acc: 0.5402\n","    Macro-F1: 0.3786\n","    Balanced Acc: 0.4045\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.3436, 0.4618]\n","    Balanced Acc 95% CI: [0.3381, 0.4850]\n","    Subject-level F1: 0.3786±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_01_minirocket.json\n","    ✓ Saved: metrics/confusion_fold_01_minirocket.png\n","\n","  Processing multirocket...\n","    Number of samples: 659\n","    Acc: 0.5341\n","    Macro-F1: 0.3619\n","    Balanced Acc: 0.3746\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.3320, 0.4475]\n","    Balanced Acc 95% CI: [0.3108, 0.4441]\n","    Subject-level F1: 0.3619±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_01_multirocket.json\n","    ✓ Saved: metrics/confusion_fold_01_multirocket.png\n","\n","  Processing tst...\n","    Number of samples: 659\n","    Acc: 0.5129\n","    Macro-F1: 0.3695\n","    Balanced Acc: 0.4010\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.3378, 0.4047]\n","    Balanced Acc 95% CI: [0.3348, 0.4734]\n","    Subject-level F1: 0.3695±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_01_tst.json\n","    ✓ Saved: metrics/confusion_fold_01_tst.png\n","\n","============================================================\n","Processing fold_02\n","============================================================\n","\n","  Processing minirocket...\n","    Number of samples: 771\n","    Acc: 0.5642\n","    Macro-F1: 0.5202\n","    Balanced Acc: 0.6078\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.4600, 0.5639]\n","    Balanced Acc 95% CI: [0.5674, 0.6499]\n","    Subject-level F1: 0.5202±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_02_minirocket.json\n","    ✓ Saved: metrics/confusion_fold_02_minirocket.png\n","\n","  Processing multirocket...\n","    Number of samples: 771\n","    Acc: 0.5344\n","    Macro-F1: 0.4852\n","    Balanced Acc: 0.5367\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.4336, 0.5242]\n","    Balanced Acc 95% CI: [0.4975, 0.5775]\n","    Subject-level F1: 0.4852±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_02_multirocket.json\n","    ✓ Saved: metrics/confusion_fold_02_multirocket.png\n","\n","  Processing tst...\n","    Number of samples: 771\n","    Acc: 0.4708\n","    Macro-F1: 0.4124\n","    Balanced Acc: 0.5129\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.3609, 0.4628]\n","    Balanced Acc 95% CI: [0.4711, 0.5556]\n","    Subject-level F1: 0.4124±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_02_tst.json\n","    ✓ Saved: metrics/confusion_fold_02_tst.png\n","\n","============================================================\n","Processing fold_03\n","============================================================\n","\n","  Processing minirocket...\n","    Number of samples: 873\n","    Acc: 0.5430\n","    Macro-F1: 0.5620\n","    Balanced Acc: 0.6029\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.5310, 0.5934]\n","    Balanced Acc 95% CI: [0.5729, 0.6320]\n","    Subject-level F1: 0.5620±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_03_minirocket.json\n","    ✓ Saved: metrics/confusion_fold_03_minirocket.png\n","\n","  Processing multirocket...\n","    Number of samples: 873\n","    Acc: 0.5166\n","    Macro-F1: 0.5170\n","    Balanced Acc: 0.5429\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.4752, 0.5510]\n","    Balanced Acc 95% CI: [0.4816, 0.5819]\n","    Subject-level F1: 0.5170±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_03_multirocket.json\n","    ✓ Saved: metrics/confusion_fold_03_multirocket.png\n","\n","  Processing tst...\n","    Number of samples: 873\n","    Acc: 0.4490\n","    Macro-F1: 0.4664\n","    Balanced Acc: 0.4990\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.4211, 0.5059]\n","    Balanced Acc 95% CI: [0.4435, 0.5324]\n","    Subject-level F1: 0.4664±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_03_tst.json\n","    ✓ Saved: metrics/confusion_fold_03_tst.png\n","\n","============================================================\n","Processing fold_04\n","============================================================\n","\n","  Processing minirocket...\n","    Number of samples: 746\n","    Acc: 0.5965\n","    Macro-F1: 0.5961\n","    Balanced Acc: 0.6189\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.5598, 0.6257]\n","    Balanced Acc 95% CI: [0.5712, 0.6546]\n","    Subject-level F1: 0.5961±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_04_minirocket.json\n","    ✓ Saved: metrics/confusion_fold_04_minirocket.png\n","\n","  Processing multirocket...\n","    Number of samples: 746\n","    Acc: 0.6314\n","    Macro-F1: 0.5841\n","    Balanced Acc: 0.5861\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.5476, 0.6187]\n","    Balanced Acc 95% CI: [0.5414, 0.6175]\n","    Subject-level F1: 0.5841±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_04_multirocket.json\n","    ✓ Saved: metrics/confusion_fold_04_multirocket.png\n","\n","  Processing tst...\n","    Number of samples: 746\n","    Acc: 0.5576\n","    Macro-F1: 0.5606\n","    Balanced Acc: 0.5889\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.5279, 0.5921]\n","    Balanced Acc 95% CI: [0.5546, 0.6203]\n","    Subject-level F1: 0.5606±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_04_tst.json\n","    ✓ Saved: metrics/confusion_fold_04_tst.png\n","\n","============================================================\n","Processing fold_05\n","============================================================\n","\n","  Processing minirocket...\n","    Number of samples: 289\n","    Acc: 0.4567\n","    Macro-F1: 0.3696\n","    Balanced Acc: 0.3580\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.3135, 0.4718]\n","    Balanced Acc 95% CI: [0.3015, 0.4305]\n","    Subject-level F1: 0.3696±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_05_minirocket.json\n","    ✓ Saved: metrics/confusion_fold_05_minirocket.png\n","\n","  Processing multirocket...\n","    Number of samples: 289\n","    Acc: 0.4498\n","    Macro-F1: 0.3911\n","    Balanced Acc: 0.3769\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.3184, 0.4849]\n","    Balanced Acc 95% CI: [0.3101, 0.4567]\n","    Subject-level F1: 0.3911±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_05_multirocket.json\n","    ✓ Saved: metrics/confusion_fold_05_multirocket.png\n","\n","  Processing tst...\n","    Number of samples: 289\n","    Acc: 0.3564\n","    Macro-F1: 0.3246\n","    Balanced Acc: 0.3043\n","    Computing BCa 95% CI (n=1000)...\n","    Macro-F1 95% CI: [0.2555, 0.3960]\n","    Balanced Acc 95% CI: [0.2360, 0.3953]\n","    Subject-level F1: 0.3246±0.0000 (n=1)\n","    ✓ Saved: metrics/fold_05_tst.json\n","    ✓ Saved: metrics/confusion_fold_05_tst.png\n","\n","============================================================\n","Aggregate across all folds\n","============================================================\n","\n","MINIROCKET:\n","------------------------------------------------------------\n","  Number of folds: 6\n","  Macro-F1: 0.4941 ± 0.0964\n","  Balanced Acc: 0.5226 ± 0.1135\n","  Mean BCa CI width (Macro-F1): 0.0954\n","  Subject-level F1 (cross-fold mean): 0.4941 ± 0.0964\n","\n","  Per-class F1 (cross-fold mean ± std):\n","    transition     : 0.4481 ± 0.1239\n","    walking        : 0.8189 ± 0.0703\n","    running        : 0.1226 ± 0.0213\n","    sitting        : 0.6435 ± 0.4299\n","    standing       : 0.5973 ± 0.0669\n","    upstairs       : 0.3341 ± 0.0897\n","\n","MULTIROCKET:\n","------------------------------------------------------------\n","  Number of folds: 6\n","  Macro-F1: 0.4739 ± 0.0830\n","  Balanced Acc: 0.4873 ± 0.0901\n","  Mean BCa CI width (Macro-F1): 0.0961\n","  Subject-level F1 (cross-fold mean): 0.4739 ± 0.0830\n","\n","  Per-class F1 (cross-fold mean ± std):\n","    transition     : 0.4177 ± 0.1166\n","    walking        : 0.7762 ± 0.0985\n","    running        : 0.1166 ± 0.0572\n","    sitting        : 0.6637 ± 0.3929\n","    standing       : 0.6102 ± 0.0623\n","    upstairs       : 0.2589 ± 0.0867\n","\n","TST:\n","------------------------------------------------------------\n","  Number of folds: 6\n","  Macro-F1: 0.4286 ± 0.0819\n","  Balanced Acc: 0.4557 ± 0.0996\n","  Mean BCa CI width (Macro-F1): 0.0896\n","  Subject-level F1 (cross-fold mean): 0.4286 ± 0.0819\n","\n","  Per-class F1 (cross-fold mean ± std):\n","    transition     : 0.3999 ± 0.1190\n","    walking        : 0.6971 ± 0.1155\n","    running        : 0.1334 ± 0.0664\n","    sitting        : 0.5851 ± 0.3379\n","    standing       : 0.5365 ± 0.0662\n","    upstairs       : 0.2199 ± 0.0825\n","\n","============================================================\n","✓ Aggregated summary saved: metrics/summary_all_folds.json\n","============================================================\n","\n","============================================================\n","Method comparison table\n","============================================================\n","\n","     Method      Macro-F1  Balanced Acc    Subject-F1  N Folds\n"," MINIROCKET 0.4941±0.0964 0.5226±0.1135 0.4941±0.0964        6\n","MULTIROCKET 0.4739±0.0830 0.4873±0.0901 0.4739±0.0830        6\n","        TST 0.4286±0.0819 0.4557±0.0996 0.4286±0.0819        6\n","\n","✓ Comparison table saved: metrics/comparison_table.csv\n","\n","============================================================\n","Generating cross-fold boxplots\n","============================================================\n","✓ Saved: metrics/boxplot_across_folds.png\n","\n","============================================================\n","Step 16 completed\n","============================================================\n","\n","Main outputs:\n","  - metrics/fold_*_*.json (per-fold detailed results)\n","  - metrics/confusion_*.png (confusion matrices)\n","  - metrics/summary_all_folds.json (aggregated results)\n","  - metrics/comparison_table.csv (comparison table)\n","  - metrics/boxplot_across_folds.png (boxplots across folds)\n","\n","Manuscript-friendly reporting format:\n","  MINIROCKET: Macro-F1 = 49.41% ± 9.64%, Subject-level F1 = 49.41% ± 9.64%\n","  MULTIROCKET: Macro-F1 = 47.39% ± 8.30%, Subject-level F1 = 47.39% ± 8.30%\n","  TST: Macro-F1 = 42.86% ± 8.19%, Subject-level F1 = 42.86% ± 8.19%\n","============================================================\n"]}]}]}