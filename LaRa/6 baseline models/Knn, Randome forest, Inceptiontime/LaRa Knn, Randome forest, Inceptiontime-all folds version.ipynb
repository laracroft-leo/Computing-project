{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyNlfzxihMcRd+ZfPKgChN8n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"katidkdZjUWf","executionInfo":{"status":"ok","timestamp":1763144173342,"user_tz":0,"elapsed":6609,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"5a59e293-4095-4f51-ffd2-d5e10c7f5edd"},"outputs":[{"output_type":"stream","name":"stdout","text":["⚠️ Note: In Jupyter/Colab, PYTHONHASHSEED must be set before the kernel starts\n","   Suggestion: After setting environment variables, restart the runtime, then run the main code\n","\n","Configured random seeds: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","Generating SEEDS.yaml...\n","Generating requirements.txt...\n","Generating env.txt...\n","Generating environment.yml...\n","Collecting hardware information...\n","Collecting Git information...\n","Saving PyTorch build information...\n","Generating data checksums...\n","  data/ directory does not exist; skipping checksums\n","Computing environment hashes...\n","\n","============================================================\n","Step 0 complete - Reproducible environment configuration (top-conf/journal grade)\n","============================================================\n","Output directory: artifacts/env/\n","  ✓ SEEDS.yaml (seeds: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","  ✓ requirements.txt\n","  ✓ env.txt (with system summary)\n","  ✓ environment.yml\n","  ✓ hardware_log.json\n","  ✓ git_info.json (dirty=False)\n","  ✓ torch_build.txt\n","  ✓ ENV.SHA256 (covers all key files)\n","\n","Strict determinism configuration:\n","  - torch.use_deterministic_algorithms: True (warn_only=False)\n","  - cudnn.deterministic: True\n","  - cudnn.benchmark: False\n","  - TF32 disabled: False\n","  - Environment variables set:\n","    PYTHONHASHSEED: 0\n","    CUBLAS_WORKSPACE_CONFIG: :4096:8\n","    Thread control: OMP/MKL/OPENBLAS/NUMEXPR=1\n","============================================================\n"]}],"source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 0: Reproducible Environment (Colab/Jupyter adapted - top-conf/journal grade)\n","Generate a complete reproducible environment configuration\n","\"\"\"\n","\n","# ===== Set environment variables directly (Colab/Jupyter env) =====\n","import os\n","import sys\n","\n","os.environ[\"PYTHONHASHSEED\"] = \"0\"\n","os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n","os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n","os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n","os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n","os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n","\n","print(\"⚠️ Note: In Jupyter/Colab, PYTHONHASHSEED must be set before the kernel starts\")\n","print(\"   Suggestion: After setting environment variables, restart the runtime, then run the main code\\n\")\n","\n","# ===== Environment variables set; continue normal flow =====\n","import json\n","import hashlib\n","import subprocess\n","from pathlib import Path\n","from datetime import datetime, timezone\n","from contextlib import redirect_stdout\n","import io\n","\n","# Check Python version\n","assert sys.version_info >= (3, 10), f\"Require Python ≥ 3.10, current: {sys.version}\"\n","\n","# Create output directory\n","output_dir = Path(\"artifacts/env\")\n","output_dir.mkdir(parents=True, exist_ok=True)\n","\n","# 1. Multiple random seeds (0–9)\n","SEEDS = list(range(10))\n","print(f\"Configured random seeds: {SEEDS}\")\n","\n","# Import and configure\n","import random\n","import numpy as np\n","import torch\n","\n","# Initialize with the first seed\n","random.seed(SEEDS[0])\n","np.random.seed(SEEDS[0])\n","torch.manual_seed(SEEDS[0])\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEEDS[0])\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # Disable TF32\n","    torch.backends.cuda.matmul.allow_tf32 = False\n","    torch.backends.cudnn.allow_tf32 = False\n","\n","# Enable strict deterministic algorithms (not using warn_only)\n","torch.use_deterministic_algorithms(True)\n","\n","# Set matmul precision\n","if hasattr(torch, 'set_float32_matmul_precision'):\n","    torch.set_float32_matmul_precision(\"high\")\n","\n","# 2. Generate SEEDS.yaml (with fallback)\n","print(\"Generating SEEDS.yaml...\")\n","seeds_config = {\n","    \"seeds\": SEEDS,\n","    \"default_seed\": SEEDS[0],\n","    \"description\": \"Random seeds for python, numpy, torch, sklearn\"\n","}\n","try:\n","    import yaml\n","    with open(output_dir / \"SEEDS.yaml\", \"w\") as f:\n","        yaml.dump(seeds_config, f, default_flow_style=False)\n","except ImportError:\n","    # Fallback if PyYAML is not installed\n","    yaml_content = f\"\"\"seeds: {SEEDS}\n","default_seed: {SEEDS[0]}\n","description: Random seeds for python, numpy, torch, sklearn\n","\"\"\"\n","    with open(output_dir / \"SEEDS.yaml\", \"w\") as f:\n","        f.write(yaml_content)\n","\n","# 3. Generate requirements.txt (frozen versions)\n","print(\"Generating requirements.txt...\")\n","result = subprocess.run(\n","    [sys.executable, \"-m\", \"pip\", \"freeze\"],\n","    capture_output=True, text=True\n",")\n","requirements = result.stdout\n","with open(output_dir / \"requirements.txt\", \"w\") as f:\n","    f.write(requirements)\n","\n","# 4. Collect system info (for env.txt header)\n","import platform\n","system_info = []\n","system_info.append(\"=\"*60)\n","system_info.append(\"Environment Snapshot - System Overview\")\n","system_info.append(\"=\"*60)\n","system_info.append(f\"Time (UTC): {datetime.now(timezone.utc).isoformat()}\")\n","system_info.append(f\"Python: {sys.version}\")\n","system_info.append(f\"Platform: {platform.system()} {platform.release()} ({platform.machine()})\")\n","\n","try:\n","    import psutil\n","    system_info.append(f\"CPU: {psutil.cpu_count(logical=False)} cores / {psutil.cpu_count(logical=True)} threads\")\n","    system_info.append(f\"Memory: {round(psutil.virtual_memory().total / (1024**3), 2)} GB\")\n","except ImportError:\n","    pass\n","\n","system_info.append(f\"PyTorch: {torch.__version__}\")\n","if torch.cuda.is_available():\n","    system_info.append(f\"CUDA: {torch.version.cuda}\")\n","    system_info.append(f\"cuDNN: {torch.backends.cudnn.version()}\")\n","    try:\n","        out = subprocess.run(\n","            [\"nvidia-smi\", \"--query-gpu=driver_version\", \"--format=csv,noheader\"],\n","            capture_output=True, text=True\n","        )\n","        if out.returncode == 0 and out.stdout.strip():\n","            system_info.append(f\"NVIDIA driver: {out.stdout.strip().splitlines()[0]}\")\n","    except:\n","        pass\n","\n","system_info.append(\"\\nEnvironment variables:\")\n","for key in [\"PYTHONHASHSEED\", \"CUBLAS_WORKSPACE_CONFIG\", \"OMP_NUM_THREADS\",\n","            \"MKL_NUM_THREADS\", \"OPENBLAS_NUM_THREADS\", \"NUMEXPR_NUM_THREADS\"]:\n","    system_info.append(f\"  {key}={os.environ.get(key, 'N/A')}\")\n","\n","system_info.append(\"\\n\" + \"=\"*60)\n","system_info.append(\"Installed packages list\")\n","system_info.append(\"=\"*60 + \"\\n\")\n","\n","# 5. Generate env.txt (human-readable + system summary)\n","print(\"Generating env.txt...\")\n","result = subprocess.run(\n","    [sys.executable, \"-m\", \"pip\", \"list\"],\n","    capture_output=True, text=True\n",")\n","with open(output_dir / \"env.txt\", \"w\") as f:\n","    f.write(\"\\n\".join(system_info))\n","    f.write(result.stdout)\n","\n","# 6. Generate environment.yml\n","print(\"Generating environment.yml...\")\n","env_yml = f\"\"\"name: har_lara\n","channels:\n","  - defaults\n","  - conda-forge\n","dependencies:\n","  - python={sys.version_info.major}.{sys.version_info.minor}\n","  - pip\n","  - pip:\n","\"\"\"\n","for line in requirements.strip().split(\"\\n\"):\n","    if line and not line.startswith(\"#\"):\n","        env_yml += f\"      - {line}\\n\"\n","\n","with open(output_dir / \"environment.yml\", \"w\") as f:\n","    f.write(env_yml)\n","\n","# 7. Collect complete hardware information\n","print(\"Collecting hardware information...\")\n","hardware_info = {\n","    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n","    \"python_version\": sys.version,\n","    \"python_executable\": sys.executable,\n","    \"platform\": sys.platform,\n","    \"os\": platform.system(),\n","    \"os_release\": platform.release(),\n","    \"os_version\": platform.version(),\n","    \"machine\": platform.machine(),\n","    \"processor\": platform.processor(),\n","}\n","\n","try:\n","    import psutil\n","    hardware_info[\"cpu_count_physical\"] = psutil.cpu_count(logical=False)\n","    hardware_info[\"cpu_count_logical\"] = psutil.cpu_count(logical=True)\n","    hardware_info[\"memory_total_gb\"] = round(psutil.virtual_memory().total / (1024**3), 2)\n","except ImportError:\n","    pass\n","\n","hardware_info[\"torch_version\"] = torch.__version__\n","\n","if torch.cuda.is_available():\n","    hardware_info[\"gpu_available\"] = True\n","    hardware_info[\"gpu_count\"] = torch.cuda.device_count()\n","    hardware_info[\"gpu_names\"] = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n","    hardware_info[\"cuda_version\"] = torch.version.cuda\n","    hardware_info[\"cudnn_version\"] = torch.backends.cudnn.version()\n","\n","    gpu_details = []\n","    for i in range(torch.cuda.device_count()):\n","        props = torch.cuda.get_device_properties(i)\n","        gpu_details.append({\n","            \"id\": i,\n","            \"name\": props.name,\n","            \"compute_capability\": f\"{props.major}.{props.minor}\",\n","            \"total_memory_gb\": round(props.total_memory / (1024**3), 2),\n","            \"multi_processor_count\": props.multi_processor_count\n","        })\n","    hardware_info[\"gpu_details\"] = gpu_details\n","\n","    try:\n","        out = subprocess.run(\n","            [\"nvidia-smi\", \"--query-gpu=driver_version\", \"--format=csv,noheader\"],\n","            capture_output=True, text=True\n","        )\n","        if out.returncode == 0 and out.stdout.strip():\n","            hardware_info[\"nvidia_driver_version\"] = out.stdout.strip().splitlines()[0]\n","    except:\n","        pass\n","else:\n","    hardware_info[\"gpu_available\"] = False\n","\n","hardware_info[\"deterministic_config\"] = {\n","    \"cudnn_deterministic\": torch.backends.cudnn.deterministic,\n","    \"cudnn_benchmark\": torch.backends.cudnn.benchmark,\n","    \"use_deterministic_algorithms\": True,\n","    \"warn_only\": False,\n","    \"tf32_disabled\": not torch.backends.cuda.matmul.allow_tf32 if torch.cuda.is_available() else \"N/A\",\n","    \"float32_matmul_precision\": \"high\" if hasattr(torch, 'set_float32_matmul_precision') else \"N/A\",\n","    \"PYTHONHASHSEED\": os.environ.get(\"PYTHONHASHSEED\"),\n","    \"CUBLAS_WORKSPACE_CONFIG\": os.environ.get(\"CUBLAS_WORKSPACE_CONFIG\"),\n","    \"OMP_NUM_THREADS\": os.environ.get(\"OMP_NUM_THREADS\"),\n","    \"MKL_NUM_THREADS\": os.environ.get(\"MKL_NUM_THREADS\"),\n","    \"OPENBLAS_NUM_THREADS\": os.environ.get(\"OPENBLAS_NUM_THREADS\"),\n","    \"NUMEXPR_NUM_THREADS\": os.environ.get(\"NUMEXPR_NUM_THREADS\"),\n","}\n","\n","with open(output_dir / \"hardware_log.json\", \"w\") as f:\n","    json.dump(hardware_info, f, indent=2)\n","\n","# 8. Git commit + dirty flag\n","print(\"Collecting Git information...\")\n","git_info = {}\n","try:\n","    git_commit = subprocess.run(\n","        [\"git\", \"rev-parse\", \"HEAD\"],\n","        capture_output=True, text=True, check=True\n","    ).stdout.strip()\n","    git_info[\"commit\"] = git_commit\n","\n","    git_branch = subprocess.run(\n","        [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n","        capture_output=True, text=True, check=True\n","    ).stdout.strip()\n","    git_info[\"branch\"] = git_branch\n","\n","    dirty = subprocess.run(\n","        [\"git\", \"status\", \"--porcelain\"],\n","        capture_output=True, text=True\n","    ).stdout.strip()\n","    git_info[\"dirty\"] = bool(dirty)\n","except:\n","    git_info[\"commit\"] = \"N/A (not a git repo)\"\n","    git_info[\"dirty\"] = False\n","\n","with open(output_dir / \"git_info.json\", \"w\") as f:\n","    json.dump(git_info, f, indent=2)\n","\n","# 9. PyTorch build information\n","print(\"Saving PyTorch build information...\")\n","try:\n","    buf = io.StringIO()\n","    with redirect_stdout(buf):\n","        torch.__config__.show()\n","    (output_dir / \"torch_build.txt\").write_text(buf.getvalue(), encoding=\"utf-8\")\n","except:\n","    pass\n","\n","# 10. Data checksums (only original archives)\n","print(\"Generating data checksums...\")\n","data_dir = Path(\"data\")\n","if data_dir.exists():\n","    sha256sums = []\n","    archive_exts = {'.zip', '.tar', '.gz', '.tgz', '.bz2', '.xz', '.7z', '.rar'}\n","    for file_path in sorted(data_dir.rglob(\"*\")):\n","        if file_path.is_file() and file_path.suffix.lower() in archive_exts:\n","            sha256 = hashlib.sha256()\n","            with open(file_path, \"rb\") as f:\n","                for chunk in iter(lambda: f.read(65536), b\"\"):\n","                    sha256.update(chunk)\n","            rel_path = file_path.relative_to(data_dir)\n","            sha256sums.append(f\"{sha256.hexdigest()}  {rel_path}\")\n","\n","    if sha256sums:\n","        with open(output_dir / \"data_SHA256SUMS.txt\", \"w\") as f:\n","            f.write(\"\\n\".join(sha256sums))\n","        print(f\"  Generated checksums for {len(sha256sums)} archives\")\n","    else:\n","        print(\"  No archives in data/ directory; skipping checksums\")\n","else:\n","    print(\"  data/ directory does not exist; skipping checksums\")\n","\n","# 11. Compute environment hashes of all key files\n","print(\"Computing environment hashes...\")\n","env_files = [\n","    \"requirements.txt\",\n","    \"environment.yml\",\n","    \"env.txt\",\n","    \"SEEDS.yaml\",\n","    \"hardware_log.json\",\n","    \"git_info.json\"\n","]\n","sha256_lines = []\n","for filename in env_files:\n","    filepath = output_dir / filename\n","    if filepath.exists():\n","        sha256 = hashlib.sha256()\n","        with open(filepath, \"rb\") as f:\n","            sha256.update(f.read())\n","        sha256_lines.append(f\"{sha256.hexdigest()}  {filename}\")\n","\n","with open(output_dir / \"ENV.SHA256\", \"w\") as f:\n","    f.write(\"\\n\".join(sha256_lines))\n","\n","# Output summary\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 0 complete - Reproducible environment configuration (top-conf/journal grade)\")\n","print(\"=\"*60)\n","print(f\"Output directory: {output_dir}/\")\n","print(f\"  ✓ SEEDS.yaml (seeds: {SEEDS})\")\n","print(f\"  ✓ requirements.txt\")\n","print(f\"  ✓ env.txt (with system summary)\")\n","print(f\"  ✓ environment.yml\")\n","print(f\"  ✓ hardware_log.json\")\n","print(f\"  ✓ git_info.json (dirty={git_info.get('dirty', False)})\")\n","print(f\"  ✓ torch_build.txt\")\n","print(f\"  ✓ ENV.SHA256 (covers all key files)\")\n","if (output_dir / \"data_SHA256SUMS.txt\").exists():\n","    print(f\"  ✓ data_SHA256SUMS.txt (archives only)\")\n","\n","print(f\"\\nStrict determinism configuration:\")\n","print(f\"  - torch.use_deterministic_algorithms: True (warn_only=False)\")\n","print(f\"  - cudnn.deterministic: {torch.backends.cudnn.deterministic}\")\n","print(f\"  - cudnn.benchmark: {torch.backends.cudnn.benchmark}\")\n","if torch.cuda.is_available():\n","    print(f\"  - TF32 disabled: {not torch.backends.cuda.matmul.allow_tf32}\")\n","print(f\"  - Environment variables set:\")\n","print(f\"    PYTHONHASHSEED: {os.environ.get('PYTHONHASHSEED')}\")\n","print(f\"    CUBLAS_WORKSPACE_CONFIG: {os.environ.get('CUBLAS_WORKSPACE_CONFIG')}\")\n","print(f\"    Thread control: OMP/MKL/OPENBLAS/NUMEXPR=1\")\n","print(\"=\"*60)"]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Steps 1–2: Data Acquisition & Unpack Standardization (top-conf/journal grade)\n","Process the uploaded LARa MbientLab IMU archive\n","\"\"\"\n","\n","import os\n","import hashlib\n","import zipfile\n","import shutil\n","import json\n","import re\n","import numpy as np\n","from pathlib import Path\n","from datetime import datetime, timezone\n","import pandas as pd\n","\n","# ========== Helper functions ==========\n","def read_any_csv(path, nrows=None):\n","    \"\"\"CSV reader with auto delimiter detection\"\"\"\n","    try:\n","        return pd.read_csv(path, nrows=nrows, sep=None, engine=\"python\")\n","    except Exception:\n","        return pd.read_csv(path, nrows=nrows)\n","\n","def infer_sampling_rate(df):\n","    \"\"\"Infer sampling rate; auto-handle ns/μs/ms/s time units\"\"\"\n","    cols = [c.lower() for c in df.columns]\n","    time_cols = [c for c in df.columns if re.search(r\"(time|timestamp|epoch)\", c.lower())]\n","    if not time_cols:\n","        return None\n","\n","    c = time_cols[0]\n","    t = pd.to_numeric(df[c], errors=\"coerce\").dropna().to_numpy()\n","    if t.size < 3:\n","        return None\n","\n","    # Infer time unit by magnitude\n","    max_val = np.nanmax(np.abs(t[:1000])) if t.size else 0\n","    if max_val >= 1e12:      # nanoseconds\n","        scale = 1e-9\n","    elif max_val >= 1e9:     # nanoseconds\n","        scale = 1e-9\n","    elif max_val >= 1e6:     # microseconds\n","        scale = 1e-6\n","    elif max_val >= 1e3:     # milliseconds\n","        scale = 1e-3\n","    else:                    # seconds\n","        scale = 1.0\n","\n","    t_sec = t * scale\n","    dt = np.diff(t_sec)\n","    dt = dt[dt > 0]\n","    if dt.size == 0:\n","        return None\n","\n","    # Use median for robustness\n","    return float(np.round(1.0 / np.median(dt), 3))\n","\n","def infer_sensor_type(cols_lower, filename):\n","    \"\"\"Infer sensor type\"\"\"\n","    if 'label' in filename.lower() or 'activity' in filename.lower():\n","        return \"labels\"\n","\n","    sensors = []\n","    if any((\"acc\" in c) or (\"accelerom\" in c) for c in cols_lower):\n","        sensors.append(\"acc\")\n","    if any((\"gyro\" in c) or re.search(r\"\\bgyr\", c) for c in cols_lower):\n","        sensors.append(\"gyro\")\n","    if any((\"mag\" in c) or (\"magnetom\" in c) for c in cols_lower):\n","        sensors.append(\"mag\")\n","\n","    return \"+\".join(sensors) if sensors else \"unknown\"\n","\n","# LARa placement mapping (per official docs)\n","PLACEMENT_MAP = {\n","    \"L01\": \"lwrist\",      # Left wrist\n","    \"L02\": \"rwrist\",      # Right wrist\n","    \"L03\": \"chest\",       # Chest\n","    \"L04\": \"belt\",        # Belt\n","    \"L05\": \"lankle\",      # Left ankle\n","    \"L06\": \"pocket\",      # Pocket\n","    \"L07\": \"lforearm\",    # Left forearm\n","    \"L08\": \"lupperarm\",   # Left upper arm\n","}\n","\n","# ========== Step 1: Acquire & verify ==========\n","print(\"=\"*60)\n","print(\"Step 1: Data acquisition & verification\")\n","print(\"=\"*60)\n","\n","# Create directory structure\n","raw_dir = Path(\"data/lara/mbientlab/raw\")\n","raw_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Find uploaded zip files (prefer annotated versions)\n","uploaded_files = list(Path(\".\").glob(\"*annotated*MbientLab*.zip\"))\n","if not uploaded_files:\n","    uploaded_files = list(Path(\".\").glob(\"*MbientLab*.zip\"))\n","if not uploaded_files:\n","    uploaded_files = list(Path(\".\").glob(\"*.zip\"))\n","\n","if not uploaded_files:\n","    raise FileNotFoundError(\"No MbientLab data archive found; please upload a zip file first\")\n","\n","if len(uploaded_files) > 1:\n","    print(f\"Warning: found multiple candidate files: {[f.name for f in uploaded_files]}\")\n","    print(f\"Using the first: {uploaded_files[0].name}\")\n","\n","zip_file = uploaded_files[0]\n","print(f\"Found archive: {zip_file}\")\n","\n","# Move to raw data directory\n","target_zip = raw_dir / zip_file.name\n","if not target_zip.exists():\n","    shutil.copy2(zip_file, target_zip)\n","    print(f\"Copied to: {target_zip}\")\n","else:\n","    print(f\"File already exists: {target_zip}\")\n","\n","# Compute SHA256 checksum\n","print(\"Computing SHA256 checksum...\")\n","sha256_hash = hashlib.sha256()\n","with open(target_zip, \"rb\") as f:\n","    for chunk in iter(lambda: f.read(65536), b\"\"):\n","        sha256_hash.update(chunk)\n","\n","checksum = sha256_hash.hexdigest()\n","print(f\"SHA256: {checksum}\")\n","\n","# Save checksum\n","sha256_file = raw_dir / \"SHA256SUMS.txt\"\n","with open(sha256_file, \"w\") as f:\n","    f.write(f\"{checksum}  {target_zip.name}\\n\")\n","print(f\"Saved checksum: {sha256_file}\")\n","\n","# Record provenance (traceability)\n","provenance = {\n","    \"dataset\": \"LARa IMU-only / MbientLab\",\n","    \"origin\": \"manual-upload\",\n","    \"official_url\": \"https://sensor.informatik.uni-mannheim.de/#dataset_lara\",\n","    \"retrieved_at_utc\": datetime.now(timezone.utc).isoformat(),\n","    \"archive\": target_zip.name,\n","    \"sha256\": checksum\n","}\n","(raw_dir / \"PROVENANCE.json\").write_text(\n","    json.dumps(provenance, indent=2, ensure_ascii=False),\n","    encoding=\"utf-8\"\n",")\n","print(f\"Recorded provenance info: {raw_dir / 'PROVENANCE.json'}\")\n","\n","# Set raw archive to read-only\n","os.chmod(target_zip, 0o444)\n","print(f\"Set read-only permission: {target_zip}\")\n","\n","# ========== Step 2: Unpack & directory standardization ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 2: Unpack & directory standardization\")\n","print(\"=\"*60)\n","\n","# Extract to temp directory\n","temp_extract = raw_dir / \"temp_extract\"\n","temp_extract.mkdir(exist_ok=True)\n","\n","print(f\"Extracting {target_zip.name}...\")\n","with zipfile.ZipFile(target_zip, 'r') as zip_ref:\n","    zip_ref.extractall(temp_extract)\n","\n","# Scan extracted files and normalize\n","file_records = []\n","problems = []  # record files that failed to parse\n","\n","# Recursively scan all CSV/TSV files\n","for file_path in temp_extract.rglob(\"*\"):\n","    if not file_path.is_file():\n","        continue\n","\n","    # Process only data files\n","    if file_path.suffix.lower() not in ['.csv', '.tsv', '.txt']:\n","        continue\n","\n","    # Parse filename: LARa pattern L01_S07_R01.csv\n","    filename = file_path.stem\n","\n","    # Extract L01/L02/L03 (placement)\n","    placement_match = re.search(r'L(\\d+)', filename)\n","    placement_raw = f\"L{placement_match.group(1).zfill(2)}\" if placement_match else \"L00\"\n","    placement = PLACEMENT_MAP.get(placement_raw, placement_raw)\n","\n","    # Extract S07 (subject)\n","    subject_match = re.search(r'S(\\d+)', filename)\n","    subject_id = f\"S{subject_match.group(1).zfill(2)}\" if subject_match else \"S00\"\n","\n","    # Extract R01 (session)\n","    session_match = re.search(r'R(\\d+)', filename)\n","    session_id = f\"R{session_match.group(1).zfill(2)}\" if session_match else \"R01\"\n","\n","    # Detect parse failures (avoid LOSO leakage)\n","    if subject_id == \"S00\" or session_id == \"R01\":\n","        if not re.search(r'R01', filename):  # exclude real R01\n","            problems.append(str(file_path.relative_to(temp_extract)))\n","\n","    # Create standardized directory structure\n","    std_dir = raw_dir / subject_id / session_id / placement\n","    std_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # Standardized filename (lowercase, underscores)\n","    std_filename = file_path.name.lower().replace(' ', '_').replace('-', '_')\n","    std_path = std_dir / std_filename\n","\n","    # Copy to standardized location\n","    if not std_path.exists():\n","        shutil.copy2(file_path, std_path)\n","\n","    # Get file info\n","    file_size = file_path.stat().st_size\n","    num_rows = 0\n","    sampling_rate = None\n","    duration = None\n","    sensor_type = \"unknown\"\n","\n","    try:\n","        # Read sample\n","        df_sample = read_any_csv(file_path, nrows=2000)\n","        columns_lower = [c.lower() for c in df_sample.columns]\n","\n","        # Infer sensor type\n","        sensor_type = infer_sensor_type(columns_lower, filename)\n","\n","        # Infer sampling rate (skip for labels)\n","        if sensor_type != \"labels\":\n","            sampling_rate = infer_sampling_rate(df_sample)\n","\n","        # Count total rows (streaming to avoid loading big files)\n","        with open(file_path, \"rb\") as fh:\n","            num_rows = sum(1 for _ in fh) - 1  # minus header\n","\n","        # Compute duration\n","        if sampling_rate and num_rows > 0:\n","            duration = round(num_rows / sampling_rate, 2)\n","\n","    except Exception:\n","        pass  # silently skip files that cannot be parsed\n","\n","    # Record file info\n","    file_records.append({\n","        \"subject_id\": subject_id,\n","        \"session_id\": session_id,\n","        \"placement\": placement,\n","        \"placement_raw\": placement_raw,\n","        \"sensor_type\": sensor_type,\n","        \"original_path\": str(file_path.relative_to(temp_extract)),\n","        \"standardized_path\": str(std_path.relative_to(raw_dir)),\n","        \"filename\": std_filename,\n","        \"file_size_bytes\": file_size,\n","        \"num_rows\": num_rows,\n","        \"sampling_rate_hz\": sampling_rate,\n","        \"duration_sec\": duration,\n","    })\n","\n","print(f\"Processed {len(file_records)} files\")\n","\n","# Check parse failures\n","if problems:\n","    problems_file = raw_dir / \"PROBLEMS.log\"\n","    problems_file.write_text(\n","        \"The following files could not parse subject/session (would break LOSO):\\n\" +\n","        \"\\n\".join(problems) + \"\\n\",\n","        encoding=\"utf-8\"\n","    )\n","    raise RuntimeError(\n","        f\"Found {len(problems)} files with unparsed subject/session; \"\n","        f\"please check {problems_file} and fix\"\n","    )\n","\n","# Remove temp extraction directory\n","shutil.rmtree(temp_extract)\n","print(\"Removed temporary files\")\n","\n","# Generate file_index (Parquet preferred; fallback to CSV)\n","if file_records:\n","    file_index = pd.DataFrame(file_records)\n","\n","    # Sort\n","    file_index = file_index.sort_values(\n","        ['subject_id', 'session_id', 'placement', 'sensor_type']\n","    )\n","\n","    # Save index\n","    index_file = raw_dir / \"file_index.parquet\"\n","    try:\n","        file_index.to_parquet(index_file, index=False)\n","        saved_index = index_file\n","        print(f\"\\nGenerated file index: {saved_index}\")\n","    except Exception as e:\n","        print(f\"Warning: Parquet write failed ({e}); falling back to CSV\")\n","        index_file_csv = raw_dir / \"file_index.csv\"\n","        file_index.to_csv(index_file_csv, index=False)\n","        saved_index = index_file_csv\n","        print(f\"Generated file index: {saved_index}\")\n","\n","    # Show dataset statistics\n","    print(\"\\nDataset statistics:\")\n","    print(f\"  Number of subjects: {file_index['subject_id'].nunique()}\")\n","    print(f\"  Number of sessions: {file_index.groupby('subject_id')['session_id'].nunique().sum()}\")\n","    print(f\"  Placements: {sorted(file_index['placement'].unique().tolist())}\")\n","    print(f\"  Sensor types: {sorted(file_index['sensor_type'].unique().tolist())}\")\n","    print(f\"  Total files: {len(file_index)}\")\n","\n","    # Sampling rate stats\n","    sensor_files = file_index[file_index['sensor_type'] != 'labels']\n","    if not sensor_files.empty:\n","        rates = sensor_files['sampling_rate_hz'].dropna()\n","        if not rates.empty:\n","            print(f\"  Sampling rate range: {rates.min():.1f} - {rates.max():.1f} Hz\")\n","            print(f\"  Median sampling rate: {rates.median():.1f} Hz\")\n","\n","    # Preview first records\n","    print(\"\\nFile index preview:\")\n","    print(file_index.head(10).to_string())\n","else:\n","    print(\"Warning: No data files found\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Steps 1–2 complete (top-conf/journal grade)\")\n","print(\"=\"*60)\n","print(f\"Raw data: {raw_dir}/\")\n","print(f\"Checksum: {sha256_file}\")\n","print(f\"Provenance record: {raw_dir / 'PROVENANCE.json'}\")\n","print(f\"File index: {saved_index}\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qAGVcRTXkyvs","executionInfo":{"status":"ok","timestamp":1763144246032,"user_tz":0,"elapsed":30458,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"077151dd-0a70-4679-d48a-3a341cbe001a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 1: Data acquisition & verification\n","============================================================\n","Found archive: IMU data (annotated) _ MbientLab.zip\n","Copied to: data/lara/mbientlab/raw/IMU data (annotated) _ MbientLab.zip\n","Computing SHA256 checksum...\n","SHA256: 70968b6b8874375e96671af67e31c27ccb63793f31191f86e732d40f24ac3106\n","Saved checksum: data/lara/mbientlab/raw/SHA256SUMS.txt\n","Recorded provenance info: data/lara/mbientlab/raw/PROVENANCE.json\n","Set read-only permission: data/lara/mbientlab/raw/IMU data (annotated) _ MbientLab.zip\n","\n","============================================================\n","Step 2: Unpack & directory standardization\n","============================================================\n","Extracting IMU data (annotated) _ MbientLab.zip...\n","Processed 386 files\n","Removed temporary files\n","\n","Generated file index: data/lara/mbientlab/raw/file_index.parquet\n","\n","Dataset statistics:\n","  Number of subjects: 8\n","  Number of sessions: 193\n","  Placements: ['chest', 'lwrist', 'rwrist']\n","  Sensor types: ['acc+gyro', 'labels']\n","  Total files: 386\n","\n","File index preview:\n","   subject_id session_id placement placement_raw sensor_type                                                original_path                      standardized_path                filename  file_size_bytes  num_rows sampling_rate_hz duration_sec\n","86        S07        R01    lwrist           L01    acc+gyro         IMU data (annotated) _ MbientLab/S07/L01_S07_R01.csv         S07/R01/lwrist/l01_s07_r01.csv         l01_s07_r01.csv          7227132     11824             None         None\n","51        S07        R01    lwrist           L01      labels  IMU data (annotated) _ MbientLab/S07/L01_S07_R01_labels.csv  S07/R01/lwrist/l01_s07_r01_labels.csv  l01_s07_r01_labels.csv           485055     11824             None         None\n","72        S07        R02    lwrist           L01    acc+gyro         IMU data (annotated) _ MbientLab/S07/L01_S07_R02.csv         S07/R02/lwrist/l01_s07_r02.csv         l01_s07_r02.csv          7196022     11776             None         None\n","37        S07        R02    lwrist           L01      labels  IMU data (annotated) _ MbientLab/S07/L01_S07_R02_labels.csv  S07/R02/lwrist/l01_s07_r02_labels.csv  l01_s07_r02_labels.csv           483087     11776             None         None\n","83        S07        R03    rwrist           L02    acc+gyro         IMU data (annotated) _ MbientLab/S07/L02_S07_R03.csv         S07/R03/rwrist/l02_s07_r03.csv         l02_s07_r03.csv          7193816     11758             None         None\n","67        S07        R03    rwrist           L02      labels  IMU data (annotated) _ MbientLab/S07/L02_S07_R03_labels.csv  S07/R03/rwrist/l02_s07_r03_labels.csv  l02_s07_r03_labels.csv           482349     11758             None         None\n","46        S07        R05    rwrist           L02    acc+gyro         IMU data (annotated) _ MbientLab/S07/L02_S07_R05.csv         S07/R05/rwrist/l02_s07_r05.csv         l02_s07_r05.csv          7197459     11766             None         None\n","66        S07        R05    rwrist           L02      labels  IMU data (annotated) _ MbientLab/S07/L02_S07_R05_labels.csv  S07/R05/rwrist/l02_s07_r05_labels.csv  l02_s07_r05_labels.csv           482677     11766             None         None\n","64        S07        R06    rwrist           L02    acc+gyro         IMU data (annotated) _ MbientLab/S07/L02_S07_R06.csv         S07/R06/rwrist/l02_s07_r06.csv         l02_s07_r06.csv          7238567     11838             None         None\n","49        S07        R06    rwrist           L02      labels  IMU data (annotated) _ MbientLab/S07/L02_S07_R06_labels.csv  S07/R06/rwrist/l02_s07_r06_labels.csv  l02_s07_r06_labels.csv           485629     11838             None         None\n","\n","============================================================\n","Steps 1–2 complete (top-conf/journal grade)\n","============================================================\n","Raw data: data/lara/mbientlab/raw/\n","Checksum: data/lara/mbientlab/raw/SHA256SUMS.txt\n","Provenance record: data/lara/mbientlab/raw/PROVENANCE.json\n","File index: data/lara/mbientlab/raw/file_index.parquet\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 3: Metadata & Quality Audit (top-conf/journal grade - final)\n","Parse subjects, activity set, sampling rate, placement, session time; empty-window cleanup\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","from datetime import datetime, timezone\n","import json\n","import re\n","\n","# ========== Config ==========\n","MISSING_THRESHOLD = 0.05      # Missing-rate threshold 5%\n","GAP_THRESHOLD = 2.0           # Single-gap absolute threshold (seconds)\n","GAP_RATIO_THRESHOLD = 0.05    # Gap ratio threshold 5%\n","\n","print(\"=\"*60)\n","print(\"Step 3: Metadata & Quality Audit\")\n","print(\"=\"*60)\n","\n","# Load file index\n","raw_dir = Path(\"data/lara/mbientlab/raw\")\n","index_file = raw_dir / \"file_index.parquet\"\n","if not index_file.exists():\n","    index_file = raw_dir / \"file_index.csv\"\n","\n","print(f\"Loading file index: {index_file}\")\n","file_index = pd.read_parquet(index_file) if index_file.suffix == '.parquet' else pd.read_csv(index_file)\n","\n","# Initialize variables (avoid undefined in edge cases)\n","subject_agg = pd.DataFrame()\n","meta_subjects_file = None\n","meta_sessions_file = None\n","keep_sessions_file = None\n","\n","# ========== Helper functions ==========\n","def pick_scale(med_raw, sr_hint=None):\n","    \"\"\"Smartly pick time unit (s/ms/μs/ns → seconds)\"\"\"\n","    cands = [1.0, 1e-3, 1e-6, 1e-9]\n","\n","    if sr_hint and sr_hint > 0:\n","        target_dt = 1.0 / sr_hint\n","        return min(cands, key=lambda s: abs(med_raw * s - target_dt))\n","\n","    # Without hint: prefer median interval mapping into 5-400 Hz, bias toward ~50 Hz\n","    best, err = 1.0, float(\"inf\")\n","    for s in cands:\n","        dt = med_raw * s\n","        if dt <= 0:\n","            continue\n","        sr = 1.0 / dt\n","        score = 0 if 5 <= sr <= 400 else abs(sr - 50) * 10\n","        if score < err:\n","            best, err = s, score\n","    return best\n","\n","def extract_time_range_and_gaps(file_path, sampling_rate_hint=None, head_rows=20000, chunksize=200000):\n","    \"\"\"Read time column in chunks; extract range and gaps (incl. inter-chunk gaps, memory-friendly)\"\"\"\n","    try:\n","        # Infer time column & unit from a small sample\n","        df_head = pd.read_csv(file_path, nrows=head_rows, sep=None, engine=\"python\")\n","        time_cols = [c for c in df_head.columns if re.search(r\"(time|timestamp|epoch|ts)\", c, re.I)]\n","        if not time_cols:\n","            return None, None, 0.0, 0.0, 0.0\n","\n","        c = time_cols[0]\n","        s = pd.to_numeric(df_head[c], errors=\"coerce\").dropna().to_numpy()\n","\n","        # Numeric timestamp branch\n","        if s.size >= 3:\n","            diffs = np.diff(s)\n","            diffs = diffs[np.isfinite(diffs) & (diffs > 0)]\n","            if diffs.size > 0:\n","                med = float(np.median(diffs))\n","                scale = pick_scale(med, sampling_rate_hint)\n","                expected = (1.0 / sampling_rate_hint) if (sampling_rate_hint and sampling_rate_hint > 0) else (med * scale)\n","\n","                # OR logic: two independent thresholds\n","                rel_threshold = 10.0 * expected  # Relative threshold: 10× expected interval\n","                abs_threshold = GAP_THRESHOLD    # Absolute threshold: 2 s\n","\n","                first = None\n","                last = None\n","                prev = None\n","                gap_sec = 0.0\n","                max_gap = 0.0\n","\n","                for chunk in pd.read_csv(file_path, usecols=[c], sep=None, engine=\"python\", chunksize=chunksize):\n","                    v = pd.to_numeric(chunk[c], errors=\"coerce\").dropna().to_numpy()\n","                    if v.size == 0:\n","                        continue\n","\n","                    if first is None:\n","                        first = v[0]\n","\n","                    # Inter-chunk gaps (fix: use max as baseline)\n","                    if prev is not None:\n","                        delta = (v[0] - prev) * scale\n","                        cond_rel = delta > rel_threshold\n","                        cond_abs = delta > abs_threshold\n","\n","                        if cond_rel or cond_abs:\n","                            # If both trigger, use max (more lenient); if only one, use that one\n","                            if cond_rel and cond_abs:\n","                                base = max(rel_threshold, abs_threshold)\n","                            elif cond_rel:\n","                                base = rel_threshold\n","                            else:\n","                                base = abs_threshold\n","\n","                            gap_this = delta - base\n","                            gap_sec += gap_this\n","                            max_gap = max(max_gap, gap_this)\n","\n","                    # Intra-chunk gaps (fix: shape + baseline)\n","                    d = np.diff(v) * scale\n","                    mask_rel = d > rel_threshold\n","                    mask_abs = d > abs_threshold\n","                    mask = mask_rel | mask_abs\n","\n","                    if mask.any():\n","                        # Vectorized: choose the threshold triggered by each gap (use max if both)\n","                        both_triggered = mask_rel & mask_abs\n","                        thr_used = np.where(\n","                            both_triggered,\n","                            max(rel_threshold, abs_threshold),\n","                            np.where(mask_rel, rel_threshold, abs_threshold)\n","                        )\n","                        gaps = d[mask] - thr_used[mask]  # Fix: also index thr_used\n","                        gap_sec += float(gaps.sum())\n","                        max_gap = max(max_gap, float(gaps.max()))\n","\n","                    prev = v[-1]\n","                    last = v[-1]\n","\n","                if first is not None and last is not None:\n","                    start_sec = float(first * scale)\n","                    end_sec = float(last * scale)\n","                    total = end_sec - start_sec\n","                    ratio = float(gap_sec / total) if total > 0 else 0.0\n","                    return start_sec, end_sec, float(round(gap_sec, 2)), float(round(ratio, 4)), float(round(max_gap, 2))\n","\n","        # Fallback branch: datetime strings\n","        t_head = pd.to_datetime(df_head[c], utc=True, errors=\"coerce\").dropna()\n","        if t_head.size >= 3:\n","            med = float(t_head.diff().dt.total_seconds().dropna().median())\n","            if med > 0:\n","                expected = (1.0 / sampling_rate_hint) if (sampling_rate_hint and sampling_rate_hint > 0) else med\n","\n","                # OR logic\n","                rel_threshold = 10.0 * expected\n","                abs_threshold = GAP_THRESHOLD\n","\n","                first = None\n","                last = None\n","                prev = None\n","                gap_sec = 0.0\n","                max_gap = 0.0\n","\n","                for chunk in pd.read_csv(file_path, usecols=[c], sep=None, engine=\"python\", chunksize=chunksize):\n","                    tt = pd.to_datetime(chunk[c], utc=True, errors=\"coerce\").dropna()\n","                    if tt.empty:\n","                        continue\n","\n","                    if first is None:\n","                        first = tt.iloc[0]\n","\n","                    # Inter-chunk gaps (fix: use max as baseline)\n","                    if prev is not None:\n","                        delta = (tt.iloc[0] - prev).total_seconds()\n","                        cond_rel = delta > rel_threshold\n","                        cond_abs = delta > abs_threshold\n","\n","                        if cond_rel or cond_abs:\n","                            if cond_rel and cond_abs:\n","                                base = max(rel_threshold, abs_threshold)\n","                            elif cond_rel:\n","                                base = rel_threshold\n","                            else:\n","                                base = abs_threshold\n","\n","                            gap_this = delta - base\n","                            gap_sec += gap_this\n","                            max_gap = max(max_gap, gap_this)\n","\n","                    # Intra-chunk gaps (fix: shape + baseline)\n","                    d = tt.diff().dt.total_seconds().dropna()\n","                    mask_rel = d > rel_threshold\n","                    mask_abs = d > abs_threshold\n","                    mask = mask_rel | mask_abs\n","\n","                    if not mask.empty and mask.any():\n","                        both_triggered = mask_rel & mask_abs\n","                        thr_used = np.where(\n","                            both_triggered,\n","                            max(rel_threshold, abs_threshold),\n","                            np.where(mask_rel, rel_threshold, abs_threshold)\n","                        )\n","                        gaps = d[mask].values - thr_used[mask]\n","                        gap_sec += float(gaps.sum())\n","                        max_gap = max(max_gap, float(gaps.max()))\n","\n","                    prev = tt.iloc[-1]\n","                    last = tt.iloc[-1]\n","\n","                if first is not None and last is not None:\n","                    total = (last - first).total_seconds()\n","                    ratio = float(gap_sec / total) if total > 0 else 0.0\n","                    return first.timestamp(), last.timestamp(), float(round(gap_sec, 2)), float(round(ratio, 4)), float(round(max_gap, 2))\n","\n","        return None, None, 0.0, 0.0, 0.0\n","\n","    except Exception:\n","        return None, None, 0.0, 0.0, 0.0\n","\n","def safe_float(x, default=0.0):\n","    \"\"\"Safely cast to float, handling NaN/Inf\"\"\"\n","    try:\n","        if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))):\n","            return default\n","        return float(x)\n","    except:\n","        return default\n","\n","# ========== 1. Parse sensor data metadata ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Parse sensor data metadata\")\n","print(\"=\"*60)\n","\n","# Determine label files directly from filenames (more reliable)\n","label_files = file_index[\n","    file_index['filename'].str.contains('label', case=False, na=False)\n","].copy()\n","sensor_files = file_index[\n","    ~file_index['filename'].str.contains('label', case=False, na=False)\n","].copy()\n","\n","print(f\"Sensor files: {len(sensor_files)}\")\n","print(f\"Label files: {len(label_files)}\")\n","\n","# Extract time ranges for sensor files (receive 5 return values)\n","print(\"Extracting time spans and gap statistics (chunked)...\")\n","time_records = []\n","for idx, row in sensor_files.iterrows():\n","    file_path = raw_dir / row['standardized_path']\n","    start, end, gap_sec, gap_ratio, max_gap = extract_time_range_and_gaps(\n","        file_path,\n","        row['sampling_rate_hz']\n","    )\n","    time_records.append({\n","        'subject_id': row['subject_id'],\n","        'session_id': row['session_id'],\n","        'placement': row['placement'],\n","        'start_time': start,\n","        'end_time': end,\n","        'gap_seconds': gap_sec,\n","        'gap_ratio': gap_ratio,\n","        'max_gap_seconds': max_gap,\n","    })\n","\n","df_time_ranges = pd.DataFrame(time_records)\n","\n","# Aggregate time ranges by session (includes max_gap)\n","session_time_agg = df_time_ranges.groupby(['subject_id', 'session_id']).agg({\n","    'start_time': 'min',\n","    'end_time': 'max',\n","    'gap_seconds': 'sum',\n","    'max_gap_seconds': 'max',\n","}).reset_index()\n","\n","session_time_agg['session_duration_sec'] = (\n","    session_time_agg['end_time'] - session_time_agg['start_time']\n",")\n","session_time_agg['gap_ratio'] = (\n","    session_time_agg['gap_seconds'] / session_time_agg['session_duration_sec']\n",").fillna(0.0).infer_objects(copy=False)\n","\n","session_time_agg.rename(columns={\n","    'start_time': 'session_start_time',\n","    'end_time': 'session_end_time'\n","}, inplace=True)\n","\n","# Add ISO8601 (human-readable) times\n","def to_iso(x):\n","    try:\n","        if pd.notna(x):\n","            return datetime.fromtimestamp(float(x), tz=timezone.utc).isoformat()\n","    except:\n","        pass\n","    return None\n","\n","session_time_agg['session_start_utc'] = session_time_agg['session_start_time'].apply(to_iso)\n","session_time_agg['session_end_utc'] = session_time_agg['session_end_time'].apply(to_iso)\n","\n","print(f\"Extracted time spans for {len(session_time_agg)} sessions\")\n","\n","# ========== 2. Parse labels & activity statistics ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Parse labels & activity statistics\")\n","print(\"=\"*60)\n","\n","activity_stats = []\n","session_records = []\n","\n","for idx, label_row in label_files.iterrows():\n","    label_path = raw_dir / label_row['standardized_path']\n","\n","    if not label_path.exists():\n","        continue\n","\n","    try:\n","        # Read label file\n","        df_label = pd.read_csv(label_path, sep=None, engine='python')\n","\n","        # Find label column (LARa dataset uses 'Class')\n","        if 'Class' in df_label.columns:\n","            label_col = 'Class'\n","        elif 'class' in df_label.columns:\n","            label_col = 'class'\n","        else:\n","            label_cols = [c for c in df_label.columns if 'label' in c.lower() or 'activity' in c.lower()]\n","            if not label_cols:\n","                print(f\"  No label column ({df_label.columns.tolist()}): {label_path.name}\")\n","                continue\n","            label_col = label_cols[0]\n","\n","        # Count activity distribution\n","        activity_counts = df_label[label_col].value_counts()\n","        total_samples = len(df_label)\n","\n","        # Check missing\n","        missing_count = df_label[label_col].isna().sum()\n","        missing_rate = missing_count / total_samples if total_samples > 0 else 0\n","\n","        # Record session info\n","        session_info = {\n","            'subject_id': label_row['subject_id'],\n","            'session_id': label_row['session_id'],\n","            'placement': label_row['placement'],\n","            'total_samples': total_samples,\n","            'missing_samples': missing_count,\n","            'missing_rate': round(missing_rate, 4),\n","            'num_activities': len(activity_counts),\n","        }\n","\n","        # Add per-activity stats\n","        for activity, count in activity_counts.items():\n","            activity_stats.append({\n","                'subject_id': label_row['subject_id'],\n","                'session_id': label_row['session_id'],\n","                'placement': label_row['placement'],\n","                'activity': str(activity),\n","                'count': int(count),\n","                'percentage': round(count / total_samples * 100, 2)\n","            })\n","\n","        session_records.append(session_info)\n","\n","    except Exception as e:\n","        print(f\"  Warning: failed to parse {label_path.name}: {e}\")\n","        continue\n","\n","print(f\"Parsed {len(session_records)} sessions\")\n","\n","# ========== 2.1 Orphan session check ==========\n","print(\"\\nChecking orphan sessions...\")\n","sess_from_sensors = set(zip(sensor_files['subject_id'], sensor_files['session_id']))\n","sess_from_labels = set(zip(label_files['subject_id'], label_files['session_id']))\n","orphans = sess_from_sensors - sess_from_labels\n","\n","if orphans:\n","    orphan_file = raw_dir / \"QA_ISSUES.log\"\n","    with open(orphan_file, \"a\", encoding=\"utf-8\") as f:\n","        f.write(\"\\nSessions with sensors but no labels (orphan sessions):\\n\")\n","        for s, r in sorted(orphans):\n","            f.write(f\"  {s}-{r}\\n\")\n","    print(f\"⚠️  Found {len(orphans)} orphan sessions; logged to QA_ISSUES.log\")\n","\n","# ========== 3. Merge session metadata ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Merge session metadata\")\n","print(\"=\"*60)\n","\n","df_sessions = pd.DataFrame(session_records)\n","df_activities = pd.DataFrame(activity_stats)\n","\n","# Merge time info\n","if not df_sessions.empty and not session_time_agg.empty:\n","    df_sessions = df_sessions.merge(\n","        session_time_agg,\n","        on=['subject_id', 'session_id'],\n","        how='left'\n","    )\n","    print(f\"Merged time span info\")\n","\n","# ========== 4. Data quality checks & empty-window cleanup ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Data quality checks & empty-window cleanup\")\n","print(\"=\"*60)\n","\n","if not df_sessions.empty:\n","    # Generate keep flag\n","    df_sessions['keep'] = True\n","    df_sessions['reject_reason'] = ''\n","\n","    # Check missing-rate exceeds threshold\n","    high_missing_mask = df_sessions['missing_rate'] > MISSING_THRESHOLD\n","    if high_missing_mask.any():\n","        df_sessions.loc[high_missing_mask, 'keep'] = False\n","        df_sessions.loc[high_missing_mask, 'reject_reason'] = 'high_missing_rate'\n","        print(f\"⚠️  {high_missing_mask.sum()} sessions marked not kept due to high missing rate\")\n","\n","    # Check time-gap ratio exceeds threshold\n","    if 'gap_ratio' in df_sessions.columns:\n","        high_gap_mask = df_sessions['gap_ratio'] > GAP_RATIO_THRESHOLD\n","        if high_gap_mask.any():\n","            # Append reason if already rejected; otherwise mark alone\n","            for idx in df_sessions[high_gap_mask].index:\n","                if df_sessions.loc[idx, 'keep']:\n","                    df_sessions.loc[idx, 'keep'] = False\n","                    df_sessions.loc[idx, 'reject_reason'] = 'high_gap_ratio'\n","                else:\n","                    df_sessions.loc[idx, 'reject_reason'] += '+high_gap_ratio'\n","            print(f\"⚠️  {high_gap_mask.sum()} sessions marked not kept due to high gap ratio\")\n","\n","    # Summary\n","    keep_count = df_sessions['keep'].sum()\n","    reject_count = (~df_sessions['keep']).sum()\n","    print(f\"✓ QC result: keep {keep_count} sessions, reject {reject_count} sessions\")\n","\n","    # Save keep list\n","    keep_sessions_file = raw_dir / \"qa_keep_sessions.csv\"\n","    df_sessions[['subject_id', 'session_id', 'placement', 'keep', 'reject_reason',\n","                 'missing_rate', 'gap_ratio']].to_csv(keep_sessions_file, index=False)\n","    print(f\"✓ Saved: {keep_sessions_file}\")\n","\n","    # Log rejection details\n","    if reject_count > 0:\n","        rejected = df_sessions[~df_sessions['keep']]\n","        qa_issues = raw_dir / \"QA_ISSUES.log\"\n","        with open(qa_issues, \"a\") as f:\n","            f.write(f\"\\nSessions rejected by QC (total {reject_count}):\\n\\n\")\n","            f.write(rejected[['subject_id', 'session_id', 'placement', 'reject_reason',\n","                             'missing_rate', 'gap_ratio']].to_string(index=False))\n","        print(f\"  Details logged to: {qa_issues}\")\n","\n","# ========== 4.1 Generate file-level empty-window list ==========\n","print(\"\\nGenerating file-level empty-window list...\")\n","if not df_time_ranges.empty:\n","    empty_segments = df_time_ranges[\n","        df_time_ranges['gap_ratio'].notna() &\n","        (df_time_ranges['gap_ratio'] > GAP_RATIO_THRESHOLD)\n","    ].copy()\n","\n","    if not empty_segments.empty:\n","        empty_todo_file = raw_dir / \"EMPTY_SEGMENTS_TODO.csv\"\n","        empty_segments[['subject_id', 'session_id', 'placement',\n","                       'gap_seconds', 'gap_ratio', 'max_gap_seconds']].to_csv(empty_todo_file, index=False)\n","        print(f\"⚠️  Generated empty-segment list: {empty_todo_file} ({len(empty_segments)} files)\")\n","\n","# ========== 5. Generate subject-level metadata ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Generate subject-level metadata\")\n","print(\"=\"*60)\n","\n","if not df_sessions.empty:\n","    # Only count kept sessions\n","    df_keep = df_sessions[df_sessions['keep']]\n","\n","    if not df_keep.empty:\n","        # Aggregate by subject\n","        subject_agg = df_keep.groupby('subject_id').agg({\n","            'session_id': 'nunique',\n","            'total_samples': 'sum',\n","            'missing_samples': 'sum',\n","            'session_duration_sec': 'sum',\n","            'num_activities': 'sum',\n","        }).reset_index()\n","\n","        subject_agg.columns = ['subject_id', 'num_sessions', 'total_samples',\n","                               'total_missing', 'total_duration_sec', 'total_activities']\n","\n","        # Compute overall missing rate\n","        subject_agg['overall_missing_rate'] = (\n","            subject_agg['total_missing'] / subject_agg['total_samples']\n","        ).round(4)\n","\n","        # Add placement coverage\n","        placement_coverage = df_keep.groupby('subject_id')['placement'].apply(\n","            lambda x: ','.join(sorted(set(x)))\n","        ).reset_index()\n","        placement_coverage.columns = ['subject_id', 'placements']\n","\n","        subject_agg = subject_agg.merge(placement_coverage, on='subject_id')\n","\n","        # Save subject metadata\n","        meta_subjects_file = raw_dir / \"meta_subjects.csv\"\n","        subject_agg.to_csv(meta_subjects_file, index=False)\n","        print(f\"✓ Saved: {meta_subjects_file}\")\n","        print(f\"  Number of subjects: {len(subject_agg)}\")\n","\n","# ========== 6. Generate session-level metadata ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Generate session-level metadata\")\n","print(\"=\"*60)\n","\n","if not df_sessions.empty:\n","    # Add activity list\n","    if not df_activities.empty:\n","        activity_list = df_activities.groupby(['subject_id', 'session_id'])['activity'].apply(\n","            lambda x: ','.join(sorted(set(x)))\n","        ).reset_index()\n","        activity_list.columns = ['subject_id', 'session_id', 'activities']\n","\n","        df_sessions_full = df_sessions.merge(\n","            activity_list,\n","            on=['subject_id', 'session_id'],\n","            how='left'\n","        )\n","    else:\n","        df_sessions_full = df_sessions\n","\n","    # Save session metadata\n","    meta_sessions_file = raw_dir / \"meta_sessions.csv\"\n","    df_sessions_full.to_csv(meta_sessions_file, index=False)\n","    print(f\"✓ Saved: {meta_sessions_file}\")\n","    print(f\"  Number of sessions: {len(df_sessions_full)}\")\n","\n","# ========== 7. Generate quality audit report ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"7. Generate quality audit report\")\n","print(\"=\"*60)\n","\n","qa_report = []\n","qa_report.append(\"=\"*70)\n","qa_report.append(\"LARa MbientLab IMU Dataset - Quality Audit Report\")\n","qa_report.append(\"=\"*70)\n","qa_report.append(f\"Generated at: {datetime.now(timezone.utc).isoformat()}\")\n","qa_report.append(f\"Data path: {raw_dir}\")\n","qa_report.append(\"\")\n","\n","# Overall stats\n","qa_report.append(\"[1. Dataset overview]\")\n","qa_report.append(\"-\"*70)\n","if not subject_agg.empty:\n","    total_hours = safe_float(subject_agg['total_duration_sec'].sum() / 3600)\n","    qa_report.append(f\"Number of subjects: {len(subject_agg)}\")\n","    qa_report.append(f\"Total sessions: {subject_agg['num_sessions'].sum()}\")\n","    qa_report.append(f\"Total duration: {total_hours:.2f} hours\")\n","    qa_report.append(f\"Total samples: {subject_agg['total_samples'].sum():,}\")\n","qa_report.append(\"\")\n","\n","# Sampling rate stats\n","qa_report.append(\"[2. Sampling rate statistics]\")\n","qa_report.append(\"-\"*70)\n","if not sensor_files.empty:\n","    rates = sensor_files['sampling_rate_hz'].dropna()\n","    if not rates.empty:\n","        qa_report.append(f\"Sampling rate range: {rates.min():.2f} - {rates.max():.2f} Hz\")\n","        qa_report.append(f\"Median sampling rate: {rates.median():.2f} Hz\")\n","        qa_report.append(f\"Mode sampling rate: {rates.mode().values[0]:.2f} Hz\")\n","qa_report.append(\"\")\n","\n","# Placement coverage\n","qa_report.append(\"[3. Sensor placement coverage]\")\n","qa_report.append(\"-\"*70)\n","if not df_sessions.empty:\n","    df_keep = df_sessions[df_sessions['keep']]\n","    if not df_keep.empty:\n","        placement_dist = df_keep['placement'].value_counts()\n","        for placement, count in placement_dist.items():\n","            percentage = count / len(df_keep) * 100\n","            qa_report.append(f\"  {placement:15s}: {count:3d} sessions ({percentage:5.1f}%)\")\n","qa_report.append(\"\")\n","\n","# Activity distribution\n","qa_report.append(\"[4. Activity distribution]\")\n","qa_report.append(\"-\"*70)\n","if not df_activities.empty:\n","    activity_total = df_activities.groupby('activity').agg({\n","        'count': 'sum',\n","    }).sort_values('count', ascending=False)\n","\n","    total_count = activity_total['count'].sum()\n","    qa_report.append(f\"Number of activity classes: {len(activity_total)}\")\n","    qa_report.append(f\"Total samples: {total_count:,}\")\n","    qa_report.append(\"\")\n","    qa_report.append(\"Per-activity share:\")\n","    for activity, row in activity_total.iterrows():\n","        percentage = row['count'] / total_count * 100\n","        qa_report.append(f\"  {str(activity):30s}: {row['count']:8,} ({percentage:5.2f}%)\")\n","qa_report.append(\"\")\n","\n","# Data quality (incl. max_gap stats)\n","qa_report.append(\"[5. Data quality assessment]\")\n","qa_report.append(\"-\"*70)\n","if not df_sessions.empty:\n","    qa_report.append(f\"Missing-rate threshold: {MISSING_THRESHOLD*100}%\")\n","    qa_report.append(f\"Gap absolute threshold: {GAP_THRESHOLD} s\")\n","    qa_report.append(f\"Gap relative threshold: 10× expected interval\")\n","    qa_report.append(f\"Gap ratio threshold: {GAP_RATIO_THRESHOLD*100}%\")\n","\n","    avg_miss = safe_float(df_sessions['missing_rate'].mean())\n","    max_miss = safe_float(df_sessions['missing_rate'].max())\n","    med_miss = safe_float(df_sessions['missing_rate'].median())\n","\n","    qa_report.append(f\"Overall average missing rate: {avg_miss*100:.2f}%\")\n","    qa_report.append(f\"Max missing rate: {max_miss*100:.2f}%\")\n","    qa_report.append(f\"Median missing rate: {med_miss*100:.2f}%\")\n","\n","    if 'gap_ratio' in df_sessions.columns:\n","        avg_gap = safe_float(df_sessions['gap_ratio'].mean())\n","        max_gap_ratio = safe_float(df_sessions['gap_ratio'].max())\n","        qa_report.append(f\"Average gap ratio: {avg_gap*100:.2f}%\")\n","        qa_report.append(f\"Max gap ratio: {max_gap_ratio*100:.2f}%\")\n","\n","    if 'max_gap_seconds' in df_sessions.columns:\n","        max_single_gap = safe_float(df_sessions['max_gap_seconds'].max())\n","        qa_report.append(f\"Max single gap: {max_single_gap:.2f} s\")\n","\n","    keep_count = df_sessions['keep'].sum()\n","    total_count = len(df_sessions)\n","    pass_rate = keep_count / total_count * 100 if total_count > 0 else 0\n","    qa_report.append(f\"\")\n","    qa_report.append(f\"Sessions passing QC: {keep_count}/{total_count} ({pass_rate:.1f}%)\")\n","\n","if (raw_dir / \"EMPTY_SEGMENTS_TODO.csv\").exists():\n","    qa_report.append(\"\")\n","    qa_report.append(\"[Note] Empty/abnormal segments found; see: EMPTY_SEGMENTS_TODO.csv (exclude during later sliding-window segmentation)\")\n","\n","qa_report.append(\"\")\n","\n","# Per-subject details\n","qa_report.append(\"[6. Subject-level details]\")\n","qa_report.append(\"-\"*70)\n","if not subject_agg.empty:\n","    for _, subj in subject_agg.iterrows():\n","        qa_report.append(f\"Subject {subj['subject_id']}:\")\n","        qa_report.append(f\"  # sessions: {subj['num_sessions']}\")\n","        qa_report.append(f\"  Total duration: {subj['total_duration_sec']/60:.1f} minutes\")\n","        qa_report.append(f\"  Total samples: {subj['total_samples']:,}\")\n","        qa_report.append(f\"  Missing rate: {subj['overall_missing_rate']*100:.2f}%\")\n","        qa_report.append(f\"  Placements: {subj['placements']}\")\n","        qa_report.append(\"\")\n","\n","qa_report.append(\"=\"*70)\n","qa_report.append(\"End of report\")\n","qa_report.append(\"=\"*70)\n","\n","# Save QA report\n","qa_report_file = raw_dir / \"QA_REPORT.txt\"\n","with open(qa_report_file, \"w\", encoding=\"utf-8\") as f:\n","    f.write(\"\\n\".join(qa_report))\n","\n","print(f\"✓ Saved quality report: {qa_report_file}\")\n","\n","# Also print to console\n","print(\"\\n\" + \"\\n\".join(qa_report))\n","\n","# ========== 8. Generate summary JSON ==========\n","summary = {\n","    \"generated_at_utc\": datetime.now(timezone.utc).isoformat(),\n","    \"num_subjects\": int(len(subject_agg)) if not subject_agg.empty else 0,\n","    \"num_sessions_total\": len(df_sessions) if not df_sessions.empty else 0,\n","    \"num_sessions_keep\": int(df_sessions['keep'].sum()) if not df_sessions.empty else 0,\n","    \"total_duration_hours\": safe_float(subject_agg['total_duration_sec'].sum() / 3600) if not subject_agg.empty else 0.0,\n","    \"missing_threshold\": MISSING_THRESHOLD,\n","    \"gap_threshold_sec\": GAP_THRESHOLD,\n","    \"gap_ratio_threshold\": GAP_RATIO_THRESHOLD,\n","    \"avg_missing_rate\": safe_float(df_sessions['missing_rate'].mean()) if not df_sessions.empty else 0.0,\n","    \"avg_gap_ratio\": safe_float(df_sessions['gap_ratio'].mean()) if not df_sessions.empty and 'gap_ratio' in df_sessions.columns else 0.0,\n","    \"max_single_gap_seconds\": safe_float(df_sessions['max_gap_seconds'].max()) if not df_sessions.empty and 'max_gap_seconds' in df_sessions.columns else 0.0,\n","    \"num_activities\": int(len(activity_total)) if not df_activities.empty else 0,\n","    \"placements\": sorted(df_sessions[df_sessions['keep']]['placement'].unique().tolist()) if not df_sessions.empty and df_sessions['keep'].any() else [],\n","}\n","\n","summary_file = raw_dir / \"qa_summary.json\"\n","with open(summary_file, \"w\") as f:\n","    json.dump(summary, f, indent=2)\n","\n","print(f\"\\n✓ Saved summary: {summary_file}\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 3 complete - Metadata & Quality Audit (top-conf/journal grade)\")\n","print(\"=\"*60)\n","print(f\"Output files:\")\n","if meta_subjects_file:\n","    print(f\"  - {meta_subjects_file}\")\n","if meta_sessions_file:\n","    print(f\"  - {meta_sessions_file}\")\n","if keep_sessions_file:\n","    print(f\"  - {keep_sessions_file}\")\n","print(f\"  - {qa_report_file}\")\n","print(f\"  - {summary_file}\")\n","if (raw_dir / \"EMPTY_SEGMENTS_TODO.csv\").exists():\n","    print(f\"  - {raw_dir / 'EMPTY_SEGMENTS_TODO.csv'} (file-level empty-window list)\")\n","if (raw_dir / \"QA_ISSUES.log\").exists():\n","    print(f\"  - {raw_dir / 'QA_ISSUES.log'} (quality issue details)\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uxvaZCQPkz5i","executionInfo":{"status":"ok","timestamp":1763144362905,"user_tz":0,"elapsed":116870,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"9e4fbe77-58af-454d-8097-65cd0d048a68"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 3: Metadata & Quality Audit\n","============================================================\n","Loading file index: data/lara/mbientlab/raw/file_index.parquet\n","\n","============================================================\n","1. Parse sensor data metadata\n","============================================================\n","Sensor files: 193\n","Label files: 193\n","Extracting time spans and gap statistics (chunked)...\n","Extracted time spans for 193 sessions\n","\n","============================================================\n","2. Parse labels & activity statistics\n","============================================================\n","Parsed 193 sessions\n","\n","Checking orphan sessions...\n","\n","============================================================\n","3. Merge session metadata\n","============================================================\n","Merged time span info\n","\n","============================================================\n","4. Data quality checks & empty-window cleanup\n","============================================================\n","✓ QC result: keep 193 sessions, reject 0 sessions\n","✓ Saved: data/lara/mbientlab/raw/qa_keep_sessions.csv\n","\n","Generating file-level empty-window list...\n","\n","============================================================\n","5. Generate subject-level metadata\n","============================================================\n","✓ Saved: data/lara/mbientlab/raw/meta_subjects.csv\n","  Number of subjects: 8\n","\n","============================================================\n","6. Generate session-level metadata\n","============================================================\n","✓ Saved: data/lara/mbientlab/raw/meta_sessions.csv\n","  Number of sessions: 193\n","\n","============================================================\n","7. Generate quality audit report\n","============================================================\n","✓ Saved quality report: data/lara/mbientlab/raw/QA_REPORT.txt\n","\n","======================================================================\n","LARa MbientLab IMU Dataset - Quality Audit Report\n","======================================================================\n","Generated at: 2025-11-14T18:19:25.436256+00:00\n","Data path: data/lara/mbientlab/raw\n","\n","[1. Dataset overview]\n","----------------------------------------------------------------------\n","Number of subjects: 8\n","Total sessions: 193\n","Total duration: 6.18 hours\n","Total samples: 2,224,452\n","\n","[2. Sampling rate statistics]\n","----------------------------------------------------------------------\n","\n","[3. Sensor placement coverage]\n","----------------------------------------------------------------------\n","  rwrist         :  96 sessions ( 49.7%)\n","  chest          :  85 sessions ( 44.0%)\n","  lwrist         :  12 sessions (  6.2%)\n","\n","[4. Activity distribution]\n","----------------------------------------------------------------------\n","Number of activity classes: 8\n","Total samples: 2,224,452\n","\n","Per-activity share:\n","  4                             : 1,198,354 (53.87%)\n","  0                             :  231,084 (10.39%)\n","  2                             :  197,087 ( 8.86%)\n","  1                             :  182,687 ( 8.21%)\n","  3                             :  150,685 ( 6.77%)\n","  5                             :  112,818 ( 5.07%)\n","  7                             :  107,298 ( 4.82%)\n","  6                             :   44,439 ( 2.00%)\n","\n","[5. Data quality assessment]\n","----------------------------------------------------------------------\n","Missing-rate threshold: 5.0%\n","Gap absolute threshold: 2.0 s\n","Gap relative threshold: 10× expected interval\n","Gap ratio threshold: 5.0%\n","Overall average missing rate: 0.00%\n","Max missing rate: 0.00%\n","Median missing rate: 0.00%\n","Average gap ratio: 0.00%\n","Max gap ratio: 0.00%\n","Max single gap: 0.00 s\n","\n","Sessions passing QC: 193/193 (100.0%)\n","\n","[6. Subject-level details]\n","----------------------------------------------------------------------\n","Subject S07:\n","  # sessions: 29\n","  Total duration: 57.1 minutes\n","  Total samples: 342,376\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","Subject S08:\n","  # sessions: 24\n","  Total duration: 47.4 minutes\n","  Total samples: 284,329\n","  Missing rate: 0.00%\n","  Placements: chest,rwrist\n","\n","Subject S09:\n","  # sessions: 29\n","  Total duration: 54.5 minutes\n","  Total samples: 326,680\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","Subject S10:\n","  # sessions: 23\n","  Total duration: 43.2 minutes\n","  Total samples: 259,019\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","Subject S11:\n","  # sessions: 14\n","  Total duration: 27.7 minutes\n","  Total samples: 165,980\n","  Missing rate: 0.00%\n","  Placements: lwrist,rwrist\n","\n","Subject S12:\n","  # sessions: 17\n","  Total duration: 30.5 minutes\n","  Total samples: 183,024\n","  Missing rate: 0.00%\n","  Placements: chest,rwrist\n","\n","Subject S13:\n","  # sessions: 29\n","  Total duration: 56.7 minutes\n","  Total samples: 340,084\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","Subject S14:\n","  # sessions: 28\n","  Total duration: 53.8 minutes\n","  Total samples: 322,960\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","======================================================================\n","End of report\n","======================================================================\n","\n","✓ Saved summary: data/lara/mbientlab/raw/qa_summary.json\n","\n","============================================================\n","Step 3 complete - Metadata & Quality Audit (top-conf/journal grade)\n","============================================================\n","Output files:\n","  - data/lara/mbientlab/raw/meta_subjects.csv\n","  - data/lara/mbientlab/raw/meta_sessions.csv\n","  - data/lara/mbientlab/raw/qa_keep_sessions.csv\n","  - data/lara/mbientlab/raw/QA_REPORT.txt\n","  - data/lara/mbientlab/raw/qa_summary.json\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 4: Channel & Placement Strategy Selection (top-conf/journal grade)\n","Select placement, raw channels, derived channels; generate config file\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import re\n","\n","print(\"=\"*60)\n","print(\"Step 4: Channel & Placement Strategy Selection\")\n","print(\"=\"*60)\n","\n","# ========== Placement → Prefix allowlist (eradicate cross-placement leakage) ==========\n","PREFIX_ALLOWLIST = {\n","    \"rwrist\": [\"RA_\"],\n","    \"lwrist\": [\"LA_\"],\n","    \"chest\":  [\"N_\"],\n","    # Extensible: \"rleg\": [\"RL_\"], \"lleg\": [\"LL_\"]\n","}\n","\n","REQ_SUFFIX = {\n","    \"ax\": \"AccelerometerX\", \"ay\": \"AccelerometerY\", \"az\": \"AccelerometerZ\",\n","    \"gx\": \"GyroscopeX\",     \"gy\": \"GyroscopeY\",     \"gz\": \"GyroscopeZ\",\n","}\n","\n","# Coverage threshold: required column presence ratio across files (1.0=100%, 0.95=95%)\n","MIN_COVERAGE = 1.0\n","\n","# Load metadata\n","raw_dir = Path(\"data/lara/mbientlab/raw\")\n","configs_dir = Path(\"configs\")\n","configs_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Load subject metadata\n","meta_subjects = pd.read_csv(raw_dir / \"meta_subjects.csv\")\n","print(f\"\\nLoaded subject metadata: {len(meta_subjects)} subjects\")\n","\n","# Load file index\n","index_file = raw_dir / \"file_index.parquet\"\n","if not index_file.exists():\n","    index_file = raw_dir / \"file_index.csv\"\n","file_index = pd.read_parquet(index_file) if index_file.suffix == '.parquet' else pd.read_csv(index_file)\n","\n","# Keep only sensor files (more robust: filter by sensor_type and filename)\n","if 'sensor_type' in file_index.columns:\n","    sensor_files = file_index[\n","        (file_index['sensor_type'].isin(['acc+gyro', 'acc', 'gyro'])) &\n","        ~file_index['filename'].str.contains('label', case=False, na=False)\n","    ].copy()\n","else:\n","    sensor_files = file_index[\n","        ~file_index['filename'].str.contains('label', case=False, na=False)\n","    ].copy()\n","\n","print(f\"Number of sensor files: {len(sensor_files)}\")\n","\n","# ========== 1. Analyze placement coverage ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Analyze placement coverage\")\n","print(\"=\"*60)\n","\n","# Count data volume per placement\n","placement_stats = sensor_files.groupby('placement').agg({\n","    'subject_id': 'nunique',\n","    'session_id': 'nunique',\n","    'file_size_bytes': 'sum',\n","    'num_rows': 'sum',\n","}).reset_index()\n","placement_stats.columns = ['placement', 'num_subjects', 'num_sessions', 'total_bytes', 'total_samples']\n","placement_stats = placement_stats.sort_values('total_samples', ascending=False)\n","\n","print(\"\\nPlacement statistics (sorted by sample count):\")\n","print(placement_stats.to_string(index=False))\n","\n","# Fix selection to right wrist (this round)\n","selected_placement = \"rwrist\"\n","print(f\"\\nFixed placement for this round: {selected_placement}\")\n","\n","# Check whether placement exists\n","if selected_placement not in placement_stats['placement'].values:\n","    raise ValueError(f\"Specified placement '{selected_placement}' does not exist in the data\")\n","\n","# Check which subjects have that placement\n","subjects_with_selected = sensor_files[sensor_files['placement'] == selected_placement]['subject_id'].unique()\n","print(f\"Subjects with {selected_placement} data: {len(subjects_with_selected)}/{len(meta_subjects)}\")\n","\n","# ========== 2. Allowlist validation & channel check ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Allowlist validation & channel check\")\n","print(\"=\"*60)\n","\n","# Read only from files of selected placement\n","placement_files = sensor_files[sensor_files['placement'] == selected_placement]\n","print(f\"Number of files for selected placement '{selected_placement}': {len(placement_files)}\")\n","\n","# Get allowlist prefixes\n","allowed_prefixes = PREFIX_ALLOWLIST.get(selected_placement, [])\n","assert allowed_prefixes, f\"Prefix allowlist for '{selected_placement}' not configured; please add it in PREFIX_ALLOWLIST\"\n","print(f\"\\nUsing placement→prefix allowlist: {selected_placement} → {allowed_prefixes}\")\n","\n","# Robust header-reading function\n","def read_cols(fp):\n","    \"\"\"Read column names (with fallback)\"\"\"\n","    try:\n","        return pd.read_csv(fp, nrows=5, sep=None, engine='python').columns.tolist()\n","    except Exception:\n","        return pd.read_csv(fp, nrows=5, sep=\",\").columns.tolist()\n","\n","# Read headers of all files\n","print(f\"\\nRead headers of all {len(placement_files)} files to check consistency...\")\n","all_columns_by_file = []\n","\n","for _, row in placement_files.iterrows():\n","    fp = raw_dir / row['standardized_path']\n","    cols = read_cols(fp)\n","    data_cols = [c for c in cols if not re.search(r'(time|timestamp|epoch|index|id|class|label)', c, re.I)]\n","    all_columns_by_file.append(data_cols)\n","\n","# Assert all files were read successfully\n","assert len(all_columns_by_file) == len(placement_files), \\\n","    f\"{len(placement_files)-len(all_columns_by_file)} '{selected_placement}' files failed header reading; fix or exclude these files first\"\n","\n","print(f\"✓ Successfully read {len(all_columns_by_file)} files\")\n","\n","# Show columns of the first file as a reference\n","if all_columns_by_file:\n","    print(f\"\\nData columns of the first file:\")\n","    for col in all_columns_by_file[0]:\n","        print(f\"  {col}\")\n","\n","# ========== 3. Build strict channel mapping (allowlist + consistency assertions) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Build strict channel mapping (allowlist + consistency assertions)\")\n","print(\"=\"*60)\n","\n","def extract_prefix(col):\n","    \"\"\"Extract column prefix\"\"\"\n","    m = re.match(r'^([A-Z]{1,}_)', col)\n","    return m.group(1) if m else None\n","\n","def build_mapping_from_allowlist(allowed_prefixes, all_cols_by_file, min_coverage=1.0):\n","    \"\"\"Compose column names from allowlist × suffix and check coverage\"\"\"\n","    mapping = {}\n","    missing_files = {}\n","\n","    for std, suf in REQ_SUFFIX.items():\n","        chosen = None\n","        for pfx in allowed_prefixes:\n","            cand = f\"{pfx}{suf}\"\n","            # Count in how many files this column exists\n","            present_files = [i for i, cols in enumerate(all_cols_by_file) if cand in cols]\n","            coverage = len(present_files) / len(all_cols_by_file)\n","\n","            if coverage >= min_coverage:\n","                chosen = cand\n","                if coverage < 1.0:\n","                    # Record indices of files missing this column (for later inspection)\n","                    missing_idx = [i for i in range(len(all_cols_by_file)) if i not in present_files]\n","                    missing_files[std] = missing_idx\n","                break\n","\n","        if not chosen:\n","            raise RuntimeError(\n","                f\"[Consistency assertion failed] {std}: Under prefixes {allowed_prefixes}, no '{suf}' meets {min_coverage*100:.0f}% coverage. \"\n","                f\"Check raw column names or change placement/prefix allowlist.\"\n","            )\n","\n","        mapping[std] = chosen\n","\n","    # Prefix consistency check: all mapped columns must come from allowlist\n","    used_prefixes = {extract_prefix(v) for v in mapping.values()}\n","    if not used_prefixes.issubset(set(allowed_prefixes)):\n","        raise RuntimeError(\n","            f\"[Consistency assertion failed] Final mapping prefixes {used_prefixes} are not all within allowlist {allowed_prefixes}\"\n","        )\n","\n","    return mapping, used_prefixes, missing_files\n","\n","# Build mapping\n","final_mapping, used_prefixes, missing_files = build_mapping_from_allowlist(\n","    allowed_prefixes, all_columns_by_file, MIN_COVERAGE\n",")\n","\n","print(\"\\nFinal channel mapping (standard_name <- original_column):\")\n","for std, orig in sorted(final_mapping.items()):\n","    print(f\"  {std} <- {orig}\")\n","\n","# Explicit hard assertions\n","assert len(used_prefixes) == 1, f\"A single prefix should be used; got {used_prefixes}\"\n","assert list(used_prefixes)[0] in set(PREFIX_ALLOWLIST[selected_placement]), \\\n","    f\"Source prefix {used_prefixes} not in allowlist {PREFIX_ALLOWLIST[selected_placement]} for {selected_placement}\"\n","\n","print(f\"\\n✓ Consistency assertions passed:\")\n","print(f\"  - Using a single prefix: {sorted(used_prefixes)}\")\n","print(f\"  - Prefix is in the allowlist: {PREFIX_ALLOWLIST[selected_placement]}\")\n","print(f\"  - Number of files checked: {len(all_columns_by_file)}\")\n","print(f\"  - Coverage requirement: {MIN_COVERAGE*100:.0f}%\")\n","\n","# If there are missing, print warnings\n","if missing_files:\n","    print(f\"\\n⚠️  The following channels are missing in some files (coverage threshold set to {MIN_COVERAGE*100:.0f}%):\")\n","    for std, idx_list in missing_files.items():\n","        print(f\"  {std}: missing in {len(idx_list)} files\")\n","\n","# ========== 4. Generate channel & placement config ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Generate channel & placement config\")\n","print(\"=\"*60)\n","\n","# Config content\n","config = {\n","    'dataset': 'LARa_MbientLab_IMU',\n","    'strategy': 'single_placement_baseline',\n","\n","    # Placement configuration\n","    'placements': {\n","        'selected': [selected_placement],\n","        'available': placement_stats['placement'].tolist(),\n","        'rationale': f'Fixed selection {selected_placement}, covering {len(subjects_with_selected)} subjects',\n","    },\n","\n","    # Raw channel configuration\n","    'channels': {\n","        'raw': ['ax', 'ay', 'az', 'gx', 'gy', 'gz'],\n","        'mapping': final_mapping,\n","        'prefix_allowlist': PREFIX_ALLOWLIST,\n","        'source_prefix': sorted(used_prefixes)[0],\n","        'min_coverage': MIN_COVERAGE,\n","        'description': {\n","            'ax': 'Accelerometer X axis (m/s² or g)',\n","            'ay': 'Accelerometer Y axis (m/s² or g)',\n","            'az': 'Accelerometer Z axis (m/s² or g)',\n","            'gx': 'Gyroscope X axis (rad/s or deg/s)',\n","            'gy': 'Gyroscope Y axis (rad/s or deg/s)',\n","            'gz': 'Gyroscope Z axis (rad/s or deg/s)',\n","        }\n","    },\n","\n","    # Derived channel configuration\n","    'derived_channels': {\n","        'acc_mag': {\n","            'formula': 'sqrt(ax^2 + ay^2 + az^2)',\n","            'description': 'Accelerometer vector magnitude',\n","        },\n","        'gyr_mag': {\n","            'formula': 'sqrt(gx^2 + gy^2 + gz^2)',\n","            'description': 'Gyroscope vector magnitude',\n","        }\n","    },\n","\n","    # Final channel order\n","    'final_channels': ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag'],\n","\n","    # Multi-placement fusion (reserved; currently disabled)\n","    'multi_placement_fusion': {\n","        'enabled': False,\n","        'strategy': None,\n","        'warning': 'If enabling multi-placement fusion, you must select the fusion strategy independently within each training fold to avoid cross-fold leakage',\n","    },\n","\n","    # Rigor notes\n","    'notes': [\n","        'Single-placement baseline: avoid cross-placement information leakage',\n","        'Channel mapping uses \"placement→prefix allowlist + consistency assertions\"; no cross-prefix voting',\n","        f'Consistency checked over all {len(all_columns_by_file)} {selected_placement} files',\n","        f'Coverage requirement: {MIN_COVERAGE*100:.0f}% (tunable tolerance)',\n","        'Derived channels are computed at feature-extraction stage to preserve raw data integrity',\n","        'Any multi-placement fusion must be chosen & validated within each LOSO fold',\n","    ]\n","}\n","\n","# Save config\n","config_file = configs_dir / \"channels.yaml\"\n","with open(config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","print(f\"✓ Saved config: {config_file}\")\n","\n","# ========== 5. Validate config (random multi-file sampling) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Validate config\")\n","print(\"=\"*60)\n","\n","# Verify coverage of selected placement across all sensor files\n","files_with_placement = sensor_files[sensor_files['placement'] == selected_placement]\n","\n","print(f\"\\nValidate placement '{selected_placement}':\")\n","print(f\"  Files: {len(files_with_placement)}\")\n","print(f\"  Subjects: {files_with_placement['subject_id'].nunique()}\")\n","print(f\"  Sessions: {files_with_placement['session_id'].nunique()}\")\n","\n","# Validate channel mapping: randomly sample multiple files\n","verify_sample_size = min(5, len(files_with_placement))\n","verify_df = files_with_placement.sample(n=verify_sample_size, random_state=0)\n","\n","print(f\"\\nValidate channel mapping (random sample of {verify_sample_size} files):\")\n","for idx, sample_file in verify_df.iterrows():\n","    sample_path = raw_dir / sample_file['standardized_path']\n","    try:\n","        df_verify = pd.read_csv(sample_path, nrows=100, sep=None, engine='python')\n","\n","        print(f\"\\nFile: {sample_file['filename']}\")\n","        all_found = True\n","        for std_name in ['ax', 'ay', 'az', 'gx', 'gy', 'gz']:\n","            if std_name in final_mapping:\n","                orig_name = final_mapping[std_name]\n","                if orig_name in df_verify.columns:\n","                    sample_val = df_verify[orig_name].iloc[0]\n","                    print(f\"  ✓ {std_name} <- {orig_name} (sample value: {sample_val:.4f})\")\n","                else:\n","                    print(f\"  ✗ {std_name} <- {orig_name} (column not found)\")\n","                    all_found = False\n","            else:\n","                print(f\"  ✗ {std_name} (not mapped)\")\n","                all_found = False\n","\n","        if not all_found:\n","            print(f\"  ⚠️  This file failed validation\")\n","\n","    except Exception as e:\n","        print(f\"\\nFile: {sample_file['filename']}\")\n","        print(f\"  ✗ Error during validation: {e}\")\n","\n","# Compute derived-channel examples on the first successfully validated file\n","for idx, sample_file in verify_df.iterrows():\n","    sample_path = raw_dir / sample_file['standardized_path']\n","    try:\n","        df_verify = pd.read_csv(sample_path, nrows=100, sep=None, engine='python')\n","        if all(final_mapping[ch] in df_verify.columns for ch in ['ax', 'ay', 'az', 'gx', 'gy', 'gz']):\n","            acc_mag = np.sqrt(\n","                df_verify[final_mapping['ax']].values**2 +\n","                df_verify[final_mapping['ay']].values**2 +\n","                df_verify[final_mapping['az']].values**2\n","            )\n","            gyr_mag = np.sqrt(\n","                df_verify[final_mapping['gx']].values**2 +\n","                df_verify[final_mapping['gy']].values**2 +\n","                df_verify[final_mapping['gz']].values**2\n","            )\n","\n","            print(f\"\\nDerived-channel example values (file: {sample_file['filename']}):\")\n","            print(f\"  acc_mag: min={acc_mag.min():.4f}, max={acc_mag.max():.4f}, mean={acc_mag.mean():.4f}\")\n","            print(f\"  gyr_mag: min={gyr_mag.min():.4f}, max={gyr_mag.max():.4f}, mean={gyr_mag.mean():.4f}\")\n","            break\n","    except:\n","        continue\n","\n","# ========== 6. Fuse check (reload config for verification) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Fuse check (reload config for verification)\")\n","print(\"=\"*60)\n","\n","with open(config_file, \"r\", encoding=\"utf-8\") as f:\n","    cfg = yaml.safe_load(f)\n","\n","# Extract prefixes of all mapped columns\n","srcs = list(cfg[\"channels\"][\"mapping\"].values())\n","pfxs = {re.match(r'^([A-Za-z]+_)', s).group(1) for s in srcs if re.match(r'^([A-Za-z]+_)', s)}\n","\n","# Assertion: all channels use the same prefix\n","assert len(pfxs) == 1, f\"ax..gz not using a single prefix: {pfxs}\"\n","\n","# Assertion: prefix in allowlist\n","sel = cfg[\"placements\"][\"selected\"][0]\n","allow = set(cfg[\"channels\"][\"prefix_allowlist\"][sel])\n","assert list(pfxs)[0] in allow, f\"Prefix {pfxs} not in {sel} allowlist {allow}\"\n","\n","print(f\"✓ Config fuse check passed:\")\n","print(f\"  - Reloaded config: {config_file}\")\n","print(f\"  - All channels use a single prefix: {pfxs}\")\n","print(f\"  - Prefix is in {sel} allowlist: {allow}\")\n","\n","# ========== 7. Summary ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 4 complete - Channels & Placement Strategy\")\n","print(\"=\"*60)\n","print(f\"\\nConfig summary:\")\n","print(f\"  Strategy: single-placement baseline\")\n","print(f\"  Fixed placement: {config['placements']['selected']}\")\n","print(f\"  Raw channels: {config['channels']['raw']}\")\n","print(f\"  Derived channels: {list(config['derived_channels'].keys())}\")\n","print(f\"  Final number of channels: {len(config['final_channels'])}\")\n","print(f\"  Prefix used: {sorted(used_prefixes)}\")\n","print(f\"  Coverage requirement: {MIN_COVERAGE*100:.0f}%\")\n","print(f\"\\nConfig file: {config_file}\")\n","print(f\"\\nRigor guarantees:\")\n","print(f\"  1. ✓ Use placement→prefix allowlist (hard-coded)\")\n","print(f\"  2. ✓ Consistency assertions across all files ({len(all_columns_by_file)} files)\")\n","print(f\"  3. ✓ No cross-prefix voting; avoid mis-selection\")\n","print(f\"  4. ✓ Error out if column names don't match allowlist\")\n","print(f\"  5. ✓ Explicit assertions: single prefix + within allowlist\")\n","print(f\"  6. ✓ Abort if header reading fails\")\n","print(f\"  7. ✓ Randomly sample {verify_sample_size} files to validate mapping\")\n","print(f\"  8. ✓ Fuse check: reload config and verify prefix\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BYOCF2FBk1R2","executionInfo":{"status":"ok","timestamp":1763144363239,"user_tz":0,"elapsed":331,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"c6b6debd-f6e7-4c37-bfc9-9ccd097a5b88"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 4: Channel & Placement Strategy Selection\n","============================================================\n","\n","Loaded subject metadata: 8 subjects\n","Number of sensor files: 193\n","\n","============================================================\n","1. Analyze placement coverage\n","============================================================\n","\n","Placement statistics (sorted by sample count):\n","placement  num_subjects  num_sessions  total_bytes  total_samples\n","   rwrist             8            14    685467785        1120045\n","    chest             7            14    595662725         972496\n","   lwrist             6             2     80626579         131911\n","\n","Fixed placement for this round: rwrist\n","Subjects with rwrist data: 8/8\n","\n","============================================================\n","2. Allowlist validation & channel check\n","============================================================\n","Number of files for selected placement 'rwrist': 96\n","\n","Using placement→prefix allowlist: rwrist → ['RA_']\n","\n","Read headers of all 96 files to check consistency...\n","✓ Successfully read 96 files\n","\n","Data columns of the first file:\n","  LA_AccelerometerX\n","  LA_AccelerometerY\n","  LA_AccelerometerZ\n","  LA_GyroscopeX\n","  LA_GyroscopeY\n","  LA_GyroscopeZ\n","  LL_AccelerometerX\n","  LL_AccelerometerY\n","  LL_AccelerometerZ\n","  LL_GyroscopeX\n","  LL_GyroscopeY\n","  LL_GyroscopeZ\n","  N_AccelerometerX\n","  N_AccelerometerY\n","  N_AccelerometerZ\n","  N_GyroscopeX\n","  N_GyroscopeY\n","  N_GyroscopeZ\n","  RA_AccelerometerX\n","  RA_AccelerometerY\n","  RA_AccelerometerZ\n","  RA_GyroscopeX\n","  RA_GyroscopeY\n","  RA_GyroscopeZ\n","  RL_AccelerometerX\n","  RL_AccelerometerY\n","  RL_AccelerometerZ\n","  RL_GyroscopeX\n","  RL_GyroscopeY\n","  RL_GyroscopeZ\n","\n","============================================================\n","3. Build strict channel mapping (allowlist + consistency assertions)\n","============================================================\n","\n","Final channel mapping (standard_name <- original_column):\n","  ax <- RA_AccelerometerX\n","  ay <- RA_AccelerometerY\n","  az <- RA_AccelerometerZ\n","  gx <- RA_GyroscopeX\n","  gy <- RA_GyroscopeY\n","  gz <- RA_GyroscopeZ\n","\n","✓ Consistency assertions passed:\n","  - Using a single prefix: ['RA_']\n","  - Prefix is in the allowlist: ['RA_']\n","  - Number of files checked: 96\n","  - Coverage requirement: 100%\n","\n","============================================================\n","4. Generate channel & placement config\n","============================================================\n","✓ Saved config: configs/channels.yaml\n","\n","============================================================\n","5. Validate config\n","============================================================\n","\n","Validate placement 'rwrist':\n","  Files: 96\n","  Subjects: 8\n","  Sessions: 14\n","\n","Validate channel mapping (random sample of 5 files):\n","\n","File: l02_s09_r05.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.3215)\n","  ✓ ay <- RA_AccelerometerY (sample value: -0.9254)\n","  ✓ az <- RA_AccelerometerZ (sample value: 0.2765)\n","  ✓ gx <- RA_GyroscopeX (sample value: -1.1439)\n","  ✓ gy <- RA_GyroscopeY (sample value: -1.3566)\n","  ✓ gz <- RA_GyroscopeZ (sample value: -0.7625)\n","\n","File: l02_s14_r03.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.2813)\n","  ✓ ay <- RA_AccelerometerY (sample value: -0.9499)\n","  ✓ az <- RA_AccelerometerZ (sample value: 0.1393)\n","  ✓ gx <- RA_GyroscopeX (sample value: 1.2430)\n","  ✓ gy <- RA_GyroscopeY (sample value: 12.4323)\n","  ✓ gz <- RA_GyroscopeZ (sample value: 4.8893)\n","\n","File: l02_s07_r06.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.1674)\n","  ✓ ay <- RA_AccelerometerY (sample value: -0.9645)\n","  ✓ az <- RA_AccelerometerZ (sample value: -0.0895)\n","  ✓ gx <- RA_GyroscopeX (sample value: -1.0865)\n","  ✓ gy <- RA_GyroscopeY (sample value: 9.2328)\n","  ✓ gz <- RA_GyroscopeZ (sample value: 55.7498)\n","\n","File: l02_s11_r06.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.5316)\n","  ✓ ay <- RA_AccelerometerY (sample value: -0.8636)\n","  ✓ az <- RA_AccelerometerZ (sample value: 0.1911)\n","  ✓ gx <- RA_GyroscopeX (sample value: 8.0598)\n","  ✓ gy <- RA_GyroscopeY (sample value: 32.2562)\n","  ✓ gz <- RA_GyroscopeZ (sample value: -1.3813)\n","\n","File: l02_s12_r15.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.9424)\n","  ✓ ay <- RA_AccelerometerY (sample value: -1.0324)\n","  ✓ az <- RA_AccelerometerZ (sample value: 0.9479)\n","  ✓ gx <- RA_GyroscopeX (sample value: 59.7400)\n","  ✓ gy <- RA_GyroscopeY (sample value: 145.9488)\n","  ✓ gz <- RA_GyroscopeZ (sample value: -34.0791)\n","\n","Derived-channel example values (file: l02_s09_r05.csv):\n","  acc_mag: min=0.9377, max=1.0473, mean=1.0126\n","  gyr_mag: min=0.4164, max=9.8511, mean=3.7085\n","\n","============================================================\n","6. Fuse check (reload config for verification)\n","============================================================\n","✓ Config fuse check passed:\n","  - Reloaded config: configs/channels.yaml\n","  - All channels use a single prefix: {'RA_'}\n","  - Prefix is in rwrist allowlist: {'RA_'}\n","\n","============================================================\n","Step 4 complete - Channels & Placement Strategy\n","============================================================\n","\n","Config summary:\n","  Strategy: single-placement baseline\n","  Fixed placement: ['rwrist']\n","  Raw channels: ['ax', 'ay', 'az', 'gx', 'gy', 'gz']\n","  Derived channels: ['acc_mag', 'gyr_mag']\n","  Final number of channels: 8\n","  Prefix used: ['RA_']\n","  Coverage requirement: 100%\n","\n","Config file: configs/channels.yaml\n","\n","Rigor guarantees:\n","  1. ✓ Use placement→prefix allowlist (hard-coded)\n","  2. ✓ Consistency assertions across all files (96 files)\n","  3. ✓ No cross-prefix voting; avoid mis-selection\n","  4. ✓ Error out if column names don't match allowlist\n","  5. ✓ Explicit assertions: single prefix + within allowlist\n","  6. ✓ Abort if header reading fails\n","  7. ✓ Randomly sample 5 files to validate mapping\n","  8. ✓ Fuse check: reload config and verify prefix\n","============================================================\n"]}]},{"cell_type":"code","source":["import os\n","\n","\"\"\"\n","Step 5: Timeline Unification & Resampling (top-conf/journal grade - flawless)\n","Unify to 50 Hz; linear interpolation/forward-fill; align start/end\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import re\n","import json\n","\n","# ========== Config ==========\n","TARGET_FREQ_HZ = 50.0           # Target sampling rate\n","MAX_INTERP_GAP_MS = 20.0        # Maximum interpolation gap (milliseconds)\n","MAX_INTERP_RATIO = 0.15         # Gap coverage threshold 15% (constant; applied globally)\n","\n","print(\"=\"*60)\n","print(\"Step 5: Timeline Unification & Resampling\")\n","print(\"=\"*60)\n","\n","# Load config and metadata\n","raw_dir = Path(\"data/lara/mbientlab/raw\")\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","proc_dir.mkdir(parents=True, exist_ok=True)\n","\n","configs_dir = Path(\"configs\")\n","with open(configs_dir / \"channels.yaml\", 'r', encoding='utf-8') as f:\n","    channel_config = yaml.safe_load(f)\n","\n","selected_placement = channel_config['placements']['selected'][0]\n","channel_mapping = channel_config['channels']['mapping']\n","print(f\"\\nTarget sampling rate: {TARGET_FREQ_HZ} Hz\")\n","print(f\"Selected placement: {selected_placement}\")\n","\n","# Load QC results (all kept sessions)\n","qa_keep = pd.read_csv(raw_dir / \"qa_keep_sessions.csv\")\n","keep_sessions = qa_keep[qa_keep['keep'] == True].copy()\n","keep_sessions = keep_sessions[keep_sessions['placement'] == selected_placement].copy()\n","\n","# Global processing (no per-fold dependency)\n","print(f\"\\nGlobal resampling over all kept sessions (no per-fold markers)\")\n","print(f\"  Total sessions: {len(keep_sessions)}\")\n","\n","# Prune switch: always ON (remove sessions with excessive gaps globally)\n","APPLY_PRUNE = True\n","keep_sessions['is_train'] = False  # Kept for compatibility with stats / logs\n","\n","# Load file index\n","index_file = raw_dir / \"file_index.parquet\"\n","if not index_file.exists():\n","    index_file = raw_dir / \"file_index.csv\"\n","file_index = pd.read_parquet(index_file) if index_file.suffix == '.parquet' else pd.read_csv(index_file)\n","\n","# ========== Helper functions ==========\n","def detect_time_column(df):\n","    \"\"\"Detect time column (avoid false positive matches on 'ts' substring)\"\"\"\n","    time_cols = [c for c in df.columns\n","                 if re.search(r'(^|_)(time|timestamp|epoch|ts)($|_)', c, re.I)]\n","    return time_cols[0] if time_cols else None\n","\n","def parse_time_to_seconds(time_series):\n","    \"\"\"Convert time to seconds (correctly infer Unix timestamp units)\"\"\"\n","    numeric = pd.to_numeric(time_series, errors='coerce')\n","    if numeric.notna().sum() > len(time_series) * 0.9:\n","        vals = numeric.dropna().values\n","        max_val = np.abs(vals[:1000]).max() if len(vals) else 0\n","\n","        # Infer by 2025 Unix timestamp magnitude\n","        if max_val > 1e17:      # nanoseconds\n","            return numeric * 1e-9\n","        elif max_val > 1e14:    # microseconds\n","            return numeric * 1e-6\n","        elif max_val > 1e11:    # milliseconds\n","            return numeric * 1e-3\n","        else:                   # seconds\n","            return numeric\n","\n","    dt = pd.to_datetime(time_series, utc=True, errors='coerce')\n","    if dt.notna().sum() > len(time_series) * 0.9:\n","        epoch = pd.Timestamp(\"1970-01-01\", tz='UTC')\n","        return (dt - epoch).dt.total_seconds()\n","\n","    return None\n","\n","def resample_sensor_data(df, time_col, data_cols, target_freq_hz=50.0, max_gap_ms=20.0):\n","    \"\"\"Resample sensor data (return cleaned time for labels)\"\"\"\n","    time_sec = parse_time_to_seconds(df[time_col])\n","    if time_sec is None:\n","        raise ValueError(\"Unable to parse time column\")\n","\n","    valid_mask = time_sec.notna() & df[data_cols].notna().all(axis=1)\n","    time_clean = time_sec[valid_mask].values\n","    data_clean = df.loc[valid_mask, data_cols].values\n","\n","    if len(time_clean) < 2:\n","        return None, 0.0, 0, 0.0, 0.0, None\n","\n","    # De-duplicate + sort\n","    unique_idx = np.unique(time_clean, return_index=True)[1]\n","    time_clean = time_clean[unique_idx]\n","    data_clean = data_clean[unique_idx]\n","\n","    order = np.argsort(time_clean)\n","    time_clean = time_clean[order]\n","    data_clean = data_clean[order]\n","\n","    # Original frequency\n","    dt_orig = np.median(np.diff(time_clean))\n","    orig_freq_hz = 1.0 / dt_orig if dt_orig > 0 else 0.0\n","\n","    # Build target timeline with integer number of samples\n","    dt = 1.0 / target_freq_hz\n","    t_start = time_clean[0]\n","    t_end = time_clean[-1]\n","    n_samples = int(np.round((t_end - t_start) / dt))\n","    target_time = t_start + np.arange(n_samples + 1) * dt\n","\n","    # Linear interpolation\n","    resampled_data = np.zeros((len(target_time), len(data_cols)))\n","    for i in range(len(data_cols)):\n","        resampled_data[:, i] = np.interp(target_time, time_clean, data_clean[:, i])\n","\n","    # Large-gap detection (account for jitter)\n","    max_gap_sec = max(max_gap_ms / 1000.0, 1.25 * dt)\n","    time_diffs = np.diff(time_clean)\n","    gap_mask = time_diffs > max_gap_sec\n","\n","    is_in_gap = np.zeros(len(target_time), dtype=int)\n","    is_forced_nan = np.zeros(len(target_time), dtype=int)\n","    actual_interp_count = 0\n","    total_gap_time = 0.0\n","\n","    if gap_mask.any():\n","        for i in range(len(time_clean) - 1):\n","            if gap_mask[i]:\n","                t_gap_start = time_clean[i]\n","                t_gap_end = time_clean[i + 1]\n","                gap_duration = t_gap_end - t_gap_start\n","                total_gap_time += gap_duration\n","\n","                idxs = np.where((target_time > t_gap_start) & (target_time < t_gap_end))[0]\n","\n","                if idxs.size > 0:\n","                    is_in_gap[idxs] = 1\n","                    actual_interp_count += 1\n","\n","                    if idxs.size > 1:\n","                        forced_nan_idxs = idxs[1:]\n","                        is_forced_nan[forced_nan_idxs] = 1\n","                        resampled_data[forced_nan_idxs, :] = np.nan\n","\n","    # Gap coverage\n","    gap_points = int(is_in_gap.sum())\n","    interp_ratio = gap_points / len(target_time) if len(target_time) > 0 else 0.0\n","\n","    # Gap time fraction\n","    total_duration = t_end - t_start\n","    gap_time_fraction = total_gap_time / total_duration if total_duration > 0 else 0.0\n","\n","    resampled_df = pd.DataFrame(resampled_data, columns=data_cols)\n","    resampled_df.insert(0, 'time_sec', target_time)\n","    resampled_df['is_in_gap'] = is_in_gap\n","    resampled_df['is_forced_nan'] = is_forced_nan\n","\n","    return resampled_df, interp_ratio, gap_points, gap_time_fraction, orig_freq_hz, time_clean\n","\n","def resample_labels(df_label, df_sensor_time_clean, label_col, target_time, label_time_col=None):\n","    \"\"\"Resample labels (boundary NaN + sorting)\"\"\"\n","    if label_time_col is not None:\n","        time_sec = parse_time_to_seconds(df_label[label_time_col])\n","        if time_sec is None:\n","            raise ValueError(\"Unable to parse label time column\")\n","\n","        valid_mask = time_sec.notna() & df_label[label_col].notna()\n","        time_clean = time_sec[valid_mask].values\n","        labels_clean = df_label.loc[valid_mask, label_col].values\n","    else:\n","        # Use cleaned sensor time as reference\n","        sensor_time_original = df_sensor_time_clean\n","        if sensor_time_original is None:\n","            raise ValueError(\"Labels have no time column and no sensor time provided\")\n","\n","        min_len = min(len(df_label), len(sensor_time_original))\n","        if abs(len(df_label) - len(sensor_time_original)) > min_len * 0.01:\n","            raise ValueError(\n","                f\"Label rows ({len(df_label)}) differ too much from sensor rows ({len(sensor_time_original)})\"\n","            )\n","\n","        time_clean = sensor_time_original[:min_len]\n","        labels_clean = df_label[label_col].iloc[:min_len].values\n","\n","        valid_mask = pd.notna(labels_clean)\n","        time_clean = time_clean[valid_mask]\n","        labels_clean = labels_clean[valid_mask]\n","\n","    if len(time_clean) == 0:\n","        return np.full(len(target_time), np.nan)\n","\n","    # Explicit sorting\n","    order = np.argsort(time_clean)\n","    time_clean = time_clean[order]\n","    labels_clean = labels_clean[order]\n","\n","    idx = np.searchsorted(time_clean, target_time, side='right') - 1\n","    idx = np.clip(idx, 0, len(time_clean) - 1)\n","\n","    labels = labels_clean[idx].copy()\n","\n","    # Fix: cast integers to float to allow NaN\n","    if labels.dtype.kind in ['i', 'u']:  # integer or unsigned integer\n","        labels = labels.astype('float64')\n","\n","    # Boundary NaNs\n","    mask_before = target_time < time_clean[0]\n","    mask_after = target_time > time_clean[-1]\n","    labels[mask_before | mask_after] = np.nan\n","\n","    return labels\n","\n","# ========== 1. Process all sessions ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Resampling\")\n","print(\"=\"*60)\n","\n","resampled_records = []\n","interp_stats = []\n","issues = []\n","\n","for idx, session in keep_sessions.iterrows():\n","    subject_id = session['subject_id']\n","    session_id = session['session_id']\n","    placement = session['placement']\n","    is_train = session['is_train']\n","\n","    print(f\"\\nProcessing {subject_id}/{session_id}/{placement} {'[TRAIN]' if is_train else '[TEST]'}...\")\n","\n","    sensor_file = file_index[\n","        (file_index['subject_id'] == subject_id) &\n","        (file_index['session_id'] == session_id) &\n","        (file_index['placement'] == placement) &\n","        (~file_index['filename'].str.contains('label', case=False, na=False))\n","    ]\n","\n","    label_file = file_index[\n","        (file_index['subject_id'] == subject_id) &\n","        (file_index['session_id'] == session_id) &\n","        (file_index['placement'] == placement) &\n","        (file_index['filename'].str.contains('label', case=False, na=False))\n","    ]\n","\n","    if sensor_file.empty or label_file.empty:\n","        print(f\"  Skip: missing files\")\n","        continue\n","\n","    sensor_path = raw_dir / sensor_file.iloc[0]['standardized_path']\n","    label_path = raw_dir / label_file.iloc[0]['standardized_path']\n","\n","    try:\n","        df_sensor = pd.read_csv(sensor_path, sep=None, engine='python')\n","        time_col = detect_time_column(df_sensor)\n","        if not time_col:\n","            print(f\"  Skip: no time column\")\n","            continue\n","\n","        data_cols = [channel_mapping[std] for std in ['ax', 'ay', 'az', 'gx', 'gy', 'gz']]\n","        missing_cols = [c for c in data_cols if c not in df_sensor.columns]\n","        if missing_cols:\n","            print(f\"  Skip: missing columns {missing_cols}\")\n","            continue\n","\n","        print(f\"  Resampling sensors ({len(df_sensor)} rows)...\")\n","        result = resample_sensor_data(\n","            df_sensor, time_col, data_cols, TARGET_FREQ_HZ, MAX_INTERP_GAP_MS\n","        )\n","\n","        if result[0] is None:\n","            print(f\"  Skip: resampling failed\")\n","            continue\n","\n","        # Receive cleaned time for labels\n","        resampled_sensor, interp_ratio, gap_points, gap_time_frac, orig_freq, sensor_time_clean = result\n","\n","        valid_samples = resampled_sensor[data_cols].notna().all(axis=1).sum()\n","        nan_samples = len(resampled_sensor) - valid_samples\n","        forced_nan_points = int(resampled_sensor['is_forced_nan'].sum())\n","\n","        print(f\"  → {len(resampled_sensor)} rows, gap coverage: {interp_ratio*100:.2f}%, NaN: {nan_samples}\")\n","\n","        # Prune based on global switch\n","        if interp_ratio > MAX_INTERP_RATIO:\n","            msg = f\"Gap coverage too high ({interp_ratio*100:.1f}%)\"\n","            print(f\"  ⚠️  {msg}\")\n","            issues.append({\n","                'subject_id': subject_id,\n","                'session_id': session_id,\n","                'placement': placement,\n","                'is_train': is_train,\n","                'issue': 'high_gap_coverage',\n","                'gap_coverage': round(interp_ratio, 4),\n","            })\n","            if APPLY_PRUNE:\n","                continue\n","\n","        interp_stats.append({\n","            'subject_id': subject_id,\n","            'session_id': session_id,\n","            'placement': placement,\n","            'is_train': is_train,\n","            'original_samples': len(df_sensor),\n","            'original_freq_hz': round(orig_freq, 2),\n","            'resampled_samples': len(resampled_sensor),\n","            'valid_samples': valid_samples,\n","            'nan_samples': nan_samples,\n","            'gap_points': gap_points,\n","            'gap_coverage': round(interp_ratio, 4),\n","            'gap_time_fraction': round(gap_time_frac, 4),\n","            'forced_nan_points': forced_nan_points,\n","        })\n","\n","        df_label = pd.read_csv(label_path, sep=None, engine='python')\n","\n","        label_col = None\n","        for col_candidate in ['Class', 'class', 'label', 'Label', 'activity', 'Activity']:\n","            if col_candidate in df_label.columns:\n","                label_col = col_candidate\n","                break\n","\n","        if not label_col:\n","            for col in df_label.columns:\n","                if any(kw in col.lower() for kw in ['label', 'activity', 'class', 'action']):\n","                    label_col = col\n","                    break\n","\n","        if not label_col:\n","            print(f\"  Skip: no label column\")\n","            issues.append({\n","                'subject_id': subject_id,\n","                'session_id': session_id,\n","                'placement': placement,\n","                'is_train': is_train,\n","                'issue': 'no_label_column',\n","            })\n","            continue\n","\n","        label_time_col = detect_time_column(df_label)\n","        target_time = resampled_sensor['time_sec'].values\n","\n","        print(f\"  Resampling labels...\")\n","        try:\n","            if label_time_col:\n","                resampled_labels = resample_labels(\n","                    df_label, sensor_time_clean, label_col, target_time,\n","                    label_time_col=label_time_col\n","                )\n","            else:\n","                resampled_labels = resample_labels(\n","                    df_label, sensor_time_clean, label_col, target_time\n","                )\n","\n","            resampled_sensor['label'] = resampled_labels\n","\n","        except Exception as e:\n","            print(f\"  Skip: label resampling failed - {e}\")\n","            issues.append({\n","                'subject_id': subject_id,\n","                'session_id': session_id,\n","                'placement': placement,\n","                'is_train': is_train,\n","                'issue': 'label_resample_error',\n","                'error': str(e),  # include error details\n","            })\n","            continue\n","\n","        resampled_sensor.rename(columns={\n","            channel_mapping['ax']: 'ax',\n","            channel_mapping['ay']: 'ay',\n","            channel_mapping['az']: 'az',\n","            channel_mapping['gx']: 'gx',\n","            channel_mapping['gy']: 'gy',\n","            channel_mapping['gz']: 'gz',\n","        }, inplace=True)\n","\n","        resampled_sensor.insert(0, 'subject_id', subject_id)\n","        resampled_sensor.insert(1, 'session_id', session_id)\n","        resampled_sensor.insert(2, 'placement', placement)\n","\n","        resampled_records.append(resampled_sensor)\n","        print(f\"  ✓ Done\")\n","\n","    except Exception as e:\n","        print(f\"  ✗ Error: {e}\")\n","        issues.append({\n","            'subject_id': subject_id,\n","            'session_id': session_id,\n","            'placement': placement,\n","            'is_train': is_train,\n","            'issue': 'processing_error',\n","            'error': str(e),  # include error details\n","        })\n","\n","print(f\"\\nSuccessfully processed: {len(resampled_records)} sessions\")\n","print(f\"Skipped/failed: {len(issues)} sessions\")\n","\n","# ========== 2. Combine & save ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Combine & Save\")\n","print(\"=\"*60)\n","\n","if resampled_records:\n","    df_all = pd.concat(resampled_records, ignore_index=True)\n","\n","    # Optimization: cast dtypes (reduce size)\n","    for c in ['ax', 'ay', 'az', 'gx', 'gy', 'gz']:\n","        df_all[c] = df_all[c].astype('float32')\n","    df_all['time_sec'] = df_all['time_sec'].astype('float64')  # Keep high precision for time\n","\n","    output_file = proc_dir / \"resampled.parquet\"\n","\n","    if output_file.exists():\n","        import shutil\n","        if output_file.is_dir():\n","            shutil.rmtree(output_file)\n","        else:\n","            output_file.unlink()\n","        print(f\"Removed old data: {output_file}\")\n","\n","    df_all.to_parquet(\n","        output_file,\n","        index=False,\n","        partition_cols=['subject_id', 'placement'],\n","        engine='pyarrow'\n","    )\n","    print(f\"✓ Saved: {output_file}\")\n","    print(f\"  Total rows: {len(df_all):,}\")\n","    print(f\"  # subjects: {df_all['subject_id'].nunique()}\")\n","    print(f\"  # sessions: {df_all.groupby(['subject_id', 'session_id']).ngroups}\")\n","\n","    valid_mask = df_all[['ax', 'ay', 'az', 'gx', 'gy', 'gz']].notna().all(axis=1)\n","    print(f\"  Valid samples: {valid_mask.sum():,} ({valid_mask.sum()/len(df_all)*100:.1f}%)\")\n","    print(f\"  Samples with NaN: {(~valid_mask).sum():,}\")\n","\n","    print(\"\\nData preview:\")\n","    print(df_all.head(10).to_string())\n","\n","    print(\"\\nNumeric column stats (valid samples):\")\n","    numeric_cols = ['ax', 'ay', 'az', 'gx', 'gy', 'gz']\n","    print(df_all.loc[valid_mask, numeric_cols].describe().round(4))\n","else:\n","    print(\"Warning: No data to save\")\n","\n","# ========== 3. Save statistics ==========\n","if interp_stats:\n","    df_interp = pd.DataFrame(interp_stats)\n","    interp_file = proc_dir / \"resample_stats.csv\"\n","    df_interp.to_csv(interp_file, index=False)\n","    print(f\"\\n✓ Saved stats: {interp_file}\")\n","\n","    if 'is_train' in df_interp.columns and df_interp['is_train'].any():\n","        train_stats = df_interp[df_interp['is_train']]\n","        print(f\"\\nGap statistics (train fold):\")\n","        print(f\"  Mean gap coverage: {train_stats['gap_coverage'].mean()*100:.2f}%\")\n","        print(f\"  Max gap coverage: {train_stats['gap_coverage'].max()*100:.2f}%\")\n","        print(f\"  Mean gap time fraction: {train_stats['gap_time_fraction'].mean()*100:.2f}%\")\n","\n","        print(f\"\\nGap statistics (overall):\")\n","        print(f\"  Mean gap coverage: {df_interp['gap_coverage'].mean()*100:.2f}%\")\n","        print(f\"  Max gap coverage: {df_interp['gap_coverage'].max()*100:.2f}%\")\n","    else:\n","        print(f\"\\nGap statistics:\")\n","        print(f\"  Mean gap coverage: {df_interp['gap_coverage'].mean()*100:.2f}%\")\n","        print(f\"  Max gap coverage: {df_interp['gap_coverage'].max()*100:.2f}%\")\n","\n","if issues:\n","    df_issues = pd.DataFrame(issues)\n","    issues_file = proc_dir / \"resample_issues.csv\"\n","    df_issues.to_csv(issues_file, index=False)\n","    print(f\"\\n⚠️  Saved issue records: {issues_file} ({len(issues)} items)\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 5 complete - Flawless version\")\n","print(\"=\"*60)\n","print(f\"\\nFinal fixes:\")\n","print(f\"  1. ✓ Prune switch (always ON; global high-gap sessions removed)\")\n","print(f\"  2. ✓ Label time harmonized (reuse cleaned time)\")\n","print(f\"  3. ✓ Complete error information (\\\"error\\\" field)\")\n","print(f\"  4. ✓ Comment fix (constant threshold 0.15)\")\n","print(f\"  5. ✓ Type optimization (float32/float64)\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sAf1CED_k9jq","executionInfo":{"status":"ok","timestamp":1763144586170,"user_tz":0,"elapsed":46736,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"15110dcd-72ba-43c6-c455-f824a5354b33"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 5: Timeline Unification & Resampling\n","============================================================\n","\n","Target sampling rate: 50.0 Hz\n","Selected placement: rwrist\n","\n","Global resampling over all kept sessions (no per-fold markers)\n","  Total sessions: 96\n","\n","============================================================\n","1. Resampling\n","============================================================\n","\n","Processing S07/R03/rwrist [TEST]...\n","  Resampling sensors (11758 rows)...\n","  → 5879 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R05/rwrist [TEST]...\n","  Resampling sensors (11766 rows)...\n","  → 5883 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R06/rwrist [TEST]...\n","  Resampling sensors (11838 rows)...\n","  → 5919 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R07/rwrist [TEST]...\n","  Resampling sensors (11795 rows)...\n","  → 5898 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R08/rwrist [TEST]...\n","  Resampling sensors (11804 rows)...\n","  → 5902 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R09/rwrist [TEST]...\n","  Resampling sensors (11779 rows)...\n","  → 5890 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R10/rwrist [TEST]...\n","  Resampling sensors (11777 rows)...\n","  → 5889 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R11/rwrist [TEST]...\n","  Resampling sensors (11775 rows)...\n","  → 5888 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R12/rwrist [TEST]...\n","  Resampling sensors (11814 rows)...\n","  → 5908 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R13/rwrist [TEST]...\n","  Resampling sensors (11842 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R14/rwrist [TEST]...\n","  Resampling sensors (11806 rows)...\n","  → 5903 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R15/rwrist [TEST]...\n","  Resampling sensors (11774 rows)...\n","  → 5888 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R16/rwrist [TEST]...\n","  Resampling sensors (11863 rows)...\n","  → 5932 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R03/rwrist [TEST]...\n","  Resampling sensors (11868 rows)...\n","  → 5935 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R04/rwrist [TEST]...\n","  Resampling sensors (11843 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R06/rwrist [TEST]...\n","  Resampling sensors (11864 rows)...\n","  → 5932 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R07/rwrist [TEST]...\n","  Resampling sensors (11901 rows)...\n","  → 5951 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R08/rwrist [TEST]...\n","  Resampling sensors (11856 rows)...\n","  → 5929 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R10/rwrist [TEST]...\n","  Resampling sensors (11926 rows)...\n","  → 5963 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R11/rwrist [TEST]...\n","  Resampling sensors (11903 rows)...\n","  → 5952 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R12/rwrist [TEST]...\n","  Resampling sensors (11924 rows)...\n","  → 5963 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R13/rwrist [TEST]...\n","  Resampling sensors (11925 rows)...\n","  → 5963 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R15/rwrist [TEST]...\n","  Resampling sensors (11270 rows)...\n","  → 5636 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R16/rwrist [TEST]...\n","  Resampling sensors (11540 rows)...\n","  → 5770 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R03/rwrist [TEST]...\n","  Resampling sensors (11866 rows)...\n","  → 5934 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R04/rwrist [TEST]...\n","  Resampling sensors (11907 rows)...\n","  → 5954 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R05/rwrist [TEST]...\n","  Resampling sensors (11895 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R06/rwrist [TEST]...\n","  Resampling sensors (11916 rows)...\n","  → 5959 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R07/rwrist [TEST]...\n","  Resampling sensors (11860 rows)...\n","  → 5931 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R08/rwrist [TEST]...\n","  Resampling sensors (11909 rows)...\n","  → 5955 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R09/rwrist [TEST]...\n","  Resampling sensors (11906 rows)...\n","  → 5953 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R10/rwrist [TEST]...\n","  Resampling sensors (11886 rows)...\n","  → 5944 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R11/rwrist [TEST]...\n","  Resampling sensors (11880 rows)...\n","  → 5941 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R12/rwrist [TEST]...\n","  Resampling sensors (11866 rows)...\n","  → 5933 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R13/rwrist [TEST]...\n","  Resampling sensors (11904 rows)...\n","  → 5953 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R14/rwrist [TEST]...\n","  Resampling sensors (4035 rows)...\n","  → 2018 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R15/rwrist [TEST]...\n","  Resampling sensors (11895 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R16/rwrist [TEST]...\n","  Resampling sensors (11895 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R03/rwrist [TEST]...\n","  Resampling sensors (11795 rows)...\n","  → 5898 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R04/rwrist [TEST]...\n","  Resampling sensors (11806 rows)...\n","  → 5903 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R05/rwrist [TEST]...\n","  Resampling sensors (11815 rows)...\n","  → 5908 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R06/rwrist [TEST]...\n","  Resampling sensors (11767 rows)...\n","  → 5884 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R07/rwrist [TEST]...\n","  Resampling sensors (11869 rows)...\n","  → 5935 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R08/rwrist [TEST]...\n","  Resampling sensors (11794 rows)...\n","  → 5898 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R09/rwrist [TEST]...\n","  Resampling sensors (11791 rows)...\n","  → 5896 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R10/rwrist [TEST]...\n","  Resampling sensors (11863 rows)...\n","  → 5932 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R11/rwrist [TEST]...\n","  Resampling sensors (11822 rows)...\n","  → 5911 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R12/rwrist [TEST]...\n","  Resampling sensors (11845 rows)...\n","  → 5923 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R13/rwrist [TEST]...\n","  Resampling sensors (11792 rows)...\n","  → 5897 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R14/rwrist [TEST]...\n","  Resampling sensors (11798 rows)...\n","  → 5900 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R15/rwrist [TEST]...\n","  Resampling sensors (11803 rows)...\n","  → 5902 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R16/rwrist [TEST]...\n","  Resampling sensors (11751 rows)...\n","  → 5876 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R03/rwrist [TEST]...\n","  Resampling sensors (11858 rows)...\n","  → 5929 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R04/rwrist [TEST]...\n","  Resampling sensors (11809 rows)...\n","  → 5905 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R05/rwrist [TEST]...\n","  Resampling sensors (11821 rows)...\n","  → 5911 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R06/rwrist [TEST]...\n","  Resampling sensors (11843 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R07/rwrist [TEST]...\n","  Resampling sensors (11897 rows)...\n","  → 5949 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R08/rwrist [TEST]...\n","  Resampling sensors (11889 rows)...\n","  → 5945 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R09/rwrist [TEST]...\n","  Resampling sensors (11906 rows)...\n","  → 5953 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R10/rwrist [TEST]...\n","  Resampling sensors (11880 rows)...\n","  → 5940 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R11/rwrist [TEST]...\n","  Resampling sensors (11859 rows)...\n","  → 5930 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R12/rwrist [TEST]...\n","  Resampling sensors (11885 rows)...\n","  → 5943 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R13/rwrist [TEST]...\n","  Resampling sensors (11841 rows)...\n","  → 5921 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R15/rwrist [TEST]...\n","  Resampling sensors (11879 rows)...\n","  → 5940 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R11/rwrist [TEST]...\n","  Resampling sensors (11894 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R12/rwrist [TEST]...\n","  Resampling sensors (2817 rows)...\n","  → 1409 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R13/rwrist [TEST]...\n","  Resampling sensors (11905 rows)...\n","  → 5953 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R14/rwrist [TEST]...\n","  Resampling sensors (11489 rows)...\n","  → 5745 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R15/rwrist [TEST]...\n","  Resampling sensors (11927 rows)...\n","  → 5964 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R16/rwrist [TEST]...\n","  Resampling sensors (11843 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R03/rwrist [TEST]...\n","  Resampling sensors (11845 rows)...\n","  → 5923 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R04/rwrist [TEST]...\n","  Resampling sensors (11878 rows)...\n","  → 5940 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R05/rwrist [TEST]...\n","  Resampling sensors (11910 rows)...\n","  → 5956 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R06/rwrist [TEST]...\n","  Resampling sensors (11902 rows)...\n","  → 5951 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R07/rwrist [TEST]...\n","  Resampling sensors (11903 rows)...\n","  → 5952 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R08/rwrist [TEST]...\n","  Resampling sensors (11896 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R09/rwrist [TEST]...\n","  Resampling sensors (11910 rows)...\n","  → 5955 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R10/rwrist [TEST]...\n","  Resampling sensors (11918 rows)...\n","  → 5959 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R11/rwrist [TEST]...\n","  Resampling sensors (11861 rows)...\n","  → 5931 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R12/rwrist [TEST]...\n","  Resampling sensors (11912 rows)...\n","  → 5956 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R13/rwrist [TEST]...\n","  Resampling sensors (11894 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R14/rwrist [TEST]...\n","  Resampling sensors (11884 rows)...\n","  → 5943 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R15/rwrist [TEST]...\n","  Resampling sensors (11892 rows)...\n","  → 5946 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R16/rwrist [TEST]...\n","  Resampling sensors (11870 rows)...\n","  → 5935 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R03/rwrist [TEST]...\n","  Resampling sensors (11849 rows)...\n","  → 5925 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R04/rwrist [TEST]...\n","  Resampling sensors (11760 rows)...\n","  → 5881 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R06/rwrist [TEST]...\n","  Resampling sensors (11856 rows)...\n","  → 5928 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R07/rwrist [TEST]...\n","  Resampling sensors (11863 rows)...\n","  → 5932 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R08/rwrist [TEST]...\n","  Resampling sensors (11921 rows)...\n","  → 5961 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R09/rwrist [TEST]...\n","  Resampling sensors (11871 rows)...\n","  → 5936 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R10/rwrist [TEST]...\n","  Resampling sensors (11911 rows)...\n","  → 5956 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R11/rwrist [TEST]...\n","  Resampling sensors (11843 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R12/rwrist [TEST]...\n","  Resampling sensors (11787 rows)...\n","  → 5894 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R14/rwrist [TEST]...\n","  Resampling sensors (11851 rows)...\n","  → 5926 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R15/rwrist [TEST]...\n","  Resampling sensors (11823 rows)...\n","  → 5912 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R16/rwrist [TEST]...\n","  Resampling sensors (11851 rows)...\n","  → 5926 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Successfully processed: 96 sessions\n","Skipped/failed: 0 sessions\n","\n","============================================================\n","2. Combine & Save\n","============================================================\n","Removed old data: data/lara/mbientlab/proc/resampled.parquet\n","✓ Saved: data/lara/mbientlab/proc/resampled.parquet\n","  Total rows: 560,070\n","  # subjects: 8\n","  # sessions: 96\n","  Valid samples: 560,070 (100.0%)\n","  Samples with NaN: 0\n","\n","Data preview:\n","  subject_id session_id placement      time_sec        ax        ay        az         gx         gy         gz  is_in_gap  is_forced_nan  label\n","0        S07        R03    rwrist  1.564739e+09 -0.281438 -0.924541  0.325248   1.414270  24.960403  -9.089332          0              0    6.0\n","1        S07        R03    rwrist  1.564739e+09 -0.298464 -0.935255  0.334803   3.294115  27.049623 -12.293148          0              0    6.0\n","2        S07        R03    rwrist  1.564739e+09 -0.263180 -1.002837  0.456556   2.767826  14.701869 -19.894005          0              0    6.0\n","3        S07        R03    rwrist  1.564739e+09 -0.174738 -1.060583  0.545962   5.506332   5.940687 -22.905399          0              0    6.0\n","4        S07        R03    rwrist  1.564739e+09 -0.125629 -1.083030  0.620053  16.071184  10.412006 -25.808487          0              0    6.0\n","5        S07        R03    rwrist  1.564739e+09 -0.148535 -1.093232  0.639439  25.785658  20.021729 -29.624060          0              0    6.0\n","6        S07        R03    rwrist  1.564739e+09 -0.210257 -1.095191  0.675926  40.849041  25.070871 -35.659191          0              0    6.0\n","7        S07        R03    rwrist  1.564739e+09 -0.222465 -1.095854  0.701550  45.951775  22.461361 -37.891891          0              0    6.0\n","8        S07        R03    rwrist  1.564739e+09 -0.214872 -1.125343  0.729511  57.344635  10.825806 -42.137077          0              0    6.0\n","9        S07        R03    rwrist  1.564739e+09 -0.218806 -1.212497  0.748468  68.151352   9.044179 -45.126099          0              0    6.0\n","\n","Numeric column stats (valid samples):\n","                ax           ay           az           gx           gy  \\\n","count  560070.0000  560070.0000  560070.0000  560070.0000  560070.0000   \n","mean       -0.6569      -0.1522       0.3342      -1.0943       0.6134   \n","std         0.4212       0.5260       0.4697      77.1153      72.2905   \n","min       -11.1498      -9.2003     -27.0872   -4185.1846   -1426.9553   \n","25%        -0.9339      -0.5590       0.1043     -19.9812     -22.0506   \n","50%        -0.7474      -0.0536       0.3487       0.1252      -0.3429   \n","75%        -0.4270       0.2280       0.6074      20.5185      21.7400   \n","max        32.7536      11.5694      14.6606    1110.7982    1697.6891   \n","\n","                gz  \n","count  560070.0000  \n","mean        0.5667  \n","std        72.9628  \n","min     -3227.1677  \n","25%       -19.0527  \n","50%         0.5039  \n","75%        22.0198  \n","max       736.3111  \n","\n","✓ Saved stats: data/lara/mbientlab/proc/resample_stats.csv\n","\n","Gap statistics:\n","  Mean gap coverage: 0.00%\n","  Max gap coverage: 0.00%\n","\n","============================================================\n","Step 5 complete - Flawless version\n","============================================================\n","\n","Final fixes:\n","  1. ✓ Prune switch (always ON; global high-gap sessions removed)\n","  2. ✓ Label time harmonized (reuse cleaned time)\n","  3. ✓ Complete error information (\"error\" field)\n","  4. ✓ Comment fix (constant threshold 0.15)\n","  5. ✓ Type optimization (float32/float64)\n","============================================================\n"]}]},{"cell_type":"code","source":["import os\n","\n","\"\"\"\n","Step 6: Sensor Preprocessing (top-conf/journal grade - final fixed version)\n","Accelerometer high-pass to remove gravity; gyroscope denoising; adaptive ±Nσ clipping (target 1%)\n","Global version: no FOLD_ID required; thresholds estimated on all data.\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import json\n","from scipy import signal\n","\n","# ========== Config ==========\n","# Accelerometer high-pass (remove gravity)\n","ACC_HPF_CUTOFF_HZ = 0.3      # Cutoff frequency\n","ACC_HPF_ORDER = 2            # Filter order\n","\n","# Gyroscope low-pass (denoise)\n","GYR_LPF_CUTOFF_HZ = 20.0     # Cutoff frequency\n","GYR_LPF_ORDER = 2            # Filter order\n","\n","# Adaptive clipping threshold (auto-tuned to target clipping rate)\n","TARGET_CLIP_RATE = 0.01      # Target clipping rate 1% (sum of both tails)\n","\n","# Sampling rate (from Step 5)\n","SAMPLING_RATE_HZ = 50.0\n","\n","# Unit conversions\n","DEG2RAD = np.pi / 180.0\n","G_TO_MS2 = 9.80665\n","\n","print(\"=\"*60)\n","print(\"Step 6: Sensor Preprocessing\")\n","print(\"=\"*60)\n","\n","# Load data\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","\n","print(f\"\\nLoading resampled data: {proc_dir / 'resampled.parquet'}\")\n","df = pd.read_parquet(proc_dir / \"resampled.parquet\")\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Number of subjects: {df['subject_id'].nunique()}\")\n","print(f\"Number of sessions: {df.groupby(['subject_id', 'session_id'], observed=True).ngroups}\")\n","\n","# ========== 0. Unit normalization ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"0. Unit normalization\")\n","print(\"=\"*60)\n","\n","acc_channels = ['ax', 'ay', 'az']\n","print(f\"\\nAccelerometer unit conversion: g → m/s²\")\n","for ch in acc_channels:\n","    if ch in df.columns:\n","        mask = df[ch].notna()\n","        df.loc[mask, ch] = df.loc[mask, ch] * G_TO_MS2\n","print(f\"✓ Conversion factor: {G_TO_MS2:.5f}\")\n","\n","gyr_channels = ['gx', 'gy', 'gz']\n","print(f\"\\nGyroscope unit conversion: deg/s → rad/s\")\n","for ch in gyr_channels:\n","    if ch in df.columns:\n","        mask = df[ch].notna()\n","        df.loc[mask, ch] = df.loc[mask, ch] * DEG2RAD\n","print(f\"✓ Conversion factor: π/180 = {DEG2RAD:.6f}\")\n","\n","# ========== Helper functions ==========\n","def design_highpass_filter(cutoff_hz, fs_hz, order=2):\n","    \"\"\"Design a high-pass Butterworth filter\"\"\"\n","    nyq = 0.5 * fs_hz\n","    normal_cutoff = cutoff_hz / nyq\n","    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n","    return b, a\n","\n","def design_lowpass_filter(cutoff_hz, fs_hz, order=2):\n","    \"\"\"Design a low-pass Butterworth filter\"\"\"\n","    nyq = 0.5 * fs_hz\n","    normal_cutoff = cutoff_hz / nyq\n","    b, a = signal.butter(order, normal_cutoff, btype='low', analog=False)\n","    return b, a\n","\n","def filtfilt_nan_safe(x, b, a):\n","    \"\"\"Zero-phase filtering tolerant to NaN (filter each contiguous non-NaN run)\"\"\"\n","    y = x.copy()\n","    good = np.isfinite(x)\n","\n","    if not good.any():\n","        return x\n","\n","    idx = np.where(good)[0]\n","    cuts = np.where(np.diff(idx) > 1)[0] + 1\n","    runs = np.split(idx, cuts)\n","\n","    padlen = 3 * (max(len(a), len(b)) - 1)\n","\n","    for run in runs:\n","        seg = x[run]\n","\n","        if len(seg) > padlen:\n","            y[run] = signal.filtfilt(b, a, seg, method=\"pad\")\n","        else:\n","            tmp = signal.lfilter(b, a, seg)\n","            y[run] = signal.lfilter(b, a, tmp[::-1])[::-1]\n","\n","    return y\n","\n","def apply_filter_by_session(df, channels, b, a):\n","    \"\"\"Apply zero-phase filtering grouped by session (include placement grouping + sorting)\"\"\"\n","    filtered_data = []\n","\n","    for (subj, sess, plc), group in df.groupby(['subject_id', 'session_id', 'placement'], observed=True):\n","        group = group.sort_values('time_sec').copy()\n","\n","        for ch in channels:\n","            if ch not in group.columns:\n","                continue\n","\n","            data = group[ch].values\n","            filtered = filtfilt_nan_safe(data, b, a)\n","            group[ch] = filtered\n","\n","        filtered_data.append(group)\n","\n","    return pd.concat(filtered_data, ignore_index=True)\n","\n","def compute_clip_thresholds_target(df, channels, target_rate=0.01, use_robust=True):\n","    \"\"\"Adaptive thresholds to a target clipping rate (Scheme A)\n","\n","    Args:\n","        target_rate: target total clipping rate for both tails (e.g., 0.01 = 1%)\n","        use_robust: if True, use Median±k·(1.4826·MAD); otherwise Mean±k·Std\n","    \"\"\"\n","    eps = 1e-6\n","    thresholds = {}\n","\n","    for ch in channels:\n","        if ch not in df.columns:\n","            continue\n","\n","        x = df[ch].dropna().values\n","        if x.size == 0:\n","            continue\n","\n","        if use_robust:\n","            # Robust estimate: Median ± k·(1.4826·MAD)\n","            median = np.median(x)\n","            mad = np.median(np.abs(x - median))\n","            robust_std = max(1.4826 * mad, eps)\n","\n","            deviations = np.abs(x - median) / robust_std\n","            k = np.quantile(deviations, 1 - target_rate)\n","\n","            lower = median - k * robust_std\n","            upper = median + k * robust_std\n","\n","            thresholds[ch] = {\n","                'center': float(median),\n","                'scale': float(robust_std),\n","                'k': float(k),\n","                'lower': float(lower),\n","                'upper': float(upper),\n","                'method': f'Median±k·MAD (k={k:.3f}, both tails total {target_rate*100:.1f}%)',\n","            }\n","        else:\n","            # Conventional estimate: Mean ± k·Std\n","            mean = np.mean(x)\n","            std = max(np.std(x), eps)\n","\n","            deviations = np.abs(x - mean) / std\n","            k = np.quantile(deviations, 1 - target_rate)\n","\n","            lower = mean - k * std\n","            upper = mean + k * std\n","\n","            thresholds[ch] = {\n","                'center': float(mean),\n","                'scale': float(std),\n","                'k': float(k),\n","                'lower': float(lower),\n","                'upper': float(upper),\n","                'method': f'Mean±k·Std (k={k:.3f}, both tails total {target_rate*100:.1f}%)',\n","            }\n","\n","    return thresholds\n","\n","def apply_clip(df, channels, thresholds):\n","    \"\"\"Apply clipping and compute actual clipping rate\"\"\"\n","    df_clipped = df.copy()\n","    clip_stats = {}\n","\n","    for ch in channels:\n","        if ch not in df_clipped.columns or ch not in thresholds:\n","            continue\n","\n","        lower = thresholds[ch]['lower']\n","        upper = thresholds[ch]['upper']\n","\n","        mask = df_clipped[ch].notna()\n","        total = mask.sum()\n","\n","        if total > 0:\n","            outliers = ((df_clipped.loc[mask, ch] < lower) | (df_clipped.loc[mask, ch] > upper)).sum()\n","            clip_rate = outliers / total\n","            clip_stats[ch] = {\n","                'outliers': int(outliers),\n","                'total': int(total),\n","                'rate': float(clip_rate),\n","            }\n","\n","        df_clipped.loc[mask, ch] = df_clipped.loc[mask, ch].clip(lower, upper)\n","\n","    return df_clipped, clip_stats\n","\n","# ========== 1. Design filters ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Design filters\")\n","print(\"=\"*60)\n","\n","print(f\"\\nAccelerometer high-pass filter:\")\n","print(f\"  Cutoff frequency: {ACC_HPF_CUTOFF_HZ} Hz\")\n","print(f\"  Order: {ACC_HPF_ORDER}\")\n","acc_b, acc_a = design_highpass_filter(ACC_HPF_CUTOFF_HZ, SAMPLING_RATE_HZ, ACC_HPF_ORDER)\n","\n","print(f\"\\nGyroscope low-pass filter:\")\n","print(f\"  Cutoff frequency: {GYR_LPF_CUTOFF_HZ} Hz\")\n","print(f\"  Order: {GYR_LPF_ORDER}\")\n","gyr_b, gyr_a = design_lowpass_filter(GYR_LPF_CUTOFF_HZ, SAMPLING_RATE_HZ, GYR_LPF_ORDER)\n","\n","# ========== 2. Apply filters (by session + placement) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Apply filters (by session + placement, zero-phase)\")\n","print(\"=\"*60)\n","\n","print(\"\\nApplying accelerometer high-pass (remove gravity)...\")\n","df_filtered = apply_filter_by_session(df, acc_channels, acc_b, acc_a)\n","print(\"✓ Done\")\n","\n","print(\"\\nApplying gyroscope low-pass (denoise)...\")\n","df_filtered = apply_filter_by_session(df_filtered, gyr_channels, gyr_b, gyr_a)\n","print(\"✓ Done\")\n","\n","# ========== 3. Compute clipping thresholds (adaptive to target rate) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Compute adaptive clipping thresholds (target clipping rate)\")\n","print(\"=\"*60)\n","\n","print(\"Estimate clipping thresholds on all data\")\n","print(f\"  Target clip rate: {TARGET_CLIP_RATE*100:.1f}%\")\n","\n","all_channels = acc_channels + gyr_channels\n","df_for_stats = df_filtered  # global estimation on all subjects\n","clip_thresholds = compute_clip_thresholds_target(\n","    df_for_stats, all_channels, TARGET_CLIP_RATE, use_robust=True\n",")\n","\n","print(f\"\\nClipping thresholds (adaptive robust estimation):\")\n","for ch, thresh in clip_thresholds.items():\n","    print(f\"  {ch}:\")\n","    print(f\"    center: {thresh['center']:.4f}\")\n","    print(f\"    scale: {thresh['scale']:.4f}\")\n","    print(f\"    k: {thresh['k']:.3f}\")\n","    print(f\"    range: [{thresh['lower']:.4f}, {thresh['upper']:.4f}]\")\n","\n","# ========== 4. Apply clipping ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Apply adaptive clipping\")\n","print(\"=\"*60)\n","\n","df_clipped, clip_stats = apply_clip(df_filtered, all_channels, clip_thresholds)\n","\n","print(\"\\nActual clipping statistics:\")\n","for ch, stats in clip_stats.items():\n","    print(f\"  {ch}: {stats['outliers']:,} / {stats['total']:,} ({stats['rate']*100:.2f}%)\")\n","\n","# ========== 5. Cast to float32 to save memory ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Data type optimization\")\n","print(\"=\"*60)\n","\n","numeric_cols = ['ax', 'ay', 'az', 'gx', 'gy', 'gz']\n","for col in numeric_cols:\n","    if col in df_clipped.columns:\n","        df_clipped[col] = df_clipped[col].astype('float32')\n","\n","print(f\"✓ Sensor columns cast to float32\")\n","print(f\"✓ time_sec kept as float64\")\n","\n","# ========== 6. Save results ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Save results\")\n","print(\"=\"*60)\n","\n","output_file = proc_dir / \"filtered.parquet\"\n","\n","if output_file.exists():\n","    import shutil\n","    if output_file.is_dir():\n","        shutil.rmtree(output_file)\n","    else:\n","        output_file.unlink()\n","    print(f\"Removed old data: {output_file}\")\n","\n","df_clipped.to_parquet(\n","    output_file,\n","    index=False,\n","    partition_cols=['subject_id', 'placement'],\n","    engine='pyarrow'\n",")\n","print(f\"✓ Saved: {output_file}\")\n","print(f\"  Data shape: {df_clipped.shape}\")\n","\n","print(\"\\nData preview:\")\n","print(df_clipped.head(10).to_string())\n","\n","print(\"\\nPost-filter numeric column stats:\")\n","valid_mask = df_clipped[numeric_cols].notna().all(axis=1)\n","print(df_clipped.loc[valid_mask, numeric_cols].describe().round(4))\n","\n","# ========== 7. Save filter configuration ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"7. Save filter configuration\")\n","print(\"=\"*60)\n","\n","filter_config = {\n","    'sampling_rate_hz': SAMPLING_RATE_HZ,\n","\n","    'units': {\n","        'accelerometer': 'm/s² (converted from g)',\n","        'gyroscope': 'rad/s (converted from deg/s)',\n","        'conversion': {\n","            'accelerometer_g_to_ms2': G_TO_MS2,\n","            'gyroscope_deg_to_rad': DEG2RAD,\n","        }\n","    },\n","\n","    'dtypes': {\n","        'sensor_channels': 'float32',\n","        'time_sec': 'float64',\n","    },\n","\n","    'accelerometer': {\n","        'filter_type': 'highpass',\n","        'purpose': 'detrend (remove gravity)',\n","        'method': 'Butterworth',\n","        'cutoff_hz': ACC_HPF_CUTOFF_HZ,\n","        'order': ACC_HPF_ORDER,\n","        'coefficients': {\n","            'b': acc_b.tolist(),\n","            'a': acc_a.tolist(),\n","        },\n","        'zero_phase': True,\n","    },\n","\n","    'gyroscope': {\n","        'filter_type': 'lowpass',\n","        'purpose': 'denoise',\n","        'method': 'Butterworth',\n","        'cutoff_hz': GYR_LPF_CUTOFF_HZ,\n","        'order': GYR_LPF_ORDER,\n","        'coefficients': {\n","            'b': gyr_b.tolist(),\n","            'a': gyr_a.tolist(),\n","        },\n","        'zero_phase': True,\n","    },\n","\n","    'clipping': {\n","        'method': 'Adaptive robust estimation (Median±k·MAD, Scheme A)',\n","        'target_clip_rate': TARGET_CLIP_RATE,\n","        'estimated_on': 'all_data',\n","        'fold_id': None,\n","        'thresholds': clip_thresholds,\n","        'actual_clip_stats': clip_stats,\n","        'rationale': (\n","            f'Auto-adjust k so the global clipping rate reaches the target '\n","            f'{TARGET_CLIP_RATE*100:.1f}% over all subjects'\n","        ),\n","    },\n","\n","    'notes': [\n","        'All filters use filtfilt for zero phase',\n","        'Filtering is grouped by session + placement, sorted by time_sec; avoid crossing session boundaries',\n","        'filtfilt_nan_safe filters each contiguous non-NaN run separately',\n","        'Accelerometer converted from g to m/s² (×9.80665)',\n","        'Gyroscope converted from deg/s to rad/s (×π/180)',\n","        f'Adaptive clipping thresholds: determine k on all data so clipping ≈ {TARGET_CLIP_RATE*100:.1f}%, then apply consistently to all data',\n","        'NaNs remain unchanged',\n","        'Sensor columns are float32; time_sec is float64',\n","    ]\n","}\n","\n","filter_config_file = configs_dir / \"filter.yaml\"\n","with open(filter_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(filter_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","print(f\"✓ Saved filter configuration: {filter_config_file}\")\n","\n","filter_config_json = configs_dir / \"filter.json\"\n","with open(filter_config_json, 'w', encoding='utf-8') as f:\n","    json.dump(filter_config, f, indent=2)\n","print(f\"✓ Saved filter configuration: {filter_config_json}\")\n","\n","# ========== 8. Summary ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 6 complete - Sensor preprocessing (global version)\")\n","print(\"=\"*60)\n","print(f\"\\nConfig:\")\n","print(f\"  Units: Acc g→m/s², Gyro deg/s→rad/s\")\n","print(f\"  Accelerometer: high-pass {ACC_HPF_CUTOFF_HZ} Hz (remove gravity)\")\n","print(f\"  Gyroscope: low-pass {GYR_LPF_CUTOFF_HZ} Hz (denoise)\")\n","print(f\"  Clipping: adaptive ±k·MAD (target {TARGET_CLIP_RATE*100:.1f}%)\")\n","print(f\"  Clipping thresholds estimated on: all data\")\n","print(f\"\\nResults:\")\n","print(f\"  Output file: {output_file}\")\n","print(f\"  Config file: {filter_config_file}\")\n","print(f\"  Data shape: {df_clipped.shape}\")\n","print(\"\\nFinal fixes:\")\n","print(f\"  ✓ Adaptive clipping thresholds (Scheme A)\")\n","print(f\"  ✓ Target clipping rate {TARGET_CLIP_RATE*100:.1f}%, auto-solve k (global)\")\n","print(f\"  ✓ Group by placement + sort by time_sec\")\n","print(f\"  ✓ Write actual clipping rate into config\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EI27oVj2k-6-","executionInfo":{"status":"ok","timestamp":1763144588181,"user_tz":0,"elapsed":1998,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"c6bf5cb8-8039-47dd-e4d4-8e17600eec3b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 6: Sensor Preprocessing\n","============================================================\n","\n","Loading resampled data: data/lara/mbientlab/proc/resampled.parquet\n","Data shape: (560070, 13)\n","Number of subjects: 8\n","Number of sessions: 96\n","\n","============================================================\n","0. Unit normalization\n","============================================================\n","\n","Accelerometer unit conversion: g → m/s²\n","✓ Conversion factor: 9.80665\n","\n","Gyroscope unit conversion: deg/s → rad/s\n","✓ Conversion factor: π/180 = 0.017453\n","\n","============================================================\n","1. Design filters\n","============================================================\n","\n","Accelerometer high-pass filter:\n","  Cutoff frequency: 0.3 Hz\n","  Order: 2\n","\n","Gyroscope low-pass filter:\n","  Cutoff frequency: 20.0 Hz\n","  Order: 2\n","\n","============================================================\n","2. Apply filters (by session + placement, zero-phase)\n","============================================================\n","\n","Applying accelerometer high-pass (remove gravity)...\n","✓ Done\n","\n","Applying gyroscope low-pass (denoise)...\n","✓ Done\n","\n","============================================================\n","3. Compute adaptive clipping thresholds (target clipping rate)\n","============================================================\n","Estimate clipping thresholds on all data\n","  Target clip rate: 1.0%\n","\n","Clipping thresholds (adaptive robust estimation):\n","  ax:\n","    center: 0.0152\n","    scale: 1.3530\n","    k: 6.712\n","    range: [-9.0660, 9.0964]\n","  ay:\n","    center: 0.0141\n","    scale: 1.3815\n","    k: 5.790\n","    range: [-7.9844, 8.0127]\n","  az:\n","    center: 0.0019\n","    scale: 1.3427\n","    k: 5.934\n","    range: [-7.9661, 7.9699]\n","  gx:\n","    center: 0.0021\n","    scale: 0.5227\n","    k: 6.479\n","    range: [-3.3845, 3.3888]\n","  gy:\n","    center: -0.0064\n","    scale: 0.5632\n","    k: 8.447\n","    range: [-4.7639, 4.7512]\n","  gz:\n","    center: 0.0088\n","    scale: 0.5295\n","    k: 7.474\n","    range: [-3.9489, 3.9665]\n","\n","============================================================\n","4. Apply adaptive clipping\n","============================================================\n","\n","Actual clipping statistics:\n","  ax: 5,601 / 560,070 (1.00%)\n","  ay: 5,601 / 560,070 (1.00%)\n","  az: 5,601 / 560,070 (1.00%)\n","  gx: 5,601 / 560,070 (1.00%)\n","  gy: 5,601 / 560,070 (1.00%)\n","  gz: 5,601 / 560,070 (1.00%)\n","\n","============================================================\n","5. Data type optimization\n","============================================================\n","✓ Sensor columns cast to float32\n","✓ time_sec kept as float64\n","\n","============================================================\n","6. Save results\n","============================================================\n","Removed old data: data/lara/mbientlab/proc/filtered.parquet\n","✓ Saved: data/lara/mbientlab/proc/filtered.parquet\n","  Data shape: (560070, 13)\n","\n","Data preview:\n","  session_id      time_sec        ax        ay        az        gx        gy        gz  is_in_gap  is_forced_nan  label subject_id placement\n","0        R03  1.564739e+09  0.891079 -2.660401  0.238257  0.025291  0.435949 -0.158735          0              0    6.0        S07    rwrist\n","1        R03  1.564739e+09  0.715069 -2.840439  0.239453  0.059089  0.468073 -0.219117          0              0    6.0        S07    rwrist\n","2        R03  1.564739e+09  1.051161 -3.582292  1.341616  0.042692  0.260707 -0.339956          0              0    6.0        S07    rwrist\n","3        R03  1.564739e+09  1.907651 -4.231893  2.127376  0.107476  0.100950 -0.407764          0              0    6.0        S07    rwrist\n","4        R03  1.564739e+09  2.377473 -4.539616  2.763897  0.263063  0.184827 -0.442223          0              0    6.0        S07    rwrist\n","5        R03  1.564739e+09  2.140075 -4.731589  2.865023  0.472918  0.345074 -0.525765          0              0    6.0        S07    rwrist\n","6        R03  1.564739e+09  1.521002 -4.847136  3.135042  0.686333  0.442941 -0.613507          0              0    6.0        S07    rwrist\n","7        R03  1.564739e+09  1.386402 -4.954428  3.299844  0.829848  0.386559 -0.669225          0              0    6.0        S07    rwrist\n","8        R03  1.564739e+09  1.444855 -5.348888  3.488989  0.973541  0.191755 -0.729564          0              0    6.0        S07    rwrist\n","9        R03  1.564739e+09  1.389077 -6.313359  3.591348  1.214971  0.159118 -0.790919          0              0    6.0        S07    rwrist\n","\n","Post-filter numeric column stats:\n","                ax           ay           az           gx           gy  \\\n","count  560070.0000  560070.0000  560070.0000  560070.0000  560070.0000   \n","mean       -0.0088       0.0070       0.0014      -0.0023       0.0101   \n","std         2.2740       2.1459       2.0872       0.8858       1.1601   \n","min        -9.0660      -7.9844      -7.9661      -3.3845      -4.7639   \n","25%        -0.9098      -0.9143      -0.9075      -0.3482      -0.3829   \n","50%         0.0152       0.0141       0.0019       0.0021      -0.0064   \n","75%         0.9153       0.9493       0.9042       0.3572       0.3771   \n","max         9.0964       8.0127       7.9699       3.3888       4.7512   \n","\n","                gz  \n","count  560070.0000  \n","mean        0.0233  \n","std         0.9875  \n","min        -3.9489  \n","25%        -0.3321  \n","50%         0.0088  \n","75%         0.3837  \n","max         3.9665  \n","\n","============================================================\n","7. Save filter configuration\n","============================================================\n","✓ Saved filter configuration: configs/filter.yaml\n","✓ Saved filter configuration: configs/filter.json\n","\n","============================================================\n","Step 6 complete - Sensor preprocessing (global version)\n","============================================================\n","\n","Config:\n","  Units: Acc g→m/s², Gyro deg/s→rad/s\n","  Accelerometer: high-pass 0.3 Hz (remove gravity)\n","  Gyroscope: low-pass 20.0 Hz (denoise)\n","  Clipping: adaptive ±k·MAD (target 1.0%)\n","  Clipping thresholds estimated on: all data\n","\n","Results:\n","  Output file: data/lara/mbientlab/proc/filtered.parquet\n","  Config file: configs/filter.yaml\n","  Data shape: (560070, 13)\n","\n","Final fixes:\n","  ✓ Adaptive clipping thresholds (Scheme A)\n","  ✓ Target clipping rate 1.0%, auto-solve k (global)\n","  ✓ Group by placement + sort by time_sec\n","  ✓ Write actual clipping rate into config\n","============================================================\n"]}]},{"cell_type":"code","source":["import os\n","\n","\"\"\"\n","Step 7: Coordinate/Magnitude Normalization (top-conf/journal grade)\n","Compute magnitude channels; z-score standardization (global statistics)\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import json\n","import pickle\n","\n","# ========== Config ==========\n","EPSILON = 1e-8  # Prevent division by zero\n","\n","print(\"=\"*60)\n","print(\"Step 7: Coordinate/Magnitude Normalization\")\n","print(\"=\"*60)\n","\n","# Load data\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","\n","print(f\"\\nLoading filtered data: {proc_dir / 'filtered.parquet'}\")\n","df = pd.read_parquet(proc_dir / \"filtered.parquet\")\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Number of subjects: {df['subject_id'].nunique()}\")\n","print(f\"Number of sessions: {df.groupby(['subject_id', 'session_id'], observed=True).ngroups}\")\n","\n","# ========== 1. Compute derived channels (magnitude) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Compute derived channels (magnitude)\")\n","print(\"=\"*60)\n","\n","# Accelerometer magnitude\n","print(\"\\nComputing acc_mag = sqrt(ax² + ay² + az²)...\")\n","df['acc_mag'] = np.sqrt(\n","    df['ax'].values**2 +\n","    df['ay'].values**2 +\n","    df['az'].values**2\n",").astype('float32')\n","\n","# Gyroscope magnitude\n","print(\"Computing gyr_mag = sqrt(gx² + gy² + gz²)...\")\n","df['gyr_mag'] = np.sqrt(\n","    df['gx'].values**2 +\n","    df['gy'].values**2 +\n","    df['gz'].values**2\n",").astype('float32')\n","\n","print(f\"✓ Added derived channels: acc_mag, gyr_mag\")\n","\n","# Show derived-channel stats\n","print(\"\\nDerived channel statistics (post-filter):\")\n","for col in ['acc_mag', 'gyr_mag']:\n","    valid_data = df[col].dropna()\n","    if len(valid_data) > 0:\n","        print(f\"  {col}:\")\n","        print(f\"    Mean: {valid_data.mean():.4f}\")\n","        print(f\"    Std: {valid_data.std():.4f}\")\n","        print(f\"    Range: [{valid_data.min():.4f}, {valid_data.max():.4f}]\")\n","\n","# ========== 2. Determine training set (global) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Determine training set (global)\")\n","print(\"=\"*60)\n","\n","# In this simplified global version, we use ALL subjects to estimate statistics\n","df_train = df\n","train_subjects = set(df['subject_id'].unique())\n","test_subjects = set()  # no explicit test set at this step\n","\n","print(\"Compute statistics on all data (no per-fold split)\")\n","print(f\"  Samples: {len(df):,}\")\n","print(f\"  Subjects: {len(train_subjects)}\")\n","\n","# ========== 3. Compute z-score parameters (global) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Compute z-score parameters (global)\")\n","print(\"=\"*60)\n","\n","# Channels to standardize\n","channels_to_normalize = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","# Compute mean and std (valid data only)\n","scaler_params = {}\n","\n","print(\"\\nz-score parameters (global):\")\n","for ch in channels_to_normalize:\n","    if ch not in df_train.columns:\n","        continue\n","\n","    valid_data = df_train[ch].dropna().values\n","\n","    if len(valid_data) > 0:\n","        mean = float(np.mean(valid_data))\n","        std = float(np.std(valid_data))\n","\n","        # Guard against zero std\n","        if std < EPSILON:\n","            std = 1.0\n","\n","        scaler_params[ch] = {\n","            'mean': mean,\n","            'std': std,\n","        }\n","\n","        print(f\"  {ch}:\")\n","        print(f\"    Mean: {mean:.6f}\")\n","        print(f\"    Std: {std:.6f}\")\n","\n","# ========== 4. Apply z-score standardization ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Apply z-score standardization\")\n","print(\"=\"*60)\n","\n","df_normalized = df.copy()\n","\n","for ch in channels_to_normalize:\n","    if ch not in scaler_params:\n","        continue\n","\n","    mean = scaler_params[ch]['mean']\n","    std = scaler_params[ch]['std']\n","\n","    # Standardize non-NaN values only; cast to float32 to avoid warnings\n","    mask = df_normalized[ch].notna()\n","    normalized_values = ((df_normalized.loc[mask, ch] - mean) / (std + EPSILON)).astype('float32')\n","    df_normalized.loc[mask, ch] = normalized_values\n","\n","print(f\"✓ Standardized {len(scaler_params)} channels\")\n","\n","# Show post-standardization stats (global)\n","print(\"\\nPost-standardization stats (global):\")\n","for ch in channels_to_normalize:\n","    if ch not in scaler_params:\n","        continue\n","\n","    valid_data = df_normalized[ch].dropna()\n","    if len(valid_data) > 0:\n","        print(f\"  {ch}:\")\n","        print(f\"    Mean: {valid_data.mean():.6f} (should be near 0)\")\n","        print(f\"    Std: {valid_data.std():.6f} (should be near 1)\")\n","\n","# ========== 5. Save results ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Save results\")\n","print(\"=\"*60)\n","\n","# Save normalized data\n","output_file = proc_dir / \"normalized.parquet\"\n","\n","# Delete existing directory/file (avoid duplicate appends)\n","if output_file.exists():\n","    import shutil\n","    if output_file.is_dir():\n","        shutil.rmtree(output_file)\n","    else:\n","        output_file.unlink()\n","    print(f\"Removed old data: {output_file}\")\n","\n","df_normalized.to_parquet(\n","    output_file,\n","    index=False,\n","    partition_cols=['subject_id', 'placement'],\n","    engine='pyarrow'\n",")\n","print(f\"✓ Saved: {output_file}\")\n","print(f\"  Data shape: {df_normalized.shape}\")\n","\n","# Show data preview\n","print(\"\\nData preview:\")\n","display_cols = ['subject_id', 'session_id', 'ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag', 'label']\n","available_cols = [c for c in display_cols if c in df_normalized.columns]\n","print(df_normalized[available_cols].head(10).to_string())\n","\n","# Post-standardization numeric stats (overall)\n","print(\"\\nPost-standardization numeric column stats (overall):\")\n","numeric_cols = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","valid_mask = df_normalized[numeric_cols].notna().all(axis=1)\n","print(df_normalized.loc[valid_mask, numeric_cols].describe().round(4))\n","\n","# ========== 6. Save scaler parameters ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Save scaler parameters\")\n","print(\"=\"*60)\n","\n","scaler_info = {\n","    'fold_id': None,             # global\n","    'epsilon': EPSILON,\n","    'train_subjects': sorted(list(train_subjects)),\n","    'test_subjects': None,       # not defined at this step\n","    'channels': channels_to_normalize,\n","    'params': scaler_params,\n","    'notes': [\n","        'z-score standardization: (x - mean) / (std + ε)',\n","        'Mean and std computed from all available samples (global statistics)',\n","        'If std < ε, set std = 1.0 to avoid divide-by-zero',\n","        'NaN values are excluded from stats and remain NaN after normalization',\n","    ]\n","}\n","\n","# Save as pickle (global)\n","scaler_file = proc_dir / \"standardization.pkl\"\n","with open(scaler_file, 'wb') as f:\n","    pickle.dump(scaler_info, f)\n","print(f\"✓ Saved scaler: {scaler_file}\")\n","\n","# Also save as JSON (human-readable)\n","scaler_json = proc_dir / \"standardization.json\"\n","with open(scaler_json, 'w') as f:\n","    json.dump(scaler_info, f, indent=2)\n","print(f\"✓ Saved scaler: {scaler_json}\")\n","\n","# ========== 7. Validate standardization ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"7. Validate standardization (global)\")\n","print(\"=\"*60)\n","\n","for ch in channels_to_normalize[:3]:  # check first 3 channels only\n","    if ch in scaler_params:\n","        valid_data = df_normalized[ch].dropna()\n","        if len(valid_data) > 0:\n","            mean_check = valid_data.mean()\n","            std_check = valid_data.std()\n","            print(f\"  {ch}: mean={mean_check:.6f}, std={std_check:.6f}\")\n","\n","# ========== 8. Summary ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 7 complete - Coordinate/Magnitude Normalization (global)\")\n","print(\"=\"*60)\n","print(f\"\\nConfig:\")\n","print(f\"  Method: z-score standardization (global)\")\n","print(f\"  ε (avoid divide-by-zero): {EPSILON}\")\n","print(f\"  Standardized channels: {len(scaler_params)}\")\n","print(f\"\\nResults:\")\n","print(f\"  Output data: {output_file}\")\n","print(f\"  Scaler (pkl): {scaler_file}\")\n","print(f\"  Scaler (json): {scaler_json}\")\n","print(f\"  Data shape: {df_normalized.shape}\")\n","print(f\"  New columns: acc_mag, gyr_mag\")\n","print(\"\\nRigor guarantees:\")\n","print(\"  1. ✓ Mean/std computed once on all data (global stats)\")\n","print(\"  2. ✓ NaNs remain unchanged\")\n","print(\"  3. ✓ ε={} prevents divide-by-zero\".format(EPSILON))\n","print(\"  4. ✓ Derived channels acc_mag, gyr_mag\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oa5y15Nik_Xa","executionInfo":{"status":"ok","timestamp":1763144588772,"user_tz":0,"elapsed":574,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"b0061b98-61ac-4195-98aa-602180bd9aed"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 7: Coordinate/Magnitude Normalization\n","============================================================\n","\n","Loading filtered data: data/lara/mbientlab/proc/filtered.parquet\n","Data shape: (560070, 13)\n","Number of subjects: 8\n","Number of sessions: 96\n","\n","============================================================\n","1. Compute derived channels (magnitude)\n","============================================================\n","\n","Computing acc_mag = sqrt(ax² + ay² + az²)...\n","Computing gyr_mag = sqrt(gx² + gy² + gz²)...\n","✓ Added derived channels: acc_mag, gyr_mag\n","\n","Derived channel statistics (post-filter):\n","  acc_mag:\n","    Mean: 2.9151\n","    Std: 2.3751\n","    Range: [0.0073, 14.5074]\n","  gyr_mag:\n","    Mean: 1.3122\n","    Std: 1.1772\n","    Range: [0.0015, 7.0648]\n","\n","============================================================\n","2. Determine training set (global)\n","============================================================\n","Compute statistics on all data (no per-fold split)\n","  Samples: 560,070\n","  Subjects: 8\n","\n","============================================================\n","3. Compute z-score parameters (global)\n","============================================================\n","\n","z-score parameters (global):\n","  ax:\n","    Mean: -0.008808\n","    Std: 2.274599\n","  ay:\n","    Mean: 0.006988\n","    Std: 2.146420\n","  az:\n","    Mean: 0.001384\n","    Std: 2.087749\n","  gx:\n","    Mean: -0.002287\n","    Std: 0.886048\n","  gy:\n","    Mean: 0.010105\n","    Std: 1.160388\n","  gz:\n","    Mean: 0.023344\n","    Std: 0.987726\n","  acc_mag:\n","    Mean: 2.915099\n","    Std: 2.375277\n","  gyr_mag:\n","    Mean: 1.312169\n","    Std: 1.177305\n","\n","============================================================\n","4. Apply z-score standardization\n","============================================================\n","✓ Standardized 8 channels\n","\n","Post-standardization stats (global):\n","  ax:\n","    Mean: -0.000000 (should be near 0)\n","    Std: 0.999755 (should be near 1)\n","  ay:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999796 (should be near 1)\n","  az:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999753 (should be near 1)\n","  gx:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999748 (should be near 1)\n","  gy:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999680 (should be near 1)\n","  gz:\n","    Mean: -0.000000 (should be near 0)\n","    Std: 0.999743 (should be near 1)\n","  acc_mag:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999905 (should be near 1)\n","  gyr_mag:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999909 (should be near 1)\n","\n","============================================================\n","5. Save results\n","============================================================\n","Removed old data: data/lara/mbientlab/proc/normalized.parquet\n","✓ Saved: data/lara/mbientlab/proc/normalized.parquet\n","  Data shape: (560070, 15)\n","\n","Data preview:\n","  subject_id session_id        ax        ay        az        gx        gy        gz   acc_mag   gyr_mag  label\n","0        S07        R03  0.395624 -1.242715  0.113459  0.031125  0.366984 -0.184342 -0.041821 -0.719891    6.0\n","1        S07        R03  0.318244 -1.326593  0.114031  0.069269  0.394669 -0.245474  0.009993 -0.672706    6.0\n","2        S07        R03  0.466002 -1.672217  0.641951  0.050763  0.215964 -0.367815  0.442886 -0.748857    6.0\n","3        S07        R03  0.842548 -1.974861  1.018318  0.123879  0.078289 -0.436465  0.922482 -0.746250    6.0\n","4        S07        R03  1.049100 -2.118226  1.323202  0.299475  0.150572 -0.471352  1.223958 -0.650154    6.0\n","5        S07        R03  0.944730 -2.207665  1.371639  0.536319  0.288670 -0.555932  1.269686 -0.446192    6.0\n","6        S07        R03  0.672562 -2.261497  1.500974  0.777181  0.373010 -0.644765  1.285975 -0.246819    6.0\n","7        S07        R03  0.613387 -2.311484  1.579912  0.939153  0.324421 -0.701175  1.345937 -0.151343    6.0\n","8        S07        R03  0.639085 -2.495260  1.670510  1.101326  0.156542 -0.762265  1.529302 -0.068443    6.0\n","9        S07        R03  0.614563 -2.944599  1.719538  1.373805  0.128417 -0.824382  1.886048  0.124236    6.0\n","\n","Post-standardization numeric column stats (overall):\n","                ax           ay           az           gx           gy  \\\n","count  560070.0000  560070.0000  560070.0000  560070.0000  560070.0000   \n","mean       -0.0000       0.0000       0.0000       0.0000       0.0000   \n","std         0.9998       0.9998       0.9998       0.9997       0.9997   \n","min        -3.9819      -3.7231      -3.8163      -3.8172      -4.1142   \n","25%        -0.3961      -0.4292      -0.4354      -0.3904      -0.3387   \n","50%         0.0106       0.0033       0.0002       0.0050      -0.0142   \n","75%         0.4063       0.4390       0.4324       0.4057       0.3163   \n","max         4.0030       3.7298       3.8168       3.8272       4.0858   \n","\n","                gz      acc_mag      gyr_mag  \n","count  560070.0000  560070.0000  560070.0000  \n","mean       -0.0000       0.0000       0.0000  \n","std         0.9997       0.9999       0.9999  \n","min        -4.0216      -1.2242      -1.1133  \n","25%        -0.3598      -0.7161      -0.7281  \n","50%        -0.0147      -0.2953      -0.3247  \n","75%         0.3648       0.4121       0.4094  \n","max         3.9921       4.8804       4.8863  \n","\n","============================================================\n","6. Save scaler parameters\n","============================================================\n","✓ Saved scaler: data/lara/mbientlab/proc/standardization.pkl\n","✓ Saved scaler: data/lara/mbientlab/proc/standardization.json\n","\n","============================================================\n","7. Validate standardization (global)\n","============================================================\n","  ax: mean=-0.000000, std=0.999755\n","  ay: mean=0.000000, std=0.999796\n","  az: mean=0.000000, std=0.999753\n","\n","============================================================\n","Step 7 complete - Coordinate/Magnitude Normalization (global)\n","============================================================\n","\n","Config:\n","  Method: z-score standardization (global)\n","  ε (avoid divide-by-zero): 1e-08\n","  Standardized channels: 8\n","\n","Results:\n","  Output data: data/lara/mbientlab/proc/normalized.parquet\n","  Scaler (pkl): data/lara/mbientlab/proc/standardization.pkl\n","  Scaler (json): data/lara/mbientlab/proc/standardization.json\n","  Data shape: (560070, 15)\n","  New columns: acc_mag, gyr_mag\n","\n","Rigor guarantees:\n","  1. ✓ Mean/std computed once on all data (global stats)\n","  2. ✓ NaNs remain unchanged\n","  3. ✓ ε=1e-08 prevents divide-by-zero\n","  4. ✓ Derived channels acc_mag, gyr_mag\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\n","\"\"\"\n","Step 8: Label Alignment & Cleaning (top-conf/journal grade - revised)\n","Clean NULL/transition, unify to a standard label set, and record mappings\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import json\n","from collections import Counter\n","\n","# ========== Config ==========\n","\n","# Label cleaning strategy\n","NULL_STRATEGY = \"remove\"  # \"remove\" or \"merge_to_transition\"\n","TRANSITION_STRATEGY = \"merge_to_nearest\"  # \"remove\" or \"merge_to_nearest\"\n","\n","# Unmapped label threshold (abort if exceeded)\n","UNMAPPED_THRESHOLD = 0.01  # 1%\n","\n","print(\"=\"*60)\n","print(\"Step 8: Label Alignment & Cleaning\")\n","print(\"=\"*60)\n","\n","# Create directories\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","reports_dir = Path(\"reports\")\n","reports_dir.mkdir(parents=True, exist_ok=True)\n","\n","print(f\"\\nLoading normalized data: {proc_dir / 'normalized.parquet'}\")\n","df = pd.read_parquet(proc_dir / \"normalized.parquet\")\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Number of subjects: {df['subject_id'].nunique()}\")\n","\n","# ========== 1. Analyze original label distribution ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Analyze original label distribution\")\n","print(\"=\"*60)\n","\n","# Count all labels\n","label_counts = df['label'].value_counts(dropna=False)\n","total_samples = len(df)\n","null_count = df['label'].isna().sum()\n","\n","print(f\"\\nOriginal label stats:\")\n","print(f\"  Total samples: {total_samples:,}\")\n","print(f\"  NULL samples: {null_count:,} ({null_count/total_samples*100:.2f}%)\")\n","print(f\"  Number of label classes: {df['label'].nunique(dropna=True)}\")\n","\n","print(f\"\\nLabel distribution (top 20):\")\n","for label, count in label_counts.head(20).items():\n","    pct = count / total_samples * 100\n","    print(f\"  {str(label):30s}: {count:8,} ({pct:5.2f}%)\")\n","\n","# ========== 2. Define label mapping rules ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Define label mapping rules\")\n","print(\"=\"*60)\n","\n","# Map LARa dataset labels to a cross-dataset unified label superset\n","# Covers LARa / RealWorld / SHL\n","LABEL_MAPPING = {\n","    # Basic activities (shared by RealWorld + LARa)\n","    1: {\"original\": \"walking\", \"mapped\": \"walking\", \"category\": \"locomotion\"},\n","    2: {\"original\": \"running\", \"mapped\": \"running\", \"category\": \"locomotion\"},\n","    3: {\"original\": \"shuffling\", \"mapped\": \"walking\", \"category\": \"locomotion\"},  # merge into walking\n","    4: {\"original\": \"stairs (ascending)\", \"mapped\": \"upstairs\", \"category\": \"locomotion\"},\n","    5: {\"original\": \"stairs (descending)\", \"mapped\": \"downstairs\", \"category\": \"locomotion\"},\n","    6: {\"original\": \"standing\", \"mapped\": \"standing\", \"category\": \"static\"},\n","    7: {\"original\": \"sitting\", \"mapped\": \"sitting\", \"category\": \"static\"},\n","    8: {\"original\": \"lying\", \"mapped\": \"lying\", \"category\": \"static\"},\n","\n","    # Transport (specific to LARa; not in RealWorld)\n","    13: {\"original\": \"cycling (sit)\", \"mapped\": \"cycling\", \"category\": \"transport\"},\n","    14: {\"original\": \"cycling (stand)\", \"mapped\": \"cycling\", \"category\": \"transport\"},\n","    130: {\"original\": \"cycling\", \"mapped\": \"cycling\", \"category\": \"transport\"},\n","\n","    17: {\"original\": \"car\", \"mapped\": \"car\", \"category\": \"transport\"},\n","    18: {\"original\": \"bus\", \"mapped\": \"bus\", \"category\": \"transport\"},\n","    19: {\"original\": \"train\", \"mapped\": \"train\", \"category\": \"transport\"},\n","    20: {\"original\": \"subway\", \"mapped\": \"subway\", \"category\": \"transport\"},\n","\n","    # Transition label\n","    0: {\"original\": \"transition\", \"mapped\": \"transition\", \"category\": \"transition\"},\n","}\n","\n","# Cross-dataset unified label superset (LARa + RealWorld + SHL)\n","UNIFIED_LABELS = {\n","    \"walking\": 1,\n","    \"running\": 2,\n","    \"sitting\": 3,\n","    \"standing\": 4,\n","    \"upstairs\": 5,\n","    \"downstairs\": 6,\n","    \"lying\": 7,\n","    \"cycling\": 8,\n","    \"car\": 9,\n","    \"bus\": 10,\n","    \"train\": 11,\n","    \"subway\": 12,\n","    \"transition\": 0,  # kept or cleaned\n","}\n","\n","print(f\"\\nDefined mapping rules: {len(LABEL_MAPPING)} original labels\")\n","print(f\"Unified label set: {len(UNIFIED_LABELS)} labels (cross-dataset superset)\")\n","\n","print(f\"\\nMapping examples:\")\n","for orig_id, info in list(LABEL_MAPPING.items())[:10]:\n","    print(f\"  {orig_id} ({info['original']}) -> {info['mapped']}\")\n","\n","# ========== 3. Audit assertion: check unmapped labels ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Audit assertion: check unmapped labels\")\n","print(\"=\"*60)\n","\n","# Find all original label IDs (excluding NULL)\n","orig_ids = set(df['label'].dropna().astype(int).unique())\n","covered_ids = set(LABEL_MAPPING.keys())\n","unmapped_ids = sorted(orig_ids - covered_ids)\n","\n","if unmapped_ids:\n","    # Count samples for unmapped labels\n","    unmapped_counts = []\n","    for uid in unmapped_ids:\n","        count = (df['label'] == uid).sum()\n","        pct = count / total_samples\n","        unmapped_counts.append({\n","            'original_label_id': uid,\n","            'sample_count': count,\n","            'percentage': round(pct * 100, 4),\n","        })\n","\n","    df_unmapped = pd.DataFrame(unmapped_counts)\n","    total_unmapped = df_unmapped['sample_count'].sum()\n","    unmapped_ratio = total_unmapped / total_samples\n","\n","    # Save list of unmapped labels\n","    unmapped_file = reports_dir / \"unmapped_labels.csv\"\n","    df_unmapped.to_csv(unmapped_file, index=False)\n","\n","    print(f\"\\n⚠️ Found unmapped labels: {len(unmapped_ids)}\")\n","    print(f\"  Unmapped sample count: {total_unmapped:,} ({unmapped_ratio*100:.2f}%)\")\n","    print(f\"  Details saved to: {unmapped_file}\")\n","    print(f\"\\nList of unmapped labels:\")\n","    print(df_unmapped.to_string(index=False))\n","\n","    # Abort if threshold exceeded\n","    if unmapped_ratio > UNMAPPED_THRESHOLD:\n","        raise RuntimeError(\n","            f\"Unmapped label ratio {unmapped_ratio*100:.2f}% exceeds threshold {UNMAPPED_THRESHOLD*100}%. \"\n","            f\"Please check {unmapped_file} and extend LABEL_MAPPING.\"\n","        )\n","    else:\n","        print(f\"\\n✓ Unmapped label ratio does not exceed threshold {UNMAPPED_THRESHOLD*100}%; continuing (will mark as NULL)\")\n","else:\n","    print(f\"\\n✓ All original labels are covered\")\n","\n","# ========== 4. Apply label mapping ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Apply label mapping\")\n","print(\"=\"*60)\n","\n","df_mapped = df.copy()\n","\n","# Keep a copy of original labels (nullable integer)\n","df_mapped['label_original'] = df_mapped['label'].astype('Int32')\n","\n","# Apply mapping\n","def map_label(label):\n","    \"\"\"Map a single label\"\"\"\n","    if pd.isna(label):\n","        return np.nan\n","\n","    label = int(label)\n","    if label in LABEL_MAPPING:\n","        mapped_name = LABEL_MAPPING[label]['mapped']\n","        return UNIFIED_LABELS[mapped_name]\n","    else:\n","        # Unknown labels marked as NaN\n","        return np.nan\n","\n","df_mapped['label'] = df_mapped['label_original'].apply(map_label)\n","\n","# Stats after mapping\n","mapped_label_counts = df_mapped['label'].value_counts(dropna=False)\n","null_after_mapping = df_mapped['label'].isna().sum()\n","\n","print(f\"\\nPost-mapping label stats:\")\n","print(f\"  NULL samples: {null_after_mapping:,} ({null_after_mapping/total_samples*100:.2f}%)\")\n","print(f\"  Number of valid label classes: {df_mapped['label'].nunique(dropna=True)}\")\n","\n","print(f\"\\nPost-mapping distribution:\")\n","for label, count in mapped_label_counts.head(15).items():\n","    pct = count / total_samples * 100\n","    # find label name\n","    label_name = \"NULL\"\n","    if not pd.isna(label):\n","        label_name = [k for k, v in UNIFIED_LABELS.items() if v == int(label)][0]\n","    print(f\"  {label_name:15s} ({str(label):2s}): {count:8,} ({pct:5.2f}%)\")\n","\n","# ========== 5. Clean NULL and transition labels (true nearest neighbor) ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Clean NULL and transition labels (true nearest neighbor)\")\n","print(\"=\"*60)\n","\n","df_cleaned = df_mapped.copy()\n","\n","# Handle NULL labels\n","if NULL_STRATEGY == \"remove\":\n","    null_mask = df_cleaned['label'].isna()\n","    removed_null = null_mask.sum()\n","    df_cleaned = df_cleaned[~null_mask].copy()\n","    print(f\"\\nNULL handling: removed {removed_null:,} samples\")\n","elif NULL_STRATEGY == \"merge_to_transition\":\n","    null_mask = df_cleaned['label'].isna()\n","    df_cleaned.loc[null_mask, 'label'] = UNIFIED_LABELS['transition']\n","    print(f\"\\nNULL handling: merged into transition ({null_mask.sum():,} samples)\")\n","\n","# Detect time column\n","time_col = None\n","for candidate in ['time_sec', 'timestamp', 'timestamp_ms', 'time', 'epoch_ms']:\n","    if candidate in df_cleaned.columns:\n","        time_col = candidate\n","        break\n","\n","if time_col:\n","    print(f\"\\nDetected time column: {time_col}\")\n","else:\n","    print(f\"\\nNo time column detected; will process by index order\")\n","\n","# Handle transition label (true nearest neighbor)\n","transition_value = UNIFIED_LABELS['transition']\n","if TRANSITION_STRATEGY == \"remove\":\n","    trans_mask = df_cleaned['label'] == transition_value\n","    removed_trans = trans_mask.sum()\n","    df_cleaned = df_cleaned[~trans_mask].copy()\n","    print(f\"Transition handling: removed {removed_trans:,} samples\")\n","\n","elif TRANSITION_STRATEGY == \"merge_to_nearest\":\n","    trans_mask = df_cleaned['label'] == transition_value\n","    trans_count = trans_mask.sum()\n","\n","    if trans_count > 0:\n","        print(f\"Transition handling: merge {trans_count:,} samples using nearest-neighbor interpolation\")\n","\n","        # Sort by time (ensure nearest-neighbor semantics)\n","        if time_col:\n","            df_cleaned = df_cleaned.sort_values(\n","                ['subject_id', 'session_id', 'placement', time_col],\n","                kind='stable'\n","            ).copy()\n","            print(f\"  ✓ Sorted by [{time_col}]\")\n","        else:\n","            df_cleaned = df_cleaned.sort_index(kind='stable').copy()\n","            print(f\"  ⚠️ Sorted by index (no time column)\")\n","\n","        # True nearest-neighbor merge\n","        merged_count = 0\n","        for (subj, sess, plc), group in df_cleaned.groupby(\n","            ['subject_id', 'session_id', 'placement'], observed=True\n","        ):\n","            idx = group.index\n","            labels = df_cleaned.loc[idx, 'label'].copy()\n","\n","            # Replace transition with NaN\n","            labels_with_nan = labels.replace(transition_value, np.nan).astype('float')\n","\n","            if labels_with_nan.isna().any():\n","                # Use nearest interpolation (true nearest neighbor)\n","                labels_filled = labels_with_nan.interpolate(\n","                    method='nearest',\n","                    limit_direction='both'\n","                )\n","\n","                # Count successfully merged items\n","                was_trans = (labels == transition_value)\n","                now_filled = labels_filled.notna()\n","                merged_this_group = (was_trans & now_filled).sum()\n","                merged_count += merged_this_group\n","\n","                # Update labels (round then cast to int)\n","                df_cleaned.loc[idx, 'label'] = labels_filled.round()\n","\n","        print(f\"  ✓ Successfully merged {merged_count:,} transition samples to nearest labels\")\n","\n","        # Remove transitions that could not be merged (entire segments are transition)\n","        remaining_trans = (df_cleaned['label'] == transition_value).sum()\n","        if remaining_trans > 0:\n","            df_cleaned = df_cleaned[df_cleaned['label'] != transition_value].copy()\n","            print(f\"  ✓ Removed remaining {remaining_trans:,} transition samples that could not be merged\")\n","\n","# Remove remaining NaNs\n","final_nan = df_cleaned['label'].isna().sum()\n","if final_nan > 0:\n","    df_cleaned = df_cleaned[df_cleaned['label'].notna()].copy()\n","    print(f\"\\nRemoved final residual NaN samples: {final_nan:,}\")\n","\n","# Cast to int32\n","df_cleaned['label'] = df_cleaned['label'].astype('int32')\n","\n","# Reset index\n","df_cleaned = df_cleaned.reset_index(drop=True)\n","\n","print(f\"\\nData after cleaning:\")\n","print(f\"  Samples: {len(df_cleaned):,}\")\n","print(f\"  Number of label classes: {df_cleaned['label'].nunique()}\")\n","print(f\"  Retention rate: {len(df_cleaned)/total_samples*100:.2f}%\")\n","\n","# ========== 6. Audit assertion: verify final label set ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Audit assertion: verify final label set\")\n","print(\"=\"*60)\n","\n","# Determine allowed label set\n","allowed_labels = set(UNIFIED_LABELS.values())\n","if TRANSITION_STRATEGY == \"remove\":\n","    allowed_labels.discard(UNIFIED_LABELS['transition'])\n","\n","# Check actual label set\n","actual_labels = set(df_cleaned['label'].unique())\n","unexpected = sorted(actual_labels - allowed_labels)\n","\n","if unexpected:\n","    raise RuntimeError(\n","        f\"Illegal labels found after cleaning: {unexpected}\\n\"\n","        f\"Allowed labels: {sorted(allowed_labels)}\"\n","    )\n","else:\n","    print(f\"✓ Final label set validation passed\")\n","    print(f\"  Allowed labels: {sorted(allowed_labels)}\")\n","    print(f\"  Actual labels: {sorted(actual_labels)}\")\n","\n","# ========== 7. Final label distribution ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"7. Final label distribution\")\n","print(\"=\"*60)\n","\n","final_label_counts = df_cleaned['label'].value_counts()\n","\n","print(f\"\\nFinal label distribution:\")\n","for label_id, count in final_label_counts.items():\n","    pct = count / len(df_cleaned) * 100\n","    label_name = [k for k, v in UNIFIED_LABELS.items() if v == int(label_id)][0]\n","    print(f\"  {label_name:15s} ({int(label_id):2d}): {count:8,} ({pct:5.2f}%)\")\n","\n","# By-category statistics\n","category_stats = {}\n","for label_id, count in final_label_counts.items():\n","    label_name = [k for k, v in UNIFIED_LABELS.items() if v == int(label_id)][0]\n","    # Find category\n","    category = None\n","    for orig_id, info in LABEL_MAPPING.items():\n","        if info['mapped'] == label_name:\n","            category = info['category']\n","            break\n","\n","    if category:\n","        category_stats[category] = category_stats.get(category, 0) + count\n","\n","print(f\"\\nBy-category statistics:\")\n","for category, count in sorted(category_stats.items()):\n","    pct = count / len(df_cleaned) * 100\n","    print(f\"  {category:15s}: {count:8,} ({pct:5.2f}%)\")\n","\n","# ========== 8. Save results ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"8. Save results\")\n","print(\"=\"*60)\n","\n","# Save cleaned data (using directory layout)\n","output_dir = proc_dir / \"labeled\"\n","if output_dir.exists():\n","    import shutil\n","    shutil.rmtree(output_dir)\n","\n","df_cleaned.to_parquet(\n","    output_dir,\n","    index=False,\n","    partition_cols=['subject_id', 'placement'],\n","    engine='pyarrow'\n",")\n","\n","print(f\"✓ Saved: {output_dir}/\")\n","print(f\"  Data shape: {df_cleaned.shape}\")\n","print(f\"  Partitions: subject_id / placement\")\n","\n","# ========== 9. Save label mapping config (rich) ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"9. Save label mapping config (rich)\")\n","print(\"=\"*60)\n","\n","# Build labels_map with more info\n","labels_map_data = []\n","for label_name, label_id in sorted(UNIFIED_LABELS.items(), key=lambda x: x[1]):\n","    if label_name == \"transition\" and TRANSITION_STRATEGY == \"remove\":\n","        continue  # exclude removed transition\n","\n","    # Find original label IDs and names\n","    original_ids = []\n","    original_names = []\n","    category = None\n","\n","    for orig_id, info in LABEL_MAPPING.items():\n","        if info['mapped'] == label_name:\n","            original_ids.append(str(orig_id))\n","            original_names.append(info['original'])\n","            if category is None:\n","                category = info['category']\n","\n","    # Actual sample count\n","    sample_count = final_label_counts.get(label_id, 0)\n","\n","    labels_map_data.append({\n","        'label_id': label_id,\n","        'label_name': label_name,\n","        'category': category or 'unknown',\n","        'sample_count': int(sample_count),\n","        'percentage': round(sample_count / len(df_cleaned) * 100, 2) if len(df_cleaned) > 0 else 0.0,\n","        'original_label_ids': ','.join(original_ids) if original_ids else '',\n","        'original_label_names': '; '.join(original_names) if original_names else '',\n","        'source_dataset': 'LARa-MbientLab',\n","        'description': f\"{label_name} activity\",\n","    })\n","\n","df_labels_map = pd.DataFrame(labels_map_data)\n","labels_map_file = proc_dir / \"labels_map.csv\"\n","df_labels_map.to_csv(labels_map_file, index=False)\n","\n","print(f\"✓ Saved label mapping: {labels_map_file}\")\n","print(f\"\\nLabel mapping table:\")\n","print(df_labels_map.to_string(index=False))\n","\n","# Save detailed configuration\n","label_config = {\n","    'dataset': 'LARa-MbientLab',\n","    'label_system': 'Cross-dataset unified label superset (covers LARa/RealWorld/SHL)',\n","    'unified_labels': UNIFIED_LABELS,\n","    'label_mapping': LABEL_MAPPING,\n","    'cleaning_strategy': {\n","        'null_strategy': NULL_STRATEGY,\n","        'transition_strategy': TRANSITION_STRATEGY,\n","        'transition_method': 'nearest-neighbor interpolation (true nearest neighbor)' if TRANSITION_STRATEGY == 'merge_to_nearest' else 'remove',\n","        'time_sorted': time_col is not None,\n","        'time_column': time_col,\n","        'unmapped_threshold': UNMAPPED_THRESHOLD,\n","    },\n","    'statistics': {\n","        'original_samples': int(total_samples),\n","        'cleaned_samples': int(len(df_cleaned)),\n","        'removed_samples': int(total_samples - len(df_cleaned)),\n","        'removal_rate': float((total_samples - len(df_cleaned)) / total_samples),\n","        'original_label_count': int(df['label'].nunique(dropna=True)),\n","        'final_label_count': int(df_cleaned['label'].nunique()),\n","        'unmapped_label_count': len(unmapped_ids) if unmapped_ids else 0,\n","    },\n","    'label_distribution': {\n","        label_name: int(final_label_counts.get(label_id, 0))\n","        for label_name, label_id in UNIFIED_LABELS.items()\n","        if label_name != 'transition' or TRANSITION_STRATEGY != 'remove'\n","    },\n","    'notes': [\n","        'Label mapping based on cross-dataset unified label superset (LARa + RealWorld + SHL)',\n","        f'NULL label strategy: {NULL_STRATEGY}',\n","        f'Transition label strategy: {TRANSITION_STRATEGY} (true nearest-neighbor interpolation)',\n","        'Unmapped original labels are automatically marked as NULL',\n","        f'Unmapped label threshold: {UNMAPPED_THRESHOLD*100}%',\n","        f'Sorted by time column: {time_col if time_col else \"No (by index)\"}',\n","        'Mapping table saved at proc/labels_map.csv',\n","        'label_original column uses nullable integer Int32',\n","        'Includes audit assertions to ensure label set integrity',\n","    ]\n","}\n","\n","label_config_file = configs_dir / \"labels.yaml\"\n","with open(label_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(label_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","print(f\"✓ Saved config: {label_config_file}\")\n","\n","label_config_json = configs_dir / \"labels.json\"\n","with open(label_config_json, 'w', encoding='utf-8') as f:\n","    json.dump(label_config, f, indent=2)\n","\n","print(f\"✓ Saved config: {label_config_json}\")\n","\n","# ========== 10. Summary ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 8 complete - Label alignment & cleaning (top-tier revised)\")\n","print(\"=\"*60)\n","\n","print(f\"\\nConfig:\")\n","print(f\"  Label system: cross-dataset unified superset (LARa/RealWorld/SHL)\")\n","print(f\"  NULL strategy: {NULL_STRATEGY}\")\n","print(f\"  Transition strategy: {TRANSITION_STRATEGY} (true nearest neighbor)\")\n","print(f\"  Unmapped threshold: {UNMAPPED_THRESHOLD*100}%\")\n","print(f\"  Time column: {time_col if time_col else 'No (by index)'}\")\n","\n","print(f\"\\nResults:\")\n","print(f\"  Original samples: {total_samples:,}\")\n","print(f\"  Cleaned samples: {len(df_cleaned):,}\")\n","print(f\"  Removed samples: {total_samples - len(df_cleaned):,}\")\n","print(f\"  Retention rate: {len(df_cleaned)/total_samples*100:.2f}%\")\n","\n","print(f\"\\nLabel stats:\")\n","print(f\"  Original label classes: {df['label'].nunique(dropna=True)}\")\n","print(f\"  Final label classes: {df_cleaned['label'].nunique()}\")\n","print(f\"  Unmapped labels: {len(unmapped_ids) if unmapped_ids else 0}\")\n","\n","print(f\"\\nOutputs:\")\n","print(f\"  Data: {output_dir}/\")\n","print(f\"  Mapping table: {labels_map_file}\")\n","print(f\"  Config: {label_config_file}\")\n","if unmapped_ids:\n","    print(f\"  Unmapped list: {reports_dir / 'unmapped_labels.csv'}\")\n","\n","print(\"\\nKey fixes (top-tier):\")\n","print(\"  1. ✓ True nearest-neighbor merge (interpolate method='nearest')\")\n","print(\"  2. ✓ Sort by time before processing (correct semantics)\")\n","print(\"  3. ✓ Record unmapped labels to reports/unmapped_labels.csv\")\n","print(\"  4. ✓ label_original uses nullable Int32\")\n","print(\"  5. ✓ Removed irrelevant MAJORITY_VOTE_THRESHOLD\")\n","print(\"  6. ✓ Audit assertions (fail-fast)\")\n","print(\"  7. ✓ labels_map.csv includes original names and source\")\n","print(\"  8. ✓ Label system described as cross-dataset superset\")\n","print(\"  9. ✓ Output directory changed to labeled/\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IF8QO8aLlA0O","executionInfo":{"status":"ok","timestamp":1763144590258,"user_tz":0,"elapsed":1464,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"55b24767-e0dd-4fbe-d25b-d61480efee1c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 8: Label Alignment & Cleaning\n","============================================================\n","\n","Loading normalized data: data/lara/mbientlab/proc/normalized.parquet\n","Data shape: (560070, 15)\n","Number of subjects: 8\n","\n","============================================================\n","1. Analyze original label distribution\n","============================================================\n","\n","Original label stats:\n","  Total samples: 560,070\n","  NULL samples: 28 (0.00%)\n","  Number of label classes: 8\n","\n","Label distribution (top 20):\n","  4.0                           :  215,988 (38.56%)\n","  2.0                           :   86,132 (15.38%)\n","  0.0                           :   73,180 (13.07%)\n","  7.0                           :   48,927 ( 8.74%)\n","  5.0                           :   43,966 ( 7.85%)\n","  1.0                           :   42,039 ( 7.51%)\n","  3.0                           :   38,224 ( 6.82%)\n","  6.0                           :   11,586 ( 2.07%)\n","  nan                           :       28 ( 0.00%)\n","\n","============================================================\n","2. Define label mapping rules\n","============================================================\n","\n","Defined mapping rules: 16 original labels\n","Unified label set: 13 labels (cross-dataset superset)\n","\n","Mapping examples:\n","  1 (walking) -> walking\n","  2 (running) -> running\n","  3 (shuffling) -> walking\n","  4 (stairs (ascending)) -> upstairs\n","  5 (stairs (descending)) -> downstairs\n","  6 (standing) -> standing\n","  7 (sitting) -> sitting\n","  8 (lying) -> lying\n","  13 (cycling (sit)) -> cycling\n","  14 (cycling (stand)) -> cycling\n","\n","============================================================\n","3. Audit assertion: check unmapped labels\n","============================================================\n","\n","✓ All original labels are covered\n","\n","============================================================\n","4. Apply label mapping\n","============================================================\n","\n","Post-mapping label stats:\n","  NULL samples: 28 (0.00%)\n","  Number of valid label classes: 7\n","\n","Post-mapping distribution:\n","  upstairs        (5.0):  215,988 (38.56%)\n","  running         (2.0):   86,132 (15.38%)\n","  walking         (1.0):   80,263 (14.33%)\n","  transition      (0.0):   73,180 (13.07%)\n","  sitting         (3.0):   48,927 ( 8.74%)\n","  downstairs      (6.0):   43,966 ( 7.85%)\n","  standing        (4.0):   11,586 ( 2.07%)\n","  NULL            (nan):       28 ( 0.00%)\n","\n","============================================================\n","5. Clean NULL and transition labels (true nearest neighbor)\n","============================================================\n","\n","NULL handling: removed 28 samples\n","\n","Detected time column: time_sec\n","Transition handling: merge 73,180 samples using nearest-neighbor interpolation\n","  ✓ Sorted by [time_sec]\n","  ✓ Successfully merged 69,642 transition samples to nearest labels\n","\n","Removed final residual NaN samples: 3,538\n","\n","Data after cleaning:\n","  Samples: 556,504\n","  Number of label classes: 6\n","  Retention rate: 99.36%\n","\n","============================================================\n","6. Audit assertion: verify final label set\n","============================================================\n","✓ Final label set validation passed\n","  Allowed labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n","  Actual labels: [np.int32(1), np.int32(2), np.int32(3), np.int32(4), np.int32(5), np.int32(6)]\n","\n","============================================================\n","7. Final label distribution\n","============================================================\n","\n","Final label distribution:\n","  upstairs        ( 5):  255,038 (45.83%)\n","  walking         ( 1):   96,957 (17.42%)\n","  running         ( 2):   89,302 (16.05%)\n","  sitting         ( 3):   51,931 ( 9.33%)\n","  downstairs      ( 6):   50,391 ( 9.05%)\n","  standing        ( 4):   12,885 ( 2.32%)\n","\n","By-category statistics:\n","  locomotion     :  491,688 (88.35%)\n","  static         :   64,816 (11.65%)\n","\n","============================================================\n","8. Save results\n","============================================================\n","✓ Saved: data/lara/mbientlab/proc/labeled/\n","  Data shape: (556504, 16)\n","  Partitions: subject_id / placement\n","\n","============================================================\n","9. Save label mapping config (rich)\n","============================================================\n","✓ Saved label mapping: data/lara/mbientlab/proc/labels_map.csv\n","\n","Label mapping table:\n"," label_id label_name   category  sample_count  percentage original_label_ids                    original_label_names source_dataset         description\n","        0 transition transition             0        0.00                  0                              transition LARa-MbientLab transition activity\n","        1    walking locomotion         96957       17.42                1,3                      walking; shuffling LARa-MbientLab    walking activity\n","        2    running locomotion         89302       16.05                  2                                 running LARa-MbientLab    running activity\n","        3    sitting     static         51931        9.33                  7                                 sitting LARa-MbientLab    sitting activity\n","        4   standing     static         12885        2.32                  6                                standing LARa-MbientLab   standing activity\n","        5   upstairs locomotion        255038       45.83                  4                      stairs (ascending) LARa-MbientLab   upstairs activity\n","        6 downstairs locomotion         50391        9.05                  5                     stairs (descending) LARa-MbientLab downstairs activity\n","        7      lying     static             0        0.00                  8                                   lying LARa-MbientLab      lying activity\n","        8    cycling  transport             0        0.00          13,14,130 cycling (sit); cycling (stand); cycling LARa-MbientLab    cycling activity\n","        9        car  transport             0        0.00                 17                                     car LARa-MbientLab        car activity\n","       10        bus  transport             0        0.00                 18                                     bus LARa-MbientLab        bus activity\n","       11      train  transport             0        0.00                 19                                   train LARa-MbientLab      train activity\n","       12     subway  transport             0        0.00                 20                                  subway LARa-MbientLab     subway activity\n","✓ Saved config: configs/labels.yaml\n","✓ Saved config: configs/labels.json\n","\n","============================================================\n","Step 8 complete - Label alignment & cleaning (top-tier revised)\n","============================================================\n","\n","Config:\n","  Label system: cross-dataset unified superset (LARa/RealWorld/SHL)\n","  NULL strategy: remove\n","  Transition strategy: merge_to_nearest (true nearest neighbor)\n","  Unmapped threshold: 1.0%\n","  Time column: time_sec\n","\n","Results:\n","  Original samples: 560,070\n","  Cleaned samples: 556,504\n","  Removed samples: 3,566\n","  Retention rate: 99.36%\n","\n","Label stats:\n","  Original label classes: 8\n","  Final label classes: 6\n","  Unmapped labels: 0\n","\n","Outputs:\n","  Data: data/lara/mbientlab/proc/labeled/\n","  Mapping table: data/lara/mbientlab/proc/labels_map.csv\n","  Config: configs/labels.yaml\n","\n","Key fixes (top-tier):\n","  1. ✓ True nearest-neighbor merge (interpolate method='nearest')\n","  2. ✓ Sort by time before processing (correct semantics)\n","  3. ✓ Record unmapped labels to reports/unmapped_labels.csv\n","  4. ✓ label_original uses nullable Int32\n","  5. ✓ Removed irrelevant MAJORITY_VOTE_THRESHOLD\n","  6. ✓ Audit assertions (fail-fast)\n","  7. ✓ labels_map.csv includes original names and source\n","  8. ✓ Label system described as cross-dataset superset\n","  9. ✓ Output directory changed to labeled/\n","============================================================\n"]}]},{"cell_type":"code","source":["import os\n","\n","\"\"\"\n","Step 9: Sliding-window Slicing (top-conf/journal grade - multi-fold version)\n","Slice with fixed window length/step; assign window label by majority label\n","For each fold in configs/splits.json, generate windows/{fold_xx}/X_train.npy, X_test.npy, etc.\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import json\n","from collections import Counter\n","\n","# ========== Config ==========\n","\n","# Sliding-window parameters\n","SAMPLING_RATE_HZ = 50.0\n","WINDOW_SIZE_SEC = 3.0\n","OVERLAP_RATIO = 0.5\n","\n","# Compute sample counts\n","WINDOW_SIZE = int(WINDOW_SIZE_SEC * SAMPLING_RATE_HZ)        # 150 samples\n","STEP_SIZE = int(WINDOW_SIZE * (1 - OVERLAP_RATIO))           # 75 samples\n","\n","# Majority label threshold\n","DOMINANT_THRESHOLD = 0.8\n","\n","# Feature columns (8 channels)\n","FEATURE_COLS = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","print(\"=\"*60)\n","print(\"Step 9: Sliding-window slicing (multi-fold)\")\n","print(\"=\"*60)\n","\n","# Base directories\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","windows_root = proc_dir / \"windows\"\n","windows_root.mkdir(parents=True, exist_ok=True)\n","\n","print(f\"\\nSliding-window parameters:\")\n","print(f\"  Window length: {WINDOW_SIZE_SEC} s = {WINDOW_SIZE} samples @ {SAMPLING_RATE_HZ} Hz\")\n","print(f\"  Step size: {STEP_SIZE} samples (overlap {OVERLAP_RATIO*100:.0f}%)\")\n","print(f\"  Dominant label threshold: {DOMINANT_THRESHOLD*100:.0f}%\")\n","print(f\"  Feature columns: {FEATURE_COLS}\")\n","\n","# ========== 1. Load data ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Load cleaned & labeled data\")\n","print(\"=\"*60)\n","\n","labeled_dir = proc_dir / \"labeled\"\n","print(f\"Loading data from: {labeled_dir}/\")\n","df = pd.read_parquet(labeled_dir)\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Number of subjects: {df['subject_id'].nunique()}\")\n","print(f\"Number of label classes: {df['label'].nunique()}\")\n","\n","# Check required columns\n","required_cols = ['subject_id', 'session_id', 'placement', 'label'] + FEATURE_COLS\n","missing_cols = [c for c in required_cols if c not in df.columns]\n","if missing_cols:\n","    raise ValueError(f\"Missing required columns: {missing_cols}\")\n","\n","# Detect time column\n","time_col = 'time_sec' if 'time_sec' in df.columns else None\n","if time_col:\n","    print(f\"Time column: {time_col}\")\n","else:\n","    print(\"No time column detected; will sort by index\")\n","\n","# ========== 2. Load splits and determine folds ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Load train/test splits (folds)\")\n","print(\"=\"*60)\n","\n","splits_path = configs_dir / \"splits.json\"\n","fold_ids = []\n","splits = None\n","\n","if splits_path.exists():\n","    with open(splits_path, \"r\") as f:\n","        splits = json.load(f)\n","\n","    # Expect keys like \"0\",\"1\",\"2\",...\n","    fold_ids = sorted(int(k) for k in splits.keys())\n","    print(f\"Detected {len(fold_ids)} folds from {splits_path}: {fold_ids}\")\n","else:\n","    # Fallback: single \"all\" fold (no LOSO config)\n","    print(\"⚠️ splits.json not found; will treat all data as a single 'all' fold\")\n","    fold_ids = [None]\n","\n","# ========== 3. Sliding-window function (with time continuity check) ==========\n","\n","def sliding_window_extract(df_subset, window_size, step_size, dominant_threshold, time_col=None):\n","    \"\"\"\n","    Perform sliding-window slicing grouped by session.\n","\n","    Returns:\n","        windows_list: list of window feature arrays\n","        metadata_list: list of window metadata dicts\n","    \"\"\"\n","    windows_list = []\n","    metadata_list = []\n","    window_id = 0\n","\n","    # Group by session + placement\n","    for (subj, sess, plc), group in df_subset.groupby(\n","        ['subject_id', 'session_id', 'placement'], observed=True\n","    ):\n","        # Sort by time column (preferred), otherwise by index\n","        if time_col and time_col in group.columns:\n","            group = group.sort_values(time_col, kind='stable').copy()\n","        else:\n","            group = group.sort_index(kind='stable').copy()\n","\n","        # Extract features and labels\n","        features = group[FEATURE_COLS].values\n","        labels = group['label'].values\n","\n","        # Extract timestamps (if any)\n","        if time_col and time_col in group.columns:\n","            timestamps = group[time_col].values\n","        else:\n","            timestamps = None\n","\n","        # Sliding-window slicing\n","        n_samples = len(group)\n","        for start_idx in range(0, n_samples - window_size + 1, step_size):\n","            end_idx = start_idx + window_size\n","\n","            # Extract window\n","            window_features = features[start_idx:end_idx]\n","            window_labels = labels[start_idx:end_idx]\n","\n","            # Check NaNs\n","            if np.isnan(window_features).any():\n","                continue\n","\n","            # Time continuity check (if timestamps exist)\n","            if timestamps is not None:\n","                expected_duration = (window_size - 1) / SAMPLING_RATE_HZ\n","                actual_duration = timestamps[end_idx - 1] - timestamps[start_idx]\n","                # Allow 10% jitter\n","                if abs(actual_duration - expected_duration) > 0.1 * expected_duration:\n","                    continue\n","\n","            # Compute dominant label\n","            label_counts = Counter(window_labels)\n","            dominant_label, dominant_count = label_counts.most_common(1)[0]\n","            dominant_ratio = dominant_count / window_size\n","\n","            # Keep only windows that meet the threshold\n","            if dominant_ratio < dominant_threshold:\n","                continue\n","\n","            # Extract time range\n","            if timestamps is not None:\n","                time_start = timestamps[start_idx]\n","                time_end = timestamps[end_idx - 1]\n","                time_range = f\"{time_start:.3f}-{time_end:.3f}\"\n","            else:\n","                time_range = f\"{start_idx}-{end_idx-1}\"\n","\n","            # Save window\n","            windows_list.append(window_features)\n","\n","            # Save metadata\n","            metadata_list.append({\n","                'window_id': window_id,\n","                'subject_id': subj,\n","                'session_id': sess,\n","                'placement': plc,\n","                'label': int(dominant_label),\n","                'label_purity': round(dominant_ratio, 4),\n","                'time_range': time_range,\n","                'start_idx': start_idx,\n","                'end_idx': end_idx,\n","            })\n","\n","            window_id += 1\n","\n","    return windows_list, metadata_list\n","\n","# ========== 4. Loop over folds and extract windows ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Extract windows for each fold\")\n","print(\"=\"*60)\n","\n","fold_stats = {}  # for global config (per-fold statistics)\n","\n","for fold_id in fold_ids:\n","    if fold_id is None:\n","        fold_tag = \"all\"\n","        print(f\"\\n--- Processing pseudo-fold: {fold_tag} (all data) ---\")\n","\n","        train_subjects = set(df['subject_id'].unique())\n","        test_subjects = set()\n","        df_train = df.copy()\n","        df_test = pd.DataFrame()\n","    else:\n","        fold_tag = f\"fold_{fold_id:02d}\"\n","        print(f\"\\n--- Processing fold {fold_id} ({fold_tag}) ---\")\n","\n","        fold_cfg = splits[str(fold_id)]\n","        train_subjects = set(fold_cfg[\"train_subjects\"])\n","        test_subjects = set(fold_cfg[\"test_subjects\"])\n","\n","        # Split data\n","        df_train = df[df['subject_id'].isin(train_subjects)].copy()\n","        df_test = df[df['subject_id'].isin(test_subjects)].copy()\n","\n","        print(f\"  Train subjects: {len(train_subjects)}\")\n","        print(f\"  Test subjects:  {len(test_subjects)}\")\n","        print(f\"  Train samples:  {len(df_train):,}\")\n","        print(f\"  Test samples:   {len(df_test):,}\")\n","\n","    # Create output directory for this fold\n","    windows_dir = windows_root / fold_tag\n","    windows_dir.mkdir(parents=True, exist_ok=True)\n","    print(f\"  Output directory: {windows_dir}\")\n","\n","    # ----- 4.1 Extract training-set windows -----\n","    print(\"\\n  [Train] Sliding-window extraction\")\n","\n","    train_windows = []\n","    df_train_meta = pd.DataFrame()\n","    train_label_counts = pd.Series(dtype=int)\n","\n","    if not df_train.empty:\n","        print(f\"  Processing train set ({len(df_train):,} samples)...\")\n","\n","        train_windows, train_metadata = sliding_window_extract(\n","            df_train, WINDOW_SIZE, STEP_SIZE, DOMINANT_THRESHOLD, time_col\n","        )\n","\n","        print(f\"  ✓ Extracted train windows: {len(train_windows):,}\")\n","\n","        if train_windows:\n","            # To numpy array\n","            X_train = np.array(train_windows, dtype='float32')  # (n_windows, window_size, n_features)\n","            df_train_meta = pd.DataFrame(train_metadata)\n","\n","            print(f\"    X_train shape: {X_train.shape}\")\n","            print(f\"    Feature dims : {X_train.shape[2]} channels × {X_train.shape[1]} timesteps\")\n","\n","            # Label distribution\n","            train_label_counts = df_train_meta['label'].value_counts().sort_index()\n","            print(f\"\\n    Train-set label distribution:\")\n","            for label, count in train_label_counts.items():\n","                pct = count / len(df_train_meta) * 100\n","                print(f\"      Label {label}: {count:6,} windows ({pct:5.2f}%)\")\n","\n","            # Label purity stats\n","            avg_purity = df_train_meta['label_purity'].mean()\n","            min_purity = df_train_meta['label_purity'].min()\n","            print(f\"\\n    Train-set label purity:\")\n","            print(f\"      Mean: {avg_purity*100:.2f}%\")\n","            print(f\"      Min:  {min_purity*100:.2f}%\")\n","\n","            # Save train set\n","            print(f\"\\n    Saving train set...\")\n","\n","            # Save features (numpy)\n","            X_train_npy_file = windows_dir / \"X_train.npy\"\n","            np.save(X_train_npy_file, X_train)\n","            print(f\"      ✓ {X_train_npy_file} (feature tensor)\")\n","\n","            # Save metadata (Parquet)\n","            X_train_meta_file = windows_dir / \"X_train.parquet\"\n","            df_train_meta[['window_id', 'subject_id', 'session_id', 'placement',\n","                           'label', 'label_purity', 'time_range', 'start_idx', 'end_idx']].to_parquet(\n","                X_train_meta_file, index=False\n","            )\n","            print(f\"      ✓ {X_train_meta_file} (metadata)\")\n","\n","            # Save label vector\n","            y_train = df_train_meta['label'].values.astype('int32')\n","            y_train_file = windows_dir / \"y_train.npy\"\n","            np.save(y_train_file, y_train)\n","            print(f\"      ✓ {y_train_file}\")\n","\n","            # Export label distribution snapshot (for audit)\n","            train_label_counts.to_csv(windows_dir / \"train_label_counts.csv\", header=['count'])\n","            print(f\"      ✓ train_label_counts.csv\")\n","        else:\n","            print(\"  ⚠️ No train windows extracted\")\n","    else:\n","        print(\"  Train set is empty; skipping\")\n","\n","    # ----- 4.2 Extract test-set windows -----\n","    print(\"\\n  [Test] Sliding-window extraction\")\n","\n","    test_windows = []\n","    df_test_meta = pd.DataFrame()\n","    test_label_counts = pd.Series(dtype=int)\n","\n","    if not df_test.empty:\n","        print(f\"  Processing test set ({len(df_test):,} samples)...\")\n","\n","        test_windows, test_metadata = sliding_window_extract(\n","            df_test, WINDOW_SIZE, STEP_SIZE, DOMINANT_THRESHOLD, time_col\n","        )\n","\n","        print(f\"  ✓ Extracted test windows: {len(test_windows):,}\")\n","\n","        if test_windows:\n","            # To numpy array\n","            X_test = np.array(test_windows, dtype='float32')\n","            df_test_meta = pd.DataFrame(test_metadata)\n","\n","            print(f\"    X_test shape: {X_test.shape}\")\n","\n","            # Label distribution\n","            test_label_counts = df_test_meta['label'].value_counts().sort_index()\n","            print(f\"\\n    Test-set label distribution:\")\n","            for label, count in test_label_counts.items():\n","                pct = count / len(df_test_meta) * 100\n","                print(f\"      Label {label}: {count:6,} windows ({pct:5.2f}%)\")\n","\n","            # Label purity stats\n","            avg_purity = df_test_meta['label_purity'].mean()\n","            min_purity = df_test_meta['label_purity'].min()\n","            print(f\"\\n    Test-set label purity:\")\n","            print(f\"      Mean: {avg_purity*100:.2f}%\")\n","            print(f\"      Min:  {min_purity*100:.2f}%\")\n","\n","            # Save test set\n","            print(f\"\\n    Saving test set...\")\n","\n","            # Save features (numpy)\n","            X_test_npy_file = windows_dir / \"X_test.npy\"\n","            np.save(X_test_npy_file, X_test)\n","            print(f\"      ✓ {X_test_npy_file} (feature tensor)\")\n","\n","            # Save metadata (Parquet)\n","            X_test_meta_file = windows_dir / \"X_test.parquet\"\n","            df_test_meta[['window_id', 'subject_id', 'session_id', 'placement',\n","                          'label', 'label_purity', 'time_range', 'start_idx', 'end_idx']].to_parquet(\n","                X_test_meta_file, index=False\n","            )\n","            print(f\"      ✓ {X_test_meta_file} (metadata)\")\n","\n","            # Save label vector\n","            y_test = df_test_meta['label'].values.astype('int32')\n","            y_test_file = windows_dir / \"y_test.npy\"\n","            np.save(y_test_file, y_test)\n","            print(f\"      ✓ {y_test_file}\")\n","\n","            # Export label distribution snapshot (for audit)\n","            test_label_counts.to_csv(windows_dir / \"test_label_counts.csv\", header=['count'])\n","            print(f\"      ✓ test_label_counts.csv\")\n","        else:\n","            print(\"  ⚠️ No test windows extracted\")\n","    else:\n","        print(\"  Test set is empty; skipping\")\n","\n","    # ----- 4.3 Collect statistics for this fold -----\n","    fold_key = \"all\" if fold_id is None else str(fold_id)\n","    fold_stats[fold_key] = {}\n","\n","    if train_windows:\n","        fold_stats[fold_key]['train'] = {\n","            'n_windows': int(len(train_windows)),\n","            'n_subjects': int(df_train_meta['subject_id'].nunique()),\n","            'n_sessions': int(df_train_meta.groupby(['subject_id', 'session_id']).ngroups),\n","            'label_distribution': {int(k): int(v) for k, v in train_label_counts.items()},\n","            'avg_label_purity': round(float(df_train_meta['label_purity'].mean()), 4),\n","            'min_label_purity': round(float(df_train_meta['label_purity'].min()), 4),\n","        }\n","\n","    if test_windows:\n","        fold_stats[fold_key]['test'] = {\n","            'n_windows': int(len(test_windows)),\n","            'n_subjects': int(df_test_meta['subject_id'].nunique()),\n","            'n_sessions': int(df_test_meta.groupby(['subject_id', 'session_id']).ngroups),\n","            'label_distribution': {int(k): int(v) for k, v in test_label_counts.items()},\n","            'avg_label_purity': round(float(df_test_meta['label_purity'].mean()), 4),\n","            'min_label_purity': round(float(df_test_meta['label_purity'].min()), 4),\n","        }\n","\n","# ========== 5. Save window configuration (global, multi-fold) ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Save window configuration (global)\")\n","print(\"=\"*60)\n","\n","fold_ids_str = [\"all\" if fid is None else str(fid) for fid in fold_ids]\n","\n","window_config = {\n","    'window_parameters': {\n","        'sampling_rate_hz': SAMPLING_RATE_HZ,\n","        'window_size_sec': WINDOW_SIZE_SEC,\n","        'window_size_samples': WINDOW_SIZE,\n","        'overlap_ratio': OVERLAP_RATIO,\n","        'step_size_samples': STEP_SIZE,\n","        'dominant_threshold': DOMINANT_THRESHOLD,\n","    },\n","    'features': {\n","        'channels': FEATURE_COLS,\n","        'n_channels': len(FEATURE_COLS),\n","        'description': '8-channel IMU features (ax,ay,az,gx,gy,gz,acc_mag,gyr_mag)',\n","    },\n","    'dataset_split': {\n","        'num_folds': len(fold_ids),\n","        'fold_ids': fold_ids_str,\n","        'source': str(splits_path) if splits_path.exists() else None,\n","    },\n","    'statistics': fold_stats,\n","    'notes': [\n","        f'Window parameters: {WINDOW_SIZE_SEC}s @ {SAMPLING_RATE_HZ}Hz = {WINDOW_SIZE} samples',\n","        f'Step size: {STEP_SIZE} samples (overlap {OVERLAP_RATIO*100:.0f}%)',\n","        f'Dominant label threshold: {DOMINANT_THRESHOLD*100:.0f}% (discard windows below threshold)',\n","        'Features: 8 channels (3-axis accelerometer + 3-axis gyroscope + 2 magnitudes)',\n","        'Data formats: X_*.npy (float32 tensor), X_*.parquet (metadata), y_*.npy (int32)',\n","        'Metadata includes: window_id/time_range/label/label_purity, etc.',\n","        'Slice per session to ensure temporal continuity',\n","        f'Order by {time_col if time_col else \"index\"}',\n","        'Discard windows containing NaN',\n","        'Time continuity check (allow 10% jitter)',\n","        'Persist by fold: windows/fold_xx/ (avoid overwrite when looping over folds)',\n","    ]\n","}\n","\n","window_config_file = configs_dir / \"windows.yaml\"\n","with open(window_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(window_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","print(f\"✓ Saved config: {window_config_file}\")\n","\n","window_config_json = configs_dir / \"windows.json\"\n","with open(window_config_json, 'w', encoding='utf-8') as f:\n","    json.dump(window_config, f, indent=2)\n","print(f\"✓ Saved config: {window_config_json}\")\n","\n","# ========== 6. Summary ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 9 complete - Sliding-window slicing (multi-fold)\")\n","print(\"=\"*60)\n","\n","print(f\"\\nWindow parameters:\")\n","print(f\"  Window length: {WINDOW_SIZE_SEC} s = {WINDOW_SIZE} samples\")\n","print(f\"  Step size: {STEP_SIZE} samples (overlap {OVERLAP_RATIO*100:.0f}%)\")\n","print(f\"  Dominant threshold: {DOMINANT_THRESHOLD*100:.0f}%\")\n","print(f\"  Feature dimension: {len(FEATURE_COLS)} channels\")\n","print(f\"  Sort order: {time_col if time_col else 'index'}\")\n","\n","print(\"\\nPer-fold window statistics:\")\n","for fold_key, stats in fold_stats.items():\n","    print(f\"\\n  Fold {fold_key}:\")\n","    if 'train' in stats:\n","        tr = stats['train']\n","        print(f\"    Train: {tr['n_windows']} windows, \"\n","              f\"{tr['n_subjects']} subjects, {tr['n_sessions']} sessions, \"\n","              f\"avg purity {tr['avg_label_purity']*100:.2f}%\")\n","    else:\n","        print(\"    Train: (no windows)\")\n","    if 'test' in stats:\n","        te = stats['test']\n","        print(f\"    Test : {te['n_windows']} windows, \"\n","              f\"{te['n_subjects']} subjects, {te['n_sessions']} sessions, \"\n","              f\"avg purity {te['avg_label_purity']*100:.2f}%\")\n","    else:\n","        print(\"    Test : (no windows)\")\n","\n","print(f\"\\nOutputs per fold:\")\n","print(f\"  Root directory: {windows_root}/\")\n","print(f\"  For each fold: X_train.npy, X_train.parquet, y_train.npy, \"\n","      f\"train_label_counts.csv (+ test equivalents when applicable)\")\n","print(f\"  Global config: {window_config_file}, {window_config_json}\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MFtWgDUzlCJq","executionInfo":{"status":"ok","timestamp":1763144595067,"user_tz":0,"elapsed":4710,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"2b572285-2356-4ebd-b8f0-4693d9356d95"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 9: Sliding-window slicing (multi-fold)\n","============================================================\n","\n","Sliding-window parameters:\n","  Window length: 3.0 s = 150 samples @ 50.0 Hz\n","  Step size: 75 samples (overlap 50%)\n","  Dominant label threshold: 80%\n","  Feature columns: ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","============================================================\n","1. Load cleaned & labeled data\n","============================================================\n","Loading data from: data/lara/mbientlab/proc/labeled/\n","Data shape: (556504, 16)\n","Number of subjects: 8\n","Number of label classes: 6\n","Time column: time_sec\n","\n","============================================================\n","2. Load train/test splits (folds)\n","============================================================\n","Detected 8 folds from configs/splits.json: [0, 1, 2, 3, 4, 5, 6, 7]\n","\n","============================================================\n","3. Extract windows for each fold\n","============================================================\n","\n","--- Processing fold 0 (fold_00) ---\n","  Train subjects: 7\n","  Test subjects:  1\n","  Train samples:  479,982\n","  Test samples:   76,522\n","  Output directory: data/lara/mbientlab/proc/windows/fold_00\n","\n","  [Train] Sliding-window extraction\n","  Processing train set (479,982 samples)...\n","  ✓ Extracted train windows: 4,965\n","    X_train shape: (4965, 150, 8)\n","    Feature dims : 8 channels × 150 timesteps\n","\n","    Train-set label distribution:\n","      Label 1:    812 windows (16.35%)\n","      Label 2:    849 windows (17.10%)\n","      Label 3:    554 windows (11.16%)\n","      Label 4:     47 windows ( 0.95%)\n","      Label 5:  2,359 windows (47.51%)\n","      Label 6:    344 windows ( 6.93%)\n","\n","    Train-set label purity:\n","      Mean: 98.47%\n","      Min:  80.00%\n","\n","    Saving train set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_00/X_train.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_00/X_train.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_00/y_train.npy\n","      ✓ train_label_counts.csv\n","\n","  [Test] Sliding-window extraction\n","  Processing test set (76,522 samples)...\n","  ✓ Extracted test windows: 766\n","    X_test shape: (766, 150, 8)\n","\n","    Test-set label distribution:\n","      Label 1:     92 windows (12.01%)\n","      Label 2:    145 windows (18.93%)\n","      Label 3:     49 windows ( 6.40%)\n","      Label 4:     19 windows ( 2.48%)\n","      Label 5:    346 windows (45.17%)\n","      Label 6:    115 windows (15.01%)\n","\n","    Test-set label purity:\n","      Mean: 98.44%\n","      Min:  80.00%\n","\n","    Saving test set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_00/X_test.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_00/X_test.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_00/y_test.npy\n","      ✓ test_label_counts.csv\n","\n","--- Processing fold 1 (fold_01) ---\n","  Train subjects: 7\n","  Test subjects:  1\n","  Train samples:  491,647\n","  Test samples:   64,857\n","  Output directory: data/lara/mbientlab/proc/windows/fold_01\n","\n","  [Train] Sliding-window extraction\n","  Processing train set (491,647 samples)...\n","  ✓ Extracted train windows: 5,072\n","    X_train shape: (5072, 150, 8)\n","    Feature dims : 8 channels × 150 timesteps\n","\n","    Train-set label distribution:\n","      Label 1:    794 windows (15.65%)\n","      Label 2:    891 windows (17.57%)\n","      Label 3:    517 windows (10.19%)\n","      Label 4:     65 windows ( 1.28%)\n","      Label 5:  2,378 windows (46.88%)\n","      Label 6:    427 windows ( 8.42%)\n","\n","    Train-set label purity:\n","      Mean: 98.46%\n","      Min:  80.00%\n","\n","    Saving train set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_01/X_train.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_01/X_train.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_01/y_train.npy\n","      ✓ train_label_counts.csv\n","\n","  [Test] Sliding-window extraction\n","  Processing test set (64,857 samples)...\n","  ✓ Extracted test windows: 659\n","    X_test shape: (659, 150, 8)\n","\n","    Test-set label distribution:\n","      Label 1:    110 windows (16.69%)\n","      Label 2:    103 windows (15.63%)\n","      Label 3:     86 windows (13.05%)\n","      Label 4:      1 windows ( 0.15%)\n","      Label 5:    327 windows (49.62%)\n","      Label 6:     32 windows ( 4.86%)\n","\n","    Test-set label purity:\n","      Mean: 98.53%\n","      Min:  80.00%\n","\n","    Saving test set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_01/X_test.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_01/X_test.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_01/y_test.npy\n","      ✓ test_label_counts.csv\n","\n","--- Processing fold 2 (fold_02) ---\n","  Train subjects: 7\n","  Test subjects:  1\n","  Train samples:  478,803\n","  Test samples:   77,701\n","  Output directory: data/lara/mbientlab/proc/windows/fold_02\n","\n","  [Train] Sliding-window extraction\n","  Processing train set (478,803 samples)...\n","  ✓ Extracted train windows: 4,960\n","    X_train shape: (4960, 150, 8)\n","    Feature dims : 8 channels × 150 timesteps\n","\n","    Train-set label distribution:\n","      Label 1:    770 windows (15.52%)\n","      Label 2:    921 windows (18.57%)\n","      Label 3:    547 windows (11.03%)\n","      Label 4:     60 windows ( 1.21%)\n","      Label 5:  2,229 windows (44.94%)\n","      Label 6:    433 windows ( 8.73%)\n","\n","    Train-set label purity:\n","      Mean: 98.49%\n","      Min:  80.00%\n","\n","    Saving train set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_02/X_train.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_02/X_train.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_02/y_train.npy\n","      ✓ train_label_counts.csv\n","\n","  [Test] Sliding-window extraction\n","  Processing test set (77,701 samples)...\n","  ✓ Extracted test windows: 771\n","    X_test shape: (771, 150, 8)\n","\n","    Test-set label distribution:\n","      Label 1:    134 windows (17.38%)\n","      Label 2:     73 windows ( 9.47%)\n","      Label 3:     56 windows ( 7.26%)\n","      Label 4:      6 windows ( 0.78%)\n","      Label 5:    476 windows (61.74%)\n","      Label 6:     26 windows ( 3.37%)\n","\n","    Test-set label purity:\n","      Mean: 98.37%\n","      Min:  80.00%\n","\n","    Saving test set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_02/X_test.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_02/X_test.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_02/y_test.npy\n","      ✓ test_label_counts.csv\n","\n","--- Processing fold 3 (fold_03) ---\n","  Train subjects: 7\n","  Test subjects:  1\n","  Train samples:  473,845\n","  Test samples:   82,659\n","  Output directory: data/lara/mbientlab/proc/windows/fold_03\n","\n","  [Train] Sliding-window extraction\n","  Processing train set (473,845 samples)...\n","  ✓ Extracted train windows: 4,858\n","    X_train shape: (4858, 150, 8)\n","    Feature dims : 8 channels × 150 timesteps\n","\n","    Train-set label distribution:\n","      Label 1:    791 windows (16.28%)\n","      Label 2:    852 windows (17.54%)\n","      Label 3:    439 windows ( 9.04%)\n","      Label 4:     55 windows ( 1.13%)\n","      Label 5:  2,327 windows (47.90%)\n","      Label 6:    394 windows ( 8.11%)\n","\n","    Train-set label purity:\n","      Mean: 98.47%\n","      Min:  80.00%\n","\n","    Saving train set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_03/X_train.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_03/X_train.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_03/y_train.npy\n","      ✓ train_label_counts.csv\n","\n","  [Test] Sliding-window extraction\n","  Processing test set (82,659 samples)...\n","  ✓ Extracted test windows: 873\n","    X_test shape: (873, 150, 8)\n","\n","    Test-set label distribution:\n","      Label 1:    113 windows (12.94%)\n","      Label 2:    142 windows (16.27%)\n","      Label 3:    164 windows (18.79%)\n","      Label 4:     11 windows ( 1.26%)\n","      Label 5:    378 windows (43.30%)\n","      Label 6:     65 windows ( 7.45%)\n","\n","    Test-set label purity:\n","      Mean: 98.47%\n","      Min:  80.00%\n","\n","    Saving test set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_03/X_test.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_03/X_test.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_03/y_test.npy\n","      ✓ test_label_counts.csv\n","\n","--- Processing fold 4 (fold_04) ---\n","  Train subjects: 7\n","  Test subjects:  1\n","  Train samples:  486,094\n","  Test samples:   70,410\n","  Output directory: data/lara/mbientlab/proc/windows/fold_04\n","\n","  [Train] Sliding-window extraction\n","  Processing train set (486,094 samples)...\n","  ✓ Extracted train windows: 4,985\n","    X_train shape: (4985, 150, 8)\n","    Feature dims : 8 channels × 150 timesteps\n","\n","    Train-set label distribution:\n","      Label 1:    767 windows (15.39%)\n","      Label 2:    829 windows (16.63%)\n","      Label 3:    547 windows (10.97%)\n","      Label 4:     53 windows ( 1.06%)\n","      Label 5:  2,387 windows (47.88%)\n","      Label 6:    402 windows ( 8.06%)\n","\n","    Train-set label purity:\n","      Mean: 98.47%\n","      Min:  80.00%\n","\n","    Saving train set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_04/X_train.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_04/X_train.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_04/y_train.npy\n","      ✓ train_label_counts.csv\n","\n","  [Test] Sliding-window extraction\n","  Processing test set (70,410 samples)...\n","  ✓ Extracted test windows: 746\n","    X_test shape: (746, 150, 8)\n","\n","    Test-set label distribution:\n","      Label 1:    137 windows (18.36%)\n","      Label 2:    165 windows (22.12%)\n","      Label 3:     56 windows ( 7.51%)\n","      Label 4:     13 windows ( 1.74%)\n","      Label 5:    318 windows (42.63%)\n","      Label 6:     57 windows ( 7.64%)\n","\n","    Test-set label purity:\n","      Mean: 98.48%\n","      Min:  80.00%\n","\n","    Saving test set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_04/X_test.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_04/X_test.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_04/y_test.npy\n","      ✓ test_label_counts.csv\n","\n","--- Processing fold 5 (fold_05) ---\n","  Train subjects: 7\n","  Test subjects:  1\n","  Train samples:  525,581\n","  Test samples:   30,923\n","  Output directory: data/lara/mbientlab/proc/windows/fold_05\n","\n","  [Train] Sliding-window extraction\n","  Processing train set (525,581 samples)...\n","  ✓ Extracted train windows: 5,442\n","    X_train shape: (5442, 150, 8)\n","    Feature dims : 8 channels × 150 timesteps\n","\n","    Train-set label distribution:\n","      Label 1:    824 windows (15.14%)\n","      Label 2:    945 windows (17.36%)\n","      Label 3:    578 windows (10.62%)\n","      Label 4:     57 windows ( 1.05%)\n","      Label 5:  2,599 windows (47.76%)\n","      Label 6:    439 windows ( 8.07%)\n","\n","    Train-set label purity:\n","      Mean: 98.49%\n","      Min:  80.00%\n","\n","    Saving train set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_05/X_train.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_05/X_train.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_05/y_train.npy\n","      ✓ train_label_counts.csv\n","\n","  [Test] Sliding-window extraction\n","  Processing test set (30,923 samples)...\n","  ✓ Extracted test windows: 289\n","    X_test shape: (289, 150, 8)\n","\n","    Test-set label distribution:\n","      Label 1:     80 windows (27.68%)\n","      Label 2:     49 windows (16.96%)\n","      Label 3:     25 windows ( 8.65%)\n","      Label 4:      9 windows ( 3.11%)\n","      Label 5:    106 windows (36.68%)\n","      Label 6:     20 windows ( 6.92%)\n","\n","    Test-set label purity:\n","      Mean: 98.00%\n","      Min:  80.00%\n","\n","    Saving test set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_05/X_test.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_05/X_test.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_05/y_test.npy\n","      ✓ test_label_counts.csv\n","\n","--- Processing fold 6 (fold_06) ---\n","  Train subjects: 7\n","  Test subjects:  1\n","  Train samples:  474,169\n","  Test samples:   82,335\n","  Output directory: data/lara/mbientlab/proc/windows/fold_06\n","\n","  [Train] Sliding-window extraction\n","  Processing train set (474,169 samples)...\n","  ✓ Extracted train windows: 4,853\n","    X_train shape: (4853, 150, 8)\n","    Feature dims : 8 channels × 150 timesteps\n","\n","    Train-set label distribution:\n","      Label 1:    785 windows (16.18%)\n","      Label 2:    789 windows (16.26%)\n","      Label 3:    507 windows (10.45%)\n","      Label 4:     64 windows ( 1.32%)\n","      Label 5:  2,291 windows (47.21%)\n","      Label 6:    417 windows ( 8.59%)\n","\n","    Train-set label purity:\n","      Mean: 98.43%\n","      Min:  80.00%\n","\n","    Saving train set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_06/X_train.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_06/X_train.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_06/y_train.npy\n","      ✓ train_label_counts.csv\n","\n","  [Test] Sliding-window extraction\n","  Processing test set (82,335 samples)...\n","  ✓ Extracted test windows: 878\n","    X_test shape: (878, 150, 8)\n","\n","    Test-set label distribution:\n","      Label 1:    119 windows (13.55%)\n","      Label 2:    205 windows (23.35%)\n","      Label 3:     96 windows (10.93%)\n","      Label 4:      2 windows ( 0.23%)\n","      Label 5:    414 windows (47.15%)\n","      Label 6:     42 windows ( 4.78%)\n","\n","    Test-set label purity:\n","      Mean: 98.71%\n","      Min:  80.00%\n","\n","    Saving test set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_06/X_test.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_06/X_test.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_06/y_test.npy\n","      ✓ test_label_counts.csv\n","\n","--- Processing fold 7 (fold_07) ---\n","  Train subjects: 7\n","  Test subjects:  1\n","  Train samples:  485,407\n","  Test samples:   71,097\n","  Output directory: data/lara/mbientlab/proc/windows/fold_07\n","\n","  [Train] Sliding-window extraction\n","  Processing train set (485,407 samples)...\n","  ✓ Extracted train windows: 4,982\n","    X_train shape: (4982, 150, 8)\n","    Feature dims : 8 channels × 150 timesteps\n","\n","    Train-set label distribution:\n","      Label 1:    785 windows (15.76%)\n","      Label 2:    882 windows (17.70%)\n","      Label 3:    532 windows (10.68%)\n","      Label 4:     61 windows ( 1.22%)\n","      Label 5:  2,365 windows (47.47%)\n","      Label 6:    357 windows ( 7.17%)\n","\n","    Train-set label purity:\n","      Mean: 98.48%\n","      Min:  80.00%\n","\n","    Saving train set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_07/X_train.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_07/X_train.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_07/y_train.npy\n","      ✓ train_label_counts.csv\n","\n","  [Test] Sliding-window extraction\n","  Processing test set (71,097 samples)...\n","  ✓ Extracted test windows: 749\n","    X_test shape: (749, 150, 8)\n","\n","    Test-set label distribution:\n","      Label 1:    119 windows (15.89%)\n","      Label 2:    112 windows (14.95%)\n","      Label 3:     71 windows ( 9.48%)\n","      Label 4:      5 windows ( 0.67%)\n","      Label 5:    340 windows (45.39%)\n","      Label 6:    102 windows (13.62%)\n","\n","    Test-set label purity:\n","      Mean: 98.43%\n","      Min:  80.00%\n","\n","    Saving test set...\n","      ✓ data/lara/mbientlab/proc/windows/fold_07/X_test.npy (feature tensor)\n","      ✓ data/lara/mbientlab/proc/windows/fold_07/X_test.parquet (metadata)\n","      ✓ data/lara/mbientlab/proc/windows/fold_07/y_test.npy\n","      ✓ test_label_counts.csv\n","\n","============================================================\n","4. Save window configuration (global)\n","============================================================\n","✓ Saved config: configs/windows.yaml\n","✓ Saved config: configs/windows.json\n","\n","============================================================\n","Step 9 complete - Sliding-window slicing (multi-fold)\n","============================================================\n","\n","Window parameters:\n","  Window length: 3.0 s = 150 samples\n","  Step size: 75 samples (overlap 50%)\n","  Dominant threshold: 80%\n","  Feature dimension: 8 channels\n","  Sort order: time_sec\n","\n","Per-fold window statistics:\n","\n","  Fold 0:\n","    Train: 4965 windows, 7 subjects, 83 sessions, avg purity 98.47%\n","    Test : 766 windows, 1 subjects, 13 sessions, avg purity 98.44%\n","\n","  Fold 1:\n","    Train: 5072 windows, 7 subjects, 85 sessions, avg purity 98.46%\n","    Test : 659 windows, 1 subjects, 11 sessions, avg purity 98.53%\n","\n","  Fold 2:\n","    Train: 4960 windows, 7 subjects, 82 sessions, avg purity 98.49%\n","    Test : 771 windows, 1 subjects, 14 sessions, avg purity 98.37%\n","\n","  Fold 3:\n","    Train: 4858 windows, 7 subjects, 82 sessions, avg purity 98.47%\n","    Test : 873 windows, 1 subjects, 14 sessions, avg purity 98.47%\n","\n","  Fold 4:\n","    Train: 4985 windows, 7 subjects, 84 sessions, avg purity 98.47%\n","    Test : 746 windows, 1 subjects, 12 sessions, avg purity 98.48%\n","\n","  Fold 5:\n","    Train: 5442 windows, 7 subjects, 90 sessions, avg purity 98.49%\n","    Test : 289 windows, 1 subjects, 6 sessions, avg purity 98.00%\n","\n","  Fold 6:\n","    Train: 4853 windows, 7 subjects, 82 sessions, avg purity 98.43%\n","    Test : 878 windows, 1 subjects, 14 sessions, avg purity 98.71%\n","\n","  Fold 7:\n","    Train: 4982 windows, 7 subjects, 84 sessions, avg purity 98.48%\n","    Test : 749 windows, 1 subjects, 12 sessions, avg purity 98.43%\n","\n","Outputs per fold:\n","  Root directory: data/lara/mbientlab/proc/windows/\n","  For each fold: X_train.npy, X_train.parquet, y_train.npy, train_label_counts.csv (+ test equivalents when applicable)\n","  Global config: configs/windows.yaml, configs/windows.json\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\n","\"\"\"\n","Step 10: LOSO Split (top-conf/journal grade)\n","Leave-One-Subject-Out: 1 subject for test per fold, the rest for training\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import json\n","import yaml\n","from collections import defaultdict\n","\n","print(\"=\"*60)\n","print(\"Step 10: LOSO split\")\n","print(\"=\"*60)\n","\n","# Path configuration\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","configs_dir.mkdir(parents=True, exist_ok=True)\n","\n","# ========== 1. Load data and get subject list ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Load data and get subject list\")\n","print(\"=\"*60)\n","\n","labeled_dir = proc_dir / \"labeled\"\n","print(f\"Loading data: {labeled_dir}/\")\n","\n","df = pd.read_parquet(labeled_dir)\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Total samples: {len(df):,}\")\n","\n","# Extract all subjects\n","all_subjects = sorted(df['subject_id'].unique().tolist())\n","n_subjects = len(all_subjects)\n","\n","print(f\"\\nSubject list:\")\n","print(f\"  Total: {n_subjects} subjects\")\n","print(f\"  IDs: {all_subjects}\")\n","\n","# Sample count per subject\n","subject_sample_counts = df['subject_id'].value_counts().sort_index()\n","print(f\"\\nSample count per subject:\")\n","for subj in all_subjects:\n","    count = subject_sample_counts.get(subj, 0)\n","    pct = count / len(df) * 100\n","    print(f\"  {subj}: {count:8,} samples ({pct:5.2f}%)\")\n","\n","# ========== 2. Generate LOSO split ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Generate LOSO split\")\n","print(\"=\"*60)\n","\n","print(f\"\\nLOSO strategy: Leave-One-Subject-Out\")\n","print(f\"  #folds = #subjects = {n_subjects}\")\n","print(f\"  Per fold: 1 subject for test, {n_subjects-1} subjects for train\")\n","\n","# Create split dict\n","splits = {}\n","\n","for fold_id, test_subject in enumerate(all_subjects):\n","    # Test set: current subject\n","    test_subjects = [test_subject]\n","\n","    # Train set: all other subjects\n","    train_subjects = [s for s in all_subjects if s != test_subject]\n","\n","    # Save split\n","    splits[str(fold_id)] = {\n","        \"fold_id\": fold_id,\n","        \"test_subject\": test_subject,\n","        \"test_subjects\": test_subjects,  # list for compatibility\n","        \"train_subjects\": train_subjects,\n","        \"n_train\": len(train_subjects),\n","        \"n_test\": len(test_subjects),\n","    }\n","\n","    print(f\"  Fold {fold_id}: test {test_subject}, train {len(train_subjects)} subjects\")\n","\n","print(f\"\\n✓ Generated {len(splits)} LOSO folds\")\n","\n","# ========== 3. Validate split integrity ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Validate split integrity\")\n","print(\"=\"*60)\n","\n","# Check 1: each subject appears exactly once in the test set\n","test_subject_appearances = defaultdict(int)\n","for fold_id, fold_info in splits.items():\n","    for subj in fold_info['test_subjects']:\n","        test_subject_appearances[subj] += 1\n","\n","print(f\"\\nCheck 1: times each subject appears as test\")\n","all_once = True\n","for subj in all_subjects:\n","    count = test_subject_appearances[subj]\n","    status = \"✓\" if count == 1 else \"✗\"\n","    print(f\"  {status} {subj}: {count} time(s)\")\n","    if count != 1:\n","        all_once = False\n","\n","if all_once:\n","    print(f\"  ✓ All subjects appear exactly once\")\n","else:\n","    raise RuntimeError(\"Split validation failed: subject test appearances not equal to 1\")\n","\n","# Check 2: train and test sets are disjoint\n","print(f\"\\nCheck 2: train and test sets are disjoint\")\n","all_disjoint = True\n","for fold_id, fold_info in splits.items():\n","    train_set = set(fold_info['train_subjects'])\n","    test_set = set(fold_info['test_subjects'])\n","    overlap = train_set & test_set\n","\n","    if overlap:\n","        print(f\"  ✗ Fold {fold_id}: overlap exists {overlap}\")\n","        all_disjoint = False\n","\n","if all_disjoint:\n","    print(f\"  ✓ Train/test sets are completely disjoint for all folds\")\n","else:\n","    raise RuntimeError(\"Split validation failed: train and test sets have overlap\")\n","\n","# Check 3: all subjects covered\n","print(f\"\\nCheck 3: all subjects covered\")\n","covered_subjects = set()\n","for fold_id, fold_info in splits.items():\n","    covered_subjects.update(fold_info['train_subjects'])\n","    covered_subjects.update(fold_info['test_subjects'])\n","\n","missing = set(all_subjects) - covered_subjects\n","extra = covered_subjects - set(all_subjects)\n","\n","if not missing and not extra:\n","    print(f\"  ✓ All subjects are covered; no missing or extra subjects\")\n","else:\n","    if missing:\n","        print(f\"  ✗ Missing subjects: {missing}\")\n","    if extra:\n","        print(f\"  ✗ Extra subjects: {extra}\")\n","    raise RuntimeError(\"Split validation failed: subject coverage incomplete\")\n","\n","# Check 4: sample count stats\n","print(f\"\\nCheck 4: per-fold sample counts\")\n","fold_sample_stats = []\n","for fold_id, fold_info in splits.items():\n","    train_subjects = fold_info['train_subjects']\n","    test_subjects = fold_info['test_subjects']\n","\n","    n_train_samples = df[df['subject_id'].isin(train_subjects)].shape[0]\n","    n_test_samples = df[df['subject_id'].isin(test_subjects)].shape[0]\n","\n","    fold_sample_stats.append({\n","        'fold_id': int(fold_id),\n","        'test_subject': fold_info['test_subject'],\n","        'n_train_samples': n_train_samples,\n","        'n_test_samples': n_test_samples,\n","        'train_ratio': round(n_train_samples / len(df), 4),\n","        'test_ratio': round(n_test_samples / len(df), 4),\n","    })\n","\n","df_fold_stats = pd.DataFrame(fold_sample_stats)\n","\n","print(f\"\\nPer-fold sample distribution:\")\n","print(df_fold_stats.to_string(index=False))\n","\n","# Summary\n","print(f\"\\nSample distribution summary:\")\n","print(f\"  Train sample count: {df_fold_stats['n_train_samples'].min():,} ~ {df_fold_stats['n_train_samples'].max():,}\")\n","print(f\"  Test sample count: {df_fold_stats['n_test_samples'].min():,} ~ {df_fold_stats['n_test_samples'].max():,}\")\n","print(f\"  Average train ratio: {df_fold_stats['train_ratio'].mean()*100:.2f}%\")\n","print(f\"  Average test ratio: {df_fold_stats['test_ratio'].mean()*100:.2f}%\")\n","\n","print(f\"\\n✓ All validations passed\")\n","\n","# ========== 4. Save split configuration ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Save split configuration\")\n","print(\"=\"*60)\n","\n","# Save splits.json\n","splits_file = configs_dir / \"splits.json\"\n","with open(splits_file, 'w', encoding='utf-8') as f:\n","    json.dump(splits, f, indent=2)\n","\n","print(f\"✓ Saved: {splits_file}\")\n","\n","# Save detailed config (with metadata)\n","loso_config = {\n","    'strategy': 'LOSO (Leave-One-Subject-Out)',\n","    'description': 'One subject for test in each fold; remaining subjects for training',\n","    'n_folds': n_subjects,\n","    'n_subjects': n_subjects,\n","    'all_subjects': all_subjects,\n","    'fold_statistics': {\n","        'train_samples_min': int(df_fold_stats['n_train_samples'].min()),\n","        'train_samples_max': int(df_fold_stats['n_train_samples'].max()),\n","        'train_samples_mean': int(df_fold_stats['n_train_samples'].mean()),\n","        'test_samples_min': int(df_fold_stats['n_test_samples'].min()),\n","        'test_samples_max': int(df_fold_stats['n_test_samples'].max()),\n","        'test_samples_mean': int(df_fold_stats['n_test_samples'].mean()),\n","        'avg_train_ratio': round(float(df_fold_stats['train_ratio'].mean()), 4),\n","        'avg_test_ratio': round(float(df_fold_stats['test_ratio'].mean()), 4),\n","    },\n","    'validation': {\n","        'no_subject_overlap': True,\n","        'all_subjects_covered': True,\n","        'each_subject_tested_once': True,\n","    },\n","    'anti_leakage_principles': [\n","        'Train and test sets are completely separated by subject',\n","        'Window slicing is performed after splitting to ensure no cross-fold leakage',\n","        'Statistics (mean/std) are computed from the training fold only',\n","        'Feature engineering is performed independently within each fold',\n","        'Hyperparameter tuning uses training-fold data only (nested CV optional)',\n","        'Final model evaluation is strictly based on the corresponding fold’s test set',\n","        'When aggregating results across folds, use metrics from independent test sets',\n","    ],\n","    'notes': [\n","        f'LOSO split: {n_subjects} folds; 1 subject per fold for test',\n","        'Ensure each subject appears exactly once in the test set',\n","        'Train/test sets are mutually exclusive with no subject overlap',\n","        'Suitable for small-sample settings with large inter-subject variability',\n","        'Report mean and standard deviation across all folds',\n","    ]\n","}\n","\n","loso_config_file = configs_dir / \"loso.yaml\"\n","with open(loso_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(loso_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","print(f\"✓ Saved: {loso_config_file}\")\n","\n","loso_config_json = configs_dir / \"loso.json\"\n","with open(loso_config_json, 'w', encoding='utf-8') as f:\n","    json.dump(loso_config, f, indent=2)\n","\n","print(f\"✓ Saved: {loso_config_json}\")\n","\n","# Save per-fold sample stats\n","fold_stats_file = configs_dir / \"loso_fold_stats.csv\"\n","df_fold_stats.to_csv(fold_stats_file, index=False)\n","print(f\"✓ Saved: {fold_stats_file}\")\n","\n","# ========== 5. Generate usage example ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Generate usage example\")\n","print(\"=\"*60)\n","\n","example_code = '''\n","# ========== LOSO Usage Example ==========\n","\n","import json\n","from pathlib import Path\n","\n","# 1. Load splits\n","with open(\"configs/splits.json\", \"r\") as f:\n","    splits = json.load(f)\n","\n","# 2. Iterate over folds\n","for fold_id in range(len(splits)):\n","    print(f\"\\\\n========== Fold {fold_id} ==========\")\n","\n","    # Get current fold split\n","    fold = splits[str(fold_id)]\n","    train_subjects = fold[\"train_subjects\"]\n","    test_subject = fold[\"test_subject\"]\n","\n","    print(f\"Train: {len(train_subjects)} subjects\")\n","    print(f\"Test: {test_subject}\")\n","\n","    # 3. Set environment variable (used by later steps)\n","    import os\n","    os.environ[\"FOLD_ID\"] = str(fold_id)\n","\n","    # 4. Run training pipeline\n","    # - Step 6: per-fold clipping (statistics from train only)\n","    # - Step 7: per-fold standardization (statistics from train only)\n","    # - Step 9: per-fold windowing\n","    # - Train model (training windows only)\n","    # - Evaluate model (test windows only)\n","\n","    # 5. Save results of current fold\n","    # results[fold_id] = {\"accuracy\": acc, \"f1\": f1, ...}\n","\n","# 6. Aggregate results across folds\n","# mean_acc = np.mean([r[\"accuracy\"] for r in results.values()])\n","# std_acc = np.std([r[\"accuracy\"] for r in results.values()])\n","# print(f\"Mean accuracy: {mean_acc:.4f} ± {std_acc:.4f}\")\n","\n","# ========== Anti-leakage Checklist ==========\n","# ✓ Train/test separated by subject\n","# ✓ Statistics (mean/std) computed from training set only\n","# ✓ Feature scaling uses parameters from training set\n","# ✓ Windowing performed after splitting\n","# ✓ Hyperparameter tuning uses training data only\n","# ✓ Test set used strictly for final evaluation\n","'''\n","\n","example_file = configs_dir / \"loso_usage_example.py\"\n","with open(example_file, 'w', encoding='utf-8') as f:\n","    f.write(example_code)\n","\n","print(f\"✓ Generated usage example: {example_file}\")\n","\n","print(\"\\nHow to use:\")\n","print(\"  1. export FOLD_ID=0  # set current fold\")\n","print(\"  2. Run steps 6–9 (they will use the corresponding fold automatically)\")\n","print(\"  3. Train the model and evaluate\")\n","print(\"  4. Repeat steps 1–3 for all folds\")\n","print(\"  5. Aggregate results (mean ± std)\")\n","\n","# ========== 6. Split visualization info ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Split visualization info\")\n","print(\"=\"*60)\n","\n","print(f\"\\nLOSO split matrix (first 5 folds):\")\n","print(f\"{'Fold':<6} {'TestSubject':<12} {'#TrainSubs':<12} {'#TestSamples':<12} {'#TrainSamples':<12}\")\n","print(\"-\" * 60)\n","\n","for i in range(min(5, len(splits))):\n","    fold = splits[str(i)]\n","    stats = df_fold_stats[df_fold_stats['fold_id'] == i].iloc[0]\n","    print(f\"{i:<6} {fold['test_subject']:<12} {fold['n_train']:<12} \"\n","          f\"{stats['n_test_samples']:<12} {stats['n_train_samples']:<12}\")\n","\n","if len(splits) > 5:\n","    print(f\"... (total {len(splits)} folds)\")\n","\n","# ========== 7. Summary ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 10 complete - LOSO split\")\n","print(\"=\"*60)\n","\n","print(f\"\\nSplit strategy:\")\n","print(f\"  Method: LOSO (Leave-One-Subject-Out)\")\n","print(f\"  #folds: {n_subjects}\")\n","print(f\"  #subjects: {n_subjects}\")\n","print(f\"  Train per fold: {n_subjects-1} subjects\")\n","print(f\"  Test per fold: 1 subject\")\n","\n","print(f\"\\nData distribution:\")\n","print(f\"  Total samples: {len(df):,}\")\n","print(f\"  Train ratio (avg): {df_fold_stats['train_ratio'].mean()*100:.2f}%\")\n","print(f\"  Test ratio (avg): {df_fold_stats['test_ratio'].mean()*100:.2f}%\")\n","\n","print(f\"\\nValidation results:\")\n","print(f\"  ✓ No subject overlap\")\n","print(f\"  ✓ All subjects covered\")\n","print(f\"  ✓ Each subject tested exactly once\")\n","print(f\"  ✓ Train/test sets are disjoint\")\n","\n","print(f\"\\nOutput files:\")\n","print(f\"  Main config: {splits_file}\")\n","print(f\"  Detailed config: {loso_config_file}\")\n","print(f\"  Fold stats: {fold_stats_file}\")\n","print(f\"  Usage example: {example_file}\")\n","\n","print(\"\\nAnti-leakage principles:\")\n","print(\"  1. ✓ Fully separated by subject\")\n","print(\"  2. ✓ Statistics computed from training fold only\")\n","print(\"  3. ✓ Feature engineering is fold-internal\")\n","print(\"  4. ✓ Window slicing performed after splitting\")\n","print(\"  5. ✓ Hyperparameter tuning limited to training data\")\n","print(\"  6. ✓ Test set used strictly for independent evaluation\")\n","print(\"  7. ✓ Cross-fold aggregation uses independent metrics\")\n","\n","print(\"\\nNext steps:\")\n","print(\"  - Set export FOLD_ID=<fold_id>\")\n","print(\"  - Re-run steps 6–9 (per-fold processing)\")\n","print(\"  - Train and evaluate models\")\n","print(\"  - Iterate all folds and aggregate results\")\n","\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6lQ1U4qPlDX8","executionInfo":{"status":"ok","timestamp":1763144595393,"user_tz":0,"elapsed":324,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"99ba0070-98f7-40b5-dd3c-2d368ec12e48"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 10: LOSO split\n","============================================================\n","\n","============================================================\n","1. Load data and get subject list\n","============================================================\n","Loading data: data/lara/mbientlab/proc/labeled/\n","Data shape: (556504, 16)\n","Total samples: 556,504\n","\n","Subject list:\n","  Total: 8 subjects\n","  IDs: ['S07', 'S08', 'S09', 'S10', 'S11', 'S12', 'S13', 'S14']\n","\n","Sample count per subject:\n","  S07:   76,522 samples (13.75%)\n","  S08:   64,857 samples (11.65%)\n","  S09:   77,701 samples (13.96%)\n","  S10:   82,659 samples (14.85%)\n","  S11:   70,410 samples (12.65%)\n","  S12:   30,923 samples ( 5.56%)\n","  S13:   82,335 samples (14.80%)\n","  S14:   71,097 samples (12.78%)\n","\n","============================================================\n","2. Generate LOSO split\n","============================================================\n","\n","LOSO strategy: Leave-One-Subject-Out\n","  #folds = #subjects = 8\n","  Per fold: 1 subject for test, 7 subjects for train\n","  Fold 0: test S07, train 7 subjects\n","  Fold 1: test S08, train 7 subjects\n","  Fold 2: test S09, train 7 subjects\n","  Fold 3: test S10, train 7 subjects\n","  Fold 4: test S11, train 7 subjects\n","  Fold 5: test S12, train 7 subjects\n","  Fold 6: test S13, train 7 subjects\n","  Fold 7: test S14, train 7 subjects\n","\n","✓ Generated 8 LOSO folds\n","\n","============================================================\n","3. Validate split integrity\n","============================================================\n","\n","Check 1: times each subject appears as test\n","  ✓ S07: 1 time(s)\n","  ✓ S08: 1 time(s)\n","  ✓ S09: 1 time(s)\n","  ✓ S10: 1 time(s)\n","  ✓ S11: 1 time(s)\n","  ✓ S12: 1 time(s)\n","  ✓ S13: 1 time(s)\n","  ✓ S14: 1 time(s)\n","  ✓ All subjects appear exactly once\n","\n","Check 2: train and test sets are disjoint\n","  ✓ Train/test sets are completely disjoint for all folds\n","\n","Check 3: all subjects covered\n","  ✓ All subjects are covered; no missing or extra subjects\n","\n","Check 4: per-fold sample counts\n","\n","Per-fold sample distribution:\n"," fold_id test_subject  n_train_samples  n_test_samples  train_ratio  test_ratio\n","       0          S07           479982           76522       0.8625      0.1375\n","       1          S08           491647           64857       0.8835      0.1165\n","       2          S09           478803           77701       0.8604      0.1396\n","       3          S10           473845           82659       0.8515      0.1485\n","       4          S11           486094           70410       0.8735      0.1265\n","       5          S12           525581           30923       0.9444      0.0556\n","       6          S13           474169           82335       0.8520      0.1480\n","       7          S14           485407           71097       0.8722      0.1278\n","\n","Sample distribution summary:\n","  Train sample count: 473,845 ~ 525,581\n","  Test sample count: 30,923 ~ 82,659\n","  Average train ratio: 87.50%\n","  Average test ratio: 12.50%\n","\n","✓ All validations passed\n","\n","============================================================\n","4. Save split configuration\n","============================================================\n","✓ Saved: configs/splits.json\n","✓ Saved: configs/loso.yaml\n","✓ Saved: configs/loso.json\n","✓ Saved: configs/loso_fold_stats.csv\n","\n","============================================================\n","5. Generate usage example\n","============================================================\n","✓ Generated usage example: configs/loso_usage_example.py\n","\n","How to use:\n","  1. export FOLD_ID=0  # set current fold\n","  2. Run steps 6–9 (they will use the corresponding fold automatically)\n","  3. Train the model and evaluate\n","  4. Repeat steps 1–3 for all folds\n","  5. Aggregate results (mean ± std)\n","\n","============================================================\n","6. Split visualization info\n","============================================================\n","\n","LOSO split matrix (first 5 folds):\n","Fold   TestSubject  #TrainSubs   #TestSamples #TrainSamples\n","------------------------------------------------------------\n","0      S07          7            76522        479982      \n","1      S08          7            64857        491647      \n","2      S09          7            77701        478803      \n","3      S10          7            82659        473845      \n","4      S11          7            70410        486094      \n","... (total 8 folds)\n","\n","============================================================\n","Step 10 complete - LOSO split\n","============================================================\n","\n","Split strategy:\n","  Method: LOSO (Leave-One-Subject-Out)\n","  #folds: 8\n","  #subjects: 8\n","  Train per fold: 7 subjects\n","  Test per fold: 1 subject\n","\n","Data distribution:\n","  Total samples: 556,504\n","  Train ratio (avg): 87.50%\n","  Test ratio (avg): 12.50%\n","\n","Validation results:\n","  ✓ No subject overlap\n","  ✓ All subjects covered\n","  ✓ Each subject tested exactly once\n","  ✓ Train/test sets are disjoint\n","\n","Output files:\n","  Main config: configs/splits.json\n","  Detailed config: configs/loso.yaml\n","  Fold stats: configs/loso_fold_stats.csv\n","  Usage example: configs/loso_usage_example.py\n","\n","Anti-leakage principles:\n","  1. ✓ Fully separated by subject\n","  2. ✓ Statistics computed from training fold only\n","  3. ✓ Feature engineering is fold-internal\n","  4. ✓ Window slicing performed after splitting\n","  5. ✓ Hyperparameter tuning limited to training data\n","  6. ✓ Test set used strictly for independent evaluation\n","  7. ✓ Cross-fold aggregation uses independent metrics\n","\n","Next steps:\n","  - Set export FOLD_ID=<fold_id>\n","  - Re-run steps 6–9 (per-fold processing)\n","  - Train and evaluate models\n","  - Iterate all folds and aggregate results\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 11: Feature Engineering (KNN/RF) - Multi-fold Final Version\n","Compute time- + frequency- + correlation-domain features per window, with magnitudes & energy\n","Loop over all folds defined in configs/splits.json and save per-fold outputs.\n","\"\"\"\n","\n","import os\n","import sys\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import json\n","import yaml\n","from scipy import stats, fft\n","from sklearn.preprocessing import StandardScaler\n","import pickle\n","\n","# ========== Config ==========\n","\n","SAMPLING_RATE_HZ = 50.0\n","\n","print(\"=\"*60)\n","print(\"Step 11: Feature Engineering (KNN/RF) - multi-fold\")\n","print(\"=\"*60)\n","\n","# Path config\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","configs_dir.mkdir(parents=True, exist_ok=True)\n","\n","windows_root = proc_dir / \"windows\"\n","features_root = proc_dir / \"features\"\n","\n","# Channel names\n","channel_names = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","# ========== 0. Determine folds ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"0. Determine folds\")\n","print(\"=\"*60)\n","\n","splits_path = configs_dir / \"splits.json\"\n","if splits_path.exists():\n","    with open(splits_path, \"r\") as f:\n","        splits = json.load(f)\n","    fold_ids = sorted(int(k) for k in splits.keys())\n","    print(f\"Detected {len(fold_ids)} folds from {splits_path}: {fold_ids}\")\n","else:\n","    print(\"⚠️ splits.json not found; will treat all data as a single 'all' fold\")\n","    splits = None\n","    fold_ids = [None]   # pseudo-fold \"all\"\n","\n","# ========== 1. Define feature extraction functions (with robustness fixes) ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Define feature extraction functions\")\n","print(\"=\"*60)\n","\n","def extract_time_features(x):\n","    \"\"\"Extract time-domain features (with robustness)\"\"\"\n","    features = {}\n","\n","    # Basic stats\n","    features['mean'] = np.mean(x)\n","    features['std'] = np.std(x)\n","    features['min'] = np.min(x)\n","    features['max'] = np.max(x)\n","    features['range'] = features['max'] - features['min']\n","\n","    # Quantiles\n","    features['q25'] = np.percentile(x, 25)\n","    features['q50'] = np.median(x)\n","    features['q75'] = np.percentile(x, 75)\n","    features['iqr'] = features['q75'] - features['q25']\n","\n","    # Robust stats\n","    features['mad'] = np.median(np.abs(x - features['q50']))\n","\n","    # Higher-order stats (unbiased)\n","    features['skew'] = stats.skew(x, bias=False, nan_policy='omit')\n","    features['kurtosis'] = stats.kurtosis(x, bias=False, fisher=True, nan_policy='omit')\n","\n","    # Energy-related\n","    features['rms'] = np.sqrt(np.mean(x**2))\n","    features['energy'] = np.sum(x**2)\n","\n","    # Zero crossing rate\n","    zero_crossings = np.sum(np.diff(np.sign(x)) != 0)\n","    features['zero_crossing_rate'] = zero_crossings / len(x)\n","\n","    # Mean absolute difference\n","    features['mean_abs_diff'] = np.mean(np.abs(np.diff(x)))\n","\n","    return features\n","\n","def extract_freq_features(x, fs=50.0):\n","    \"\"\"Extract frequency-domain features (skip DC)\"\"\"\n","    features = {}\n","\n","    # FFT (Hann window to reduce spectral leakage)\n","    window = np.hanning(len(x))\n","    x_windowed = x * window\n","\n","    fft_vals = fft.fft(x_windowed)\n","    fft_mag = np.abs(fft_vals[:len(x)//2])\n","    fft_freq = fft.fftfreq(len(x), 1/fs)[:len(x)//2]\n","\n","    # Normalized power spectrum\n","    psd = fft_mag**2\n","    psd_norm = psd / np.sum(psd) if np.sum(psd) > 0 else psd\n","\n","    # Spectral energy\n","    features['spectral_energy'] = np.sum(psd)\n","\n","    # Spectral entropy\n","    psd_norm_pos = psd_norm[psd_norm > 0]\n","    features['spectral_entropy'] = -np.sum(psd_norm_pos * np.log2(psd_norm_pos)) if len(psd_norm_pos) > 0 else 0\n","\n","    # Peak frequency (skip DC to avoid 0 Hz dominating)\n","    fft_mag_no_dc = fft_mag.copy()\n","    fft_mag_no_dc[0] = 0.0\n","    peak_idx = np.argmax(fft_mag_no_dc)\n","    features['peak_frequency'] = fft_freq[peak_idx]\n","\n","    # Spectral centroid\n","    features['spectral_centroid'] = np.sum(fft_freq * psd_norm) if np.sum(psd_norm) > 0 else 0\n","\n","    # Spectral bandwidth (std)\n","    features['spectral_bandwidth'] = np.sqrt(\n","        np.sum(((fft_freq - features['spectral_centroid'])**2) * psd_norm)\n","    ) if np.sum(psd_norm) > 0 else 0\n","\n","    # Spectral rolloff (85% energy)\n","    cumsum_psd = np.cumsum(psd_norm)\n","    rolloff_idx = np.where(cumsum_psd >= 0.85)[0]\n","    features['spectral_rolloff'] = fft_freq[rolloff_idx[0]] if len(rolloff_idx) > 0 else fft_freq[-1]\n","\n","    # Low-frequency energy ratio (0–5 Hz / total energy)\n","    low_freq_mask = fft_freq <= 5.0\n","    features['low_freq_energy_ratio'] = (\n","        np.sum(psd[low_freq_mask]) / np.sum(psd) if np.sum(psd) > 0 else 0\n","    )\n","\n","    return features\n","\n","def extract_window_features(window, channel_names, fs=50.0):\n","    \"\"\"Extract all features for a single window (with deduplicated cross-correlation)\"\"\"\n","    all_features = {}\n","    n_channels = window.shape[1]\n","\n","    # Time & frequency features per channel\n","    for ch_idx, ch_name in enumerate(channel_names):\n","        signal_data = window[:, ch_idx]\n","\n","        # Time-domain\n","        time_feats = extract_time_features(signal_data)\n","        for feat_name, feat_val in time_feats.items():\n","            all_features[f'{ch_name}_{feat_name}'] = feat_val\n","\n","        # Frequency-domain\n","        freq_feats = extract_freq_features(signal_data, fs)\n","        for feat_name, feat_val in freq_feats.items():\n","            all_features[f'{ch_name}_{feat_name}'] = feat_val\n","\n","    # Correlation features (deduplicated)\n","    corr_mat = np.corrcoef(window, rowvar=False)\n","\n","    # Autocorrelation (lag=1) for each channel\n","    for ch_idx, ch_name in enumerate(channel_names):\n","        signal_data = window[:, ch_idx]\n","        if len(signal_data) > 1:\n","            autocorr = np.corrcoef(signal_data[:-1], signal_data[1:])[0, 1]\n","            all_features[f'{ch_name}_autocorr_lag1'] = autocorr if not np.isnan(autocorr) else 0\n","        else:\n","            all_features[f'{ch_name}_autocorr_lag1'] = 0\n","\n","    # Cross-correlation (upper triangle only; deduplicated): C(8,2)=28 pairs\n","    for i in range(n_channels):\n","        for j in range(i + 1, n_channels):\n","            val = corr_mat[i, j]\n","            key = f'crosscorr_{channel_names[i]}_{channel_names[j]}'\n","            all_features[key] = 0 if np.isnan(val) else val\n","\n","    return all_features\n","\n","print(\"Feature types:\")\n","print(\"  Time-domain: mean, std, min, max, range, q25, q50, q75, iqr, mad,\")\n","print(\"               skew, kurtosis, rms, energy, zero_crossing_rate, mean_abs_diff\")\n","print(\"  Frequency-domain: spectral_energy, spectral_entropy, peak_frequency (skip DC),\")\n","print(\"                    spectral_centroid, spectral_bandwidth, spectral_rolloff,\")\n","print(\"                    low_freq_energy_ratio\")\n","print(\"  Correlation: autocorr_lag1 (8 dims), crosscorr_* (28 dims, deduplicated)\")\n","print(f\"  Expected total dimension: 16×8 + 7×8 + 8 + 28 = 220 dims\")\n","\n","# ========== 2. Loop over folds and run feature engineering ==========\n","\n","for fold_id in fold_ids:\n","    fold_tag = f\"fold_{fold_id:02d}\" if fold_id is not None else \"all\"\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(f\"2. Processing fold: {fold_tag}\")\n","    print(\"=\"*60)\n","\n","    # ----- Paths for this fold -----\n","    windows_dir = windows_root / fold_tag\n","    features_dir = features_root / fold_tag\n","    features_dir.mkdir(parents=True, exist_ok=True)\n","\n","    if not windows_dir.exists():\n","        print(f\"⚠️ Windows dir not found for {fold_tag}: {windows_dir}, skipping this fold.\")\n","        continue\n","\n","    print(f\"Windows dir:  {windows_dir}\")\n","    print(f\"Features dir: {features_dir}\")\n","\n","    # ========== 3. Load window data & metadata ==========\n","    print(\"\\n\" + \"-\"*60)\n","    print(\"3. Load window data & metadata\")\n","    print(\"-\"*60)\n","\n","    # Train\n","    X_train_file = windows_dir / \"X_train.npy\"\n","    y_train_file = windows_dir / \"y_train.npy\"\n","    train_meta_file = windows_dir / \"X_train.parquet\"\n","\n","    if not X_train_file.exists() or not y_train_file.exists():\n","        print(f\"⚠️ Train window data not found for {fold_tag}; please run Step 9 first. Skipping.\")\n","        continue\n","\n","    X_train = np.load(X_train_file)\n","    y_train = np.load(y_train_file)\n","    df_train_meta = pd.read_parquet(train_meta_file)\n","\n","    assert X_train.shape[0] == len(df_train_meta) == len(y_train), \\\n","        f\"[{fold_tag}] Inconsistent train data: X={X_train.shape[0]}, meta={len(df_train_meta)}, y={len(y_train)}\"\n","\n","    print(f\"\\nTrain windows:\")\n","    print(f\"  X_train shape: {X_train.shape}\")\n","    print(f\"  y_train shape: {y_train.shape}\")\n","    print(f\"  metadata:      {len(df_train_meta)} rows\")\n","    print(f\"  ✓ Consistency check passed\")\n","\n","    # Test set (may be empty)\n","    X_test_file = windows_dir / \"X_test.npy\"\n","    y_test_file = windows_dir / \"y_test.npy\"\n","    test_meta_file = windows_dir / \"X_test.parquet\"\n","\n","    if X_test_file.exists() and y_test_file.exists():\n","        X_test = np.load(X_test_file)\n","        y_test = np.load(y_test_file)\n","        df_test_meta = pd.read_parquet(test_meta_file)\n","\n","        assert X_test.shape[0] == len(df_test_meta) == len(y_test), \\\n","            f\"[{fold_tag}] Inconsistent test data: X={X_test.shape[0]}, meta={len(df_test_meta)}, y={len(y_test)}\"\n","\n","        print(f\"\\nTest windows:\")\n","        print(f\"  X_test shape: {X_test.shape}\")\n","        print(f\"  y_test shape: {y_test.shape}\")\n","        print(f\"  metadata:     {len(df_test_meta)} rows\")\n","        print(f\"  ✓ Consistency check passed\")\n","        has_test = True\n","    else:\n","        print(f\"\\nTest windows not found for {fold_tag}; this fold will be train-only.\")\n","        X_test = None\n","        y_test = None\n","        df_test_meta = None\n","        has_test = False\n","\n","    # ========== Anti-leakage assertions ==========\n","    train_subs = set(df_train_meta[\"subject_id\"].unique())\n","    test_subs = set(df_test_meta[\"subject_id\"].unique()) if has_test else set()\n","    assert train_subs.isdisjoint(test_subs), f\"[{fold_tag}] Train/Test subjects overlap: {train_subs & test_subs}\"\n","    if has_test and len(test_subs) > 0:\n","        assert len(test_subs) == 1, f\"[{fold_tag}] Test set contains multiple subjects: {test_subs}\"\n","    print(f\"\\n✓ Anti-leakage check passed:\")\n","    print(f\"  Train subjects: {len(train_subs)}\")\n","    print(f\"  Test subjects:  {len(test_subs)}\")\n","\n","    # ========== 4. Extract training-set features ==========\n","    print(\"\\n\" + \"-\"*60)\n","    print(\"4. Extract training-set features\")\n","    print(\"-\"*60)\n","\n","    print(f\"Processing {X_train.shape[0]:,} training windows...\")\n","\n","    train_features_list = []\n","    for i in range(X_train.shape[0]):\n","        window = X_train[i]\n","        features = extract_window_features(window, channel_names, SAMPLING_RATE_HZ)\n","        train_features_list.append(features)\n","\n","        if (i + 1) % 1000 == 0:\n","            print(f\"  Processed {i+1:,} / {X_train.shape[0]:,} windows\")\n","\n","    df_train_features = pd.DataFrame(train_features_list)\n","\n","    print(f\"\\n✓ Training-set features:\")\n","    print(f\"  #samples: {len(df_train_features):,}\")\n","    print(f\"  feature dimension: {df_train_features.shape[1]}\")\n","\n","    # Check NaN & Inf\n","    nan_count = df_train_features.isna().sum().sum()\n","    inf_count = np.isinf(df_train_features.values).sum()\n","\n","    if nan_count > 0 or inf_count > 0:\n","        print(f\"\\n⚠️ Found invalid values in training features:\")\n","        print(f\"  NaN: {nan_count}\")\n","        print(f\"  Inf: {inf_count}\")\n","        df_train_features = df_train_features.replace([np.inf, -np.inf], np.nan)\n","        df_train_features = df_train_features.fillna(0)\n","        print(f\"  ✓ Filled with 0\")\n","\n","    # Feature names\n","    feature_names = df_train_features.columns.tolist()\n","    print(f\"\\nSample feature names (first 10):\")\n","    for name in feature_names[:10]:\n","        print(f\"  - {name}\")\n","\n","    # Low-variance check (raw features, before standardization)\n","    print(f\"\\nChecking low-variance features (raw):\")\n","    raw_variance = df_train_features.var()\n","    low_var_features = raw_variance[raw_variance < 1e-8].index.tolist()\n","\n","    if low_var_features:\n","        print(f\"  ⚠️ Found {len(low_var_features)} low-variance features (var < 1e-8):\")\n","        for feat in low_var_features[:5]:\n","            print(f\"    - {feat}: var={raw_variance[feat]:.2e}\")\n","        if len(low_var_features) > 5:\n","            print(f\"    ... (total {len(low_var_features)} features)\")\n","\n","        low_var_file = features_dir / \"low_variance_features.txt\"\n","        pd.Series(low_var_features).to_csv(low_var_file, index=False, header=False)\n","        print(f\"  ✓ Saved low-variance list: {low_var_file}\")\n","    else:\n","        print(f\"  ✓ All feature variances look OK\")\n","\n","    # ========== 5. Extract test-set features ==========\n","    if has_test:\n","        print(\"\\n\" + \"-\"*60)\n","        print(\"5. Extract test-set features\")\n","        print(\"-\"*60)\n","\n","        print(f\"Processing {X_test.shape[0]:,} test windows...\")\n","\n","        test_features_list = []\n","        for i in range(X_test.shape[0]):\n","            window = X_test[i]\n","            features = extract_window_features(window, channel_names, SAMPLING_RATE_HZ)\n","            test_features_list.append(features)\n","\n","            if (i + 1) % 1000 == 0:\n","                print(f\"  Processed {i+1:,} / {X_test.shape[0]:,} windows\")\n","\n","        df_test_features = pd.DataFrame(test_features_list)\n","\n","        print(f\"\\n✓ Test-set features:\")\n","        print(f\"  #samples: {len(df_test_features):,}\")\n","        print(f\"  feature dimension: {df_test_features.shape[1]}\")\n","\n","        nan_count = df_test_features.isna().sum().sum()\n","        inf_count = np.isinf(df_test_features.values).sum()\n","\n","        if nan_count > 0 or inf_count > 0:\n","            print(f\"\\n⚠️ Found invalid values in test features:\")\n","            print(f\"  NaN: {nan_count}\")\n","            print(f\"  Inf: {inf_count}\")\n","            df_test_features = df_test_features.replace([np.inf, -np.inf], np.nan)\n","            df_test_features = df_test_features.fillna(0)\n","            print(f\"  ✓ Filled with 0\")\n","\n","    # ========== 6. Feature standardization (train-only stats) ==========\n","    print(\"\\n\" + \"-\"*60)\n","    print(\"6. Feature standardization (train-only stats)\")\n","    print(\"-\"*60)\n","\n","    scaler = StandardScaler()\n","    X_train_scaled = scaler.fit_transform(df_train_features)\n","\n","    print(f\"Scaler parameters (train set):\")\n","    print(f\"  mean range: [{scaler.mean_.min():.4f}, {scaler.mean_.max():.4f}]\")\n","    print(f\"  std range:  [{scaler.scale_.min():.4f}, {scaler.scale_.max():.4f}]\")\n","\n","    df_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names).astype('float32')\n","\n","    if has_test:\n","        X_test_scaled = scaler.transform(df_test_features)\n","        df_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_names).astype('float32')\n","        print(f\"\\n✓ Standardized test set using train-set statistics\")\n","\n","    print(f\"✓ Features cast to: float32\")\n","\n","    # ========== 7. Save feature data (with metadata) ==========\n","    print(\"\\n\" + \"-\"*60)\n","    print(\"7. Save feature data (with metadata)\")\n","    print(\"-\"*60)\n","\n","    # Train set\n","    train_X_file = features_dir / \"train_X.parquet\"\n","    train_y_file = features_dir / \"train_y.parquet\"\n","    train_meta_output = features_dir / \"train_meta.parquet\"\n","\n","    df_train_scaled.to_parquet(train_X_file, index=False)\n","    pd.DataFrame({'label': y_train}).to_parquet(train_y_file, index=False)\n","    df_train_meta[['window_id', 'subject_id', 'session_id', 'placement',\n","                   'time_range', 'label', 'label_purity']].to_parquet(\n","        train_meta_output, index=False\n","    )\n","\n","    print(f\"✓ Train set:\")\n","    print(f\"  features: {train_X_file}\")\n","    print(f\"  labels:   {train_y_file}\")\n","    print(f\"  metadata: {train_meta_output}\")\n","\n","    # Test set\n","    if has_test:\n","        test_X_file = features_dir / \"test_X.parquet\"\n","        test_y_file = features_dir / \"test_y.parquet\"\n","        test_meta_output = features_dir / \"test_meta.parquet\"\n","\n","        df_test_scaled.to_parquet(test_X_file, index=False)\n","        pd.DataFrame({'label': y_test}).to_parquet(test_y_file, index=False)\n","        df_test_meta[['window_id', 'subject_id', 'session_id', 'placement',\n","                      'time_range', 'label', 'label_purity']].to_parquet(\n","            test_meta_output, index=False\n","        )\n","\n","        print(f\"✓ Test set:\")\n","        print(f\"  features: {test_X_file}\")\n","        print(f\"  labels:   {test_y_file}\")\n","        print(f\"  metadata: {test_meta_output}\")\n","\n","    # Scaler\n","    scaler_file = features_dir / \"scaler.pkl\"\n","    with open(scaler_file, 'wb') as f:\n","        pickle.dump(scaler, f)\n","    print(f\"✓ Scaler: {scaler_file}\")\n","\n","    # ========== 8. Feature type stats (exact counts, revised) ==========\n","    print(\"\\n\" + \"-\"*60)\n","    print(\"8. Feature type stats (exact counts, revised)\")\n","    print(\"-\"*60)\n","\n","    freq_features = [\n","        f for f in feature_names\n","        if 'spectral_' in f\n","        or f.endswith('_peak_frequency')\n","        or f.endswith('_low_freq_energy_ratio')\n","    ]\n","\n","    corr_features = [f for f in feature_names\n","                     if 'autocorr' in f or 'crosscorr' in f]\n","\n","    time_features = [f for f in feature_names\n","                     if f not in freq_features and f not in corr_features]\n","\n","    print(f\"\\nFeature counts (exact) for {fold_tag}:\")\n","    print(f\"  Total: {len(feature_names)}\")\n","    print(f\"  Time-domain:      {len(time_features)}\")\n","    print(f\"  Frequency-domain: {len(freq_features)}\")\n","    print(f\"  Correlation:      {len(corr_features)}\")\n","    print(f\"    - Autocorr: {len([f for f in corr_features if 'autocorr' in f])}\")\n","    print(f\"    - Crosscorr: {len([f for f in corr_features if 'crosscorr' in f])} (deduplicated)\")\n","\n","    total_check = len(time_features) + len(freq_features) + len(corr_features)\n","    assert total_check == len(feature_names), \\\n","        f\"[{fold_tag}] Feature count mismatch: {total_check} ≠ {len(feature_names)}\"\n","    print(f\"✓ Count verification: {len(time_features)} + {len(freq_features)} + {len(corr_features)} = {len(feature_names)}\")\n","\n","    # ========== 9. Save feature config (named by fold) ==========\n","    print(\"\\n\" + \"-\"*60)\n","    print(\"9. Save feature config (named by fold)\")\n","    print(\"-\"*60)\n","\n","    feature_config = {\n","        'feature_extraction': {\n","            'method': 'handcrafted (time + frequency + correlation)',\n","            'sampling_rate_hz': SAMPLING_RATE_HZ,\n","            'n_channels': len(channel_names),\n","            'channel_names': channel_names,\n","        },\n","        'feature_types': {\n","            'time_domain': [\n","                'mean', 'std', 'min', 'max', 'range',\n","                'q25', 'q50', 'q75', 'iqr', 'mad',\n","                'skew (bias=False)', 'kurtosis (bias=False, fisher=True)',\n","                'rms', 'energy',\n","                'zero_crossing_rate', 'mean_abs_diff'\n","            ],\n","            'frequency_domain': [\n","                'spectral_energy', 'spectral_entropy',\n","                'peak_frequency (skip DC)', 'spectral_centroid',\n","                'spectral_bandwidth', 'spectral_rolloff',\n","                'low_freq_energy_ratio'\n","            ],\n","            'correlation': [\n","                'autocorr_lag1 (8 dims: per channel)',\n","                'crosscorr_* (28 dims: C(8,2) pairs, deduplicated)'\n","            ]\n","        },\n","        'feature_dimensions': {\n","            'total': len(feature_names),\n","            'time_domain': len(time_features),\n","            'frequency_domain': len(freq_features),\n","            'correlation': len(corr_features),\n","            'correlation_breakdown': {\n","                'autocorr': len([f for f in corr_features if 'autocorr' in f]),\n","                'crosscorr': len([f for f in corr_features if 'crosscorr' in f]),\n","            }\n","        },\n","        'preprocessing': {\n","            'scaler': 'StandardScaler',\n","            'fit_on': 'training set only',\n","            'nan_inf_handling': 'fill with 0',\n","            'dtype': 'float32',\n","        },\n","        'dataset': {\n","            'fold_id': fold_id if fold_id is not None else None,\n","            'fold_tag': fold_tag,\n","            'train_samples': int(len(df_train_scaled)),\n","            'test_samples': int(len(df_test_scaled)) if has_test else 0,\n","        },\n","        'feature_names': feature_names,\n","        'improvements': [\n","            'Cross-correlation deduplication: keep upper triangle only, C(8,2)=28 pairs',\n","            'Higher-order stats: bias=False, fisher=True, nan_policy=omit',\n","            'Feature counting: exact (time = not frequency and not correlation)',\n","            'Frequency robustness: Hann window + skip DC for peak frequency',\n","            'Data type: float32 to save space',\n","            'Traceability: save window_id/session/time metadata',\n","            'Per-fold output: avoid overwriting different folds',\n","            'Consistency assertions: ensure X/y/meta alignment',\n","            'Low-variance check: on raw features (before standardization)',\n","            'Anti-leakage assertions: train/test subjects disjoint + single test subject under LOSO',\n","        ],\n","        'notes': [\n","            f'Total feature dimension: {len(feature_names)} (currently ~220)',\n","            f'Time-domain: {len(time_features)} dims',\n","            f'Frequency-domain: {len(freq_features)} dims',\n","            f'Correlation: {len(corr_features)} dims (autocorr + crosscorr)',\n","            'Standardization uses train set statistics only (anti-leakage)',\n","            'NaN/Inf filled with 0',\n","            'Suitable for KNN/RF/SVM and other classical ML models',\n","        ]\n","    }\n","\n","    feature_config_file = configs_dir / f\"features_{fold_tag}.yaml\"\n","    with open(feature_config_file, 'w', encoding='utf-8') as f:\n","        yaml.dump(feature_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","    print(f\"✓ Saved config: {feature_config_file}\")\n","\n","    feature_config_json = configs_dir / f\"features_{fold_tag}.json\"\n","    with open(feature_config_json, 'w', encoding='utf-8') as f:\n","        json.dump(feature_config, f, indent=2)\n","    print(f\"✓ Saved config: {feature_config_json}\")\n","\n","    # Save feature name list\n","    feature_names_file = features_dir / \"feature_names.txt\"\n","    with open(feature_names_file, 'w') as f:\n","        for name in feature_names:\n","            f.write(f\"{name}\\n\")\n","    print(f\"✓ Saved feature names: {feature_names_file}\")\n","\n","    # ========== 10. Feature statistics (train standardized) ==========\n","    print(\"\\n\" + \"-\"*60)\n","    print(\"10. Train feature statistics (standardized)\")\n","    print(\"-\"*60)\n","\n","    print(f\"\\nTraining-set feature stats (standardized, {fold_tag}):\")\n","    print(df_train_scaled.describe().T[['mean', 'std', 'min', 'max']].head(10))\n","\n","    # ========== 11. Per-fold summary ==========\n","    print(\"\\n\" + \"-\"*60)\n","    print(f\"11. Summary for fold {fold_tag}\")\n","    print(\"-\"*60)\n","\n","    print(f\"\\nFeature extraction:\")\n","    print(f\"  Method: handcrafted (time + frequency + correlation)\")\n","    print(f\"  Total dims:      {len(feature_names)}\")\n","    print(f\"  Time-domain:     {len(time_features)} dims\")\n","    print(f\"  Frequency-domain:{len(freq_features)} dims\")\n","    print(f\"  Correlation:     {len(corr_features)} dims\")\n","\n","    print(f\"\\nDataset:\")\n","    print(f\"  Fold:           {fold_tag}\")\n","    print(f\"  Train samples:  {len(df_train_scaled):,}\")\n","    if has_test:\n","        print(f\"  Test samples:   {len(df_test_scaled):,}\")\n","\n","    print(f\"\\nPreprocessing:\")\n","    print(f\"  Standardization: StandardScaler (fit on train only)\")\n","    print(f\"  Invalid values:  NaN/Inf filled with 0\")\n","    print(f\"  Dtype:           float32\")\n","\n","    print(f\"\\nOutputs ({fold_tag}):\")\n","    print(f\"  Dir:    {features_dir}/\")\n","    print(f\"  Train:  train_X.parquet, train_y.parquet, train_meta.parquet\")\n","    if has_test:\n","        print(f\"  Test:   test_X.parquet, test_y.parquet, test_meta.parquet\")\n","    print(f\"  Scaler: scaler.pkl\")\n","    print(f\"  Config: {feature_config_file.name}\")\n","    print(f\"  Feature list: feature_names.txt\")\n","    if low_var_features:\n","        print(f\"  Low variance: low_variance_features.txt\")\n","\n","# ========== Global summary ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 11 complete - Feature Engineering for all folds\")\n","print(\"=\"*60)\n","print(\"Next steps per fold:\")\n","print(\"  - Use train_X.parquet / train_y.parquet to train classical machine-learning models (e.g., KNN, Random Forest, SVM)\")\n","print(\"  - Use test_X.parquet / test_y.parquet to evaluate model performance\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ettLm0q1xng4","executionInfo":{"status":"ok","timestamp":1763148216319,"user_tz":0,"elapsed":800536,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"350da28f-2854-4cbe-dfff-5aaa7e668680"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 11: Feature Engineering (KNN/RF) - multi-fold\n","============================================================\n","\n","============================================================\n","0. Determine folds\n","============================================================\n","Detected 8 folds from configs/splits.json: [0, 1, 2, 3, 4, 5, 6, 7]\n","\n","============================================================\n","1. Define feature extraction functions\n","============================================================\n","Feature types:\n","  Time-domain: mean, std, min, max, range, q25, q50, q75, iqr, mad,\n","               skew, kurtosis, rms, energy, zero_crossing_rate, mean_abs_diff\n","  Frequency-domain: spectral_energy, spectral_entropy, peak_frequency (skip DC),\n","                    spectral_centroid, spectral_bandwidth, spectral_rolloff,\n","                    low_freq_energy_ratio\n","  Correlation: autocorr_lag1 (8 dims), crosscorr_* (28 dims, deduplicated)\n","  Expected total dimension: 16×8 + 7×8 + 8 + 28 = 220 dims\n","\n","============================================================\n","2. Processing fold: fold_00\n","============================================================\n","Windows dir:  data/lara/mbientlab/proc/windows/fold_00\n","Features dir: data/lara/mbientlab/proc/features/fold_00\n","\n","------------------------------------------------------------\n","3. Load window data & metadata\n","------------------------------------------------------------\n","\n","Train windows:\n","  X_train shape: (4965, 150, 8)\n","  y_train shape: (4965,)\n","  metadata:      4965 rows\n","  ✓ Consistency check passed\n","\n","Test windows:\n","  X_test shape: (766, 150, 8)\n","  y_test shape: (766,)\n","  metadata:     766 rows\n","  ✓ Consistency check passed\n","\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test subjects:  1\n","\n","------------------------------------------------------------\n","4. Extract training-set features\n","------------------------------------------------------------\n","Processing 4,965 training windows...\n","  Processed 1,000 / 4,965 windows\n","  Processed 2,000 / 4,965 windows\n","  Processed 3,000 / 4,965 windows\n","  Processed 4,000 / 4,965 windows\n","\n","✓ Training-set features:\n","  #samples: 4,965\n","  feature dimension: 220\n","\n","Sample feature names (first 10):\n","  - ax_mean\n","  - ax_std\n","  - ax_min\n","  - ax_max\n","  - ax_range\n","  - ax_q25\n","  - ax_q50\n","  - ax_q75\n","  - ax_iqr\n","  - ax_mad\n","\n","Checking low-variance features (raw):\n","  ✓ All feature variances look OK\n","\n","------------------------------------------------------------\n","5. Extract test-set features\n","------------------------------------------------------------\n","Processing 766 test windows...\n","\n","✓ Test-set features:\n","  #samples: 766\n","  feature dimension: 220\n","\n","------------------------------------------------------------\n","6. Feature standardization (train-only stats)\n","------------------------------------------------------------\n","Scaler parameters (train set):\n","  mean range: [-2.3800, 5011.4682]\n","  std range:  [0.0275, 5989.5515]\n","\n","✓ Standardized test set using train-set statistics\n","✓ Features cast to: float32\n","\n","------------------------------------------------------------\n","7. Save feature data (with metadata)\n","------------------------------------------------------------\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/features/fold_00/train_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_00/train_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_00/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/features/fold_00/test_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_00/test_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_00/test_meta.parquet\n","✓ Scaler: data/lara/mbientlab/proc/features/fold_00/scaler.pkl\n","\n","------------------------------------------------------------\n","8. Feature type stats (exact counts, revised)\n","------------------------------------------------------------\n","\n","Feature counts (exact) for fold_00:\n","  Total: 220\n","  Time-domain:      128\n","  Frequency-domain: 56\n","  Correlation:      36\n","    - Autocorr: 8\n","    - Crosscorr: 28 (deduplicated)\n","✓ Count verification: 128 + 56 + 36 = 220\n","\n","------------------------------------------------------------\n","9. Save feature config (named by fold)\n","------------------------------------------------------------\n","✓ Saved config: configs/features_fold_00.yaml\n","✓ Saved config: configs/features_fold_00.json\n","✓ Saved feature names: data/lara/mbientlab/proc/features/fold_00/feature_names.txt\n","\n","------------------------------------------------------------\n","10. Train feature statistics (standardized)\n","------------------------------------------------------------\n","\n","Training-set feature stats (standardized, fold_00):\n","                  mean       std        min       max\n","ax_mean  -1.152477e-09  1.000100 -15.219729  6.306524\n","ax_std    0.000000e+00  1.000100  -1.764076  4.245049\n","ax_min   -1.229308e-08  1.000101  -1.376173  2.003343\n","ax_max   -1.229308e-08  1.000102  -1.874545  1.459454\n","ax_range -6.146541e-09  1.000100  -2.084974  1.526482\n","ax_q25    0.000000e+00  1.000100  -7.923995  1.791004\n","ax_q50   -1.536635e-09  1.000101 -14.619161  6.822258\n","ax_q75   -6.146541e-09  1.000100  -4.762974  6.220499\n","ax_iqr    0.000000e+00  1.000101  -1.488466  6.504924\n","ax_mad    0.000000e+00  1.000101  -1.485339  6.554254\n","\n","------------------------------------------------------------\n","11. Summary for fold fold_00\n","------------------------------------------------------------\n","\n","Feature extraction:\n","  Method: handcrafted (time + frequency + correlation)\n","  Total dims:      220\n","  Time-domain:     128 dims\n","  Frequency-domain:56 dims\n","  Correlation:     36 dims\n","\n","Dataset:\n","  Fold:           fold_00\n","  Train samples:  4,965\n","  Test samples:   766\n","\n","Preprocessing:\n","  Standardization: StandardScaler (fit on train only)\n","  Invalid values:  NaN/Inf filled with 0\n","  Dtype:           float32\n","\n","Outputs (fold_00):\n","  Dir:    data/lara/mbientlab/proc/features/fold_00/\n","  Train:  train_X.parquet, train_y.parquet, train_meta.parquet\n","  Test:   test_X.parquet, test_y.parquet, test_meta.parquet\n","  Scaler: scaler.pkl\n","  Config: features_fold_00.yaml\n","  Feature list: feature_names.txt\n","\n","============================================================\n","2. Processing fold: fold_01\n","============================================================\n","Windows dir:  data/lara/mbientlab/proc/windows/fold_01\n","Features dir: data/lara/mbientlab/proc/features/fold_01\n","\n","------------------------------------------------------------\n","3. Load window data & metadata\n","------------------------------------------------------------\n","\n","Train windows:\n","  X_train shape: (5072, 150, 8)\n","  y_train shape: (5072,)\n","  metadata:      5072 rows\n","  ✓ Consistency check passed\n","\n","Test windows:\n","  X_test shape: (659, 150, 8)\n","  y_test shape: (659,)\n","  metadata:     659 rows\n","  ✓ Consistency check passed\n","\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test subjects:  1\n","\n","------------------------------------------------------------\n","4. Extract training-set features\n","------------------------------------------------------------\n","Processing 5,072 training windows...\n","  Processed 1,000 / 5,072 windows\n","  Processed 2,000 / 5,072 windows\n","  Processed 3,000 / 5,072 windows\n","  Processed 4,000 / 5,072 windows\n","  Processed 5,000 / 5,072 windows\n","\n","✓ Training-set features:\n","  #samples: 5,072\n","  feature dimension: 220\n","\n","Sample feature names (first 10):\n","  - ax_mean\n","  - ax_std\n","  - ax_min\n","  - ax_max\n","  - ax_range\n","  - ax_q25\n","  - ax_q50\n","  - ax_q75\n","  - ax_iqr\n","  - ax_mad\n","\n","Checking low-variance features (raw):\n","  ✓ All feature variances look OK\n","\n","------------------------------------------------------------\n","5. Extract test-set features\n","------------------------------------------------------------\n","Processing 659 test windows...\n","\n","✓ Test-set features:\n","  #samples: 659\n","  feature dimension: 220\n","\n","------------------------------------------------------------\n","6. Feature standardization (train-only stats)\n","------------------------------------------------------------\n","Scaler parameters (train set):\n","  mean range: [-2.4010, 5070.7869]\n","  std range:  [0.0312, 5966.3047]\n","\n","✓ Standardized test set using train-set statistics\n","✓ Features cast to: float32\n","\n","------------------------------------------------------------\n","7. Save feature data (with metadata)\n","------------------------------------------------------------\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/features/fold_01/train_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_01/train_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_01/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/features/fold_01/test_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_01/test_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_01/test_meta.parquet\n","✓ Scaler: data/lara/mbientlab/proc/features/fold_01/scaler.pkl\n","\n","------------------------------------------------------------\n","8. Feature type stats (exact counts, revised)\n","------------------------------------------------------------\n","\n","Feature counts (exact) for fold_01:\n","  Total: 220\n","  Time-domain:      128\n","  Frequency-domain: 56\n","  Correlation:      36\n","    - Autocorr: 8\n","    - Crosscorr: 28 (deduplicated)\n","✓ Count verification: 128 + 56 + 36 = 220\n","\n","------------------------------------------------------------\n","9. Save feature config (named by fold)\n","------------------------------------------------------------\n","✓ Saved config: configs/features_fold_01.yaml\n","✓ Saved config: configs/features_fold_01.json\n","✓ Saved feature names: data/lara/mbientlab/proc/features/fold_01/feature_names.txt\n","\n","------------------------------------------------------------\n","10. Train feature statistics (standardized)\n","------------------------------------------------------------\n","\n","Training-set feature stats (standardized, fold_01):\n","                  mean       std        min       max\n","ax_mean   7.521091e-10  1.000099 -15.658089  6.498771\n","ax_std    0.000000e+00  1.000098  -1.762424  4.855567\n","ax_min   -1.203375e-08  1.000098  -1.367046  2.034536\n","ax_max   -6.016873e-09  1.000096  -1.907545  1.440462\n","ax_range  6.016873e-09  1.000097  -2.119454  1.511371\n","ax_q25   -6.016873e-09  1.000098  -8.087412  1.688494\n","ax_q50    0.000000e+00  1.000099 -14.433160  6.727624\n","ax_q75    0.000000e+00  1.000099  -4.673830  9.872938\n","ax_iqr    0.000000e+00  1.000098  -1.451072  8.628301\n","ax_mad   -6.016873e-09  1.000098  -1.448191  8.466000\n","\n","------------------------------------------------------------\n","11. Summary for fold fold_01\n","------------------------------------------------------------\n","\n","Feature extraction:\n","  Method: handcrafted (time + frequency + correlation)\n","  Total dims:      220\n","  Time-domain:     128 dims\n","  Frequency-domain:56 dims\n","  Correlation:     36 dims\n","\n","Dataset:\n","  Fold:           fold_01\n","  Train samples:  5,072\n","  Test samples:   659\n","\n","Preprocessing:\n","  Standardization: StandardScaler (fit on train only)\n","  Invalid values:  NaN/Inf filled with 0\n","  Dtype:           float32\n","\n","Outputs (fold_01):\n","  Dir:    data/lara/mbientlab/proc/features/fold_01/\n","  Train:  train_X.parquet, train_y.parquet, train_meta.parquet\n","  Test:   test_X.parquet, test_y.parquet, test_meta.parquet\n","  Scaler: scaler.pkl\n","  Config: features_fold_01.yaml\n","  Feature list: feature_names.txt\n","\n","============================================================\n","2. Processing fold: fold_02\n","============================================================\n","Windows dir:  data/lara/mbientlab/proc/windows/fold_02\n","Features dir: data/lara/mbientlab/proc/features/fold_02\n","\n","------------------------------------------------------------\n","3. Load window data & metadata\n","------------------------------------------------------------\n","\n","Train windows:\n","  X_train shape: (4960, 150, 8)\n","  y_train shape: (4960,)\n","  metadata:      4960 rows\n","  ✓ Consistency check passed\n","\n","Test windows:\n","  X_test shape: (771, 150, 8)\n","  y_test shape: (771,)\n","  metadata:     771 rows\n","  ✓ Consistency check passed\n","\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test subjects:  1\n","\n","------------------------------------------------------------\n","4. Extract training-set features\n","------------------------------------------------------------\n","Processing 4,960 training windows...\n","  Processed 1,000 / 4,960 windows\n","  Processed 2,000 / 4,960 windows\n","  Processed 3,000 / 4,960 windows\n","  Processed 4,000 / 4,960 windows\n","\n","✓ Training-set features:\n","  #samples: 4,960\n","  feature dimension: 220\n","\n","Sample feature names (first 10):\n","  - ax_mean\n","  - ax_std\n","  - ax_min\n","  - ax_max\n","  - ax_range\n","  - ax_q25\n","  - ax_q50\n","  - ax_q75\n","  - ax_iqr\n","  - ax_mad\n","\n","Checking low-variance features (raw):\n","  ✓ All feature variances look OK\n","\n","------------------------------------------------------------\n","5. Extract test-set features\n","------------------------------------------------------------\n","Processing 771 test windows...\n","\n","✓ Test-set features:\n","  #samples: 771\n","  feature dimension: 220\n","\n","------------------------------------------------------------\n","6. Feature standardization (train-only stats)\n","------------------------------------------------------------\n","Scaler parameters (train set):\n","  mean range: [-2.4118, 5163.1632]\n","  std range:  [0.0313, 5997.5821]\n","\n","✓ Standardized test set using train-set statistics\n","✓ Features cast to: float32\n","\n","------------------------------------------------------------\n","7. Save feature data (with metadata)\n","------------------------------------------------------------\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/features/fold_02/train_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_02/train_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_02/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/features/fold_02/test_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_02/test_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_02/test_meta.parquet\n","✓ Scaler: data/lara/mbientlab/proc/features/fold_02/scaler.pkl\n","\n","------------------------------------------------------------\n","8. Feature type stats (exact counts, revised)\n","------------------------------------------------------------\n","\n","Feature counts (exact) for fold_02:\n","  Total: 220\n","  Time-domain:      128\n","  Frequency-domain: 56\n","  Correlation:      36\n","    - Autocorr: 8\n","    - Crosscorr: 28 (deduplicated)\n","✓ Count verification: 128 + 56 + 36 = 220\n","\n","------------------------------------------------------------\n","9. Save feature config (named by fold)\n","------------------------------------------------------------\n","✓ Saved config: configs/features_fold_02.yaml\n","✓ Saved config: configs/features_fold_02.json\n","✓ Saved feature names: data/lara/mbientlab/proc/features/fold_02/feature_names.txt\n","\n","------------------------------------------------------------\n","10. Train feature statistics (standardized)\n","------------------------------------------------------------\n","\n","Training-set feature stats (standardized, fold_02):\n","                  mean       std        min       max\n","ax_mean   3.845461e-10  1.000101 -15.721042  6.512682\n","ax_std   -6.152737e-09  1.000100  -1.726814  4.839357\n","ax_min    6.152737e-09  1.000104  -1.344651  2.001993\n","ax_max   -1.845821e-08  1.000105  -1.898916  1.399420\n","ax_range  0.000000e+00  1.000101  -2.083400  1.476355\n","ax_q25    0.000000e+00  1.000101  -8.157734  1.740477\n","ax_q50   -1.538184e-09  1.000100 -14.754944  5.939607\n","ax_q75    0.000000e+00  1.000100  -4.653211  9.883879\n","ax_iqr    0.000000e+00  1.000100  -1.420665  8.686911\n","ax_mad    0.000000e+00  1.000100  -1.425497  8.553444\n","\n","------------------------------------------------------------\n","11. Summary for fold fold_02\n","------------------------------------------------------------\n","\n","Feature extraction:\n","  Method: handcrafted (time + frequency + correlation)\n","  Total dims:      220\n","  Time-domain:     128 dims\n","  Frequency-domain:56 dims\n","  Correlation:     36 dims\n","\n","Dataset:\n","  Fold:           fold_02\n","  Train samples:  4,960\n","  Test samples:   771\n","\n","Preprocessing:\n","  Standardization: StandardScaler (fit on train only)\n","  Invalid values:  NaN/Inf filled with 0\n","  Dtype:           float32\n","\n","Outputs (fold_02):\n","  Dir:    data/lara/mbientlab/proc/features/fold_02/\n","  Train:  train_X.parquet, train_y.parquet, train_meta.parquet\n","  Test:   test_X.parquet, test_y.parquet, test_meta.parquet\n","  Scaler: scaler.pkl\n","  Config: features_fold_02.yaml\n","  Feature list: feature_names.txt\n","\n","============================================================\n","2. Processing fold: fold_03\n","============================================================\n","Windows dir:  data/lara/mbientlab/proc/windows/fold_03\n","Features dir: data/lara/mbientlab/proc/features/fold_03\n","\n","------------------------------------------------------------\n","3. Load window data & metadata\n","------------------------------------------------------------\n","\n","Train windows:\n","  X_train shape: (4858, 150, 8)\n","  y_train shape: (4858,)\n","  metadata:      4858 rows\n","  ✓ Consistency check passed\n","\n","Test windows:\n","  X_test shape: (873, 150, 8)\n","  y_test shape: (873,)\n","  metadata:     873 rows\n","  ✓ Consistency check passed\n","\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test subjects:  1\n","\n","------------------------------------------------------------\n","4. Extract training-set features\n","------------------------------------------------------------\n","Processing 4,858 training windows...\n","  Processed 1,000 / 4,858 windows\n","  Processed 2,000 / 4,858 windows\n","  Processed 3,000 / 4,858 windows\n","  Processed 4,000 / 4,858 windows\n","\n","✓ Training-set features:\n","  #samples: 4,858\n","  feature dimension: 220\n","\n","Sample feature names (first 10):\n","  - ax_mean\n","  - ax_std\n","  - ax_min\n","  - ax_max\n","  - ax_range\n","  - ax_q25\n","  - ax_q50\n","  - ax_q75\n","  - ax_iqr\n","  - ax_mad\n","\n","Checking low-variance features (raw):\n","  ✓ All feature variances look OK\n","\n","------------------------------------------------------------\n","5. Extract test-set features\n","------------------------------------------------------------\n","Processing 873 test windows...\n","\n","✓ Test-set features:\n","  #samples: 873\n","  feature dimension: 220\n","\n","------------------------------------------------------------\n","6. Feature standardization (train-only stats)\n","------------------------------------------------------------\n","Scaler parameters (train set):\n","  mean range: [-2.4757, 5321.1833]\n","  std range:  [0.0317, 6128.3903]\n","\n","✓ Standardized test set using train-set statistics\n","✓ Features cast to: float32\n","\n","------------------------------------------------------------\n","7. Save feature data (with metadata)\n","------------------------------------------------------------\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/features/fold_03/train_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_03/train_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_03/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/features/fold_03/test_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_03/test_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_03/test_meta.parquet\n","✓ Scaler: data/lara/mbientlab/proc/features/fold_03/scaler.pkl\n","\n","------------------------------------------------------------\n","8. Feature type stats (exact counts, revised)\n","------------------------------------------------------------\n","\n","Feature counts (exact) for fold_03:\n","  Total: 220\n","  Time-domain:      128\n","  Frequency-domain: 56\n","  Correlation:      36\n","    - Autocorr: 8\n","    - Crosscorr: 28 (deduplicated)\n","✓ Count verification: 128 + 56 + 36 = 220\n","\n","------------------------------------------------------------\n","9. Save feature config (named by fold)\n","------------------------------------------------------------\n","✓ Saved config: configs/features_fold_03.yaml\n","✓ Saved config: configs/features_fold_03.json\n","✓ Saved feature names: data/lara/mbientlab/proc/features/fold_03/feature_names.txt\n","\n","------------------------------------------------------------\n","10. Train feature statistics (standardized)\n","------------------------------------------------------------\n","\n","Training-set feature stats (standardized, fold_03):\n","                  mean       std        min       max\n","ax_mean   0.000000e+00  1.000102 -14.766671  6.127156\n","ax_std    0.000000e+00  1.000103  -1.768069  4.642880\n","ax_min   -1.256384e-08  1.000107  -1.287500  2.075149\n","ax_max    1.256384e-08  1.000100  -1.946005  1.343034\n","ax_range  0.000000e+00  1.000102  -2.156586  1.412719\n","ax_q25    0.000000e+00  1.000104  -7.787620  1.744398\n","ax_q50   -7.852403e-10  1.000103 -13.990578  6.562627\n","ax_q75    0.000000e+00  1.000103  -4.510034  9.377128\n","ax_iqr    0.000000e+00  1.000102  -1.450560  8.272748\n","ax_mad    1.256384e-08  1.000102  -1.453880  8.145726\n","\n","------------------------------------------------------------\n","11. Summary for fold fold_03\n","------------------------------------------------------------\n","\n","Feature extraction:\n","  Method: handcrafted (time + frequency + correlation)\n","  Total dims:      220\n","  Time-domain:     128 dims\n","  Frequency-domain:56 dims\n","  Correlation:     36 dims\n","\n","Dataset:\n","  Fold:           fold_03\n","  Train samples:  4,858\n","  Test samples:   873\n","\n","Preprocessing:\n","  Standardization: StandardScaler (fit on train only)\n","  Invalid values:  NaN/Inf filled with 0\n","  Dtype:           float32\n","\n","Outputs (fold_03):\n","  Dir:    data/lara/mbientlab/proc/features/fold_03/\n","  Train:  train_X.parquet, train_y.parquet, train_meta.parquet\n","  Test:   test_X.parquet, test_y.parquet, test_meta.parquet\n","  Scaler: scaler.pkl\n","  Config: features_fold_03.yaml\n","  Feature list: feature_names.txt\n","\n","============================================================\n","2. Processing fold: fold_04\n","============================================================\n","Windows dir:  data/lara/mbientlab/proc/windows/fold_04\n","Features dir: data/lara/mbientlab/proc/features/fold_04\n","\n","------------------------------------------------------------\n","3. Load window data & metadata\n","------------------------------------------------------------\n","\n","Train windows:\n","  X_train shape: (4985, 150, 8)\n","  y_train shape: (4985,)\n","  metadata:      4985 rows\n","  ✓ Consistency check passed\n","\n","Test windows:\n","  X_test shape: (746, 150, 8)\n","  y_test shape: (746,)\n","  metadata:     746 rows\n","  ✓ Consistency check passed\n","\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test subjects:  1\n","\n","------------------------------------------------------------\n","4. Extract training-set features\n","------------------------------------------------------------\n","Processing 4,985 training windows...\n","  Processed 1,000 / 4,985 windows\n","  Processed 2,000 / 4,985 windows\n","  Processed 3,000 / 4,985 windows\n","  Processed 4,000 / 4,985 windows\n","\n","✓ Training-set features:\n","  #samples: 4,985\n","  feature dimension: 220\n","\n","Sample feature names (first 10):\n","  - ax_mean\n","  - ax_std\n","  - ax_min\n","  - ax_max\n","  - ax_range\n","  - ax_q25\n","  - ax_q50\n","  - ax_q75\n","  - ax_iqr\n","  - ax_mad\n","\n","Checking low-variance features (raw):\n","  ✓ All feature variances look OK\n","\n","------------------------------------------------------------\n","5. Extract test-set features\n","------------------------------------------------------------\n","Processing 746 test windows...\n","\n","✓ Test-set features:\n","  #samples: 746\n","  feature dimension: 220\n","\n","------------------------------------------------------------\n","6. Feature standardization (train-only stats)\n","------------------------------------------------------------\n","Scaler parameters (train set):\n","  mean range: [-2.5589, 5222.4780]\n","  std range:  [0.0316, 6126.2452]\n","\n","✓ Standardized test set using train-set statistics\n","✓ Features cast to: float32\n","\n","------------------------------------------------------------\n","7. Save feature data (with metadata)\n","------------------------------------------------------------\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/features/fold_04/train_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_04/train_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_04/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/features/fold_04/test_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_04/test_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_04/test_meta.parquet\n","✓ Scaler: data/lara/mbientlab/proc/features/fold_04/scaler.pkl\n","\n","------------------------------------------------------------\n","8. Feature type stats (exact counts, revised)\n","------------------------------------------------------------\n","\n","Feature counts (exact) for fold_04:\n","  Total: 220\n","  Time-domain:      128\n","  Frequency-domain: 56\n","  Correlation:      36\n","    - Autocorr: 8\n","    - Crosscorr: 28 (deduplicated)\n","✓ Count verification: 128 + 56 + 36 = 220\n","\n","------------------------------------------------------------\n","9. Save feature config (named by fold)\n","------------------------------------------------------------\n","✓ Saved config: configs/features_fold_04.yaml\n","✓ Saved config: configs/features_fold_04.json\n","✓ Saved feature names: data/lara/mbientlab/proc/features/fold_04/feature_names.txt\n","\n","------------------------------------------------------------\n","10. Train feature statistics (standardized)\n","------------------------------------------------------------\n","\n","Training-set feature stats (standardized, fold_04):\n","                  mean       std        min       max\n","ax_mean   0.000000e+00  1.000100 -10.293209  6.280050\n","ax_std   -6.121881e-09  1.000100  -1.906931  4.760089\n","ax_min    1.224376e-08  1.000101  -1.252809  2.210406\n","ax_max    0.000000e+00  1.000100  -2.076604  1.332540\n","ax_range -6.121881e-09  1.000101  -2.316359  1.398646\n","ax_q25    0.000000e+00  1.000101  -8.129154  1.875501\n","ax_q50    0.000000e+00  1.000100  -8.452626  6.556899\n","ax_q75   -1.224376e-08  1.000099  -2.165071  9.574411\n","ax_iqr    0.000000e+00  1.000099  -1.564367  8.534094\n","ax_mad    0.000000e+00  1.000100  -1.573568  8.447431\n","\n","------------------------------------------------------------\n","11. Summary for fold fold_04\n","------------------------------------------------------------\n","\n","Feature extraction:\n","  Method: handcrafted (time + frequency + correlation)\n","  Total dims:      220\n","  Time-domain:     128 dims\n","  Frequency-domain:56 dims\n","  Correlation:     36 dims\n","\n","Dataset:\n","  Fold:           fold_04\n","  Train samples:  4,985\n","  Test samples:   746\n","\n","Preprocessing:\n","  Standardization: StandardScaler (fit on train only)\n","  Invalid values:  NaN/Inf filled with 0\n","  Dtype:           float32\n","\n","Outputs (fold_04):\n","  Dir:    data/lara/mbientlab/proc/features/fold_04/\n","  Train:  train_X.parquet, train_y.parquet, train_meta.parquet\n","  Test:   test_X.parquet, test_y.parquet, test_meta.parquet\n","  Scaler: scaler.pkl\n","  Config: features_fold_04.yaml\n","  Feature list: feature_names.txt\n","\n","============================================================\n","2. Processing fold: fold_05\n","============================================================\n","Windows dir:  data/lara/mbientlab/proc/windows/fold_05\n","Features dir: data/lara/mbientlab/proc/features/fold_05\n","\n","------------------------------------------------------------\n","3. Load window data & metadata\n","------------------------------------------------------------\n","\n","Train windows:\n","  X_train shape: (5442, 150, 8)\n","  y_train shape: (5442,)\n","  metadata:      5442 rows\n","  ✓ Consistency check passed\n","\n","Test windows:\n","  X_test shape: (289, 150, 8)\n","  y_test shape: (289,)\n","  metadata:     289 rows\n","  ✓ Consistency check passed\n","\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test subjects:  1\n","\n","------------------------------------------------------------\n","4. Extract training-set features\n","------------------------------------------------------------\n","Processing 5,442 training windows...\n","  Processed 1,000 / 5,442 windows\n","  Processed 2,000 / 5,442 windows\n","  Processed 3,000 / 5,442 windows\n","  Processed 4,000 / 5,442 windows\n","  Processed 5,000 / 5,442 windows\n","\n","✓ Training-set features:\n","  #samples: 5,442\n","  feature dimension: 220\n","\n","Sample feature names (first 10):\n","  - ax_mean\n","  - ax_std\n","  - ax_min\n","  - ax_max\n","  - ax_range\n","  - ax_q25\n","  - ax_q50\n","  - ax_q75\n","  - ax_iqr\n","  - ax_mad\n","\n","Checking low-variance features (raw):\n","  ✓ All feature variances look OK\n","\n","------------------------------------------------------------\n","5. Extract test-set features\n","------------------------------------------------------------\n","Processing 289 test windows...\n","\n","✓ Test-set features:\n","  #samples: 289\n","  feature dimension: 220\n","\n","------------------------------------------------------------\n","6. Feature standardization (train-only stats)\n","------------------------------------------------------------\n","Scaler parameters (train set):\n","  mean range: [-2.4452, 5169.8673]\n","  std range:  [0.0312, 5954.7184]\n","\n","✓ Standardized test set using train-set statistics\n","✓ Features cast to: float32\n","\n","------------------------------------------------------------\n","7. Save feature data (with metadata)\n","------------------------------------------------------------\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/features/fold_05/train_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_05/train_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_05/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/features/fold_05/test_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_05/test_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_05/test_meta.parquet\n","✓ Scaler: data/lara/mbientlab/proc/features/fold_05/scaler.pkl\n","\n","------------------------------------------------------------\n","8. Feature type stats (exact counts, revised)\n","------------------------------------------------------------\n","\n","Feature counts (exact) for fold_05:\n","  Total: 220\n","  Time-domain:      128\n","  Frequency-domain: 56\n","  Correlation:      36\n","    - Autocorr: 8\n","    - Crosscorr: 28 (deduplicated)\n","✓ Count verification: 128 + 56 + 36 = 220\n","\n","------------------------------------------------------------\n","9. Save feature config (named by fold)\n","------------------------------------------------------------\n","✓ Saved config: configs/features_fold_05.yaml\n","✓ Saved config: configs/features_fold_05.json\n","✓ Saved feature names: data/lara/mbientlab/proc/features/fold_05/feature_names.txt\n","\n","------------------------------------------------------------\n","10. Train feature statistics (standardized)\n","------------------------------------------------------------\n","\n","Training-set feature stats (standardized, fold_05):\n","                  mean       std        min       max\n","ax_mean   7.009734e-10  1.000092 -15.316504  5.749540\n","ax_std    0.000000e+00  1.000091  -1.783584  4.776817\n","ax_min    2.243115e-08  1.000096  -1.319837  2.058828\n","ax_max   -2.243115e-08  1.000093  -1.931526  1.382036\n","ax_range  0.000000e+00  1.000093  -2.142370  1.452528\n","ax_q25    0.000000e+00  1.000092  -8.074915  1.777462\n","ax_q50    7.009734e-10  1.000091 -14.569479  6.798544\n","ax_q75    0.000000e+00  1.000092  -4.642829  9.704402\n","ax_iqr    1.121557e-08  1.000094  -1.475706  8.569535\n","ax_mad    1.121557e-08  1.000092  -1.473538  8.406415\n","\n","------------------------------------------------------------\n","11. Summary for fold fold_05\n","------------------------------------------------------------\n","\n","Feature extraction:\n","  Method: handcrafted (time + frequency + correlation)\n","  Total dims:      220\n","  Time-domain:     128 dims\n","  Frequency-domain:56 dims\n","  Correlation:     36 dims\n","\n","Dataset:\n","  Fold:           fold_05\n","  Train samples:  5,442\n","  Test samples:   289\n","\n","Preprocessing:\n","  Standardization: StandardScaler (fit on train only)\n","  Invalid values:  NaN/Inf filled with 0\n","  Dtype:           float32\n","\n","Outputs (fold_05):\n","  Dir:    data/lara/mbientlab/proc/features/fold_05/\n","  Train:  train_X.parquet, train_y.parquet, train_meta.parquet\n","  Test:   test_X.parquet, test_y.parquet, test_meta.parquet\n","  Scaler: scaler.pkl\n","  Config: features_fold_05.yaml\n","  Feature list: feature_names.txt\n","\n","============================================================\n","2. Processing fold: fold_06\n","============================================================\n","Windows dir:  data/lara/mbientlab/proc/windows/fold_06\n","Features dir: data/lara/mbientlab/proc/features/fold_06\n","\n","------------------------------------------------------------\n","3. Load window data & metadata\n","------------------------------------------------------------\n","\n","Train windows:\n","  X_train shape: (4853, 150, 8)\n","  y_train shape: (4853,)\n","  metadata:      4853 rows\n","  ✓ Consistency check passed\n","\n","Test windows:\n","  X_test shape: (878, 150, 8)\n","  y_test shape: (878,)\n","  metadata:     878 rows\n","  ✓ Consistency check passed\n","\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test subjects:  1\n","\n","------------------------------------------------------------\n","4. Extract training-set features\n","------------------------------------------------------------\n","Processing 4,853 training windows...\n","  Processed 1,000 / 4,853 windows\n","  Processed 2,000 / 4,853 windows\n","  Processed 3,000 / 4,853 windows\n","  Processed 4,000 / 4,853 windows\n","\n","✓ Training-set features:\n","  #samples: 4,853\n","  feature dimension: 220\n","\n","Sample feature names (first 10):\n","  - ax_mean\n","  - ax_std\n","  - ax_min\n","  - ax_max\n","  - ax_range\n","  - ax_q25\n","  - ax_q50\n","  - ax_q75\n","  - ax_iqr\n","  - ax_mad\n","\n","Checking low-variance features (raw):\n","  ✓ All feature variances look OK\n","\n","------------------------------------------------------------\n","5. Extract test-set features\n","------------------------------------------------------------\n","Processing 878 test windows...\n","\n","✓ Test-set features:\n","  #samples: 878\n","  feature dimension: 220\n","\n","------------------------------------------------------------\n","6. Feature standardization (train-only stats)\n","------------------------------------------------------------\n","Scaler parameters (train set):\n","  mean range: [-2.4534, 5011.9481]\n","  std range:  [0.0314, 6069.1681]\n","\n","✓ Standardized test set using train-set statistics\n","✓ Features cast to: float32\n","\n","------------------------------------------------------------\n","7. Save feature data (with metadata)\n","------------------------------------------------------------\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/features/fold_06/train_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_06/train_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_06/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/features/fold_06/test_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_06/test_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_06/test_meta.parquet\n","✓ Scaler: data/lara/mbientlab/proc/features/fold_06/scaler.pkl\n","\n","------------------------------------------------------------\n","8. Feature type stats (exact counts, revised)\n","------------------------------------------------------------\n","\n","Feature counts (exact) for fold_06:\n","  Total: 220\n","  Time-domain:      128\n","  Frequency-domain: 56\n","  Correlation:      36\n","    - Autocorr: 8\n","    - Crosscorr: 28 (deduplicated)\n","✓ Count verification: 128 + 56 + 36 = 220\n","\n","------------------------------------------------------------\n","9. Save feature config (named by fold)\n","------------------------------------------------------------\n","✓ Saved config: configs/features_fold_06.yaml\n","✓ Saved config: configs/features_fold_06.json\n","✓ Saved feature names: data/lara/mbientlab/proc/features/fold_06/feature_names.txt\n","\n","------------------------------------------------------------\n","10. Train feature statistics (standardized)\n","------------------------------------------------------------\n","\n","Training-set feature stats (standardized, fold_06):\n","                  mean       std        min       max\n","ax_mean   2.063379e-09  1.000103 -15.094825  6.266769\n","ax_std    0.000000e+00  1.000103  -1.805886  4.789556\n","ax_min    0.000000e+00  1.000102  -1.324800  2.084815\n","ax_max    0.000000e+00  1.000106  -1.957317  1.407256\n","ax_range  2.515358e-08  1.000104  -2.172825  1.469073\n","ax_q25    0.000000e+00  1.000103  -7.892141  1.762524\n","ax_q50    0.000000e+00  1.000103 -14.151514  6.617874\n","ax_q75    1.257679e-08  1.000103  -4.682328  9.741939\n","ax_iqr    0.000000e+00  1.000104  -1.486318  8.500404\n","ax_mad    1.257679e-08  1.000103  -1.486236  8.367360\n","\n","------------------------------------------------------------\n","11. Summary for fold fold_06\n","------------------------------------------------------------\n","\n","Feature extraction:\n","  Method: handcrafted (time + frequency + correlation)\n","  Total dims:      220\n","  Time-domain:     128 dims\n","  Frequency-domain:56 dims\n","  Correlation:     36 dims\n","\n","Dataset:\n","  Fold:           fold_06\n","  Train samples:  4,853\n","  Test samples:   878\n","\n","Preprocessing:\n","  Standardization: StandardScaler (fit on train only)\n","  Invalid values:  NaN/Inf filled with 0\n","  Dtype:           float32\n","\n","Outputs (fold_06):\n","  Dir:    data/lara/mbientlab/proc/features/fold_06/\n","  Train:  train_X.parquet, train_y.parquet, train_meta.parquet\n","  Test:   test_X.parquet, test_y.parquet, test_meta.parquet\n","  Scaler: scaler.pkl\n","  Config: features_fold_06.yaml\n","  Feature list: feature_names.txt\n","\n","============================================================\n","2. Processing fold: fold_07\n","============================================================\n","Windows dir:  data/lara/mbientlab/proc/windows/fold_07\n","Features dir: data/lara/mbientlab/proc/features/fold_07\n","\n","------------------------------------------------------------\n","3. Load window data & metadata\n","------------------------------------------------------------\n","\n","Train windows:\n","  X_train shape: (4982, 150, 8)\n","  y_train shape: (4982,)\n","  metadata:      4982 rows\n","  ✓ Consistency check passed\n","\n","Test windows:\n","  X_test shape: (749, 150, 8)\n","  y_test shape: (749,)\n","  metadata:     749 rows\n","  ✓ Consistency check passed\n","\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test subjects:  1\n","\n","------------------------------------------------------------\n","4. Extract training-set features\n","------------------------------------------------------------\n","Processing 4,982 training windows...\n","  Processed 1,000 / 4,982 windows\n","  Processed 2,000 / 4,982 windows\n","  Processed 3,000 / 4,982 windows\n","  Processed 4,000 / 4,982 windows\n","\n","✓ Training-set features:\n","  #samples: 4,982\n","  feature dimension: 220\n","\n","Sample feature names (first 10):\n","  - ax_mean\n","  - ax_std\n","  - ax_min\n","  - ax_max\n","  - ax_range\n","  - ax_q25\n","  - ax_q50\n","  - ax_q75\n","  - ax_iqr\n","  - ax_mad\n","\n","Checking low-variance features (raw):\n","  ✓ All feature variances look OK\n","\n","------------------------------------------------------------\n","5. Extract test-set features\n","------------------------------------------------------------\n","Processing 749 test windows...\n","\n","✓ Test-set features:\n","  #samples: 749\n","  feature dimension: 220\n","\n","------------------------------------------------------------\n","6. Feature standardization (train-only stats)\n","------------------------------------------------------------\n","Scaler parameters (train set):\n","  mean range: [-2.4813, 5299.1593]\n","  std range:  [0.0304, 6142.6317]\n","\n","✓ Standardized test set using train-set statistics\n","✓ Features cast to: float32\n","\n","------------------------------------------------------------\n","7. Save feature data (with metadata)\n","------------------------------------------------------------\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/features/fold_07/train_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_07/train_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_07/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/features/fold_07/test_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_07/test_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_07/test_meta.parquet\n","✓ Scaler: data/lara/mbientlab/proc/features/fold_07/scaler.pkl\n","\n","------------------------------------------------------------\n","8. Feature type stats (exact counts, revised)\n","------------------------------------------------------------\n","\n","Feature counts (exact) for fold_07:\n","  Total: 220\n","  Time-domain:      128\n","  Frequency-domain: 56\n","  Correlation:      36\n","    - Autocorr: 8\n","    - Crosscorr: 28 (deduplicated)\n","✓ Count verification: 128 + 56 + 36 = 220\n","\n","------------------------------------------------------------\n","9. Save feature config (named by fold)\n","------------------------------------------------------------\n","✓ Saved config: configs/features_fold_07.yaml\n","✓ Saved config: configs/features_fold_07.json\n","✓ Saved feature names: data/lara/mbientlab/proc/features/fold_07/feature_names.txt\n","\n","------------------------------------------------------------\n","10. Train feature statistics (standardized)\n","------------------------------------------------------------\n","\n","Training-set feature stats (standardized, fold_07):\n","                  mean       std        min       max\n","ax_mean  -9.571199e-10  1.000100 -14.825350  6.153957\n","ax_std   -1.225114e-08  1.000100  -1.812426  4.662156\n","ax_min    0.000000e+00  1.000104  -1.300499  2.108643\n","ax_max    1.225114e-08  1.000101  -1.960191  1.363585\n","ax_range  0.000000e+00  1.000101  -2.182723  1.431348\n","ax_q25   -6.125568e-09  1.000100  -7.786971  1.781375\n","ax_q50    3.062784e-09  1.000102 -13.866883  6.482078\n","ax_q75    1.225114e-08  1.000100  -4.608577  9.474944\n","ax_iqr   -1.225114e-08  1.000101  -1.500025  8.319974\n","ax_mad    0.000000e+00  1.000099  -1.500909  8.183527\n","\n","------------------------------------------------------------\n","11. Summary for fold fold_07\n","------------------------------------------------------------\n","\n","Feature extraction:\n","  Method: handcrafted (time + frequency + correlation)\n","  Total dims:      220\n","  Time-domain:     128 dims\n","  Frequency-domain:56 dims\n","  Correlation:     36 dims\n","\n","Dataset:\n","  Fold:           fold_07\n","  Train samples:  4,982\n","  Test samples:   749\n","\n","Preprocessing:\n","  Standardization: StandardScaler (fit on train only)\n","  Invalid values:  NaN/Inf filled with 0\n","  Dtype:           float32\n","\n","Outputs (fold_07):\n","  Dir:    data/lara/mbientlab/proc/features/fold_07/\n","  Train:  train_X.parquet, train_y.parquet, train_meta.parquet\n","  Test:   test_X.parquet, test_y.parquet, test_meta.parquet\n","  Scaler: scaler.pkl\n","  Config: features_fold_07.yaml\n","  Feature list: feature_names.txt\n","\n","============================================================\n","Step 11 complete - Feature Engineering for all folds\n","============================================================\n","Next steps per fold:\n","  - Use train_X.parquet / train_y.parquet to train classical machine-learning models (e.g., KNN, Random Forest, SVM)\n","  - Use test_X.parquet / test_y.parquet to evaluate model performance\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 12: Normalization (feature/deep) - top-conf/journal grade\n","\n","Classical models: features already standardized in Step 11\n","Deep models: per-channel z-score on windowed data (train-set statistics only)\n","Multi-fold version: loop over all folds in configs/splits.json\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import pickle\n","import json\n","import yaml\n","import os\n","import hashlib\n","\n","# ========== Config ==========\n","EPSILON = 1e-8  # avoid divide-by-zero\n","\n","print(\"=\" * 60)\n","print(\"Step 12: Normalization (feature/deep) - multi-fold\")\n","print(\"=\" * 60)\n","\n","# Path config\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","configs_dir.mkdir(parents=True, exist_ok=True)\n","\n","windows_root = proc_dir / \"windows\"\n","scalers_root = proc_dir / \"scalers\"\n","scalers_root.mkdir(parents=True, exist_ok=True)\n","\n","# ========== 0. Determine folds ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"0. Determine folds\")\n","print(\"=\" * 60)\n","\n","splits_path = configs_dir / \"splits.json\"\n","if splits_path.exists():\n","    with open(splits_path, \"r\") as f:\n","        splits = json.load(f)\n","    fold_ids = sorted(int(k) for k in splits.keys())\n","    print(f\"Detected {len(fold_ids)} folds from {splits_path}: {fold_ids}\")\n","else:\n","    # Under strict LOSO protocol, splits.json should exist; this is a conservative fallback\n","    print(\"⚠️ splits.json not found; normalization for deep models expects LOSO folds.\")\n","    fold_ids = [0]  # Fallback (e.g., when you only have a single fold); ideally, splits.json should be present\n","\n","# ========== 0.1 Load channel config ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"0.1 Load channel config\")\n","print(\"=\" * 60)\n","\n","channels_config_file = configs_dir / \"channels.yaml\"\n","if channels_config_file.exists():\n","    with open(channels_config_file, 'r', encoding='utf-8') as f:\n","        channels_config = yaml.safe_load(f)\n","    channel_names = channels_config['final_channels']\n","    print(f\"✓ Channels read from config: {channel_names}\")\n","else:\n","    channel_names = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","    print(f\"⚠️ channels.yaml not found; using default: {channel_names}\")\n","\n","n_channels_config = len(channel_names)\n","\n","# ========== Helper: SHA256 ==========\n","def _sha256(p: Path):\n","    h = hashlib.sha256()\n","    with open(p, \"rb\") as f:\n","        h.update(f.read())\n","    return h.hexdigest()\n","\n","def _check_finite(name, arr):\n","    if not np.isfinite(arr).all():\n","        bad = np.logical_not(np.isfinite(arr))\n","        nbad = int(bad.sum())\n","        raise ValueError(f\"{name} contains NaN/Inf, total: {nbad}\")\n","\n","# ========== Loop over folds ==========\n","for fold_id in fold_ids:\n","    fold_tag = f\"fold_{fold_id:02d}\"\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(f\"Processing fold: {fold_tag}\")\n","    print(\"=\" * 60)\n","\n","    windows_dir = windows_root / fold_tag\n","    scalers_dir = scalers_root / fold_tag\n","    scalers_dir.mkdir(parents=True, exist_ok=True)\n","\n","    print(f\"Input dir (windows):  {windows_dir}\")\n","    print(f\"Output dir (scalers): {scalers_dir}\")\n","\n","    if not windows_dir.exists():\n","        print(f\"⚠️ Windows dir not found for {fold_tag}: {windows_dir}, skipping this fold.\")\n","        continue\n","\n","    # ========== 1. Classical models: confirm features already standardized ==========\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"1. Classical models: confirm features already standardized\")\n","    print(\"=\" * 60)\n","\n","    features_dir = proc_dir / \"features\" / fold_tag\n","    feature_scaler_file = features_dir / \"scaler.pkl\"\n","\n","    if feature_scaler_file.exists():\n","        with open(feature_scaler_file, 'rb') as f:\n","            feature_scaler = pickle.load(f)\n","        print(f\"✓ Feature scaler was generated in Step 11: {feature_scaler_file}\")\n","        print(f\"  StandardScaler parameters:\")\n","        print(f\"  - #features: {len(feature_scaler.mean_)}\")\n","        print(f\"  - mean range: [{feature_scaler.mean_.min():.4f}, {feature_scaler.mean_.max():.4f}]\")\n","        print(f\"  - std  range: [{feature_scaler.scale_.min():.4f}, {feature_scaler.scale_.max():.4f}]\")\n","    else:\n","        print(f\"⚠️ Feature scaler not found for {fold_tag}: {feature_scaler_file}\")\n","        print(f\"   Classical (KNN/RF) features for this fold may not yet have been processed in Step 11\")\n","\n","    # ========== 2. Deep models: load window data ==========\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"2. Deep models: load window data\")\n","    print(\"=\" * 60)\n","\n","    # Train set\n","    X_train_file = windows_dir / \"X_train.npy\"\n","    y_train_file = windows_dir / \"y_train.npy\"\n","    train_meta_file = windows_dir / \"X_train.parquet\"\n","\n","    if not X_train_file.exists():\n","        print(f\"⚠️ Training windows not found for {fold_tag}: {X_train_file}, skipping this fold.\")\n","        continue\n","\n","    X_train = np.load(X_train_file)\n","    y_train = np.load(y_train_file)\n","    df_train_meta = pd.read_parquet(train_meta_file)\n","\n","    print(f\"\\nTrain set:\")\n","    print(f\"  X_train shape: {X_train.shape}, dtype={X_train.dtype}\")\n","\n","    # Test set\n","    X_test_file = windows_dir / \"X_test.npy\"\n","    y_test_file = windows_dir / \"y_test.npy\"\n","    test_meta_file = windows_dir / \"X_test.parquet\"\n","\n","    has_test = X_test_file.exists()\n","    if has_test:\n","        X_test = np.load(X_test_file)\n","        y_test = np.load(y_test_file)\n","        df_test_meta = pd.read_parquet(test_meta_file)\n","        print(f\"\\nTest set:\")\n","        print(f\"  X_test shape:  {X_test.shape}, dtype={X_test.dtype}\")\n","    else:\n","        print(f\"\\nTest set not found for {fold_tag}; train-only mode for deep normalization.\")\n","        X_test = None\n","        y_test = None\n","        df_test_meta = None\n","\n","    # ========== 2.1. Numerical robustness self-check ==========\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"2.1. Numerical robustness self-check\")\n","    print(\"=\" * 60)\n","\n","    _check_finite(\"X_train\", X_train)\n","    print(f\"✓ X_train has no NaN/Inf\")\n","\n","    if has_test:\n","        _check_finite(\"X_test\", X_test)\n","        print(f\"✓ X_test has no NaN/Inf\")\n","\n","    # ========== 2.2. Anti-leakage & consistency checks ==========\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"2.2. Anti-leakage & consistency checks\")\n","    print(\"=\" * 60)\n","\n","    # Channel count\n","    n_channels = X_train.shape[2]\n","    assert n_channels == n_channels_config, \\\n","        f\"[{fold_tag}] Channel count mismatch: data {n_channels} vs config {n_channels_config}\"\n","    print(f\"✓ Channel count consistent: {n_channels}\")\n","\n","    # Validate channel order (sidecar)\n","    channels_sidecar = windows_dir / \"channels.json\"\n","    if channels_sidecar.exists():\n","        with open(channels_sidecar, \"r\", encoding=\"utf-8\") as f:\n","            side = json.load(f)[\"channel_names\"]\n","        assert side == channel_names, \\\n","            f\"[{fold_tag}] Channel order inconsistent:\\n  windows={side}\\n  config ={channel_names}\"\n","        print(f\"✓ Channel order consistent: {channel_names}\")\n","    else:\n","        print(f\"⚠️ channels.json sidecar not found in {windows_dir}; only checking channel count\")\n","\n","    # Train/Test subjects\n","    train_subjects = set(df_train_meta['subject_id'].unique())\n","    if has_test:\n","        test_subjects = set(df_test_meta['subject_id'].unique())\n","        assert train_subjects.isdisjoint(test_subjects), \\\n","            f\"[{fold_tag}] Train/test subjects overlap {train_subjects & test_subjects}, violates LOSO!\"\n","        assert len(test_subjects) == 1, \\\n","            f\"[{fold_tag}] LOSO test set should contain exactly 1 subject; got: {len(test_subjects)}\"\n","        print(f\"✓ Anti-leakage check passed:\")\n","        print(f\"  Train subjects: {len(train_subjects)}\")\n","        print(f\"  Test  subjects: {len(test_subjects)} {test_subjects}\")\n","        print(f\"  Subject sets disjoint: True\")\n","    else:\n","        test_subjects = set()\n","        print(f\"✓ Train-only mode (train subjects: {len(train_subjects)})\")\n","\n","    # ========== 3. Compute channel-wise statistics (train only) ==========\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"3. Compute channel-wise statistics (train only)\")\n","    print(\"=\" * 60)\n","\n","    print(f\"\\n#channels: {n_channels}\")\n","    print(f\"Channel names: {channel_names}\")\n","\n","    # Check whether data already appears z-scored to avoid double-normalization\n","    probe_mean = np.mean(X_train, axis=(0, 1))\n","    probe_std = np.std(X_train, axis=(0, 1))\n","    already_z = (np.all(np.abs(probe_mean) < 1e-3) and\n","                 np.all(np.abs(probe_std - 1.0) < 1e-2))\n","\n","    if already_z:\n","        print(f\"\\n⚠️ Detected X_train appears already z-scored (mean≈0, std≈1); skipping normalization.\")\n","        print(f\"  Probed mean range: [{probe_mean.min():.4f}, {probe_mean.max():.4f}]\")\n","        print(f\"  Probed std  range: [{probe_std.min():.4f}, {probe_std.max():.4f}]\")\n","\n","        X_train_scaled = X_train.astype('float32', copy=True)\n","        if has_test:\n","            X_test_scaled = X_test.astype('float32', copy=True)\n","\n","        channel_mean = probe_mean\n","        channel_std = probe_std\n","        skip_normalization = True\n","        print(f\"  ✓ Skipped normalization; copied as float32\")\n","    else:\n","        skip_normalization = False\n","        channel_mean = np.mean(X_train, axis=(0, 1))\n","        channel_std = np.std(X_train, axis=(0, 1))\n","        channel_std = np.maximum(channel_std, EPSILON)\n","\n","        print(f\"\\nTrain-set channel statistics:\")\n","        for i, ch_name in enumerate(channel_names):\n","            print(f\"  {ch_name}: mean={channel_mean[i]:7.4f}, std={channel_std[i]:7.4f}\")\n","\n","    # ========== 4. Apply channel-wise z-score normalization ==========\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"4. Apply channel-wise z-score normalization\")\n","    print(\"=\" * 60)\n","\n","    if not skip_normalization:\n","        X_train_scaled = (X_train - channel_mean) / channel_std\n","        print(f\"\\nTrain normalization:\")\n","        print(f\"  input:  {X_train.shape}, {X_train.dtype}\")\n","        print(f\"  output: {X_train_scaled.shape}, {X_train_scaled.dtype}\")\n","\n","        # Verify\n","        train_scaled_mean = np.mean(X_train_scaled, axis=(0, 1))\n","        train_scaled_std = np.std(X_train_scaled, axis=(0, 1))\n","        print(f\"\\nTrain-set stats after normalization (should be ~0 and ~1):\")\n","        for i, ch_name in enumerate(channel_names):\n","            print(f\"  {ch_name}: mean={train_scaled_mean[i]:7.4f}, std={train_scaled_std[i]:7.4f}\")\n","\n","        if has_test:\n","            X_test_scaled = (X_test - channel_mean) / channel_std\n","            print(f\"\\nTest normalization (using train statistics):\")\n","            print(f\"  input:  {X_test.shape}, {X_test.dtype}\")\n","            print(f\"  output: {X_test_scaled.shape}, {X_test_scaled.dtype}\")\n","\n","            test_scaled_mean = np.mean(X_test_scaled, axis=(0, 1))\n","            test_scaled_std = np.std(X_test_scaled, axis=(0, 1))\n","            print(f\"\\nTest-set stats after normalization (train params; not 0/1):\")\n","            for i, ch_name in enumerate(channel_names):\n","                print(f\"  {ch_name}: mean={test_scaled_mean[i]:7.4f}, std={test_scaled_std[i]:7.4f}\")\n","    else:\n","        print(f\"\\n✓ Skipped normalization (data already normalized)\")\n","        print(f\"  Train: {X_train_scaled.shape}\")\n","        if has_test:\n","            print(f\"  Test:  {X_test_scaled.shape}\")\n","\n","    # ========== 5. Save normalized data ==========\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"5. Save normalized data\")\n","    print(\"=\" * 60)\n","\n","    X_train_scaled_file = scalers_dir / \"X_train_scaled.npy\"\n","    y_train_scaled_file = scalers_dir / \"y_train.npy\"\n","    train_meta_scaled_file = scalers_dir / \"train_meta.parquet\"\n","\n","    np.save(X_train_scaled_file, X_train_scaled.astype('float32'))\n","    np.save(y_train_scaled_file, y_train)\n","    df_train_meta.to_parquet(train_meta_scaled_file, index=False)\n","\n","    print(f\"✓ Train set:\")\n","    print(f\"  features: {X_train_scaled_file}\")\n","    print(f\"  labels:   {y_train_scaled_file}\")\n","    print(f\"  metadata: {train_meta_scaled_file}\")\n","\n","    if has_test:\n","        X_test_scaled_file = scalers_dir / \"X_test_scaled.npy\"\n","        y_test_scaled_file = scalers_dir / \"y_test.npy\"\n","        test_meta_scaled_file = scalers_dir / \"test_meta.parquet\"\n","\n","        np.save(X_test_scaled_file, X_test_scaled.astype('float32'))\n","        np.save(y_test_scaled_file, y_test)\n","        df_test_meta.to_parquet(test_meta_scaled_file, index=False)\n","\n","        print(f\"✓ Test set:\")\n","        print(f\"  features: {X_test_scaled_file}\")\n","        print(f\"  labels:   {y_test_scaled_file}\")\n","        print(f\"  metadata: {test_meta_scaled_file}\")\n","\n","    # ========== 6. Save scaler parameters (with traceability) ==========\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"6. Save scaler parameters (with traceability)\")\n","    print(\"=\" * 60)\n","\n","    print(f\"\\nComputing input file SHA256...\")\n","    x_train_sha = _sha256(X_train_file)\n","    print(f\"  X_train: {x_train_sha[:16]}...\")\n","    if has_test:\n","        x_test_sha = _sha256(X_test_file)\n","        print(f\"  X_test:  {x_test_sha[:16]}...\")\n","    else:\n","        x_test_sha = None\n","\n","    channel_scaler = {\n","        'fold_id': fold_id,\n","        'fold_tag': fold_tag,\n","        'epsilon': EPSILON,\n","        'n_channels': int(n_channels),\n","        'channel_names': channel_names,\n","        'channel_mean': channel_mean.tolist(),\n","        'channel_std': channel_std.tolist(),\n","        'train_subjects': sorted(list(train_subjects)),\n","        'test_subjects': sorted(list(test_subjects)) if has_test else [],\n","        'formula': '(x - channel_mean) / max(channel_std, ε)' if not skip_normalization else 'identity (already normalized)',\n","        'method': 'channel-wise z-score with ε floor' if not skip_normalization else 'skip (data already normalized)',\n","        'skip_normalization': bool(skip_normalization),\n","        'input_files': {\n","            'x_train_file': str(X_train_file.name),\n","            'x_train_sha256': x_train_sha,\n","            'x_test_file': str(X_test_file.name) if has_test else None,\n","            'x_test_sha256': x_test_sha,\n","        },\n","        'data_shape': {\n","            'window_size': int(X_train.shape[1]),\n","            'n_channels': int(X_train.shape[2]),\n","            'n_windows_train': int(X_train.shape[0]),\n","            'n_windows_test': int(X_test.shape[0]) if has_test else 0,\n","        },\n","        'notes': [\n","            'Per-channel normalization (8 channels independently compute mean/std)',\n","            'Statistics computed from train set only',\n","            'Test set normalized using train-set statistics',\n","            'Use np.maximum(std, ε) to avoid divide-by-zero',\n","            'Auto-detect double-standardization and skip if needed',\n","            'Consistent with Step 7 anti-leakage principles',\n","            'Suitable for CNN/LSTM/Transformer and other deep models',\n","            'Data shape: (n_windows, window_size, n_channels)',\n","            'Includes input file SHA256 for traceability',\n","        ]\n","    }\n","\n","    channel_scaler_pkl = scalers_dir / \"channel_scaler.pkl\"\n","    with open(channel_scaler_pkl, 'wb') as f:\n","        pickle.dump(channel_scaler, f)\n","    print(f\"✓ Saved scaler (pkl): {channel_scaler_pkl}\")\n","\n","    channel_scaler_json = scalers_dir / \"channel_scaler.json\"\n","    with open(channel_scaler_json, 'w') as f:\n","        json.dump(channel_scaler, f, indent=2)\n","    print(f\"✓ Saved scaler (json): {channel_scaler_json}\")\n","\n","    # ========== 7. Save config ==========\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"7. Save config\")\n","    print(\"=\" * 60)\n","\n","    normalization_config = {\n","        'traditional_models': {\n","            'method': 'feature-wise StandardScaler',\n","            'scaler_file': str(feature_scaler_file.relative_to(proc_dir)) if feature_scaler_file.exists() else None,\n","            'description': 'Feature-level z-score standardization (completed in Step 11)',\n","            'input': 'features/*/train_X.parquet / test_X.parquet',\n","            'output': 'features/*/*.parquet (already standardized)',\n","        },\n","        'deep_learning_models': {\n","            'method': 'channel-wise z-score',\n","            'scaler_file': str(channel_scaler_pkl.relative_to(proc_dir)),\n","            'description': 'Channel-wise z-score standardization (per-channel)',\n","            'input': 'windows/*/X_*.npy',\n","            'output': 'scalers/*/X_*_scaled.npy',\n","            'formula': '(x - channel_mean) / max(channel_std, ε)',\n","            'epsilon': EPSILON,\n","        },\n","        'fold_info': {\n","            'fold_id': fold_id,\n","            'fold_tag': fold_tag,\n","        },\n","        'anti_leakage': [\n","            'All statistics (mean/std) computed from train set only',\n","            'Test set uses train-set statistics for normalization',\n","            'Fully consistent with Step 7 principles',\n","            'Each fold computes and saves its own scaler',\n","            'Strict LOSO: per-fold processing',\n","        ],\n","        'notes': [\n","            'Classical models use feature-level standardization (Step 11)',\n","            'Deep models use channel-level standardization (this step)',\n","            'The two normalization routes are independent',\n","            'Post-normalization data cast to float32 to save space',\n","            f'Use np.maximum(std, {EPSILON}) to avoid divide-by-zero',\n","            'Channel names read from channels.yaml to avoid hard-coding',\n","            'Anti-leakage assertion: train/test subjects are disjoint',\n","            'Auto-detect double-standardization and skip (risk mitigation)',\n","            'Numerical robustness check: NaN/Inf self-check',\n","            'Channel order validation: compare with channels.json sidecar',\n","            'Enhanced traceability: record input file SHA256 and window size',\n","        ]\n","    }\n","\n","    norm_config_file = configs_dir / f\"normalization_{fold_tag}.yaml\"\n","    with open(norm_config_file, 'w', encoding='utf-8') as f:\n","        yaml.dump(normalization_config, f, default_flow_style=False,\n","                  allow_unicode=True, sort_keys=False)\n","    print(f\"✓ Saved config: {norm_config_file}\")\n","\n","    # ========== 8. Summary for this fold ==========\n","    print(\"\\n\" + \"=\" * 60)\n","    print(f\"Summary for fold {fold_tag}\")\n","    print(\"=\" * 60)\n","\n","    print(f\"\\nClassical models path:\")\n","    print(f\"  Method: feature-level StandardScaler\")\n","    print(f\"  Status: {'✓ completed (Step 11)' if feature_scaler_file.exists() else '⚠️ missing for this fold'}\")\n","\n","    print(f\"\\nDeep-learning path:\")\n","    print(f\"  Method: channel-wise z-score\")\n","    print(f\"  Formula: (x - mean) / max(std, ε)\")\n","    print(f\"  Skip normalization: {'Yes (data already normalized)' if skip_normalization else 'No'}\")\n","    print(f\"  #channels: {n_channels}\")\n","    print(f\"  Train windows: {X_train_scaled.shape[0]:,}\")\n","    if has_test:\n","        print(f\"  Test  windows: {X_test_scaled.shape[0]:,}\")\n","\n","    print(f\"\\nOutputs ({fold_tag}):\")\n","    print(f\"  Normalized data: {scalers_dir}/\")\n","    print(f\"  Scaler params:   {channel_scaler_pkl.name}\")\n","    print(f\"  Config file:     {norm_config_file.name}\")\n","\n","# ========== Global summary ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"Step 12 complete - Normalization (feature/deep) for all folds\")\n","print(\"=\" * 60)\n","print(\"Next steps (per fold):\")\n","print(\"  Classical: train with features/fold_xx/train_X.parquet, train_y.parquet\")\n","print(\"  Deep:      train with scalers/fold_xx/X_train_scaled.npy, y_train.npy\")\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g0SUMRQb2ISV","executionInfo":{"status":"ok","timestamp":1763148601560,"user_tz":0,"elapsed":2935,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"02a70f43-2462-452d-81f2-71b8245bab9c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 12: Normalization (feature/deep) - multi-fold\n","============================================================\n","\n","============================================================\n","0. Determine folds\n","============================================================\n","Detected 8 folds from configs/splits.json: [0, 1, 2, 3, 4, 5, 6, 7]\n","\n","============================================================\n","0.1 Load channel config\n","============================================================\n","✓ Channels read from config: ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","============================================================\n","Processing fold: fold_00\n","============================================================\n","Input dir (windows):  data/lara/mbientlab/proc/windows/fold_00\n","Output dir (scalers): data/lara/mbientlab/proc/scalers/fold_00\n","\n","============================================================\n","1. Classical models: confirm features already standardized\n","============================================================\n","✓ Feature scaler was generated in Step 11: data/lara/mbientlab/proc/features/fold_00/scaler.pkl\n","  StandardScaler parameters:\n","  - #features: 220\n","  - mean range: [-2.3800, 5011.4682]\n","  - std  range: [0.0275, 5989.5515]\n","\n","============================================================\n","2. Deep models: load window data\n","============================================================\n","\n","Train set:\n","  X_train shape: (4965, 150, 8), dtype=float32\n","\n","Test set:\n","  X_test shape:  (766, 150, 8), dtype=float32\n","\n","============================================================\n","2.1. Numerical robustness self-check\n","============================================================\n","✓ X_train has no NaN/Inf\n","✓ X_test has no NaN/Inf\n","\n","============================================================\n","2.2. Anti-leakage & consistency checks\n","============================================================\n","✓ Channel count consistent: 8\n","⚠️ channels.json sidecar not found in data/lara/mbientlab/proc/windows/fold_00; only checking channel count\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test  subjects: 1 {'S07'}\n","  Subject sets disjoint: True\n","\n","============================================================\n","3. Compute channel-wise statistics (train only)\n","============================================================\n","\n","#channels: 8\n","Channel names: ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","Train-set channel statistics:\n","  ax: mean= 0.0028, std= 0.9330\n","  ay: mean=-0.0018, std= 0.9427\n","  az: mean=-0.0011, std= 0.9656\n","  gx: mean= 0.0054, std= 0.9443\n","  gy: mean=-0.0022, std= 0.9839\n","  gz: mean=-0.0079, std= 0.9440\n","  acc_mag: mean=-0.0691, std= 0.9510\n","  gyr_mag: mean=-0.0458, std= 0.9657\n","\n","============================================================\n","4. Apply channel-wise z-score normalization\n","============================================================\n","\n","Train normalization:\n","  input:  (4965, 150, 8), float32\n","  output: (4965, 150, 8), float32\n","\n","Train-set stats after normalization (should be ~0 and ~1):\n","  ax: mean= 0.0000, std= 1.0000\n","  ay: mean= 0.0000, std= 1.0000\n","  az: mean= 0.0000, std= 1.0000\n","  gx: mean= 0.0000, std= 1.0000\n","  gy: mean=-0.0000, std= 1.0000\n","  gz: mean=-0.0000, std= 1.0000\n","  acc_mag: mean=-0.0000, std= 1.0000\n","  gyr_mag: mean= 0.0000, std= 1.0000\n","\n","Test normalization (using train statistics):\n","  input:  (766, 150, 8), float32\n","  output: (766, 150, 8), float32\n","\n","Test-set stats after normalization (train params; not 0/1):\n","  ax: mean=-0.0025, std= 1.2028\n","  ay: mean= 0.0106, std= 1.1077\n","  az: mean= 0.0057, std= 1.1277\n","  gx: mean= 0.0098, std= 1.1067\n","  gy: mean=-0.0185, std= 1.0091\n","  gz: mean= 0.0121, std= 0.9754\n","  acc_mag: mean= 0.2101, std= 1.1116\n","  gyr_mag: mean= 0.0186, std= 1.0319\n","\n","============================================================\n","5. Save normalized data\n","============================================================\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/scalers/fold_00/X_train_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_00/y_train.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_00/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/scalers/fold_00/X_test_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_00/y_test.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_00/test_meta.parquet\n","\n","============================================================\n","6. Save scaler parameters (with traceability)\n","============================================================\n","\n","Computing input file SHA256...\n","  X_train: 2df9978fd74c55e0...\n","  X_test:  093776fd01d5116d...\n","✓ Saved scaler (pkl): data/lara/mbientlab/proc/scalers/fold_00/channel_scaler.pkl\n","✓ Saved scaler (json): data/lara/mbientlab/proc/scalers/fold_00/channel_scaler.json\n","\n","============================================================\n","7. Save config\n","============================================================\n","✓ Saved config: configs/normalization_fold_00.yaml\n","\n","============================================================\n","Summary for fold fold_00\n","============================================================\n","\n","Classical models path:\n","  Method: feature-level StandardScaler\n","  Status: ✓ completed (Step 11)\n","\n","Deep-learning path:\n","  Method: channel-wise z-score\n","  Formula: (x - mean) / max(std, ε)\n","  Skip normalization: No\n","  #channels: 8\n","  Train windows: 4,965\n","  Test  windows: 766\n","\n","Outputs (fold_00):\n","  Normalized data: data/lara/mbientlab/proc/scalers/fold_00/\n","  Scaler params:   channel_scaler.pkl\n","  Config file:     normalization_fold_00.yaml\n","\n","============================================================\n","Processing fold: fold_01\n","============================================================\n","Input dir (windows):  data/lara/mbientlab/proc/windows/fold_01\n","Output dir (scalers): data/lara/mbientlab/proc/scalers/fold_01\n","\n","============================================================\n","1. Classical models: confirm features already standardized\n","============================================================\n","✓ Feature scaler was generated in Step 11: data/lara/mbientlab/proc/features/fold_01/scaler.pkl\n","  StandardScaler parameters:\n","  - #features: 220\n","  - mean range: [-2.4010, 5070.7869]\n","  - std  range: [0.0312, 5966.3047]\n","\n","============================================================\n","2. Deep models: load window data\n","============================================================\n","\n","Train set:\n","  X_train shape: (5072, 150, 8), dtype=float32\n","\n","Test set:\n","  X_test shape:  (659, 150, 8), dtype=float32\n","\n","============================================================\n","2.1. Numerical robustness self-check\n","============================================================\n","✓ X_train has no NaN/Inf\n","✓ X_test has no NaN/Inf\n","\n","============================================================\n","2.2. Anti-leakage & consistency checks\n","============================================================\n","✓ Channel count consistent: 8\n","⚠️ channels.json sidecar not found in data/lara/mbientlab/proc/windows/fold_01; only checking channel count\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test  subjects: 1 {'S08'}\n","  Subject sets disjoint: True\n","\n","============================================================\n","3. Compute channel-wise statistics (train only)\n","============================================================\n","\n","#channels: 8\n","Channel names: ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","Train-set channel statistics:\n","  ax: mean= 0.0021, std= 0.9413\n","  ay: mean=-0.0007, std= 0.9425\n","  az: mean= 0.0002, std= 0.9611\n","  gx: mean= 0.0072, std= 0.9378\n","  gy: mean=-0.0049, std= 0.9593\n","  gz: mean=-0.0077, std= 0.9170\n","  acc_mag: mean=-0.0689, std= 0.9546\n","  gyr_mag: mean=-0.0739, std= 0.9502\n","\n","============================================================\n","4. Apply channel-wise z-score normalization\n","============================================================\n","\n","Train normalization:\n","  input:  (5072, 150, 8), float32\n","  output: (5072, 150, 8), float32\n","\n","Train-set stats after normalization (should be ~0 and ~1):\n","  ax: mean=-0.0000, std= 1.0000\n","  ay: mean= 0.0000, std= 1.0000\n","  az: mean= 0.0000, std= 1.0000\n","  gx: mean= 0.0000, std= 1.0000\n","  gy: mean= 0.0000, std= 1.0001\n","  gz: mean=-0.0000, std= 1.0000\n","  acc_mag: mean=-0.0000, std= 1.0000\n","  gyr_mag: mean= 0.0000, std= 1.0000\n","\n","Test normalization (using train statistics):\n","  input:  (659, 150, 8), float32\n","  output: (659, 150, 8), float32\n","\n","Test-set stats after normalization (train params; not 0/1):\n","  ax: mean= 0.0036, std= 1.1649\n","  ay: mean= 0.0017, std= 1.1258\n","  az: mean=-0.0045, std= 1.1837\n","  gx: mean=-0.0051, std= 1.1771\n","  gy: mean= 0.0021, std= 1.2146\n","  gz: mean= 0.0127, std= 1.2082\n","  acc_mag: mean= 0.2415, std= 1.0952\n","  gyr_mag: mean= 0.2788, std= 1.1380\n","\n","============================================================\n","5. Save normalized data\n","============================================================\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/scalers/fold_01/X_train_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_01/y_train.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_01/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/scalers/fold_01/X_test_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_01/y_test.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_01/test_meta.parquet\n","\n","============================================================\n","6. Save scaler parameters (with traceability)\n","============================================================\n","\n","Computing input file SHA256...\n","  X_train: 535552cc5ba9bd4a...\n","  X_test:  3a92aa04c20b4022...\n","✓ Saved scaler (pkl): data/lara/mbientlab/proc/scalers/fold_01/channel_scaler.pkl\n","✓ Saved scaler (json): data/lara/mbientlab/proc/scalers/fold_01/channel_scaler.json\n","\n","============================================================\n","7. Save config\n","============================================================\n","✓ Saved config: configs/normalization_fold_01.yaml\n","\n","============================================================\n","Summary for fold fold_01\n","============================================================\n","\n","Classical models path:\n","  Method: feature-level StandardScaler\n","  Status: ✓ completed (Step 11)\n","\n","Deep-learning path:\n","  Method: channel-wise z-score\n","  Formula: (x - mean) / max(std, ε)\n","  Skip normalization: No\n","  #channels: 8\n","  Train windows: 5,072\n","  Test  windows: 659\n","\n","Outputs (fold_01):\n","  Normalized data: data/lara/mbientlab/proc/scalers/fold_01/\n","  Scaler params:   channel_scaler.pkl\n","  Config file:     normalization_fold_01.yaml\n","\n","============================================================\n","Processing fold: fold_02\n","============================================================\n","Input dir (windows):  data/lara/mbientlab/proc/windows/fold_02\n","Output dir (scalers): data/lara/mbientlab/proc/scalers/fold_02\n","\n","============================================================\n","1. Classical models: confirm features already standardized\n","============================================================\n","✓ Feature scaler was generated in Step 11: data/lara/mbientlab/proc/features/fold_02/scaler.pkl\n","  StandardScaler parameters:\n","  - #features: 220\n","  - mean range: [-2.4118, 5163.1632]\n","  - std  range: [0.0313, 5997.5821]\n","\n","============================================================\n","2. Deep models: load window data\n","============================================================\n","\n","Train set:\n","  X_train shape: (4960, 150, 8), dtype=float32\n","\n","Test set:\n","  X_test shape:  (771, 150, 8), dtype=float32\n","\n","============================================================\n","2.1. Numerical robustness self-check\n","============================================================\n","✓ X_train has no NaN/Inf\n","✓ X_test has no NaN/Inf\n","\n","============================================================\n","2.2. Anti-leakage & consistency checks\n","============================================================\n","✓ Channel count consistent: 8\n","⚠️ channels.json sidecar not found in data/lara/mbientlab/proc/windows/fold_02; only checking channel count\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test  subjects: 1 {'S09'}\n","  Subject sets disjoint: True\n","\n","============================================================\n","3. Compute channel-wise statistics (train only)\n","============================================================\n","\n","#channels: 8\n","Channel names: ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","Train-set channel statistics:\n","  ax: mean= 0.0029, std= 0.9398\n","  ay: mean=-0.0002, std= 0.9451\n","  az: mean= 0.0006, std= 0.9746\n","  gx: mean= 0.0070, std= 0.9449\n","  gy: mean=-0.0047, std= 0.9733\n","  gz: mean=-0.0035, std= 0.9080\n","  acc_mag: mean=-0.0692, std= 0.9662\n","  gyr_mag: mean=-0.0718, std= 0.9596\n","\n","============================================================\n","4. Apply channel-wise z-score normalization\n","============================================================\n","\n","Train normalization:\n","  input:  (4960, 150, 8), float32\n","  output: (4960, 150, 8), float32\n","\n","Train-set stats after normalization (should be ~0 and ~1):\n","  ax: mean= 0.0000, std= 1.0000\n","  ay: mean= 0.0000, std= 1.0000\n","  az: mean= 0.0000, std= 1.0000\n","  gx: mean=-0.0000, std= 1.0000\n","  gy: mean= 0.0000, std= 1.0000\n","  gz: mean= 0.0000, std= 1.0000\n","  acc_mag: mean= 0.0000, std= 1.0000\n","  gyr_mag: mean=-0.0000, std= 1.0000\n","\n","Test normalization (using train statistics):\n","  input:  (771, 150, 8), float32\n","  output: (771, 150, 8), float32\n","\n","Test-set stats after normalization (train params; not 0/1):\n","  ax: mean=-0.0033, std= 1.1533\n","  ay: mean=-0.0021, std= 1.0891\n","  az: mean=-0.0070, std= 1.0625\n","  gx: mean=-0.0031, std= 1.1011\n","  gy: mean= 0.0002, std= 1.0876\n","  gz: mean=-0.0232, std= 1.2451\n","  acc_mag: mean= 0.2063, std= 0.9972\n","  gyr_mag: mean= 0.2200, std= 1.0574\n","\n","============================================================\n","5. Save normalized data\n","============================================================\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/scalers/fold_02/X_train_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_02/y_train.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_02/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/scalers/fold_02/X_test_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_02/y_test.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_02/test_meta.parquet\n","\n","============================================================\n","6. Save scaler parameters (with traceability)\n","============================================================\n","\n","Computing input file SHA256...\n","  X_train: c613ff40c76f8fa2...\n","  X_test:  48f42fedd674c8f3...\n","✓ Saved scaler (pkl): data/lara/mbientlab/proc/scalers/fold_02/channel_scaler.pkl\n","✓ Saved scaler (json): data/lara/mbientlab/proc/scalers/fold_02/channel_scaler.json\n","\n","============================================================\n","7. Save config\n","============================================================\n","✓ Saved config: configs/normalization_fold_02.yaml\n","\n","============================================================\n","Summary for fold fold_02\n","============================================================\n","\n","Classical models path:\n","  Method: feature-level StandardScaler\n","  Status: ✓ completed (Step 11)\n","\n","Deep-learning path:\n","  Method: channel-wise z-score\n","  Formula: (x - mean) / max(std, ε)\n","  Skip normalization: No\n","  #channels: 8\n","  Train windows: 4,960\n","  Test  windows: 771\n","\n","Outputs (fold_02):\n","  Normalized data: data/lara/mbientlab/proc/scalers/fold_02/\n","  Scaler params:   channel_scaler.pkl\n","  Config file:     normalization_fold_02.yaml\n","\n","============================================================\n","Processing fold: fold_03\n","============================================================\n","Input dir (windows):  data/lara/mbientlab/proc/windows/fold_03\n","Output dir (scalers): data/lara/mbientlab/proc/scalers/fold_03\n","\n","============================================================\n","1. Classical models: confirm features already standardized\n","============================================================\n","✓ Feature scaler was generated in Step 11: data/lara/mbientlab/proc/features/fold_03/scaler.pkl\n","  StandardScaler parameters:\n","  - #features: 220\n","  - mean range: [-2.4757, 5321.1833]\n","  - std  range: [0.0317, 6128.3903]\n","\n","============================================================\n","2. Deep models: load window data\n","============================================================\n","\n","Train set:\n","  X_train shape: (4858, 150, 8), dtype=float32\n","\n","Test set:\n","  X_test shape:  (873, 150, 8), dtype=float32\n","\n","============================================================\n","2.1. Numerical robustness self-check\n","============================================================\n","✓ X_train has no NaN/Inf\n","✓ X_test has no NaN/Inf\n","\n","============================================================\n","2.2. Anti-leakage & consistency checks\n","============================================================\n","✓ Channel count consistent: 8\n","⚠️ channels.json sidecar not found in data/lara/mbientlab/proc/windows/fold_03; only checking channel count\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test  subjects: 1 {'S10'}\n","  Subject sets disjoint: True\n","\n","============================================================\n","3. Compute channel-wise statistics (train only)\n","============================================================\n","\n","#channels: 8\n","Channel names: ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","Train-set channel statistics:\n","  ax: mean= 0.0022, std= 0.9737\n","  ay: mean= 0.0003, std= 0.9679\n","  az: mean=-0.0007, std= 0.9862\n","  gx: mean= 0.0052, std= 0.9718\n","  gy: mean=-0.0065, std= 0.9908\n","  gz: mean=-0.0069, std= 0.9474\n","  acc_mag: mean=-0.0351, std= 0.9828\n","  gyr_mag: mean=-0.0404, std= 0.9839\n","\n","============================================================\n","4. Apply channel-wise z-score normalization\n","============================================================\n","\n","Train normalization:\n","  input:  (4858, 150, 8), float32\n","  output: (4858, 150, 8), float32\n","\n","Train-set stats after normalization (should be ~0 and ~1):\n","  ax: mean= 0.0000, std= 1.0000\n","  ay: mean=-0.0000, std= 1.0000\n","  az: mean= 0.0000, std= 1.0001\n","  gx: mean=-0.0000, std= 1.0000\n","  gy: mean= 0.0000, std= 1.0000\n","  gz: mean=-0.0000, std= 1.0000\n","  acc_mag: mean=-0.0000, std= 1.0000\n","  gyr_mag: mean=-0.0000, std= 1.0000\n","\n","Test normalization (using train statistics):\n","  input:  (873, 150, 8), float32\n","  output: (873, 150, 8), float32\n","\n","Test-set stats after normalization (train params; not 0/1):\n","  ax: mean= 0.0019, std= 0.9069\n","  ay: mean=-0.0054, std= 0.9230\n","  az: mean= 0.0022, std= 0.9788\n","  gx: mean= 0.0094, std= 0.9054\n","  gy: mean= 0.0124, std= 0.9623\n","  gz: mean= 0.0036, std= 0.9549\n","  acc_mag: mean=-0.0489, std= 0.8972\n","  gyr_mag: mean=-0.0205, std= 0.9018\n","\n","============================================================\n","5. Save normalized data\n","============================================================\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/scalers/fold_03/X_train_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_03/y_train.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_03/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/scalers/fold_03/X_test_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_03/y_test.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_03/test_meta.parquet\n","\n","============================================================\n","6. Save scaler parameters (with traceability)\n","============================================================\n","\n","Computing input file SHA256...\n","  X_train: 3f94c478f6fa5335...\n","  X_test:  35b4c25bfebadc17...\n","✓ Saved scaler (pkl): data/lara/mbientlab/proc/scalers/fold_03/channel_scaler.pkl\n","✓ Saved scaler (json): data/lara/mbientlab/proc/scalers/fold_03/channel_scaler.json\n","\n","============================================================\n","7. Save config\n","============================================================\n","✓ Saved config: configs/normalization_fold_03.yaml\n","\n","============================================================\n","Summary for fold fold_03\n","============================================================\n","\n","Classical models path:\n","  Method: feature-level StandardScaler\n","  Status: ✓ completed (Step 11)\n","\n","Deep-learning path:\n","  Method: channel-wise z-score\n","  Formula: (x - mean) / max(std, ε)\n","  Skip normalization: No\n","  #channels: 8\n","  Train windows: 4,858\n","  Test  windows: 873\n","\n","Outputs (fold_03):\n","  Normalized data: data/lara/mbientlab/proc/scalers/fold_03/\n","  Scaler params:   channel_scaler.pkl\n","  Config file:     normalization_fold_03.yaml\n","\n","============================================================\n","Processing fold: fold_04\n","============================================================\n","Input dir (windows):  data/lara/mbientlab/proc/windows/fold_04\n","Output dir (scalers): data/lara/mbientlab/proc/scalers/fold_04\n","\n","============================================================\n","1. Classical models: confirm features already standardized\n","============================================================\n","✓ Feature scaler was generated in Step 11: data/lara/mbientlab/proc/features/fold_04/scaler.pkl\n","  StandardScaler parameters:\n","  - #features: 220\n","  - mean range: [-2.5589, 5222.4780]\n","  - std  range: [0.0316, 6126.2452]\n","\n","============================================================\n","2. Deep models: load window data\n","============================================================\n","\n","Train set:\n","  X_train shape: (4985, 150, 8), dtype=float32\n","\n","Test set:\n","  X_test shape:  (746, 150, 8), dtype=float32\n","\n","============================================================\n","2.1. Numerical robustness self-check\n","============================================================\n","✓ X_train has no NaN/Inf\n","✓ X_test has no NaN/Inf\n","\n","============================================================\n","2.2. Anti-leakage & consistency checks\n","============================================================\n","✓ Channel count consistent: 8\n","⚠️ channels.json sidecar not found in data/lara/mbientlab/proc/windows/fold_04; only checking channel count\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test  subjects: 1 {'S11'}\n","  Subject sets disjoint: True\n","\n","============================================================\n","3. Compute channel-wise statistics (train only)\n","============================================================\n","\n","#channels: 8\n","Channel names: ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","Train-set channel statistics:\n","  ax: mean= 0.0031, std= 0.9921\n","  ay: mean=-0.0001, std= 0.9799\n","  az: mean=-0.0007, std= 1.0216\n","  gx: mean= 0.0042, std= 0.9743\n","  gy: mean=-0.0041, std= 1.0196\n","  gz: mean=-0.0060, std= 0.9675\n","  acc_mag: mean= 0.0100, std= 0.9816\n","  gyr_mag: mean=-0.0037, std= 0.9871\n","\n","============================================================\n","4. Apply channel-wise z-score normalization\n","============================================================\n","\n","Train normalization:\n","  input:  (4985, 150, 8), float32\n","  output: (4985, 150, 8), float32\n","\n","Train-set stats after normalization (should be ~0 and ~1):\n","  ax: mean= 0.0000, std= 1.0000\n","  ay: mean= 0.0000, std= 1.0000\n","  az: mean= 0.0000, std= 1.0000\n","  gx: mean=-0.0000, std= 1.0000\n","  gy: mean= 0.0000, std= 1.0000\n","  gz: mean=-0.0000, std= 1.0000\n","  acc_mag: mean=-0.0000, std= 1.0000\n","  gyr_mag: mean= 0.0000, std= 1.0000\n","\n","Test normalization (using train statistics):\n","  input:  (746, 150, 8), float32\n","  output: (746, 150, 8), float32\n","\n","Test-set stats after normalization (train params; not 0/1):\n","  ax: mean=-0.0051, std= 0.7200\n","  ay: mean=-0.0031, std= 0.8017\n","  az: mean= 0.0026, std= 0.6565\n","  gx: mean= 0.0189, std= 0.8664\n","  gy: mean=-0.0039, std= 0.7002\n","  gz: mean=-0.0025, std= 0.7642\n","  acc_mag: mean=-0.4103, std= 0.8041\n","  gyr_mag: mean=-0.3090, std= 0.8070\n","\n","============================================================\n","5. Save normalized data\n","============================================================\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/scalers/fold_04/X_train_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_04/y_train.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_04/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/scalers/fold_04/X_test_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_04/y_test.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_04/test_meta.parquet\n","\n","============================================================\n","6. Save scaler parameters (with traceability)\n","============================================================\n","\n","Computing input file SHA256...\n","  X_train: 58548df17f5ead31...\n","  X_test:  2b4ccc569f258000...\n","✓ Saved scaler (pkl): data/lara/mbientlab/proc/scalers/fold_04/channel_scaler.pkl\n","✓ Saved scaler (json): data/lara/mbientlab/proc/scalers/fold_04/channel_scaler.json\n","\n","============================================================\n","7. Save config\n","============================================================\n","✓ Saved config: configs/normalization_fold_04.yaml\n","\n","============================================================\n","Summary for fold fold_04\n","============================================================\n","\n","Classical models path:\n","  Method: feature-level StandardScaler\n","  Status: ✓ completed (Step 11)\n","\n","Deep-learning path:\n","  Method: channel-wise z-score\n","  Formula: (x - mean) / max(std, ε)\n","  Skip normalization: No\n","  #channels: 8\n","  Train windows: 4,985\n","  Test  windows: 746\n","\n","Outputs (fold_04):\n","  Normalized data: data/lara/mbientlab/proc/scalers/fold_04/\n","  Scaler params:   channel_scaler.pkl\n","  Config file:     normalization_fold_04.yaml\n","\n","============================================================\n","Processing fold: fold_05\n","============================================================\n","Input dir (windows):  data/lara/mbientlab/proc/windows/fold_05\n","Output dir (scalers): data/lara/mbientlab/proc/scalers/fold_05\n","\n","============================================================\n","1. Classical models: confirm features already standardized\n","============================================================\n","✓ Feature scaler was generated in Step 11: data/lara/mbientlab/proc/features/fold_05/scaler.pkl\n","  StandardScaler parameters:\n","  - #features: 220\n","  - mean range: [-2.4452, 5169.8673]\n","  - std  range: [0.0312, 5954.7184]\n","\n","============================================================\n","2. Deep models: load window data\n","============================================================\n","\n","Train set:\n","  X_train shape: (5442, 150, 8), dtype=float32\n","\n","Test set:\n","  X_test shape:  (289, 150, 8), dtype=float32\n","\n","============================================================\n","2.1. Numerical robustness self-check\n","============================================================\n","✓ X_train has no NaN/Inf\n","✓ X_test has no NaN/Inf\n","\n","============================================================\n","2.2. Anti-leakage & consistency checks\n","============================================================\n","✓ Channel count consistent: 8\n","⚠️ channels.json sidecar not found in data/lara/mbientlab/proc/windows/fold_05; only checking channel count\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test  subjects: 1 {'S12'}\n","  Subject sets disjoint: True\n","\n","============================================================\n","3. Compute channel-wise statistics (train only)\n","============================================================\n","\n","#channels: 8\n","Channel names: ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","Train-set channel statistics:\n","  ax: mean= 0.0027, std= 0.9579\n","  ay: mean=-0.0007, std= 0.9523\n","  az: mean=-0.0009, std= 0.9808\n","  gx: mean= 0.0113, std= 0.9551\n","  gy: mean=-0.0052, std= 0.9809\n","  gz: mean=-0.0055, std= 0.9351\n","  acc_mag: mean=-0.0483, std= 0.9680\n","  gyr_mag: mean=-0.0502, std= 0.9674\n","\n","============================================================\n","4. Apply channel-wise z-score normalization\n","============================================================\n","\n","Train normalization:\n","  input:  (5442, 150, 8), float32\n","  output: (5442, 150, 8), float32\n","\n","Train-set stats after normalization (should be ~0 and ~1):\n","  ax: mean= 0.0000, std= 1.0000\n","  ay: mean= 0.0000, std= 1.0000\n","  az: mean= 0.0000, std= 1.0000\n","  gx: mean= 0.0000, std= 1.0000\n","  gy: mean= 0.0000, std= 1.0000\n","  gz: mean=-0.0000, std= 1.0000\n","  acc_mag: mean=-0.0000, std= 1.0000\n","  gyr_mag: mean= 0.0000, std= 1.0000\n","\n","Test normalization (using train statistics):\n","  input:  (289, 150, 8), float32\n","  output: (289, 150, 8), float32\n","\n","Test-set stats after normalization (train params; not 0/1):\n","  ax: mean=-0.0041, std= 1.0492\n","  ay: mean= 0.0045, std= 1.0900\n","  az: mean= 0.0112, std= 1.0411\n","  gx: mean=-0.0977, std= 1.0608\n","  gy: mean= 0.0108, std= 1.0813\n","  gz: mean=-0.0174, std= 1.1169\n","  acc_mag: mean= 0.1213, std= 0.9969\n","  gyr_mag: mean= 0.1381, std= 1.0393\n","\n","============================================================\n","5. Save normalized data\n","============================================================\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/scalers/fold_05/X_train_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_05/y_train.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_05/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/scalers/fold_05/X_test_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_05/y_test.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_05/test_meta.parquet\n","\n","============================================================\n","6. Save scaler parameters (with traceability)\n","============================================================\n","\n","Computing input file SHA256...\n","  X_train: 4d1ecb52f936af84...\n","  X_test:  db61b552bf277b24...\n","✓ Saved scaler (pkl): data/lara/mbientlab/proc/scalers/fold_05/channel_scaler.pkl\n","✓ Saved scaler (json): data/lara/mbientlab/proc/scalers/fold_05/channel_scaler.json\n","\n","============================================================\n","7. Save config\n","============================================================\n","✓ Saved config: configs/normalization_fold_05.yaml\n","\n","============================================================\n","Summary for fold fold_05\n","============================================================\n","\n","Classical models path:\n","  Method: feature-level StandardScaler\n","  Status: ✓ completed (Step 11)\n","\n","Deep-learning path:\n","  Method: channel-wise z-score\n","  Formula: (x - mean) / max(std, ε)\n","  Skip normalization: No\n","  #channels: 8\n","  Train windows: 5,442\n","  Test  windows: 289\n","\n","Outputs (fold_05):\n","  Normalized data: data/lara/mbientlab/proc/scalers/fold_05/\n","  Scaler params:   channel_scaler.pkl\n","  Config file:     normalization_fold_05.yaml\n","\n","============================================================\n","Processing fold: fold_06\n","============================================================\n","Input dir (windows):  data/lara/mbientlab/proc/windows/fold_06\n","Output dir (scalers): data/lara/mbientlab/proc/scalers/fold_06\n","\n","============================================================\n","1. Classical models: confirm features already standardized\n","============================================================\n","✓ Feature scaler was generated in Step 11: data/lara/mbientlab/proc/features/fold_06/scaler.pkl\n","  StandardScaler parameters:\n","  - #features: 220\n","  - mean range: [-2.4534, 5011.9481]\n","  - std  range: [0.0314, 6069.1681]\n","\n","============================================================\n","2. Deep models: load window data\n","============================================================\n","\n","Train set:\n","  X_train shape: (4853, 150, 8), dtype=float32\n","\n","Test set:\n","  X_test shape:  (878, 150, 8), dtype=float32\n","\n","============================================================\n","2.1. Numerical robustness self-check\n","============================================================\n","✓ X_train has no NaN/Inf\n","✓ X_test has no NaN/Inf\n","\n","============================================================\n","2.2. Anti-leakage & consistency checks\n","============================================================\n","✓ Channel count consistent: 8\n","⚠️ channels.json sidecar not found in data/lara/mbientlab/proc/windows/fold_06; only checking channel count\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test  subjects: 1 {'S13'}\n","  Subject sets disjoint: True\n","\n","============================================================\n","3. Compute channel-wise statistics (train only)\n","============================================================\n","\n","#channels: 8\n","Channel names: ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","Train-set channel statistics:\n","  ax: mean= 0.0020, std= 0.9621\n","  ay: mean=-0.0006, std= 0.9502\n","  az: mean=-0.0001, std= 0.9680\n","  gx: mean= 0.0011, std= 0.9563\n","  gy: mean=-0.0046, std= 0.9601\n","  gz: mean=-0.0072, std= 0.9436\n","  acc_mag: mean=-0.0448, std= 0.9556\n","  gyr_mag: mean=-0.0487, std= 0.9518\n","\n","============================================================\n","4. Apply channel-wise z-score normalization\n","============================================================\n","\n","Train normalization:\n","  input:  (4853, 150, 8), float32\n","  output: (4853, 150, 8), float32\n","\n","Train-set stats after normalization (should be ~0 and ~1):\n","  ax: mean= 0.0000, std= 1.0000\n","  ay: mean=-0.0000, std= 1.0000\n","  az: mean=-0.0000, std= 1.0000\n","  gx: mean=-0.0000, std= 1.0000\n","  gy: mean= 0.0000, std= 1.0000\n","  gz: mean=-0.0000, std= 0.9999\n","  acc_mag: mean=-0.0000, std= 1.0000\n","  gyr_mag: mean= 0.0000, std= 1.0000\n","\n","Test normalization (using train statistics):\n","  input:  (878, 150, 8), float32\n","  output: (878, 150, 8), float32\n","\n","Test-set stats after normalization (train params; not 0/1):\n","  ax: mean= 0.0035, std= 0.9891\n","  ay: mean= 0.0006, std= 1.0451\n","  az: mean=-0.0015, std= 1.0973\n","  gx: mean= 0.0375, std= 1.0132\n","  gy: mean=-0.0006, std= 1.1596\n","  gz: mean= 0.0056, std= 0.9819\n","  acc_mag: mean= 0.0164, std= 1.0834\n","  gyr_mag: mean= 0.0360, std= 1.1173\n","\n","============================================================\n","5. Save normalized data\n","============================================================\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/scalers/fold_06/X_train_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_06/y_train.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_06/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/scalers/fold_06/X_test_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_06/y_test.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_06/test_meta.parquet\n","\n","============================================================\n","6. Save scaler parameters (with traceability)\n","============================================================\n","\n","Computing input file SHA256...\n","  X_train: 6699602226936fa0...\n","  X_test:  2346046661de22aa...\n","✓ Saved scaler (pkl): data/lara/mbientlab/proc/scalers/fold_06/channel_scaler.pkl\n","✓ Saved scaler (json): data/lara/mbientlab/proc/scalers/fold_06/channel_scaler.json\n","\n","============================================================\n","7. Save config\n","============================================================\n","✓ Saved config: configs/normalization_fold_06.yaml\n","\n","============================================================\n","Summary for fold fold_06\n","============================================================\n","\n","Classical models path:\n","  Method: feature-level StandardScaler\n","  Status: ✓ completed (Step 11)\n","\n","Deep-learning path:\n","  Method: channel-wise z-score\n","  Formula: (x - mean) / max(std, ε)\n","  Skip normalization: No\n","  #channels: 8\n","  Train windows: 4,853\n","  Test  windows: 878\n","\n","Outputs (fold_06):\n","  Normalized data: data/lara/mbientlab/proc/scalers/fold_06/\n","  Scaler params:   channel_scaler.pkl\n","  Config file:     normalization_fold_06.yaml\n","\n","============================================================\n","Processing fold: fold_07\n","============================================================\n","Input dir (windows):  data/lara/mbientlab/proc/windows/fold_07\n","Output dir (scalers): data/lara/mbientlab/proc/scalers/fold_07\n","\n","============================================================\n","1. Classical models: confirm features already standardized\n","============================================================\n","✓ Feature scaler was generated in Step 11: data/lara/mbientlab/proc/features/fold_07/scaler.pkl\n","  StandardScaler parameters:\n","  - #features: 220\n","  - mean range: [-2.4813, 5299.1593]\n","  - std  range: [0.0304, 6142.6317]\n","\n","============================================================\n","2. Deep models: load window data\n","============================================================\n","\n","Train set:\n","  X_train shape: (4982, 150, 8), dtype=float32\n","\n","Test set:\n","  X_test shape:  (749, 150, 8), dtype=float32\n","\n","============================================================\n","2.1. Numerical robustness self-check\n","============================================================\n","✓ X_train has no NaN/Inf\n","✓ X_test has no NaN/Inf\n","\n","============================================================\n","2.2. Anti-leakage & consistency checks\n","============================================================\n","✓ Channel count consistent: 8\n","⚠️ channels.json sidecar not found in data/lara/mbientlab/proc/windows/fold_07; only checking channel count\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test  subjects: 1 {'S14'}\n","  Subject sets disjoint: True\n","\n","============================================================\n","3. Compute channel-wise statistics (train only)\n","============================================================\n","\n","#channels: 8\n","Channel names: ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","Train-set channel statistics:\n","  ax: mean= 0.0020, std= 0.9823\n","  ay: mean=-0.0001, std= 0.9739\n","  az: mean= 0.0001, std= 1.0043\n","  gx: mean= 0.0109, std= 0.9818\n","  gy: mean=-0.0050, std= 1.0115\n","  gz: mean=-0.0062, std= 0.9642\n","  acc_mag: mean=-0.0129, std= 0.9826\n","  gyr_mag: mean=-0.0120, std= 0.9902\n","\n","============================================================\n","4. Apply channel-wise z-score normalization\n","============================================================\n","\n","Train normalization:\n","  input:  (4982, 150, 8), float32\n","  output: (4982, 150, 8), float32\n","\n","Train-set stats after normalization (should be ~0 and ~1):\n","  ax: mean= 0.0000, std= 1.0000\n","  ay: mean= 0.0000, std= 1.0000\n","  az: mean=-0.0000, std= 1.0000\n","  gx: mean=-0.0000, std= 1.0000\n","  gy: mean= 0.0000, std= 1.0000\n","  gz: mean=-0.0000, std= 0.9999\n","  acc_mag: mean=-0.0000, std= 1.0000\n","  gyr_mag: mean= 0.0000, std= 1.0000\n","\n","Test normalization (using train statistics):\n","  input:  (749, 150, 8), float32\n","  output: (749, 150, 8), float32\n","\n","Test-set stats after normalization (train params; not 0/1):\n","  ax: mean= 0.0035, std= 0.8142\n","  ay: mean=-0.0032, std= 0.8571\n","  az: mean=-0.0029, std= 0.8236\n","  gx: mean=-0.0331, std= 0.7987\n","  gy: mean= 0.0028, std= 0.7793\n","  gz: mean=-0.0012, std= 0.7970\n","  acc_mag: mean=-0.2299, std= 0.8557\n","  gyr_mag: mean=-0.2429, std= 0.7985\n","\n","============================================================\n","5. Save normalized data\n","============================================================\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/scalers/fold_07/X_train_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_07/y_train.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_07/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/scalers/fold_07/X_test_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_07/y_test.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_07/test_meta.parquet\n","\n","============================================================\n","6. Save scaler parameters (with traceability)\n","============================================================\n","\n","Computing input file SHA256...\n","  X_train: cd82ffc27b166fb6...\n","  X_test:  4ab370794db61df3...\n","✓ Saved scaler (pkl): data/lara/mbientlab/proc/scalers/fold_07/channel_scaler.pkl\n","✓ Saved scaler (json): data/lara/mbientlab/proc/scalers/fold_07/channel_scaler.json\n","\n","============================================================\n","7. Save config\n","============================================================\n","✓ Saved config: configs/normalization_fold_07.yaml\n","\n","============================================================\n","Summary for fold fold_07\n","============================================================\n","\n","Classical models path:\n","  Method: feature-level StandardScaler\n","  Status: ✓ completed (Step 11)\n","\n","Deep-learning path:\n","  Method: channel-wise z-score\n","  Formula: (x - mean) / max(std, ε)\n","  Skip normalization: No\n","  #channels: 8\n","  Train windows: 4,982\n","  Test  windows: 749\n","\n","Outputs (fold_07):\n","  Normalized data: data/lara/mbientlab/proc/scalers/fold_07/\n","  Scaler params:   channel_scaler.pkl\n","  Config file:     normalization_fold_07.yaml\n","\n","============================================================\n","Step 12 complete - Normalization (feature/deep) for all folds\n","============================================================\n","Next steps (per fold):\n","  Classical: train with features/fold_xx/train_X.parquet, train_y.parquet\n","  Deep:      train with scalers/fold_xx/X_train_scaled.npy, y_train.npy\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 13: Model Configuration & Training — top-conf/journal grade\n","\n","Deep model: InceptionTime (native 1D convolution)\n","Classical models: RandomForest, KNN\n","Training: bf16/fp32, early stopping, single random seed\n","Multi-fold version: loop over all folds in configs/splits.json\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import pickle\n","import json\n","import yaml\n","import os\n","import time\n","from datetime import datetime, timezone\n","\n","# PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.cuda.amp import GradScaler\n","\n","# Classical models\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","# ========== Config ==========\n","RANDOM_SEED = 42\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Auto-select mixed precision\n","USE_AMP = False\n","AMP_DTYPE = None\n","USE_SCALER = False  # GradScaler only for fp16\n","if torch.cuda.is_available():\n","    # Prefer bf16 (more stable, no GradScaler)\n","    if torch.cuda.is_bf16_supported():\n","        USE_AMP = True\n","        AMP_DTYPE = torch.bfloat16\n","        USE_SCALER = False\n","    else:\n","        # Fall back to fp16 (needs GradScaler)\n","        USE_AMP = True\n","        AMP_DTYPE = torch.float16\n","        USE_SCALER = True\n","\n","# Deep model hyperparameters\n","DEEP_CONFIG = {\n","    'batch_size': 64,\n","    'epochs': 100,\n","    'learning_rate': 1e-3,\n","    'weight_decay': 1e-4,\n","    'patience': 10,  # early stopping\n","    'min_delta': 1e-4,\n","    'val_split': 0.15,  # validation ratio (split from train set)\n","    'num_workers': 4,\n","}\n","\n","# InceptionTime hyperparameters\n","INCEPTION_CONFIG = {\n","    'n_filters': 32,\n","    'kernel_sizes': [9, 19, 39],\n","    'bottleneck_channels': 32,\n","    'use_residual': True,\n","    'depth': 6,\n","}\n","\n","# Classical model hyperparameters\n","RF_CONFIG = {\n","    'n_estimators': 200,\n","    'max_depth': 30,\n","    'min_samples_split': 5,\n","    'min_samples_leaf': 2,\n","    'random_state': RANDOM_SEED,\n","    'n_jobs': -1,\n","}\n","\n","KNN_CONFIG = {\n","    'n_neighbors': 5,\n","    'weights': 'distance',\n","    'metric': 'euclidean',\n","    'n_jobs': -1,\n","}\n","\n","print(\"=\" * 60)\n","print(\"Step 13: Model Configuration & Training (multi-fold)\")\n","print(\"=\" * 60)\n","\n","# Paths\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","models_root = Path(\"models\")\n","models_root.mkdir(parents=True, exist_ok=True)\n","\n","# Set random seed\n","torch.manual_seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(RANDOM_SEED)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","print(f\"\\nDevice: {DEVICE}\")\n","if USE_AMP:\n","    dtype_str = 'bf16' if AMP_DTYPE == torch.bfloat16 else 'fp16'\n","    scaler_str = ' (using GradScaler)' if USE_SCALER else ' (no GradScaler)'\n","    print(f\"Mixed precision: {dtype_str}{scaler_str}\")\n","else:\n","    print(\"Mixed precision: OFF (CPU)\")\n","print(f\"Random seed: {RANDOM_SEED}\")\n","\n","# ========== InceptionTime definition ==========\n","class InceptionModule(nn.Module):\n","    \"\"\"InceptionTime base module\"\"\"\n","    def __init__(self, in_channels, n_filters, kernel_sizes, bottleneck_channels):\n","        super().__init__()\n","        self.bottleneck = nn.Conv1d(in_channels, bottleneck_channels, 1, bias=False)\n","\n","        self.conv_list = nn.ModuleList([\n","            nn.Conv1d(bottleneck_channels, n_filters, k, padding=k//2, bias=False)\n","            for k in kernel_sizes\n","        ])\n","\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool1d(3, stride=1, padding=1),\n","            nn.Conv1d(in_channels, n_filters, 1, bias=False)\n","        )\n","\n","        out_channels = n_filters * (len(kernel_sizes) + 1)\n","        self.bn = nn.BatchNorm1d(out_channels)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        bottleneck = self.bottleneck(x)\n","        conv_outputs = [conv(bottleneck) for conv in self.conv_list]\n","        maxpool_output = self.maxpool_conv(x)\n","\n","        out = torch.cat([*conv_outputs, maxpool_output], dim=1)\n","        out = self.bn(out)\n","        out = self.relu(out)\n","        return out\n","\n","class InceptionTime(nn.Module):\n","    \"\"\"InceptionTime classifier\"\"\"\n","    def __init__(self, n_channels, n_classes, config):\n","        super().__init__()\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","\n","        n_filters = config['n_filters']\n","        kernel_sizes = config['kernel_sizes']\n","        bottleneck_channels = config['bottleneck_channels']\n","        depth = config['depth']\n","        use_residual = config['use_residual']\n","\n","        self.inception_modules = nn.ModuleList()\n","        in_ch = n_channels\n","        out_ch = n_filters * (len(kernel_sizes) + 1)\n","\n","        for _ in range(depth):\n","            self.inception_modules.append(\n","                InceptionModule(in_ch, n_filters, kernel_sizes, bottleneck_channels)\n","            )\n","            in_ch = out_ch\n","\n","        self.use_residual = use_residual\n","        if use_residual:\n","            self.residual_conv = nn.ModuleList([\n","                nn.Conv1d(n_channels if i == 0 else out_ch, out_ch, 1, bias=False)\n","                for i in range(depth)\n","            ])\n","\n","        self.gap = nn.AdaptiveAvgPool1d(1)\n","        self.fc = nn.Linear(out_ch, n_classes)\n","\n","    def forward(self, x):\n","        # x: (batch, time, channels) -> (batch, channels, time)\n","        x = x.transpose(1, 2)\n","\n","        for i, inception in enumerate(self.inception_modules):\n","            residual = x\n","            x = inception(x)\n","\n","            if self.use_residual:\n","                residual = self.residual_conv[i](residual)\n","                x = x + residual\n","\n","        x = self.gap(x)   # (batch, channels, 1)\n","        x = x.squeeze(-1) # (batch, channels)\n","        x = self.fc(x)\n","        return x\n","\n","# ========== Dataset ==========\n","class WindowDataset(Dataset):\n","    \"\"\"Windowed dataset\"\"\"\n","    def __init__(self, X, y):\n","        self.X = torch.FloatTensor(X)\n","        self.y = torch.LongTensor(y)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","# ========== Train / Eval helpers ==========\n","def train_epoch(model, loader, criterion, optimizer, scaler, device, use_amp, amp_dtype, use_scaler):\n","    \"\"\"Train one epoch\"\"\"\n","    model.train()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for X, y in loader:\n","        X, y = X.to(device), y.to(device)\n","        optimizer.zero_grad()\n","\n","        if use_amp:\n","            with torch.amp.autocast('cuda', dtype=amp_dtype):\n","                outputs = model(X)\n","                loss = criterion(outputs, y)\n","\n","            if use_scaler:\n","                scaler.scale(loss).backward()\n","                scaler.step(optimizer)\n","                scaler.update()\n","            else:\n","                loss.backward()\n","                optimizer.step()\n","        else:\n","            outputs = model(X)\n","            loss = criterion(outputs, y)\n","            loss.backward()\n","            optimizer.step()\n","\n","        total_loss += loss.item() * X.size(0)\n","        _, predicted = outputs.max(1)\n","        total += y.size(0)\n","        correct += predicted.eq(y).sum().item()\n","\n","    return total_loss / total, correct / total\n","\n","def evaluate(model, loader, criterion, device):\n","    \"\"\"Evaluate model\"\"\"\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for X, y in loader:\n","            X, y = X.to(device), y.to(device)\n","            outputs = model(X)\n","            loss = criterion(outputs, y)\n","\n","            total_loss += loss.item() * X.size(0)\n","            _, predicted = outputs.max(1)\n","            total += y.size(0)\n","            correct += predicted.eq(y).sum().item()\n","\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(y.cpu().numpy())\n","\n","    return total_loss / total, correct / total, np.array(all_preds), np.array(all_labels)\n","\n","# ========== 0. Determine folds ==========\n","splits_path = configs_dir / \"splits.json\"\n","if splits_path.exists():\n","    with open(splits_path, \"r\") as f:\n","        splits = json.load(f)\n","    fold_ids = sorted(int(k) for k in splits.keys())\n","    print(f\"\\nDetected folds from {splits_path}: {fold_ids}\")\n","else:\n","    raise RuntimeError(\"splits.json not found; LOSO Step 13 expects defined folds.\")\n","\n","# Used to record the final metrics for each fold\n","fold_summaries = []\n","\n","# ========== Loop over folds ==========\n","for fold_id in fold_ids:\n","    fold_tag = f\"fold_{fold_id:02d}\"\n","    models_dir = models_root / fold_tag\n","    models_dir.mkdir(parents=True, exist_ok=True)\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(f\"Processing fold {fold_id} ({fold_tag})\")\n","    print(\"=\" * 60)\n","\n","    # ========== 1. Train deep model (InceptionTime) ==========\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"1. Train deep model (InceptionTime)\")\n","    print(\"=\" * 60)\n","\n","    scalers_dir = proc_dir / \"scalers\" / fold_tag\n","    X_train_file = scalers_dir / \"X_train_scaled.npy\"\n","    y_train_file = scalers_dir / \"y_train.npy\"\n","    train_meta_file = scalers_dir / \"train_meta.parquet\"\n","    X_test_file = scalers_dir / \"X_test_scaled.npy\"\n","    y_test_file = scalers_dir / \"y_test.npy\"\n","\n","    if not X_train_file.exists():\n","        raise FileNotFoundError(f\"[{fold_tag}] Standardized data not found: {X_train_file}, please run Step 12 first\")\n","\n","    X_train_full = np.load(X_train_file)\n","    y_train_full = np.load(y_train_file)\n","    df_train_meta = pd.read_parquet(train_meta_file)\n","\n","    if not X_test_file.exists():\n","        raise FileNotFoundError(f\"[{fold_tag}] Test standardized data not found: {X_test_file}\")\n","    X_test = np.load(X_test_file)\n","    y_test = np.load(y_test_file)\n","\n","    print(f\"\\nData loaded:\")\n","    print(f\"  X_train_full: {X_train_full.shape}\")\n","    print(f\"  y_train_full: {y_train_full.shape}\")\n","    print(f\"  X_test:       {X_test.shape}\")\n","    print(f\"  y_test:       {y_test.shape}\")\n","\n","    # ----- 1.0 Label remapping -----\n","    print(f\"\\nLabel remapping (ensure contiguous 0..n_classes-1):\")\n","    unique_labels_train = np.unique(y_train_full)\n","    unique_labels_test = np.unique(y_test)\n","    all_unique_labels = np.unique(np.concatenate([unique_labels_train, unique_labels_test]))\n","\n","    print(f\"  Original label set: {all_unique_labels.tolist()}\")\n","\n","    label_map = {old_label: new_label for new_label, old_label in enumerate(sorted(all_unique_labels))}\n","    label_map_inverse = {v: k for k, v in label_map.items()}\n","\n","    print(f\"  Mapping: {label_map}\")\n","\n","    y_train_full_mapped = np.array([label_map[y] for y in y_train_full], dtype=np.int64)\n","    y_test_mapped = np.array([label_map[y] for y in y_test], dtype=np.int64)\n","\n","    print(f\"  Mapped label range: {np.unique(y_train_full_mapped).tolist()}\")\n","    print(f\"  #classes: {len(all_unique_labels)}\")\n","\n","    # ----- 1.1 Subject-exclusive validation split -----\n","    print(f\"\\nValidation split by subject from train (subject-exclusive):\")\n","\n","    train_subjects = df_train_meta['subject_id'].unique()\n","    n_train_subjects = len(train_subjects)\n","    print(f\"  #train subjects: {n_train_subjects}\")\n","\n","    np.random.seed(RANDOM_SEED)\n","    n_val_subjects = max(1, int(n_train_subjects * DEEP_CONFIG['val_split']))\n","    val_subjects = np.random.choice(train_subjects, size=n_val_subjects, replace=False)\n","    train_subjects_final = [s for s in train_subjects if s not in val_subjects]\n","\n","    print(f\"  #val subjects: {n_val_subjects} ({n_val_subjects/n_train_subjects*100:.1f}%)\")\n","    print(f\"  Val subjects: {sorted(val_subjects.tolist())}\")\n","    print(f\"  Final #train subjects: {len(train_subjects_final)}\")\n","\n","    val_mask = df_train_meta['subject_id'].isin(val_subjects)\n","    train_mask = ~val_mask\n","\n","    X_train = X_train_full[train_mask]\n","    y_train = y_train_full_mapped[train_mask]\n","    X_val = X_train_full[val_mask]\n","    y_val = y_train_full_mapped[val_mask]\n","\n","    print(f\"\\nAfter split:\")\n","    print(f\"  Train: {X_train.shape}\")\n","    print(f\"  Val:   {X_val.shape}\")\n","    print(f\"  Test:  {X_test.shape}\")\n","\n","    assert set(df_train_meta[train_mask]['subject_id'].unique()).isdisjoint(\n","        set(df_train_meta[val_mask]['subject_id'].unique())\n","    ), f\"[{fold_tag}] Train/Val subjects overlap!\"\n","    print(\"  ✓ Train/Val subjects disjoint\")\n","\n","    n_channels = X_train.shape[2]\n","    n_classes = len(all_unique_labels)\n","    print(f\"\\nModel params:\")\n","    print(f\"  Input channels: {n_channels}\")\n","    print(f\"  #classes:       {n_classes}\")\n","\n","    # Datasets & loaders\n","    train_dataset = WindowDataset(X_train, y_train)\n","    val_dataset = WindowDataset(X_val, y_val)\n","    test_dataset = WindowDataset(X_test, y_test_mapped)\n","\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=DEEP_CONFIG['batch_size'],\n","        shuffle=True,\n","        num_workers=DEEP_CONFIG['num_workers'],\n","        pin_memory=torch.cuda.is_available()\n","    )\n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=DEEP_CONFIG['batch_size'],\n","        shuffle=False,\n","        num_workers=DEEP_CONFIG['num_workers'],\n","        pin_memory=torch.cuda.is_available()\n","    )\n","    test_loader = DataLoader(\n","        test_dataset,\n","        batch_size=DEEP_CONFIG['batch_size'],\n","        shuffle=False,\n","        num_workers=DEEP_CONFIG['num_workers'],\n","        pin_memory=torch.cuda.is_available()\n","    )\n","\n","    model = InceptionTime(n_channels, n_classes, INCEPTION_CONFIG).to(DEVICE)\n","    print(f\"\\nModel summary ({fold_tag}):\")\n","    print(f\"  #params: {sum(p.numel() for p in model.parameters()):,}\")\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(\n","        model.parameters(),\n","        lr=DEEP_CONFIG['learning_rate'],\n","        weight_decay=DEEP_CONFIG['weight_decay']\n","    )\n","    scaler = GradScaler() if USE_SCALER else None\n","\n","    best_val_loss = float('inf')\n","    patience_counter = 0\n","    best_epoch = 0\n","    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n","\n","    print(f\"\\nStart training ({fold_tag}):\")\n","    print(f\"  Epochs:       {DEEP_CONFIG['epochs']}\")\n","    print(f\"  Batch size:   {DEEP_CONFIG['batch_size']}\")\n","    print(f\"  Learning rate:{DEEP_CONFIG['learning_rate']}\")\n","    print(f\"  Patience:     {DEEP_CONFIG['patience']}\")\n","    if USE_AMP:\n","        dtype_str = 'bf16' if AMP_DTYPE == torch.bfloat16 else 'fp16'\n","        scaler_str = ' (using GradScaler)' if USE_SCALER else ' (no GradScaler)'\n","        print(f\"  Mixed precision: {dtype_str}{scaler_str}\")\n","    else:\n","        print(\"  Mixed precision: OFF\")\n","\n","    start_time = time.time()\n","\n","    for epoch in range(DEEP_CONFIG['epochs']):\n","        train_loss, train_acc = train_epoch(\n","            model, train_loader, criterion, optimizer, scaler, DEVICE,\n","            USE_AMP, AMP_DTYPE, USE_SCALER\n","        )\n","\n","        val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, DEVICE)\n","\n","        history['train_loss'].append(train_loss)\n","        history['train_acc'].append(train_acc)\n","        history['val_loss'].append(val_loss)\n","        history['val_acc'].append(val_acc)\n","\n","        print(f\"Epoch {epoch+1:3d}/{DEEP_CONFIG['epochs']}: \"\n","              f\"Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n","              f\"Val Loss={val_loss:.4f}, Acc={val_acc:.4f}\")\n","\n","        if val_loss < best_val_loss - DEEP_CONFIG['min_delta']:\n","            best_val_loss = val_loss\n","            best_epoch = epoch\n","            patience_counter = 0\n","\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'val_loss': val_loss,\n","                'val_acc': val_acc,\n","                'val_subjects': sorted(val_subjects.tolist()),\n","            }, models_dir / \"inception_time_best.pt\")\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= DEEP_CONFIG['patience']:\n","                print(f\"\\nEarly stopping triggered, best epoch: {best_epoch+1}\")\n","                break\n","\n","    train_time = time.time() - start_time\n","    print(f\"\\nTraining finished ({fold_tag}), time: {train_time:.2f}s\")\n","\n","    # Load best model and evaluate on test set\n","    print(f\"\\nFinal evaluation on test set ({fold_tag}):\")\n","    checkpoint = torch.load(models_dir / \"inception_time_best.pt\", map_location=DEVICE)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    test_loss, test_acc, test_preds, test_labels = evaluate(model, test_loader, criterion, DEVICE)\n","\n","    test_f1_macro = f1_score(test_labels, test_preds, average='macro')\n","    print(f\"  Test Loss:       {test_loss:.4f}\")\n","    print(f\"  Test Accuracy:   {test_acc:.4f}\")\n","    print(f\"  Test F1 (macro): {test_f1_macro:.4f}\")\n","\n","    deep_results = {\n","        'model': 'InceptionTime',\n","        'fold_id': fold_id,\n","        'random_seed': RANDOM_SEED,\n","        'best_epoch': int(best_epoch + 1),\n","        'train_time': float(train_time),\n","        'label_mapping': {\n","            'original_labels': sorted(all_unique_labels.tolist()),\n","            'mapped_labels': list(range(len(all_unique_labels))),\n","            'label_map': {int(k): int(v) for k, v in label_map.items()},\n","            'label_map_inverse': {int(k): int(v) for k, v in label_map_inverse.items()},\n","        },\n","        'validation': {\n","            'val_subjects': sorted(val_subjects.tolist()),\n","            'n_val_subjects': int(n_val_subjects),\n","            'n_train_subjects': len(train_subjects_final),\n","            'best_val_loss': float(best_val_loss),\n","            'best_val_acc': float(checkpoint['val_acc']),\n","        },\n","        'test': {\n","            'test_accuracy': float(test_acc),\n","            'test_loss': float(test_loss),\n","            'test_f1_macro': float(test_f1_macro),\n","        },\n","        'history': history,\n","        'config': {**DEEP_CONFIG, **INCEPTION_CONFIG},\n","        'mixed_precision': {\n","            'enabled': USE_AMP,\n","            'dtype': 'bf16' if AMP_DTYPE == torch.bfloat16 else 'fp16' if USE_AMP else 'None',\n","            'use_grad_scaler': USE_SCALER,\n","            'note': 'GradScaler is only used for fp16; bf16 does not need it',\n","        },\n","        'notes': [\n","            'Validation split by subject from training set (subject-exclusive)',\n","            'Early stopping based on validation set to avoid test leakage',\n","            'Test set used only for final evaluation',\n","            f'Mixed precision: {\"bf16 (no GradScaler)\" if AMP_DTYPE == torch.bfloat16 else \"fp16 (with GradScaler)\" if USE_AMP else \"OFF\"}',\n","            'Labels remapped to contiguous 0..n_classes-1',\n","        ]\n","    }\n","\n","    with open(models_dir / \"inception_time_results.json\", 'w') as f:\n","        json.dump(deep_results, f, indent=2)\n","\n","    print(\"✓ Saved: inception_time_best.pt\")\n","    print(\"✓ Saved: inception_time_results.json\")\n","\n","    # ========== 2. Train classical models (RF/KNN) ==========\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"2. Train classical models (RF/KNN)\")\n","    print(\"=\" * 60)\n","\n","    features_dir = proc_dir / \"features\" / fold_tag\n","    train_X_file = features_dir / \"train_X.parquet\"\n","    train_y_file = features_dir / \"train_y.parquet\"\n","    test_X_file = features_dir / \"test_X.parquet\"\n","    test_y_file = features_dir / \"test_y.parquet\"\n","\n","    if not train_X_file.exists():\n","        raise FileNotFoundError(f\"[{fold_tag}] Feature data not found: {train_X_file}, please run Step 11 (handcrafted features) first\")\n","\n","    df_train_X = pd.read_parquet(train_X_file)\n","    df_train_y = pd.read_parquet(train_y_file)\n","    df_test_X = pd.read_parquet(test_X_file)\n","    df_test_y = pd.read_parquet(test_y_file)\n","\n","    X_train_feat = df_train_X.values\n","    y_train_feat_raw = df_train_y['label'].values\n","    X_test_feat = df_test_X.values\n","    y_test_feat_raw = df_test_y['label'].values\n","\n","    # Apply same label mapping\n","    y_train_feat = np.array([label_map[y] for y in y_train_feat_raw], dtype=np.int64)\n","    y_test_feat = np.array([label_map[y] for y in y_test_feat_raw], dtype=np.int64)\n","\n","    print(f\"\\nFeature data ({fold_tag}):\")\n","    print(f\"  X_train: {X_train_feat.shape}\")\n","    print(f\"  X_test:  {X_test_feat.shape}\")\n","    print(\"  Labels remapped\")\n","\n","    # ----- 2.1 RandomForest -----\n","    print(\"\\nTraining RandomForest...\")\n","    print(f\"  Config: {RF_CONFIG}\")\n","\n","    rf_start = time.time()\n","    rf_model = RandomForestClassifier(**RF_CONFIG)\n","    rf_model.fit(X_train_feat, y_train_feat)\n","    rf_train_time = time.time() - rf_start\n","\n","    rf_train_pred = rf_model.predict(X_train_feat)\n","    rf_test_pred = rf_model.predict(X_test_feat)\n","\n","    rf_train_acc = accuracy_score(y_train_feat, rf_train_pred)\n","    rf_test_acc = accuracy_score(y_test_feat, rf_test_pred)\n","    rf_test_f1 = f1_score(y_test_feat, rf_test_pred, average='macro')\n","\n","    print(\"✓ RandomForest training complete:\")\n","    print(f\"  Train time: {rf_train_time:.2f}s\")\n","    print(f\"  Train Acc:  {rf_train_acc:.4f}\")\n","    print(f\"  Test  Acc:  {rf_test_acc:.4f}\")\n","    print(f\"  Test  F1 (macro): {rf_test_f1:.4f}\")\n","\n","    with open(models_dir / \"random_forest.pkl\", 'wb') as f:\n","        pickle.dump(rf_model, f)\n","\n","    rf_results = {\n","        'model': 'RandomForest',\n","        'fold_id': fold_id,\n","        'random_state': RF_CONFIG['random_state'],\n","        'train_time': float(rf_train_time),\n","        'train_accuracy': float(rf_train_acc),\n","        'test_accuracy': float(rf_test_acc),\n","        'test_f1_macro': float(rf_test_f1),\n","        'config': RF_CONFIG,\n","        'label_mapping': {\n","            'original_labels': sorted(all_unique_labels.tolist()),\n","            'mapped_labels': list(range(len(all_unique_labels))),\n","            'label_map': {int(k): int(v) for k, v in label_map.items()},\n","        },\n","    }\n","\n","    with open(models_dir / \"random_forest_results.json\", 'w') as f:\n","        json.dump(rf_results, f, indent=2)\n","\n","    print(\"✓ Saved: random_forest.pkl\")\n","    print(\"✓ Saved: random_forest_results.json\")\n","\n","    # ----- 2.2 KNN -----\n","    print(\"\\nTraining KNN...\")\n","    print(f\"  Config: {KNN_CONFIG}\")\n","\n","    knn_start = time.time()\n","    knn_model = KNeighborsClassifier(**KNN_CONFIG)\n","    knn_model.fit(X_train_feat, y_train_feat)\n","    knn_train_time = time.time() - knn_start\n","\n","    knn_train_pred = knn_model.predict(X_train_feat)\n","    knn_test_pred = knn_model.predict(X_test_feat)\n","\n","    knn_train_acc = accuracy_score(y_train_feat, knn_train_pred)\n","    knn_test_acc = accuracy_score(y_test_feat, knn_test_pred)\n","    knn_test_f1 = f1_score(y_test_feat, knn_test_pred, average='macro')\n","\n","    print(\"✓ KNN training complete:\")\n","    print(f\"  Train time: {knn_train_time:.2f}s\")\n","    print(f\"  Train Acc:  {knn_train_acc:.4f}\")\n","    print(f\"  Test  Acc:  {knn_test_acc:.4f}\")\n","    print(f\"  Test  F1 (macro): {knn_test_f1:.4f}\")\n","\n","    with open(models_dir / \"knn.pkl\", 'wb') as f:\n","        pickle.dump(knn_model, f)\n","\n","    knn_results = {\n","        'model': 'KNN',\n","        'fold_id': fold_id,\n","        'train_time': float(knn_train_time),\n","        'train_accuracy': float(knn_train_acc),\n","        'test_accuracy': float(knn_test_acc),\n","        'test_f1_macro': float(knn_test_f1),\n","        'config': KNN_CONFIG,\n","        'label_mapping': {\n","            'original_labels': sorted(all_unique_labels.tolist()),\n","            'mapped_labels': list(range(len(all_unique_labels))),\n","            'label_map': {int(k): int(v) for k, v in label_map.items()},\n","        },\n","    }\n","\n","    with open(models_dir / \"knn_results.json\", 'w') as f:\n","        json.dump(knn_results, f, indent=2)\n","\n","    print(\"✓ Saved: knn.pkl\")\n","    print(\"✓ Saved: knn_results.json\")\n","\n","    # ----- 3. Save training config (per fold) -----\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"3. Save training config\")\n","    print(\"=\" * 60)\n","\n","    training_config = {\n","        'fold_id': fold_id,\n","        'fold_tag': fold_tag,\n","        'random_seed': RANDOM_SEED,\n","        'device': str(DEVICE),\n","        'timestamp': datetime.now(timezone.utc).isoformat(),\n","        'mixed_precision': {\n","            'enabled': USE_AMP,\n","            'dtype': 'bf16' if AMP_DTYPE == torch.bfloat16 else 'fp16' if USE_AMP else 'None',\n","            'use_grad_scaler': USE_SCALER,\n","            'auto_selection': 'bf16 first (no GradScaler), fp16 second (with GradScaler), OFF on CPU',\n","            'note': 'GradScaler is only used for fp16 to prevent underflow; bf16 has sufficient dynamic range',\n","        },\n","        'models': {\n","            'inception_time': {\n","                'architecture': 'InceptionTime (1D Conv)',\n","                'config': {**DEEP_CONFIG, **INCEPTION_CONFIG},\n","                'validation': {\n","                    'val_subjects': sorted(val_subjects.tolist()),\n","                    'n_val_subjects': int(n_val_subjects),\n","                    'split_ratio': DEEP_CONFIG['val_split'],\n","                    'subject_exclusive': True,\n","                },\n","                'test_accuracy': float(test_acc),\n","                'test_f1_macro': float(test_f1_macro),\n","                'train_time': float(train_time),\n","                'best_epoch': int(best_epoch + 1),\n","            },\n","            'random_forest': {\n","                'config': RF_CONFIG,\n","                'test_accuracy': float(rf_test_acc),\n","                'test_f1_macro': float(rf_test_f1),\n","                'train_time': float(rf_train_time),\n","            },\n","            'knn': {\n","                'config': KNN_CONFIG,\n","                'test_accuracy': float(knn_test_acc),\n","                'test_f1_macro': float(knn_test_f1),\n","                'train_time': float(knn_train_time),\n","            },\n","        },\n","        'notes': [\n","            'Deep model uses InceptionTime (native 1D convolution)',\n","            'Validation subjects split from train (subject-exclusive)',\n","            'Early stopping based on validation set (no test leakage)',\n","            'Test set only for final evaluation',\n","            f'Mixed precision: {\"bf16 (no GradScaler)\" if AMP_DTYPE == torch.bfloat16 else \"fp16 (with GradScaler)\" if USE_AMP else \"OFF (CPU)\"}',\n","            'GradScaler only used for fp16',\n","            'Single random seed for reproducibility',\n","            'RF records random_state for reproducibility',\n","            'Labels remapped to contiguous 0..n_classes-1 (prevent IndexError)',\n","            'All models use the same label mapping',\n","            'All models evaluated on the same fold’s independent test set',\n","            'Strict LOSO: train/val/test subjects fully disjoint',\n","        ]\n","    }\n","\n","    config_file = models_dir / \"training_config.yaml\"\n","    with open(config_file, 'w', encoding='utf-8') as f:\n","        yaml.dump(training_config, f, default_flow_style=False, allow_unicode=True)\n","\n","    print(f\"✓ Saved: {config_file}\")\n","\n","    # Collect key metrics for this fold into the global summary list\n","    fold_summaries.append({\n","        'fold_id': fold_id,\n","        'fold_tag': fold_tag,\n","        'deep_test_acc': float(test_acc),\n","        'deep_test_f1': float(test_f1_macro),\n","        'rf_test_acc': float(rf_test_acc),\n","        'rf_test_f1': float(rf_test_f1),\n","        'knn_test_acc': float(knn_test_acc),\n","        'knn_test_f1': float(knn_test_f1),\n","    })\n","\n","    # Short per-fold summary for logging\n","    print(\"\\n\" + \"=\" * 60)\n","    print(f\"Summary for fold {fold_tag}\")\n","    print(\"=\" * 60)\n","    print(f\"  InceptionTime: Acc={test_acc:.4f}, F1={test_f1_macro:.4f}, best_epoch={best_epoch+1}\")\n","    print(f\"  RandomForest:  Acc={rf_test_acc:.4f}, F1={rf_test_f1:.4f}\")\n","    print(f\"  KNN:           Acc={knn_test_acc:.4f}, F1={knn_test_f1:.4f}\")\n","    print(f\"  Outputs: {models_dir}/\")\n","\n","# ========== Global summary ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"Step 13 complete - Model Configuration & Training for all folds\")\n","print(\"=\" * 60)\n","\n","if fold_summaries:\n","    df_summary = pd.DataFrame(fold_summaries).sort_values('fold_id')\n","    print(\"\\nPer-fold test performance (deep / RF / KNN):\")\n","    print(df_summary[['fold_id',\n","                      'deep_test_acc', 'deep_test_f1',\n","                      'rf_test_acc', 'rf_test_f1',\n","                      'knn_test_acc', 'knn_test_f1']])\n","\n","    print(\"\\nCross-fold mean ± std (deep model):\")\n","    print(f\"  Acc: {df_summary['deep_test_acc'].mean():.4f} ± {df_summary['deep_test_acc'].std():.4f}\")\n","    print(f\"  F1 : {df_summary['deep_test_f1'].mean():.4f} ± {df_summary['deep_test_f1'].std():.4f}\")\n","else:\n","    print(\"No folds were processed (check previous steps).\")\n","\n","print(\"\\nNext steps:\")\n","print(\"  - Compute cross-fold mean ± standard deviation for each model (a high-level overview for the deep model is printed above).\")\n","print(\"  - In subsequent Step 16 / error analysis, read the JSON files under models/*/ and aggregate them into tables and plots.\")\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vlCI5J2020Ew","executionInfo":{"status":"ok","timestamp":1763148987216,"user_tz":0,"elapsed":209219,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"2b838ce1-f0da-4fa3-8541-01462403a9c5"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 13: Model Configuration & Training (multi-fold)\n","============================================================\n","\n","Device: cuda\n","Mixed precision: bf16 (no GradScaler)\n","Random seed: 42\n","\n","Detected folds from configs/splits.json: [0, 1, 2, 3, 4, 5, 6, 7]\n","\n","============================================================\n","Processing fold 0 (fold_00)\n","============================================================\n","\n","============================================================\n","1. Train deep model (InceptionTime)\n","============================================================\n","\n","Data loaded:\n","  X_train_full: (4965, 150, 8)\n","  y_train_full: (4965,)\n","  X_test:       (766, 150, 8)\n","  y_test:       (766,)\n","\n","Label remapping (ensure contiguous 0..n_classes-1):\n","  Original label set: [1, 2, 3, 4, 5, 6]\n","  Mapping: {np.int32(1): 0, np.int32(2): 1, np.int32(3): 2, np.int32(4): 3, np.int32(5): 4, np.int32(6): 5}\n","  Mapped label range: [0, 1, 2, 3, 4, 5]\n","  #classes: 6\n","\n","Validation split by subject from train (subject-exclusive):\n","  #train subjects: 7\n","  #val subjects: 1 (14.3%)\n","  Val subjects: ['S08']\n","  Final #train subjects: 6\n","\n","After split:\n","  Train: (4306, 150, 8)\n","  Val:   (659, 150, 8)\n","  Test:  (766, 150, 8)\n","  ✓ Train/Val subjects disjoint\n","\n","Model params:\n","  Input channels: 8\n","  #classes:       6\n","\n","Model summary (fold_00):\n","  #params: 538,374\n","\n","Start training (fold_00):\n","  Epochs:       100\n","  Batch size:   64\n","  Learning rate:0.001\n","  Patience:     10\n","  Mixed precision: bf16 (no GradScaler)\n","Epoch   1/100: Train Loss=1.1266, Acc=0.6043 | Val Loss=1.2812, Acc=0.6267\n","Epoch   2/100: Train Loss=0.9887, Acc=0.6461 | Val Loss=1.0421, Acc=0.6373\n","Epoch   3/100: Train Loss=0.9022, Acc=0.6807 | Val Loss=1.1513, Acc=0.5766\n","Epoch   4/100: Train Loss=0.8367, Acc=0.6974 | Val Loss=1.2690, Acc=0.5736\n","Epoch   5/100: Train Loss=0.7996, Acc=0.7106 | Val Loss=1.0623, Acc=0.6343\n","Epoch   6/100: Train Loss=0.7489, Acc=0.7299 | Val Loss=1.1939, Acc=0.5524\n","Epoch   7/100: Train Loss=0.7168, Acc=0.7441 | Val Loss=1.1013, Acc=0.5964\n","Epoch   8/100: Train Loss=0.7153, Acc=0.7420 | Val Loss=1.3570, Acc=0.5918\n","Epoch   9/100: Train Loss=0.6713, Acc=0.7578 | Val Loss=1.1490, Acc=0.6404\n","Epoch  10/100: Train Loss=0.6256, Acc=0.7747 | Val Loss=1.2626, Acc=0.6297\n","Epoch  11/100: Train Loss=0.6084, Acc=0.7798 | Val Loss=1.3317, Acc=0.6055\n","Epoch  12/100: Train Loss=0.5583, Acc=0.7908 | Val Loss=1.6394, Acc=0.5675\n","\n","Early stopping triggered, best epoch: 2\n","\n","Training finished (fold_00), time: 18.95s\n","\n","Final evaluation on test set (fold_00):\n","  Test Loss:       1.1145\n","  Test Accuracy:   0.6110\n","  Test F1 (macro): 0.4617\n","✓ Saved: inception_time_best.pt\n","✓ Saved: inception_time_results.json\n","\n","============================================================\n","2. Train classical models (RF/KNN)\n","============================================================\n","\n","Feature data (fold_00):\n","  X_train: (4965, 220)\n","  X_test:  (766, 220)\n","  Labels remapped\n","\n","Training RandomForest...\n","  Config: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'random_state': 42, 'n_jobs': -1}\n","✓ RandomForest training complete:\n","  Train time: 2.44s\n","  Train Acc:  0.9881\n","  Test  Acc:  0.6292\n","  Test  F1 (macro): 0.4092\n","✓ Saved: random_forest.pkl\n","✓ Saved: random_forest_results.json\n","\n","Training KNN...\n","  Config: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'n_jobs': -1}\n","✓ KNN training complete:\n","  Train time: 0.00s\n","  Train Acc:  1.0000\n","  Test  Acc:  0.5627\n","  Test  F1 (macro): 0.4577\n","✓ Saved: knn.pkl\n","✓ Saved: knn_results.json\n","\n","============================================================\n","3. Save training config\n","============================================================\n","✓ Saved: models/fold_00/training_config.yaml\n","\n","============================================================\n","Summary for fold fold_00\n","============================================================\n","  InceptionTime: Acc=0.6110, F1=0.4617, best_epoch=2\n","  RandomForest:  Acc=0.6292, F1=0.4092\n","  KNN:           Acc=0.5627, F1=0.4577\n","  Outputs: models/fold_00/\n","\n","============================================================\n","Processing fold 1 (fold_01)\n","============================================================\n","\n","============================================================\n","1. Train deep model (InceptionTime)\n","============================================================\n","\n","Data loaded:\n","  X_train_full: (5072, 150, 8)\n","  y_train_full: (5072,)\n","  X_test:       (659, 150, 8)\n","  y_test:       (659,)\n","\n","Label remapping (ensure contiguous 0..n_classes-1):\n","  Original label set: [1, 2, 3, 4, 5, 6]\n","  Mapping: {np.int32(1): 0, np.int32(2): 1, np.int32(3): 2, np.int32(4): 3, np.int32(5): 4, np.int32(6): 5}\n","  Mapped label range: [0, 1, 2, 3, 4, 5]\n","  #classes: 6\n","\n","Validation split by subject from train (subject-exclusive):\n","  #train subjects: 7\n","  #val subjects: 1 (14.3%)\n","  Val subjects: ['S07']\n","  Final #train subjects: 6\n","\n","After split:\n","  Train: (4306, 150, 8)\n","  Val:   (766, 150, 8)\n","  Test:  (659, 150, 8)\n","  ✓ Train/Val subjects disjoint\n","\n","Model params:\n","  Input channels: 8\n","  #classes:       6\n","\n","Model summary (fold_01):\n","  #params: 538,374\n","\n","Start training (fold_01):\n","  Epochs:       100\n","  Batch size:   64\n","  Learning rate:0.001\n","  Patience:     10\n","  Mixed precision: bf16 (no GradScaler)\n","Epoch   1/100: Train Loss=1.1009, Acc=0.6105 | Val Loss=1.1528, Acc=0.6057\n","Epoch   2/100: Train Loss=0.9174, Acc=0.6744 | Val Loss=1.1122, Acc=0.6240\n","Epoch   3/100: Train Loss=0.8922, Acc=0.6788 | Val Loss=0.9231, Acc=0.6684\n","Epoch   4/100: Train Loss=0.8336, Acc=0.7027 | Val Loss=0.9594, Acc=0.6684\n","Epoch   5/100: Train Loss=0.7898, Acc=0.7185 | Val Loss=0.9485, Acc=0.6828\n","Epoch   6/100: Train Loss=0.7455, Acc=0.7325 | Val Loss=0.8873, Acc=0.6906\n","Epoch   7/100: Train Loss=0.7256, Acc=0.7413 | Val Loss=1.0118, Acc=0.6554\n","Epoch   8/100: Train Loss=0.6928, Acc=0.7471 | Val Loss=0.9791, Acc=0.6893\n","Epoch   9/100: Train Loss=0.6538, Acc=0.7657 | Val Loss=1.0214, Acc=0.6815\n","Epoch  10/100: Train Loss=0.5992, Acc=0.7833 | Val Loss=1.1145, Acc=0.6867\n","Epoch  11/100: Train Loss=0.5860, Acc=0.7829 | Val Loss=1.7218, Acc=0.6201\n","Epoch  12/100: Train Loss=0.5677, Acc=0.7956 | Val Loss=1.1719, Acc=0.6671\n","Epoch  13/100: Train Loss=0.5066, Acc=0.8191 | Val Loss=1.1951, Acc=0.6723\n","Epoch  14/100: Train Loss=0.4451, Acc=0.8365 | Val Loss=1.3707, Acc=0.6279\n","Epoch  15/100: Train Loss=0.4352, Acc=0.8393 | Val Loss=1.8035, Acc=0.4791\n","Epoch  16/100: Train Loss=0.4359, Acc=0.8402 | Val Loss=1.4384, Acc=0.6410\n","\n","Early stopping triggered, best epoch: 6\n","\n","Training finished (fold_01), time: 22.74s\n","\n","Final evaluation on test set (fold_01):\n","  Test Loss:       1.1558\n","  Test Accuracy:   0.6115\n","  Test F1 (macro): 0.3955\n","✓ Saved: inception_time_best.pt\n","✓ Saved: inception_time_results.json\n","\n","============================================================\n","2. Train classical models (RF/KNN)\n","============================================================\n","\n","Feature data (fold_01):\n","  X_train: (5072, 220)\n","  X_test:  (659, 220)\n","  Labels remapped\n","\n","Training RandomForest...\n","  Config: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'random_state': 42, 'n_jobs': -1}\n","✓ RandomForest training complete:\n","  Train time: 2.57s\n","  Train Acc:  0.9901\n","  Test  Acc:  0.6267\n","  Test  F1 (macro): 0.3324\n","✓ Saved: random_forest.pkl\n","✓ Saved: random_forest_results.json\n","\n","Training KNN...\n","  Config: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'n_jobs': -1}\n","✓ KNN training complete:\n","  Train time: 0.00s\n","  Train Acc:  1.0000\n","  Test  Acc:  0.6085\n","  Test  F1 (macro): 0.3687\n","✓ Saved: knn.pkl\n","✓ Saved: knn_results.json\n","\n","============================================================\n","3. Save training config\n","============================================================\n","✓ Saved: models/fold_01/training_config.yaml\n","\n","============================================================\n","Summary for fold fold_01\n","============================================================\n","  InceptionTime: Acc=0.6115, F1=0.3955, best_epoch=6\n","  RandomForest:  Acc=0.6267, F1=0.3324\n","  KNN:           Acc=0.6085, F1=0.3687\n","  Outputs: models/fold_01/\n","\n","============================================================\n","Processing fold 2 (fold_02)\n","============================================================\n","\n","============================================================\n","1. Train deep model (InceptionTime)\n","============================================================\n","\n","Data loaded:\n","  X_train_full: (4960, 150, 8)\n","  y_train_full: (4960,)\n","  X_test:       (771, 150, 8)\n","  y_test:       (771,)\n","\n","Label remapping (ensure contiguous 0..n_classes-1):\n","  Original label set: [1, 2, 3, 4, 5, 6]\n","  Mapping: {np.int32(1): 0, np.int32(2): 1, np.int32(3): 2, np.int32(4): 3, np.int32(5): 4, np.int32(6): 5}\n","  Mapped label range: [0, 1, 2, 3, 4, 5]\n","  #classes: 6\n","\n","Validation split by subject from train (subject-exclusive):\n","  #train subjects: 7\n","  #val subjects: 1 (14.3%)\n","  Val subjects: ['S07']\n","  Final #train subjects: 6\n","\n","After split:\n","  Train: (4194, 150, 8)\n","  Val:   (766, 150, 8)\n","  Test:  (771, 150, 8)\n","  ✓ Train/Val subjects disjoint\n","\n","Model params:\n","  Input channels: 8\n","  #classes:       6\n","\n","Model summary (fold_02):\n","  #params: 538,374\n","\n","Start training (fold_02):\n","  Epochs:       100\n","  Batch size:   64\n","  Learning rate:0.001\n","  Patience:     10\n","  Mixed precision: bf16 (no GradScaler)\n","Epoch   1/100: Train Loss=1.1385, Acc=0.5999 | Val Loss=1.0021, Acc=0.6384\n","Epoch   2/100: Train Loss=0.9679, Acc=0.6559 | Val Loss=0.9287, Acc=0.6932\n","Epoch   3/100: Train Loss=0.9001, Acc=0.6781 | Val Loss=1.0393, Acc=0.6410\n","Epoch   4/100: Train Loss=0.8466, Acc=0.6938 | Val Loss=0.8954, Acc=0.6945\n","Epoch   5/100: Train Loss=0.8169, Acc=0.7005 | Val Loss=0.9736, Acc=0.6684\n","Epoch   6/100: Train Loss=0.7787, Acc=0.7172 | Val Loss=1.0291, Acc=0.6684\n","Epoch   7/100: Train Loss=0.7234, Acc=0.7358 | Val Loss=0.9291, Acc=0.6854\n","Epoch   8/100: Train Loss=0.7139, Acc=0.7382 | Val Loss=1.1542, Acc=0.6475\n","Epoch   9/100: Train Loss=0.6489, Acc=0.7632 | Val Loss=1.0377, Acc=0.6984\n","Epoch  10/100: Train Loss=0.6200, Acc=0.7680 | Val Loss=1.1312, Acc=0.6319\n","Epoch  11/100: Train Loss=0.5812, Acc=0.7854 | Val Loss=1.2506, Acc=0.5914\n","Epoch  12/100: Train Loss=0.5441, Acc=0.7983 | Val Loss=1.2566, Acc=0.6488\n","Epoch  13/100: Train Loss=0.4880, Acc=0.8264 | Val Loss=1.1234, Acc=0.6436\n","Epoch  14/100: Train Loss=0.4407, Acc=0.8460 | Val Loss=1.6425, Acc=0.5953\n","\n","Early stopping triggered, best epoch: 4\n","\n","Training finished (fold_02), time: 19.49s\n","\n","Final evaluation on test set (fold_02):\n","  Test Loss:       1.0119\n","  Test Accuracy:   0.7043\n","  Test F1 (macro): 0.4943\n","✓ Saved: inception_time_best.pt\n","✓ Saved: inception_time_results.json\n","\n","============================================================\n","2. Train classical models (RF/KNN)\n","============================================================\n","\n","Feature data (fold_02):\n","  X_train: (4960, 220)\n","  X_test:  (771, 220)\n","  Labels remapped\n","\n","Training RandomForest...\n","  Config: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'random_state': 42, 'n_jobs': -1}\n","✓ RandomForest training complete:\n","  Train time: 2.43s\n","  Train Acc:  0.9915\n","  Test  Acc:  0.7224\n","  Test  F1 (macro): 0.4514\n","✓ Saved: random_forest.pkl\n","✓ Saved: random_forest_results.json\n","\n","Training KNN...\n","  Config: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'n_jobs': -1}\n","✓ KNN training complete:\n","  Train time: 0.00s\n","  Train Acc:  1.0000\n","  Test  Acc:  0.6329\n","  Test  F1 (macro): 0.4859\n","✓ Saved: knn.pkl\n","✓ Saved: knn_results.json\n","\n","============================================================\n","3. Save training config\n","============================================================\n","✓ Saved: models/fold_02/training_config.yaml\n","\n","============================================================\n","Summary for fold fold_02\n","============================================================\n","  InceptionTime: Acc=0.7043, F1=0.4943, best_epoch=4\n","  RandomForest:  Acc=0.7224, F1=0.4514\n","  KNN:           Acc=0.6329, F1=0.4859\n","  Outputs: models/fold_02/\n","\n","============================================================\n","Processing fold 3 (fold_03)\n","============================================================\n","\n","============================================================\n","1. Train deep model (InceptionTime)\n","============================================================\n","\n","Data loaded:\n","  X_train_full: (4858, 150, 8)\n","  y_train_full: (4858,)\n","  X_test:       (873, 150, 8)\n","  y_test:       (873,)\n","\n","Label remapping (ensure contiguous 0..n_classes-1):\n","  Original label set: [1, 2, 3, 4, 5, 6]\n","  Mapping: {np.int32(1): 0, np.int32(2): 1, np.int32(3): 2, np.int32(4): 3, np.int32(5): 4, np.int32(6): 5}\n","  Mapped label range: [0, 1, 2, 3, 4, 5]\n","  #classes: 6\n","\n","Validation split by subject from train (subject-exclusive):\n","  #train subjects: 7\n","  #val subjects: 1 (14.3%)\n","  Val subjects: ['S07']\n","  Final #train subjects: 6\n","\n","After split:\n","  Train: (4092, 150, 8)\n","  Val:   (766, 150, 8)\n","  Test:  (873, 150, 8)\n","  ✓ Train/Val subjects disjoint\n","\n","Model params:\n","  Input channels: 8\n","  #classes:       6\n","\n","Model summary (fold_03):\n","  #params: 538,374\n","\n","Start training (fold_03):\n","  Epochs:       100\n","  Batch size:   64\n","  Learning rate:0.001\n","  Patience:     10\n","  Mixed precision: bf16 (no GradScaler)\n","Epoch   1/100: Train Loss=1.1080, Acc=0.6158 | Val Loss=1.0511, Acc=0.6162\n","Epoch   2/100: Train Loss=0.9431, Acc=0.6823 | Val Loss=0.9697, Acc=0.6540\n","Epoch   3/100: Train Loss=0.8753, Acc=0.6957 | Val Loss=0.9912, Acc=0.6384\n","Epoch   4/100: Train Loss=0.8262, Acc=0.7141 | Val Loss=0.9360, Acc=0.6789\n","Epoch   5/100: Train Loss=0.7749, Acc=0.7317 | Val Loss=1.0298, Acc=0.6449\n","Epoch   6/100: Train Loss=0.7537, Acc=0.7319 | Val Loss=0.9857, Acc=0.6527\n","Epoch   7/100: Train Loss=0.7294, Acc=0.7458 | Val Loss=0.9967, Acc=0.6462\n","Epoch   8/100: Train Loss=0.6793, Acc=0.7612 | Val Loss=0.9515, Acc=0.6762\n","Epoch   9/100: Train Loss=0.6566, Acc=0.7652 | Val Loss=1.2461, Acc=0.6332\n","Epoch  10/100: Train Loss=0.6041, Acc=0.7840 | Val Loss=1.0451, Acc=0.6749\n","Epoch  11/100: Train Loss=0.5793, Acc=0.7955 | Val Loss=1.2493, Acc=0.5822\n","Epoch  12/100: Train Loss=0.5425, Acc=0.8111 | Val Loss=1.5684, Acc=0.5979\n","Epoch  13/100: Train Loss=0.4949, Acc=0.8201 | Val Loss=1.3452, Acc=0.5927\n","Epoch  14/100: Train Loss=0.4671, Acc=0.8341 | Val Loss=1.6098, Acc=0.5078\n","\n","Early stopping triggered, best epoch: 4\n","\n","Training finished (fold_03), time: 19.02s\n","\n","Final evaluation on test set (fold_03):\n","  Test Loss:       1.0680\n","  Test Accuracy:   0.5853\n","  Test F1 (macro): 0.5416\n","✓ Saved: inception_time_best.pt\n","✓ Saved: inception_time_results.json\n","\n","============================================================\n","2. Train classical models (RF/KNN)\n","============================================================\n","\n","Feature data (fold_03):\n","  X_train: (4858, 220)\n","  X_test:  (873, 220)\n","  Labels remapped\n","\n","Training RandomForest...\n","  Config: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'random_state': 42, 'n_jobs': -1}\n","✓ RandomForest training complete:\n","  Train time: 2.43s\n","  Train Acc:  0.9874\n","  Test  Acc:  0.5979\n","  Test  F1 (macro): 0.4779\n","✓ Saved: random_forest.pkl\n","✓ Saved: random_forest_results.json\n","\n","Training KNN...\n","  Config: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'n_jobs': -1}\n","✓ KNN training complete:\n","  Train time: 0.01s\n","  Train Acc:  1.0000\n","  Test  Acc:  0.5590\n","  Test  F1 (macro): 0.5045\n","✓ Saved: knn.pkl\n","✓ Saved: knn_results.json\n","\n","============================================================\n","3. Save training config\n","============================================================\n","✓ Saved: models/fold_03/training_config.yaml\n","\n","============================================================\n","Summary for fold fold_03\n","============================================================\n","  InceptionTime: Acc=0.5853, F1=0.5416, best_epoch=4\n","  RandomForest:  Acc=0.5979, F1=0.4779\n","  KNN:           Acc=0.5590, F1=0.5045\n","  Outputs: models/fold_03/\n","\n","============================================================\n","Processing fold 4 (fold_04)\n","============================================================\n","\n","============================================================\n","1. Train deep model (InceptionTime)\n","============================================================\n","\n","Data loaded:\n","  X_train_full: (4985, 150, 8)\n","  y_train_full: (4985,)\n","  X_test:       (746, 150, 8)\n","  y_test:       (746,)\n","\n","Label remapping (ensure contiguous 0..n_classes-1):\n","  Original label set: [1, 2, 3, 4, 5, 6]\n","  Mapping: {np.int32(1): 0, np.int32(2): 1, np.int32(3): 2, np.int32(4): 3, np.int32(5): 4, np.int32(6): 5}\n","  Mapped label range: [0, 1, 2, 3, 4, 5]\n","  #classes: 6\n","\n","Validation split by subject from train (subject-exclusive):\n","  #train subjects: 7\n","  #val subjects: 1 (14.3%)\n","  Val subjects: ['S07']\n","  Final #train subjects: 6\n","\n","After split:\n","  Train: (4219, 150, 8)\n","  Val:   (766, 150, 8)\n","  Test:  (746, 150, 8)\n","  ✓ Train/Val subjects disjoint\n","\n","Model params:\n","  Input channels: 8\n","  #classes:       6\n","\n","Model summary (fold_04):\n","  #params: 538,374\n","\n","Start training (fold_04):\n","  Epochs:       100\n","  Batch size:   64\n","  Learning rate:0.001\n","  Patience:     10\n","  Mixed precision: bf16 (no GradScaler)\n","Epoch   1/100: Train Loss=1.1381, Acc=0.6153 | Val Loss=1.0753, Acc=0.6123\n","Epoch   2/100: Train Loss=0.9851, Acc=0.6504 | Val Loss=1.0186, Acc=0.6279\n","Epoch   3/100: Train Loss=0.9463, Acc=0.6599 | Val Loss=0.9843, Acc=0.6462\n","Epoch   4/100: Train Loss=0.8805, Acc=0.6904 | Val Loss=1.0092, Acc=0.6332\n","Epoch   5/100: Train Loss=0.8261, Acc=0.7028 | Val Loss=0.9507, Acc=0.6645\n","Epoch   6/100: Train Loss=0.7851, Acc=0.7149 | Val Loss=1.0255, Acc=0.6619\n","Epoch   7/100: Train Loss=0.7572, Acc=0.7281 | Val Loss=1.0602, Acc=0.6554\n","Epoch   8/100: Train Loss=0.7344, Acc=0.7329 | Val Loss=1.0577, Acc=0.6762\n","Epoch   9/100: Train Loss=0.6797, Acc=0.7528 | Val Loss=1.2403, Acc=0.6358\n","Epoch  10/100: Train Loss=0.6390, Acc=0.7682 | Val Loss=1.0028, Acc=0.6854\n","Epoch  11/100: Train Loss=0.6193, Acc=0.7727 | Val Loss=1.2077, Acc=0.6684\n","Epoch  12/100: Train Loss=0.5813, Acc=0.7893 | Val Loss=1.2361, Acc=0.6488\n","Epoch  13/100: Train Loss=0.5255, Acc=0.8080 | Val Loss=1.3253, Acc=0.6567\n","Epoch  14/100: Train Loss=0.4761, Acc=0.8229 | Val Loss=1.3987, Acc=0.6110\n","Epoch  15/100: Train Loss=0.4090, Acc=0.8502 | Val Loss=1.5525, Acc=0.6501\n","\n","Early stopping triggered, best epoch: 5\n","\n","Training finished (fold_04), time: 21.14s\n","\n","Final evaluation on test set (fold_04):\n","  Test Loss:       0.7477\n","  Test Accuracy:   0.7306\n","  Test F1 (macro): 0.6020\n","✓ Saved: inception_time_best.pt\n","✓ Saved: inception_time_results.json\n","\n","============================================================\n","2. Train classical models (RF/KNN)\n","============================================================\n","\n","Feature data (fold_04):\n","  X_train: (4985, 220)\n","  X_test:  (746, 220)\n","  Labels remapped\n","\n","Training RandomForest...\n","  Config: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'random_state': 42, 'n_jobs': -1}\n","✓ RandomForest training complete:\n","  Train time: 2.53s\n","  Train Acc:  0.9888\n","  Test  Acc:  0.7198\n","  Test  F1 (macro): 0.5428\n","✓ Saved: random_forest.pkl\n","✓ Saved: random_forest_results.json\n","\n","Training KNN...\n","  Config: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'n_jobs': -1}\n","✓ KNN training complete:\n","  Train time: 0.00s\n","  Train Acc:  1.0000\n","  Test  Acc:  0.6635\n","  Test  F1 (macro): 0.5729\n","✓ Saved: knn.pkl\n","✓ Saved: knn_results.json\n","\n","============================================================\n","3. Save training config\n","============================================================\n","✓ Saved: models/fold_04/training_config.yaml\n","\n","============================================================\n","Summary for fold fold_04\n","============================================================\n","  InceptionTime: Acc=0.7306, F1=0.6020, best_epoch=5\n","  RandomForest:  Acc=0.7198, F1=0.5428\n","  KNN:           Acc=0.6635, F1=0.5729\n","  Outputs: models/fold_04/\n","\n","============================================================\n","Processing fold 5 (fold_05)\n","============================================================\n","\n","============================================================\n","1. Train deep model (InceptionTime)\n","============================================================\n","\n","Data loaded:\n","  X_train_full: (5442, 150, 8)\n","  y_train_full: (5442,)\n","  X_test:       (289, 150, 8)\n","  y_test:       (289,)\n","\n","Label remapping (ensure contiguous 0..n_classes-1):\n","  Original label set: [1, 2, 3, 4, 5, 6]\n","  Mapping: {np.int32(1): 0, np.int32(2): 1, np.int32(3): 2, np.int32(4): 3, np.int32(5): 4, np.int32(6): 5}\n","  Mapped label range: [0, 1, 2, 3, 4, 5]\n","  #classes: 6\n","\n","Validation split by subject from train (subject-exclusive):\n","  #train subjects: 7\n","  #val subjects: 1 (14.3%)\n","  Val subjects: ['S07']\n","  Final #train subjects: 6\n","\n","After split:\n","  Train: (4676, 150, 8)\n","  Val:   (766, 150, 8)\n","  Test:  (289, 150, 8)\n","  ✓ Train/Val subjects disjoint\n","\n","Model params:\n","  Input channels: 8\n","  #classes:       6\n","\n","Model summary (fold_05):\n","  #params: 538,374\n","\n","Start training (fold_05):\n","  Epochs:       100\n","  Batch size:   64\n","  Learning rate:0.001\n","  Patience:     10\n","  Mixed precision: bf16 (no GradScaler)\n","Epoch   1/100: Train Loss=1.0924, Acc=0.6168 | Val Loss=1.0062, Acc=0.6227\n","Epoch   2/100: Train Loss=0.9376, Acc=0.6630 | Val Loss=0.9476, Acc=0.6606\n","Epoch   3/100: Train Loss=0.8516, Acc=0.6912 | Val Loss=1.0295, Acc=0.6540\n","Epoch   4/100: Train Loss=0.8178, Acc=0.7098 | Val Loss=0.9896, Acc=0.6671\n","Epoch   5/100: Train Loss=0.7835, Acc=0.7207 | Val Loss=1.0086, Acc=0.6606\n","Epoch   6/100: Train Loss=0.7720, Acc=0.7265 | Val Loss=1.0469, Acc=0.6423\n","Epoch   7/100: Train Loss=0.7358, Acc=0.7355 | Val Loss=1.0429, Acc=0.6671\n","Epoch   8/100: Train Loss=0.6729, Acc=0.7560 | Val Loss=1.1971, Acc=0.6606\n","Epoch   9/100: Train Loss=0.7059, Acc=0.7440 | Val Loss=0.9255, Acc=0.6906\n","Epoch  10/100: Train Loss=0.6377, Acc=0.7673 | Val Loss=1.0983, Acc=0.6606\n","Epoch  11/100: Train Loss=0.5940, Acc=0.7817 | Val Loss=1.2612, Acc=0.5770\n","Epoch  12/100: Train Loss=0.6673, Acc=0.7496 | Val Loss=1.1095, Acc=0.6449\n","Epoch  13/100: Train Loss=0.6030, Acc=0.7744 | Val Loss=0.9297, Acc=0.7076\n","Epoch  14/100: Train Loss=0.5276, Acc=0.8086 | Val Loss=1.6301, Acc=0.5992\n","Epoch  15/100: Train Loss=0.5013, Acc=0.8174 | Val Loss=1.0924, Acc=0.6684\n","Epoch  16/100: Train Loss=0.4561, Acc=0.8343 | Val Loss=1.1328, Acc=0.6867\n","Epoch  17/100: Train Loss=0.4566, Acc=0.8306 | Val Loss=1.3687, Acc=0.6893\n","Epoch  18/100: Train Loss=0.4760, Acc=0.8246 | Val Loss=1.4208, Acc=0.6345\n","Epoch  19/100: Train Loss=0.3108, Acc=0.8924 | Val Loss=1.9482, Acc=0.6475\n","\n","Early stopping triggered, best epoch: 9\n","\n","Training finished (fold_05), time: 29.08s\n","\n","Final evaluation on test set (fold_05):\n","  Test Loss:       1.6749\n","  Test Accuracy:   0.4602\n","  Test F1 (macro): 0.3894\n","✓ Saved: inception_time_best.pt\n","✓ Saved: inception_time_results.json\n","\n","============================================================\n","2. Train classical models (RF/KNN)\n","============================================================\n","\n","Feature data (fold_05):\n","  X_train: (5442, 220)\n","  X_test:  (289, 220)\n","  Labels remapped\n","\n","Training RandomForest...\n","  Config: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'random_state': 42, 'n_jobs': -1}\n","✓ RandomForest training complete:\n","  Train time: 2.76s\n","  Train Acc:  0.9890\n","  Test  Acc:  0.4498\n","  Test  F1 (macro): 0.2958\n","✓ Saved: random_forest.pkl\n","✓ Saved: random_forest_results.json\n","\n","Training KNN...\n","  Config: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'n_jobs': -1}\n","✓ KNN training complete:\n","  Train time: 0.00s\n","  Train Acc:  1.0000\n","  Test  Acc:  0.4464\n","  Test  F1 (macro): 0.3305\n","✓ Saved: knn.pkl\n","✓ Saved: knn_results.json\n","\n","============================================================\n","3. Save training config\n","============================================================\n","✓ Saved: models/fold_05/training_config.yaml\n","\n","============================================================\n","Summary for fold fold_05\n","============================================================\n","  InceptionTime: Acc=0.4602, F1=0.3894, best_epoch=9\n","  RandomForest:  Acc=0.4498, F1=0.2958\n","  KNN:           Acc=0.4464, F1=0.3305\n","  Outputs: models/fold_05/\n","\n","============================================================\n","Processing fold 6 (fold_06)\n","============================================================\n","\n","============================================================\n","1. Train deep model (InceptionTime)\n","============================================================\n","\n","Data loaded:\n","  X_train_full: (4853, 150, 8)\n","  y_train_full: (4853,)\n","  X_test:       (878, 150, 8)\n","  y_test:       (878,)\n","\n","Label remapping (ensure contiguous 0..n_classes-1):\n","  Original label set: [1, 2, 3, 4, 5, 6]\n","  Mapping: {np.int32(1): 0, np.int32(2): 1, np.int32(3): 2, np.int32(4): 3, np.int32(5): 4, np.int32(6): 5}\n","  Mapped label range: [0, 1, 2, 3, 4, 5]\n","  #classes: 6\n","\n","Validation split by subject from train (subject-exclusive):\n","  #train subjects: 7\n","  #val subjects: 1 (14.3%)\n","  Val subjects: ['S07']\n","  Final #train subjects: 6\n","\n","After split:\n","  Train: (4087, 150, 8)\n","  Val:   (766, 150, 8)\n","  Test:  (878, 150, 8)\n","  ✓ Train/Val subjects disjoint\n","\n","Model params:\n","  Input channels: 8\n","  #classes:       6\n","\n","Model summary (fold_06):\n","  #params: 538,374\n","\n","Start training (fold_06):\n","  Epochs:       100\n","  Batch size:   64\n","  Learning rate:0.001\n","  Patience:     10\n","  Mixed precision: bf16 (no GradScaler)\n","Epoch   1/100: Train Loss=1.1703, Acc=0.5911 | Val Loss=1.1171, Acc=0.6044\n","Epoch   2/100: Train Loss=0.9911, Acc=0.6437 | Val Loss=1.0110, Acc=0.6345\n","Epoch   3/100: Train Loss=0.9332, Acc=0.6675 | Val Loss=0.9730, Acc=0.6410\n","Epoch   4/100: Train Loss=0.8835, Acc=0.6839 | Val Loss=0.9958, Acc=0.6449\n","Epoch   5/100: Train Loss=0.8550, Acc=0.6890 | Val Loss=0.9250, Acc=0.6802\n","Epoch   6/100: Train Loss=0.7887, Acc=0.7176 | Val Loss=1.0922, Acc=0.6554\n","Epoch   7/100: Train Loss=0.7540, Acc=0.7269 | Val Loss=1.0423, Acc=0.6462\n","Epoch   8/100: Train Loss=0.7168, Acc=0.7453 | Val Loss=1.0605, Acc=0.6188\n","Epoch   9/100: Train Loss=0.6765, Acc=0.7470 | Val Loss=1.3631, Acc=0.5366\n","Epoch  10/100: Train Loss=0.6473, Acc=0.7622 | Val Loss=1.2856, Acc=0.6358\n","Epoch  11/100: Train Loss=0.6274, Acc=0.7666 | Val Loss=1.2353, Acc=0.5966\n","Epoch  12/100: Train Loss=0.5525, Acc=0.7989 | Val Loss=1.0727, Acc=0.6906\n","Epoch  13/100: Train Loss=0.5532, Acc=0.8011 | Val Loss=1.1556, Acc=0.6345\n","Epoch  14/100: Train Loss=0.4756, Acc=0.8265 | Val Loss=1.3506, Acc=0.5483\n","Epoch  15/100: Train Loss=0.4155, Acc=0.8444 | Val Loss=1.5802, Acc=0.6514\n","\n","Early stopping triggered, best epoch: 5\n","\n","Training finished (fold_06), time: 20.64s\n","\n","Final evaluation on test set (fold_06):\n","  Test Loss:       0.8984\n","  Test Accuracy:   0.6845\n","  Test F1 (macro): 0.5718\n","✓ Saved: inception_time_best.pt\n","✓ Saved: inception_time_results.json\n","\n","============================================================\n","2. Train classical models (RF/KNN)\n","============================================================\n","\n","Feature data (fold_06):\n","  X_train: (4853, 220)\n","  X_test:  (878, 220)\n","  Labels remapped\n","\n","Training RandomForest...\n","  Config: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'random_state': 42, 'n_jobs': -1}\n","✓ RandomForest training complete:\n","  Train time: 2.62s\n","  Train Acc:  0.9903\n","  Test  Acc:  0.7073\n","  Test  F1 (macro): 0.3821\n","✓ Saved: random_forest.pkl\n","✓ Saved: random_forest_results.json\n","\n","Training KNN...\n","  Config: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'n_jobs': -1}\n","✓ KNN training complete:\n","  Train time: 0.01s\n","  Train Acc:  1.0000\n","  Test  Acc:  0.6355\n","  Test  F1 (macro): 0.5555\n","✓ Saved: knn.pkl\n","✓ Saved: knn_results.json\n","\n","============================================================\n","3. Save training config\n","============================================================\n","✓ Saved: models/fold_06/training_config.yaml\n","\n","============================================================\n","Summary for fold fold_06\n","============================================================\n","  InceptionTime: Acc=0.6845, F1=0.5718, best_epoch=5\n","  RandomForest:  Acc=0.7073, F1=0.3821\n","  KNN:           Acc=0.6355, F1=0.5555\n","  Outputs: models/fold_06/\n","\n","============================================================\n","Processing fold 7 (fold_07)\n","============================================================\n","\n","============================================================\n","1. Train deep model (InceptionTime)\n","============================================================\n","\n","Data loaded:\n","  X_train_full: (4982, 150, 8)\n","  y_train_full: (4982,)\n","  X_test:       (749, 150, 8)\n","  y_test:       (749,)\n","\n","Label remapping (ensure contiguous 0..n_classes-1):\n","  Original label set: [1, 2, 3, 4, 5, 6]\n","  Mapping: {np.int32(1): 0, np.int32(2): 1, np.int32(3): 2, np.int32(4): 3, np.int32(5): 4, np.int32(6): 5}\n","  Mapped label range: [0, 1, 2, 3, 4, 5]\n","  #classes: 6\n","\n","Validation split by subject from train (subject-exclusive):\n","  #train subjects: 7\n","  #val subjects: 1 (14.3%)\n","  Val subjects: ['S07']\n","  Final #train subjects: 6\n","\n","After split:\n","  Train: (4216, 150, 8)\n","  Val:   (766, 150, 8)\n","  Test:  (749, 150, 8)\n","  ✓ Train/Val subjects disjoint\n","\n","Model params:\n","  Input channels: 8\n","  #classes:       6\n","\n","Model summary (fold_07):\n","  #params: 538,374\n","\n","Start training (fold_07):\n","  Epochs:       100\n","  Batch size:   64\n","  Learning rate:0.001\n","  Patience:     10\n","  Mixed precision: bf16 (no GradScaler)\n","Epoch   1/100: Train Loss=1.1012, Acc=0.6188 | Val Loss=1.2280, Acc=0.5836\n","Epoch   2/100: Train Loss=0.9359, Acc=0.6620 | Val Loss=1.1389, Acc=0.6292\n","Epoch   3/100: Train Loss=0.8809, Acc=0.6829 | Val Loss=1.0223, Acc=0.6384\n","Epoch   4/100: Train Loss=0.8504, Acc=0.6964 | Val Loss=1.1256, Acc=0.5888\n","Epoch   5/100: Train Loss=0.8112, Acc=0.7094 | Val Loss=1.0393, Acc=0.6606\n","Epoch   6/100: Train Loss=0.7589, Acc=0.7301 | Val Loss=1.0278, Acc=0.6501\n","Epoch   7/100: Train Loss=0.7344, Acc=0.7398 | Val Loss=0.9863, Acc=0.6802\n","Epoch   8/100: Train Loss=0.7198, Acc=0.7448 | Val Loss=1.1974, Acc=0.6149\n","Epoch   9/100: Train Loss=0.6699, Acc=0.7595 | Val Loss=1.2243, Acc=0.6462\n","Epoch  10/100: Train Loss=0.6357, Acc=0.7690 | Val Loss=1.0920, Acc=0.6345\n","Epoch  11/100: Train Loss=0.5850, Acc=0.7915 | Val Loss=1.2037, Acc=0.6501\n","Epoch  12/100: Train Loss=0.5538, Acc=0.8000 | Val Loss=1.2487, Acc=0.6136\n","Epoch  13/100: Train Loss=0.5066, Acc=0.8228 | Val Loss=1.1841, Acc=0.6410\n","Epoch  14/100: Train Loss=0.4620, Acc=0.8321 | Val Loss=1.1730, Acc=0.6514\n","Epoch  15/100: Train Loss=0.4125, Acc=0.8489 | Val Loss=2.0622, Acc=0.5822\n","Epoch  16/100: Train Loss=0.4184, Acc=0.8489 | Val Loss=1.6337, Acc=0.6240\n","Epoch  17/100: Train Loss=0.3231, Acc=0.8805 | Val Loss=2.0055, Acc=0.5783\n","\n","Early stopping triggered, best epoch: 7\n","\n","Training finished (fold_07), time: 24.02s\n","\n","Final evaluation on test set (fold_07):\n","  Test Loss:       1.0464\n","  Test Accuracy:   0.6315\n","  Test F1 (macro): 0.5826\n","✓ Saved: inception_time_best.pt\n","✓ Saved: inception_time_results.json\n","\n","============================================================\n","2. Train classical models (RF/KNN)\n","============================================================\n","\n","Feature data (fold_07):\n","  X_train: (4982, 220)\n","  X_test:  (749, 220)\n","  Labels remapped\n","\n","Training RandomForest...\n","  Config: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'random_state': 42, 'n_jobs': -1}\n","✓ RandomForest training complete:\n","  Train time: 2.55s\n","  Train Acc:  0.9880\n","  Test  Acc:  0.6208\n","  Test  F1 (macro): 0.4249\n","✓ Saved: random_forest.pkl\n","✓ Saved: random_forest_results.json\n","\n","Training KNN...\n","  Config: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'n_jobs': -1}\n","✓ KNN training complete:\n","  Train time: 0.00s\n","  Train Acc:  1.0000\n","  Test  Acc:  0.5701\n","  Test  F1 (macro): 0.4968\n","✓ Saved: knn.pkl\n","✓ Saved: knn_results.json\n","\n","============================================================\n","3. Save training config\n","============================================================\n","✓ Saved: models/fold_07/training_config.yaml\n","\n","============================================================\n","Summary for fold fold_07\n","============================================================\n","  InceptionTime: Acc=0.6315, F1=0.5826, best_epoch=7\n","  RandomForest:  Acc=0.6208, F1=0.4249\n","  KNN:           Acc=0.5701, F1=0.4968\n","  Outputs: models/fold_07/\n","\n","============================================================\n","Step 13 complete - Model Configuration & Training for all folds\n","============================================================\n","\n","Per-fold test performance (deep / RF / KNN):\n","   fold_id  deep_test_acc  deep_test_f1  rf_test_acc  rf_test_f1  \\\n","0        0       0.610966      0.461657     0.629243    0.409174   \n","1        1       0.611533      0.395533     0.626707    0.332400   \n","2        2       0.704280      0.494273     0.722438    0.451361   \n","3        3       0.585338      0.541620     0.597938    0.477910   \n","4        4       0.730563      0.602041     0.719839    0.542803   \n","5        5       0.460208      0.389364     0.449827    0.295785   \n","6        6       0.684510      0.571799     0.707289    0.382071   \n","7        7       0.631509      0.582605     0.620828    0.424871   \n","\n","   knn_test_acc  knn_test_f1  \n","0      0.562663     0.457746  \n","1      0.608498     0.368706  \n","2      0.632944     0.485850  \n","3      0.558992     0.504471  \n","4      0.663539     0.572946  \n","5      0.446367     0.330471  \n","6      0.635535     0.555452  \n","7      0.570093     0.496796  \n","\n","Cross-fold mean ± std (deep model):\n","  Acc: 0.6274 ± 0.0846\n","  F1 : 0.5049 ± 0.0833\n","\n","Next steps:\n","  - Compute cross-fold mean ± standard deviation for each model (a high-level overview for the deep model is printed above).\n","  - In subsequent Step 16 / error analysis, read the JSON files under models/*/ and aggregate them into tables and plots.\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 14: Inner-layer Tuning (multi-fold, warning-fix version)\n","\n","- Inner CV for each outer LOSO fold (GroupKFold by subject)\n","- Models: RandomForest, KNN, InceptionTime (lightweight version)\n","- Fixes:\n","  * Use new torch.amp.autocast API (no deprecation warning)\n","  * Use unstandardized windows for inner-layer normalization (no leakage)\n","  * Per-fold tuning outputs under tuning/fold_xx/\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import json\n","import yaml\n","import os\n","import time\n","from itertools import product\n","from functools import lru_cache\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import GroupKFold\n","from sklearn.metrics import f1_score\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.amp import autocast\n","\n","# ========== Global config ==========\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","\n","RANDOM_SEED = 42\n","INNER_CV_FOLDS = 5\n","USE_AMP_BF16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n","NUM_WORKERS = min(4, os.cpu_count() or 2)\n","\n","# Which models to tune\n","TUNE_MODELS = [\"rf\", \"knn\", \"inception\"]\n","\n","RF_PARAM_GRID = {\n","    \"n_estimators\": [100, 200],\n","    \"max_depth\": [20, 30],\n","    \"min_samples_split\": [2, 5],\n","}\n","\n","KNN_PARAM_GRID = {\n","    \"n_neighbors\": [5, 7],\n","    \"weights\": [\"uniform\", \"distance\"],\n","    \"metric\": [\"euclidean\", \"manhattan\"],\n","}\n","\n","INCEPTION_PARAM_TRIALS = [\n","    {\"learning_rate\": 1e-3, \"n_filters\": 32, \"depth\": 6},\n","    {\"learning_rate\": 5e-4, \"n_filters\": 32, \"depth\": 6},\n","]\n","\n","print(\"=\" * 60)\n","print(\"Step 14: Inner-layer Tuning (multi-fold, warning-fix version)\")\n","print(\"=\" * 60)\n","\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","configs_dir.mkdir(parents=True, exist_ok=True)\n","\n","# These globals will be updated for each fold\n","FOLD_ID = None\n","fold_tag = None\n","scalers_dir = None\n","train_meta_file = None\n","CLASS_LIST = None\n","N_CLASSES = None\n","CLASS_TO_INDEX = None\n","INDEX_LABELS = None\n","n_inner = None\n","inner_cv_splits = None\n","\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(RANDOM_SEED)\n","\n","# ========== 0. Determine outer folds from splits.json ==========\n","splits_path = configs_dir / \"splits.json\"\n","if splits_path.exists():\n","    with open(splits_path, \"r\", encoding=\"utf-8\") as f:\n","        splits_cfg = json.load(f)\n","    outer_fold_ids = sorted(int(k) for k in splits_cfg.keys())\n","else:\n","    raise RuntimeError(\"splits.json not found; LOSO tuning expects defined outer folds.\")\n","\n","print(f\"Outer folds detected: {outer_fold_ids}\")\n","print(f\"BF16 mixed precision available: {USE_AMP_BF16}\")\n","\n","# ========== 1. Data loading helpers (using globals per fold) ==========\n","\n","@lru_cache(maxsize=32)\n","def cached_window_subset(train_key, val_key):\n","    \"\"\"\n","    Cache standardized window subsets for deep models.\n","\n","    Important:\n","    - Uses UNSTANDARDIZED X_train.npy from the windows folder,\n","      to avoid reusing outer-fold statistics inside inner-layer tuning.\n","    - Normalization is recomputed per inner train split (channel-wise).\n","    \"\"\"\n","    train_subjects_local = list(train_key)\n","    val_subjects_local = list(val_key)\n","\n","    # Try unstandardized windows first (preferred)\n","    candidates = [\n","        proc_dir / \"windows\" / fold_tag / \"X_train.npy\",\n","        scalers_dir / \"X_train.npy\",  # optional fallback\n","    ]\n","    for p in candidates:\n","        if p.exists():\n","            X_full = np.load(p, mmap_mode=\"r\")\n","            break\n","    else:\n","        raise FileNotFoundError(\n","            \"Unstandardized X_train.npy is required for inner-layer normalization; \"\n","            \"having only X_train_scaled.npy would introduce validation-subject statistics. \"\n","            \"Please persist the unstandardized window array in Step 9/12.\"\n","        )\n","\n","    y_full = np.load(scalers_dir / \"y_train.npy\", mmap_mode=\"r\")\n","    df_meta = pd.read_parquet(train_meta_file)\n","\n","    train_mask = df_meta[\"subject_id\"].isin(train_subjects_local)\n","    val_mask = df_meta[\"subject_id\"].isin(val_subjects_local)\n","\n","    X_train_raw = X_full[train_mask].copy().astype(np.float32)\n","    X_val_raw = X_full[val_mask].copy().astype(np.float32)\n","    y_train_raw = y_full[train_mask].copy()\n","    y_val_raw = y_full[val_mask].copy()\n","\n","    # Inner-layer independent normalization (channel-wise)\n","    channel_mean = np.mean(X_train_raw, axis=(0, 1))\n","    channel_std = np.maximum(np.std(X_train_raw, axis=(0, 1)), 1e-8)\n","\n","    X_train = (X_train_raw - channel_mean) / channel_std\n","    X_val = (X_val_raw - channel_mean) / channel_std\n","\n","    y_train = np.array([CLASS_TO_INDEX[int(v)] for v in y_train_raw], dtype=np.int64)\n","    y_val = np.array([CLASS_TO_INDEX[int(v)] for v in y_val_raw], dtype=np.int64)\n","\n","    return X_train, y_train, X_val, y_val\n","\n","\n","def load_feature_data_subset(train_subjects_local, val_subjects_local):\n","    \"\"\"\n","    Load handcrafted feature data for classical models.\n","    Inner-layer StandardScaler is recomputed per inner train split.\n","    \"\"\"\n","    features_dir = proc_dir / \"features\" / fold_tag\n","    df_X = pd.read_parquet(features_dir / \"train_X.parquet\")\n","    df_y = pd.read_parquet(features_dir / \"train_y.parquet\")\n","    df_meta = pd.read_parquet(features_dir / \"train_meta.parquet\")\n","\n","    train_mask = df_meta[\"subject_id\"].isin(train_subjects_local)\n","    val_mask = df_meta[\"subject_id\"].isin(val_subjects_local)\n","\n","    X_train_raw = df_X.values[train_mask]\n","    X_val_raw = df_X.values[val_mask]\n","\n","    y_col = df_y[\"label\"]\n","    y_train_raw = y_col.values[train_mask]\n","    y_val_raw = y_col.values[val_mask]\n","\n","    y_train = np.array([CLASS_TO_INDEX[int(v)] for v in y_train_raw], dtype=np.int64)\n","    y_val = np.array([CLASS_TO_INDEX[int(v)] for v in y_val_raw], dtype=np.int64)\n","\n","    scaler = StandardScaler()\n","    X_train = scaler.fit_transform(X_train_raw)\n","    X_val = scaler.transform(X_val_raw)\n","\n","    return X_train, y_train, X_val, y_val\n","\n","# ========== 2. Evaluation functions ==========\n","\n","def evaluate_rf(params, inner_splits_local):\n","    f1_scores = []\n","    for split in inner_splits_local:\n","        X_train, y_train, X_val, y_val = load_feature_data_subset(\n","            split[\"train_subjects\"], split[\"val_subjects\"]\n","        )\n","\n","        model = RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1, **params)\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_val)\n","\n","        f1 = f1_score(y_val, y_pred, average=\"macro\", labels=INDEX_LABELS, zero_division=0)\n","        f1_scores.append(f1)\n","\n","    return np.mean(f1_scores), np.std(f1_scores)\n","\n","\n","def evaluate_knn(params, inner_splits_local):\n","    f1_scores = []\n","    for split in inner_splits_local:\n","        X_train, y_train, X_val, y_val = load_feature_data_subset(\n","            split[\"train_subjects\"], split[\"val_subjects\"]\n","        )\n","\n","        model = KNeighborsClassifier(n_jobs=-1, **params)\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_val)\n","\n","        f1 = f1_score(y_val, y_pred, average=\"macro\", labels=INDEX_LABELS, zero_division=0)\n","        f1_scores.append(f1)\n","\n","    return np.mean(f1_scores), np.std(f1_scores)\n","\n","\n","def evaluate_inception(params, inner_splits_local, max_epochs=20):\n","    \"\"\"\n","    InceptionTime evaluation with inner CV.\n","    Uses the new torch.amp.autocast API to avoid deprecation warnings.\n","    \"\"\"\n","    torch.manual_seed(RANDOM_SEED)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(RANDOM_SEED)\n","\n","    class InceptionModule(nn.Module):\n","        def __init__(self, in_channels, n_filters, kernel_sizes, bottleneck_channels):\n","            super().__init__()\n","            self.bottleneck = nn.Conv1d(in_channels, bottleneck_channels, 1, bias=False)\n","            self.conv_list = nn.ModuleList([\n","                nn.Conv1d(bottleneck_channels, n_filters, k, padding=k // 2, bias=False)\n","                for k in kernel_sizes\n","            ])\n","            self.maxpool_conv = nn.Sequential(\n","                nn.MaxPool1d(3, stride=1, padding=1),\n","                nn.Conv1d(in_channels, n_filters, 1, bias=False),\n","            )\n","            out_channels = n_filters * (len(kernel_sizes) + 1)\n","            self.bn = nn.BatchNorm1d(out_channels)\n","            self.relu = nn.ReLU()\n","\n","        def forward(self, x):\n","            bottleneck = self.bottleneck(x)\n","            conv_outputs = [conv(bottleneck) for conv in self.conv_list]\n","            maxpool_output = self.maxpool_conv(x)\n","            out = torch.cat([*conv_outputs, maxpool_output], dim=1)\n","            return self.relu(self.bn(out))\n","\n","    class InceptionTimeInner(nn.Module):\n","        def __init__(self, n_channels, n_classes, n_filters, depth):\n","            super().__init__()\n","            kernel_sizes = [9, 19, 39]\n","            bottleneck_channels = 32\n","\n","            self.inception_modules = nn.ModuleList()\n","            in_ch = n_channels\n","            out_ch = n_filters * (len(kernel_sizes) + 1)\n","\n","            for _ in range(depth):\n","                self.inception_modules.append(\n","                    InceptionModule(in_ch, n_filters, kernel_sizes, bottleneck_channels)\n","                )\n","                in_ch = out_ch\n","\n","            self.gap = nn.AdaptiveAvgPool1d(1)\n","            self.fc = nn.Linear(out_ch, n_classes)\n","\n","        def forward(self, x):\n","            x = x.transpose(1, 2).contiguous()\n","            for inception in self.inception_modules:\n","                x = inception(x)\n","            x = self.gap(x).squeeze(-1)\n","            return self.fc(x)\n","\n","    class WindowDataset(Dataset):\n","        def __init__(self, X, y):\n","            self.X = torch.FloatTensor(X)\n","            self.y = torch.LongTensor(y)\n","\n","        def __len__(self):\n","            return len(self.X)\n","\n","        def __getitem__(self, idx):\n","            return self.X[idx], self.y[idx]\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    pin = torch.cuda.is_available()\n","    f1_scores = []\n","\n","    for split in inner_splits_local:\n","        X_train, y_train, X_val, y_val = cached_window_subset(\n","            tuple(split[\"train_subjects\"]),\n","            tuple(split[\"val_subjects\"]),\n","        )\n","\n","        train_loader = DataLoader(\n","            WindowDataset(X_train, y_train),\n","            batch_size=64,\n","            shuffle=True,\n","            num_workers=NUM_WORKERS,\n","            pin_memory=pin,\n","        )\n","        val_loader = DataLoader(\n","            WindowDataset(X_val, y_val),\n","            batch_size=64,\n","            shuffle=False,\n","            num_workers=NUM_WORKERS,\n","            pin_memory=pin,\n","        )\n","\n","        n_channels_local = X_train.shape[2]\n","        model = InceptionTimeInner(\n","            n_channels_local, N_CLASSES, params[\"n_filters\"], params[\"depth\"]\n","        ).to(device)\n","\n","        optimizer = torch.optim.Adam(\n","            model.parameters(), lr=params[\"learning_rate\"], weight_decay=1e-4\n","        )\n","        criterion = nn.CrossEntropyLoss()\n","\n","        best_f1 = 0.0\n","        patience_counter = 0\n","\n","        for _ in range(max_epochs):\n","            model.train()\n","            for X_batch, y_batch in train_loader:\n","                X_batch = X_batch.to(device, non_blocking=True)\n","                y_batch = y_batch.to(device, non_blocking=True)\n","\n","                optimizer.zero_grad(set_to_none=True)\n","\n","                if USE_AMP_BF16:\n","                    with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n","                        outputs = model(X_batch)\n","                        loss = criterion(outputs, y_batch)\n","                else:\n","                    outputs = model(X_batch)\n","                    loss = criterion(outputs, y_batch)\n","\n","                loss.backward()\n","                optimizer.step()\n","\n","            model.eval()\n","            all_preds, all_labels = [], []\n","            with torch.no_grad():\n","                for X_batch, y_batch in val_loader:\n","                    X_batch = X_batch.to(device, non_blocking=True)\n","\n","                    if USE_AMP_BF16:\n","                        with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n","                            outputs = model(X_batch)\n","                    else:\n","                        outputs = model(X_batch)\n","\n","                    _, preds = outputs.max(1)\n","                    all_preds.extend(preds.cpu().numpy())\n","                    all_labels.extend(y_batch.numpy())\n","\n","            f1 = f1_score(\n","                all_labels, all_preds,\n","                average=\"macro\", labels=INDEX_LABELS, zero_division=0\n","            )\n","\n","            if f1 > best_f1 + 1e-4:\n","                best_f1 = f1\n","                patience_counter = 0\n","            else:\n","                patience_counter += 1\n","                if patience_counter >= 5:\n","                    break\n","\n","        f1_scores.append(best_f1)\n","\n","    return np.mean(f1_scores), np.std(f1_scores)\n","\n","# ========== 3. Loop over outer folds ==========\n","\n","for FOLD_ID in outer_fold_ids:\n","    fold_tag = f\"fold_{FOLD_ID:02d}\"\n","    tuning_dir = Path(\"tuning\") / fold_tag\n","    tuning_dir.mkdir(parents=True, exist_ok=True)\n","\n","    scalers_dir = proc_dir / \"scalers\" / fold_tag\n","    train_meta_file = scalers_dir / \"train_meta.parquet\"\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(f\"Outer fold: FOLD_ID={FOLD_ID} ({fold_tag})\")\n","    print(\"=\" * 60)\n","    print(f\"BF16 mixed precision: {USE_AMP_BF16}\")\n","    print(f\"Scalers dir: {scalers_dir}\")\n","    print(f\"Tuning dir:  {tuning_dir}\")\n","\n","    # Reset cached inner subsets between folds\n","    cached_window_subset.cache_clear()\n","\n","    # ---------- 3.1 Load global class set for this fold ----------\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"1. Load global class set\")\n","    print(\"=\" * 60)\n","\n","    df_train_meta = pd.read_parquet(train_meta_file)\n","    train_subjects = sorted(df_train_meta[\"subject_id\"].unique().tolist())\n","    n_train_subjects = len(train_subjects)\n","\n","    all_train_y = np.load(scalers_dir / \"y_train.npy\")\n","    CLASS_LIST = sorted(np.unique(all_train_y).tolist())\n","    N_CLASSES = len(CLASS_LIST)\n","\n","    CLASS_TO_INDEX = {c: i for i, c in enumerate(CLASS_LIST)}\n","    INDEX_LABELS = list(range(N_CLASSES))\n","\n","    print(f\"#Train subjects: {n_train_subjects}\")\n","    print(f\"Global class set: {CLASS_LIST} -> {INDEX_LABELS}\")\n","\n","    n_inner = min(INNER_CV_FOLDS, n_train_subjects)\n","    assert n_inner >= 2, \"Need at least 2 inner folds\"\n","    if n_inner < INNER_CV_FOLDS:\n","        print(f\"⚠️ Adjust inner folds: {INNER_CV_FOLDS} -> {n_inner}\")\n","\n","    # ---------- 3.2 Inner CV splitting ----------\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"2. Inner CV splitting (GroupKFold)\")\n","    print(\"=\" * 60)\n","\n","    df_train_meta = df_train_meta.sort_values(\n","        [\"subject_id\", \"session_id\", \"window_id\"],\n","        kind=\"mergesort\"\n","    ).reset_index(drop=True)\n","\n","    window_subjects = df_train_meta[\"subject_id\"].values\n","    window_indices = np.arange(len(df_train_meta))\n","\n","    gkf = GroupKFold(n_splits=n_inner)\n","    inner_cv_splits = []\n","\n","    for fold_idx, (train_idx, val_idx) in enumerate(\n","        gkf.split(window_indices, groups=window_subjects)\n","    ):\n","        val_subjects_inner = sorted(set(window_subjects[val_idx]))\n","        train_subjects_inner = sorted(set(window_subjects[train_idx]))\n","\n","        inner_cv_splits.append({\n","            \"fold\": fold_idx,\n","            \"train_subjects\": train_subjects_inner,\n","            \"val_subjects\": val_subjects_inner,\n","            \"train_window_indices\": train_idx.tolist(),\n","            \"val_window_indices\": val_idx.tolist(),\n","        })\n","        print(\n","            f\"  Inner fold {fold_idx}: \"\n","            f\"train {len(train_subjects_inner)} subs, val {len(val_subjects_inner)} subs\"\n","        )\n","\n","    for split in inner_cv_splits:\n","        assert set(split[\"train_subjects\"]).isdisjoint(set(split[\"val_subjects\"]))\n","    print(\"✓ Disjointness check passed\")\n","\n","    inner_splits_file = tuning_dir / \"inner_cv_splits.json\"\n","    with open(inner_splits_file, \"w\", encoding=\"utf-8\") as f:\n","        splits_to_save = [{\n","            \"fold\": s[\"fold\"],\n","            \"train_subjects\": s[\"train_subjects\"],\n","            \"val_subjects\": s[\"val_subjects\"],\n","            \"n_train_windows\": len(s[\"train_window_indices\"]),\n","            \"n_val_windows\": len(s[\"val_window_indices\"]),\n","        } for s in inner_cv_splits]\n","        json.dump(splits_to_save, f, ensure_ascii=False, indent=2)\n","    print(f\"✓ Saved: {inner_splits_file}\")\n","\n","    # ---------- 4. Tune RandomForest ----------\n","    if \"rf\" in TUNE_MODELS:\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"5. Tune RandomForest\")\n","        print(\"=\" * 60)\n","\n","        param_names = list(RF_PARAM_GRID.keys())\n","        param_values = [RF_PARAM_GRID[k] for k in param_names]\n","        rf_param_combinations = list(product(*param_values))\n","\n","        print(f\"Number of parameter combinations: {len(rf_param_combinations)}\")\n","\n","        rf_trials = []\n","        best_rf_f1 = 0.0\n","        best_rf_params = None\n","\n","        for i, combo in enumerate(rf_param_combinations):\n","            params = dict(zip(param_names, combo))\n","            print(f\"\\nTrial {i+1}/{len(rf_param_combinations)}: {params}\")\n","\n","            start_time = time.time()\n","            mean_f1, std_f1 = evaluate_rf(params, inner_cv_splits)\n","            elapsed = time.time() - start_time\n","\n","            print(f\"  Macro-F1: {mean_f1:.4f} ± {std_f1:.4f} ({elapsed:.2f}s)\")\n","\n","            rf_trials.append({\n","                \"trial\": i,\n","                \"params\": params,\n","                \"mean_f1\": float(mean_f1),\n","                \"std_f1\": float(std_f1),\n","                \"time\": float(elapsed),\n","            })\n","\n","            if mean_f1 > best_rf_f1:\n","                best_rf_f1 = mean_f1\n","                best_rf_params = params\n","                print(\"  ✓ New best!\")\n","\n","        print(f\"\\n✓ Best RandomForest: {best_rf_params} (F1={best_rf_f1:.4f})\")\n","\n","        pd.DataFrame(rf_trials).to_csv(tuning_dir / \"rf_trials.csv\", index=False)\n","        with open(tuning_dir / \"rf_best_params.json\", \"w\") as f:\n","            json.dump(\n","                {\"params\": best_rf_params, \"mean_f1\": float(best_rf_f1)},\n","                f, indent=2\n","            )\n","\n","    # ---------- 5. Tune KNN ----------\n","    if \"knn\" in TUNE_MODELS:\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"6. Tune KNN\")\n","        print(\"=\" * 60)\n","\n","        param_names = list(KNN_PARAM_GRID.keys())\n","        param_values = [KNN_PARAM_GRID[k] for k in param_names]\n","        knn_param_combinations = list(product(*param_values))\n","\n","        print(f\"Number of parameter combinations: {len(knn_param_combinations)}\")\n","\n","        knn_trials = []\n","        best_knn_f1 = 0.0\n","        best_knn_params = None\n","\n","        for i, combo in enumerate(knn_param_combinations):\n","            params = dict(zip(param_names, combo))\n","            print(f\"\\nTrial {i+1}/{len(knn_param_combinations)}: {params}\")\n","\n","            start_time = time.time()\n","            mean_f1, std_f1 = evaluate_knn(params, inner_cv_splits)\n","            elapsed = time.time() - start_time\n","\n","            print(f\"  Macro-F1: {mean_f1:.4f} ± {std_f1:.4f} ({elapsed:.2f}s)\")\n","\n","            knn_trials.append({\n","                \"trial\": i,\n","                \"params\": params,\n","                \"mean_f1\": float(mean_f1),\n","                \"std_f1\": float(std_f1),\n","                \"time\": float(elapsed),\n","            })\n","\n","            if mean_f1 > best_knn_f1:\n","                best_knn_f1 = mean_f1\n","                best_knn_params = params\n","                print(\"  ✓ New best!\")\n","\n","        print(f\"\\n✓ Best KNN: {best_knn_params} (F1={best_knn_f1:.4f})\")\n","\n","        pd.DataFrame(knn_trials).to_csv(tuning_dir / \"knn_trials.csv\", index=False)\n","        with open(tuning_dir / \"knn_best_params.json\", \"w\") as f:\n","            json.dump(\n","                {\"params\": best_knn_params, \"mean_f1\": float(best_knn_f1)},\n","                f, indent=2\n","            )\n","\n","    # ---------- 6. Tune InceptionTime ----------\n","    if \"inception\" in TUNE_MODELS:\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"7. Tune InceptionTime\")\n","        print(\"=\" * 60)\n","\n","        print(f\"#Trials: {len(INCEPTION_PARAM_TRIALS)}\")\n","\n","        inception_trials = []\n","        best_inception_f1 = 0.0\n","        best_inception_params = None\n","\n","        for i, params in enumerate(INCEPTION_PARAM_TRIALS):\n","            print(f\"\\nTrial {i+1}/{len(INCEPTION_PARAM_TRIALS)}: {params}\")\n","\n","            start_time = time.time()\n","            mean_f1, std_f1 = evaluate_inception(params, inner_cv_splits, max_epochs=20)\n","            elapsed = time.time() - start_time\n","\n","            print(f\"  Macro-F1: {mean_f1:.4f} ± {std_f1:.4f} ({elapsed:.2f}s)\")\n","\n","            inception_trials.append({\n","                \"trial\": i,\n","                \"params\": params,\n","                \"mean_f1\": float(mean_f1),\n","                \"std_f1\": float(std_f1),\n","                \"time\": float(elapsed),\n","            })\n","\n","            if mean_f1 > best_inception_f1:\n","                best_inception_f1 = mean_f1\n","                best_inception_params = params\n","                print(\"  ✓ New best!\")\n","\n","        print(f\"\\n✓ Best InceptionTime: {best_inception_params} (F1={best_inception_f1:.4f})\")\n","\n","        pd.DataFrame(inception_trials).to_csv(tuning_dir / \"inception_trials.csv\", index=False)\n","        with open(tuning_dir / \"inception_best_params.json\", \"w\") as f:\n","            json.dump(\n","                {\"params\": best_inception_params, \"mean_f1\": float(best_inception_f1)},\n","                f, indent=2\n","            )\n","\n","    # ---------- 7. Save per-fold tuning config ----------\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"8. Save config\")\n","    print(\"=\" * 60)\n","\n","    tuning_config = {\n","        \"fold_id\": FOLD_ID,\n","        \"fold_tag\": fold_tag,\n","        \"random_seed\": RANDOM_SEED,\n","        \"n_inner_folds\": n_inner,\n","        \"tuned_models\": TUNE_MODELS,\n","        \"best_params\": {},\n","    }\n","\n","    if \"rf\" in TUNE_MODELS:\n","        tuning_config[\"best_params\"][\"rf\"] = {\n","            \"params\": best_rf_params,\n","            \"mean_f1\": float(best_rf_f1),\n","        }\n","\n","    if \"knn\" in TUNE_MODELS:\n","        tuning_config[\"best_params\"][\"knn\"] = {\n","            \"params\": best_knn_params,\n","            \"mean_f1\": float(best_knn_f1),\n","        }\n","\n","    if \"inception\" in TUNE_MODELS:\n","        tuning_config[\"best_params\"][\"inception\"] = {\n","            \"params\": best_inception_params,\n","            \"mean_f1\": float(best_inception_f1),\n","        }\n","\n","    with open(tuning_dir / \"tuning_config.yaml\", \"w\", encoding=\"utf-8\") as f:\n","        yaml.dump(tuning_config, f, default_flow_style=False, allow_unicode=True)\n","\n","    print(f\"✓ Saved: {tuning_dir / 'tuning_config.yaml'}\")\n","    print(\"\\n\" + \"=\" * 60)\n","    print(f\"Step 14 complete for outer fold {FOLD_ID} ({fold_tag})\")\n","    print(\"=\" * 60)\n","    print(f\"Output dir: {tuning_dir}/\")\n","    print(\"  - inner_cv_splits.json\")\n","    if \"rf\" in TUNE_MODELS:\n","        print(\"  - rf_trials.csv, rf_best_params.json\")\n","    if \"knn\" in TUNE_MODELS:\n","        print(\"  - knn_trials.csv, knn_best_params.json\")\n","    if \"inception\" in TUNE_MODELS:\n","        print(\"  - inception_trials.csv, inception_best_params.json\")\n","    print(\"  - tuning_config.yaml\")\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"Step 14 complete for all outer folds\")\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vx-lU6wF36Br","executionInfo":{"status":"ok","timestamp":1763150948320,"user_tz":0,"elapsed":1884469,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"05cb8ade-b6c6-4fa7-b6fd-b0f098721845"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 14: Inner-layer Tuning (multi-fold, warning-fix version)\n","============================================================\n","Outer folds detected: [0, 1, 2, 3, 4, 5, 6, 7]\n","BF16 mixed precision available: True\n","\n","============================================================\n","Outer fold: FOLD_ID=0 (fold_00)\n","============================================================\n","BF16 mixed precision: True\n","Scalers dir: data/lara/mbientlab/proc/scalers/fold_00\n","Tuning dir:  tuning/fold_00\n","\n","============================================================\n","1. Load global class set\n","============================================================\n","#Train subjects: 7\n","Global class set: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","============================================================\n","2. Inner CV splitting (GroupKFold)\n","============================================================\n","  Inner fold 0: train 6 subs, val 1 subs\n","  Inner fold 1: train 6 subs, val 1 subs\n","  Inner fold 2: train 6 subs, val 1 subs\n","  Inner fold 3: train 5 subs, val 2 subs\n","  Inner fold 4: train 5 subs, val 2 subs\n","✓ Disjointness check passed\n","✓ Saved: tuning/fold_00/inner_cv_splits.json\n","\n","============================================================\n","5. Tune RandomForest\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4267 ± 0.0689 (6.04s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4243 ± 0.0737 (5.79s)\n","\n","Trial 3/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4303 ± 0.0737 (6.38s)\n","  ✓ New best!\n","\n","Trial 4/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4266 ± 0.0764 (6.04s)\n","\n","Trial 5/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4279 ± 0.0685 (10.36s)\n","\n","Trial 6/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4207 ± 0.0744 (10.28s)\n","\n","Trial 7/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4303 ± 0.0734 (10.99s)\n","  ✓ New best!\n","\n","Trial 8/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4306 ± 0.0717 (10.80s)\n","  ✓ New best!\n","\n","✓ Best RandomForest: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5} (F1=0.4306)\n","\n","============================================================\n","6. Tune KNN\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.4775 ± 0.0495 (0.34s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.4806 ± 0.0559 (1.82s)\n","  ✓ New best!\n","\n","Trial 3/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.4815 ± 0.0479 (0.29s)\n","  ✓ New best!\n","\n","Trial 4/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.4867 ± 0.0545 (1.44s)\n","  ✓ New best!\n","\n","Trial 5/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.4829 ± 0.0380 (0.29s)\n","\n","Trial 6/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.4763 ± 0.0519 (1.82s)\n","\n","Trial 7/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.4880 ± 0.0366 (0.31s)\n","  ✓ New best!\n","\n","Trial 8/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.4990 ± 0.0598 (1.45s)\n","  ✓ New best!\n","\n","✓ Best KNN: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'} (F1=0.4990)\n","\n","============================================================\n","7. Tune InceptionTime\n","============================================================\n","#Trials: 2\n","\n","Trial 1/2: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5504 ± 0.0372 (88.01s)\n","  ✓ New best!\n","\n","Trial 2/2: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5426 ± 0.0328 (94.07s)\n","\n","✓ Best InceptionTime: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6} (F1=0.5504)\n","\n","============================================================\n","8. Save config\n","============================================================\n","✓ Saved: tuning/fold_00/tuning_config.yaml\n","\n","============================================================\n","Step 14 complete for outer fold 0 (fold_00)\n","============================================================\n","Output dir: tuning/fold_00/\n","  - inner_cv_splits.json\n","  - rf_trials.csv, rf_best_params.json\n","  - knn_trials.csv, knn_best_params.json\n","  - inception_trials.csv, inception_best_params.json\n","  - tuning_config.yaml\n","\n","============================================================\n","Outer fold: FOLD_ID=1 (fold_01)\n","============================================================\n","BF16 mixed precision: True\n","Scalers dir: data/lara/mbientlab/proc/scalers/fold_01\n","Tuning dir:  tuning/fold_01\n","\n","============================================================\n","1. Load global class set\n","============================================================\n","#Train subjects: 7\n","Global class set: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","============================================================\n","2. Inner CV splitting (GroupKFold)\n","============================================================\n","  Inner fold 0: train 6 subs, val 1 subs\n","  Inner fold 1: train 6 subs, val 1 subs\n","  Inner fold 2: train 6 subs, val 1 subs\n","  Inner fold 3: train 5 subs, val 2 subs\n","  Inner fold 4: train 5 subs, val 2 subs\n","✓ Disjointness check passed\n","✓ Saved: tuning/fold_01/inner_cv_splits.json\n","\n","============================================================\n","5. Tune RandomForest\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4291 ± 0.0475 (5.60s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4565 ± 0.0468 (5.60s)\n","  ✓ New best!\n","\n","Trial 3/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4301 ± 0.0444 (5.88s)\n","\n","Trial 4/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4614 ± 0.0452 (5.99s)\n","  ✓ New best!\n","\n","Trial 5/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4283 ± 0.0502 (10.67s)\n","\n","Trial 6/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4602 ± 0.0467 (10.36s)\n","\n","Trial 7/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4527 ± 0.0456 (11.29s)\n","\n","Trial 8/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4630 ± 0.0439 (11.20s)\n","  ✓ New best!\n","\n","✓ Best RandomForest: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5} (F1=0.4630)\n","\n","============================================================\n","6. Tune KNN\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.4885 ± 0.0305 (0.41s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.5035 ± 0.0395 (1.85s)\n","  ✓ New best!\n","\n","Trial 3/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.5041 ± 0.0394 (0.27s)\n","  ✓ New best!\n","\n","Trial 4/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.5137 ± 0.0452 (1.55s)\n","  ✓ New best!\n","\n","Trial 5/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.5027 ± 0.0502 (0.32s)\n","\n","Trial 6/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.5026 ± 0.0477 (1.86s)\n","\n","Trial 7/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.5070 ± 0.0468 (0.29s)\n","\n","Trial 8/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.5130 ± 0.0476 (1.47s)\n","\n","✓ Best KNN: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan'} (F1=0.5137)\n","\n","============================================================\n","7. Tune InceptionTime\n","============================================================\n","#Trials: 2\n","\n","Trial 1/2: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5609 ± 0.0264 (82.14s)\n","  ✓ New best!\n","\n","Trial 2/2: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5529 ± 0.0321 (68.77s)\n","\n","✓ Best InceptionTime: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6} (F1=0.5609)\n","\n","============================================================\n","8. Save config\n","============================================================\n","✓ Saved: tuning/fold_01/tuning_config.yaml\n","\n","============================================================\n","Step 14 complete for outer fold 1 (fold_01)\n","============================================================\n","Output dir: tuning/fold_01/\n","  - inner_cv_splits.json\n","  - rf_trials.csv, rf_best_params.json\n","  - knn_trials.csv, knn_best_params.json\n","  - inception_trials.csv, inception_best_params.json\n","  - tuning_config.yaml\n","\n","============================================================\n","Outer fold: FOLD_ID=2 (fold_02)\n","============================================================\n","BF16 mixed precision: True\n","Scalers dir: data/lara/mbientlab/proc/scalers/fold_02\n","Tuning dir:  tuning/fold_02\n","\n","============================================================\n","1. Load global class set\n","============================================================\n","#Train subjects: 7\n","Global class set: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","============================================================\n","2. Inner CV splitting (GroupKFold)\n","============================================================\n","  Inner fold 0: train 6 subs, val 1 subs\n","  Inner fold 1: train 6 subs, val 1 subs\n","  Inner fold 2: train 6 subs, val 1 subs\n","  Inner fold 3: train 5 subs, val 2 subs\n","  Inner fold 4: train 5 subs, val 2 subs\n","✓ Disjointness check passed\n","✓ Saved: tuning/fold_02/inner_cv_splits.json\n","\n","============================================================\n","5. Tune RandomForest\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4601 ± 0.0405 (5.41s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4754 ± 0.0519 (5.45s)\n","  ✓ New best!\n","\n","Trial 3/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4749 ± 0.0660 (5.60s)\n","\n","Trial 4/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4737 ± 0.0585 (5.68s)\n","\n","Trial 5/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4704 ± 0.0511 (10.19s)\n","\n","Trial 6/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4700 ± 0.0572 (10.05s)\n","\n","Trial 7/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4680 ± 0.0529 (10.74s)\n","\n","Trial 8/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4735 ± 0.0584 (10.66s)\n","\n","✓ Best RandomForest: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5} (F1=0.4754)\n","\n","============================================================\n","6. Tune KNN\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.5021 ± 0.0525 (0.42s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.4919 ± 0.0629 (1.81s)\n","\n","Trial 3/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.5033 ± 0.0473 (0.27s)\n","  ✓ New best!\n","\n","Trial 4/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.5052 ± 0.0603 (1.41s)\n","  ✓ New best!\n","\n","Trial 5/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.4938 ± 0.0457 (0.30s)\n","\n","Trial 6/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.4979 ± 0.0661 (1.85s)\n","\n","Trial 7/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.4949 ± 0.0432 (0.32s)\n","\n","Trial 8/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.5090 ± 0.0626 (1.49s)\n","  ✓ New best!\n","\n","✓ Best KNN: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'} (F1=0.5090)\n","\n","============================================================\n","7. Tune InceptionTime\n","============================================================\n","#Trials: 2\n","\n","Trial 1/2: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5619 ± 0.0438 (71.28s)\n","  ✓ New best!\n","\n","Trial 2/2: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5647 ± 0.0377 (61.81s)\n","  ✓ New best!\n","\n","✓ Best InceptionTime: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6} (F1=0.5647)\n","\n","============================================================\n","8. Save config\n","============================================================\n","✓ Saved: tuning/fold_02/tuning_config.yaml\n","\n","============================================================\n","Step 14 complete for outer fold 2 (fold_02)\n","============================================================\n","Output dir: tuning/fold_02/\n","  - inner_cv_splits.json\n","  - rf_trials.csv, rf_best_params.json\n","  - knn_trials.csv, knn_best_params.json\n","  - inception_trials.csv, inception_best_params.json\n","  - tuning_config.yaml\n","\n","============================================================\n","Outer fold: FOLD_ID=3 (fold_03)\n","============================================================\n","BF16 mixed precision: True\n","Scalers dir: data/lara/mbientlab/proc/scalers/fold_03\n","Tuning dir:  tuning/fold_03\n","\n","============================================================\n","1. Load global class set\n","============================================================\n","#Train subjects: 7\n","Global class set: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","============================================================\n","2. Inner CV splitting (GroupKFold)\n","============================================================\n","  Inner fold 0: train 6 subs, val 1 subs\n","  Inner fold 1: train 6 subs, val 1 subs\n","  Inner fold 2: train 6 subs, val 1 subs\n","  Inner fold 3: train 5 subs, val 2 subs\n","  Inner fold 4: train 5 subs, val 2 subs\n","✓ Disjointness check passed\n","✓ Saved: tuning/fold_03/inner_cv_splits.json\n","\n","============================================================\n","5. Tune RandomForest\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4274 ± 0.0535 (5.49s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4362 ± 0.0486 (5.30s)\n","  ✓ New best!\n","\n","Trial 3/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4304 ± 0.0437 (5.69s)\n","\n","Trial 4/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4555 ± 0.0513 (5.76s)\n","  ✓ New best!\n","\n","Trial 5/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4299 ± 0.0483 (10.18s)\n","\n","Trial 6/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4288 ± 0.0545 (10.00s)\n","\n","Trial 7/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4505 ± 0.0469 (10.76s)\n","\n","Trial 8/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4314 ± 0.0529 (10.79s)\n","\n","✓ Best RandomForest: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5} (F1=0.4555)\n","\n","============================================================\n","6. Tune KNN\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.4682 ± 0.0587 (0.43s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.4617 ± 0.0570 (1.79s)\n","\n","Trial 3/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.4761 ± 0.0548 (0.30s)\n","  ✓ New best!\n","\n","Trial 4/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.4735 ± 0.0549 (1.48s)\n","\n","Trial 5/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.4758 ± 0.0587 (0.32s)\n","\n","Trial 6/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.4649 ± 0.0520 (1.78s)\n","\n","Trial 7/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.4762 ± 0.0659 (0.27s)\n","  ✓ New best!\n","\n","Trial 8/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.4723 ± 0.0575 (1.39s)\n","\n","✓ Best KNN: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'} (F1=0.4762)\n","\n","============================================================\n","7. Tune InceptionTime\n","============================================================\n","#Trials: 2\n","\n","Trial 1/2: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5573 ± 0.0431 (83.06s)\n","  ✓ New best!\n","\n","Trial 2/2: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5633 ± 0.0418 (77.41s)\n","  ✓ New best!\n","\n","✓ Best InceptionTime: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6} (F1=0.5633)\n","\n","============================================================\n","8. Save config\n","============================================================\n","✓ Saved: tuning/fold_03/tuning_config.yaml\n","\n","============================================================\n","Step 14 complete for outer fold 3 (fold_03)\n","============================================================\n","Output dir: tuning/fold_03/\n","  - inner_cv_splits.json\n","  - rf_trials.csv, rf_best_params.json\n","  - knn_trials.csv, knn_best_params.json\n","  - inception_trials.csv, inception_best_params.json\n","  - tuning_config.yaml\n","\n","============================================================\n","Outer fold: FOLD_ID=4 (fold_04)\n","============================================================\n","BF16 mixed precision: True\n","Scalers dir: data/lara/mbientlab/proc/scalers/fold_04\n","Tuning dir:  tuning/fold_04\n","\n","============================================================\n","1. Load global class set\n","============================================================\n","#Train subjects: 7\n","Global class set: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","============================================================\n","2. Inner CV splitting (GroupKFold)\n","============================================================\n","  Inner fold 0: train 6 subs, val 1 subs\n","  Inner fold 1: train 6 subs, val 1 subs\n","  Inner fold 2: train 6 subs, val 1 subs\n","  Inner fold 3: train 5 subs, val 2 subs\n","  Inner fold 4: train 5 subs, val 2 subs\n","✓ Disjointness check passed\n","✓ Saved: tuning/fold_04/inner_cv_splits.json\n","\n","============================================================\n","5. Tune RandomForest\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4288 ± 0.0439 (5.45s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4239 ± 0.0418 (5.54s)\n","\n","Trial 3/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4564 ± 0.0472 (5.75s)\n","  ✓ New best!\n","\n","Trial 4/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4564 ± 0.0422 (5.85s)\n","\n","Trial 5/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4313 ± 0.0406 (10.36s)\n","\n","Trial 6/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4256 ± 0.0372 (10.39s)\n","\n","Trial 7/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4527 ± 0.0396 (11.13s)\n","\n","Trial 8/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4615 ± 0.0404 (11.01s)\n","  ✓ New best!\n","\n","✓ Best RandomForest: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5} (F1=0.4615)\n","\n","============================================================\n","6. Tune KNN\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.4836 ± 0.0433 (0.44s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.4713 ± 0.0360 (1.83s)\n","\n","Trial 3/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.4893 ± 0.0452 (0.27s)\n","  ✓ New best!\n","\n","Trial 4/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.4931 ± 0.0517 (1.44s)\n","  ✓ New best!\n","\n","Trial 5/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.4815 ± 0.0352 (0.28s)\n","\n","Trial 6/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.4727 ± 0.0349 (1.85s)\n","\n","Trial 7/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.5011 ± 0.0559 (0.31s)\n","  ✓ New best!\n","\n","Trial 8/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.4980 ± 0.0512 (1.49s)\n","\n","✓ Best KNN: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'} (F1=0.5011)\n","\n","============================================================\n","7. Tune InceptionTime\n","============================================================\n","#Trials: 2\n","\n","Trial 1/2: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5431 ± 0.0109 (85.64s)\n","  ✓ New best!\n","\n","Trial 2/2: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5493 ± 0.0328 (79.78s)\n","  ✓ New best!\n","\n","✓ Best InceptionTime: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6} (F1=0.5493)\n","\n","============================================================\n","8. Save config\n","============================================================\n","✓ Saved: tuning/fold_04/tuning_config.yaml\n","\n","============================================================\n","Step 14 complete for outer fold 4 (fold_04)\n","============================================================\n","Output dir: tuning/fold_04/\n","  - inner_cv_splits.json\n","  - rf_trials.csv, rf_best_params.json\n","  - knn_trials.csv, knn_best_params.json\n","  - inception_trials.csv, inception_best_params.json\n","  - tuning_config.yaml\n","\n","============================================================\n","Outer fold: FOLD_ID=5 (fold_05)\n","============================================================\n","BF16 mixed precision: True\n","Scalers dir: data/lara/mbientlab/proc/scalers/fold_05\n","Tuning dir:  tuning/fold_05\n","\n","============================================================\n","1. Load global class set\n","============================================================\n","#Train subjects: 7\n","Global class set: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","============================================================\n","2. Inner CV splitting (GroupKFold)\n","============================================================\n","  Inner fold 0: train 6 subs, val 1 subs\n","  Inner fold 1: train 6 subs, val 1 subs\n","  Inner fold 2: train 6 subs, val 1 subs\n","  Inner fold 3: train 5 subs, val 2 subs\n","  Inner fold 4: train 5 subs, val 2 subs\n","✓ Disjointness check passed\n","✓ Saved: tuning/fold_05/inner_cv_splits.json\n","\n","============================================================\n","5. Tune RandomForest\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4720 ± 0.0255 (6.27s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4510 ± 0.0395 (6.07s)\n","\n","Trial 3/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4747 ± 0.0253 (6.42s)\n","  ✓ New best!\n","\n","Trial 4/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4494 ± 0.0365 (6.34s)\n","\n","Trial 5/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4747 ± 0.0211 (11.50s)\n","\n","Trial 6/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4524 ± 0.0417 (11.39s)\n","\n","Trial 7/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4757 ± 0.0229 (12.21s)\n","  ✓ New best!\n","\n","Trial 8/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4564 ± 0.0356 (12.05s)\n","\n","✓ Best RandomForest: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 2} (F1=0.4757)\n","\n","============================================================\n","6. Tune KNN\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.5152 ± 0.0272 (0.46s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.5101 ± 0.0278 (1.98s)\n","\n","Trial 3/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.5184 ± 0.0281 (0.29s)\n","  ✓ New best!\n","\n","Trial 4/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.5220 ± 0.0272 (1.61s)\n","  ✓ New best!\n","\n","Trial 5/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.5034 ± 0.0164 (0.31s)\n","\n","Trial 6/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.5094 ± 0.0381 (1.98s)\n","\n","Trial 7/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.5053 ± 0.0153 (0.31s)\n","\n","Trial 8/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.5231 ± 0.0336 (1.67s)\n","  ✓ New best!\n","\n","✓ Best KNN: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'} (F1=0.5231)\n","\n","============================================================\n","7. Tune InceptionTime\n","============================================================\n","#Trials: 2\n","\n","Trial 1/2: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5820 ± 0.0171 (93.45s)\n","  ✓ New best!\n","\n","Trial 2/2: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5724 ± 0.0169 (84.71s)\n","\n","✓ Best InceptionTime: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6} (F1=0.5820)\n","\n","============================================================\n","8. Save config\n","============================================================\n","✓ Saved: tuning/fold_05/tuning_config.yaml\n","\n","============================================================\n","Step 14 complete for outer fold 5 (fold_05)\n","============================================================\n","Output dir: tuning/fold_05/\n","  - inner_cv_splits.json\n","  - rf_trials.csv, rf_best_params.json\n","  - knn_trials.csv, knn_best_params.json\n","  - inception_trials.csv, inception_best_params.json\n","  - tuning_config.yaml\n","\n","============================================================\n","Outer fold: FOLD_ID=6 (fold_06)\n","============================================================\n","BF16 mixed precision: True\n","Scalers dir: data/lara/mbientlab/proc/scalers/fold_06\n","Tuning dir:  tuning/fold_06\n","\n","============================================================\n","1. Load global class set\n","============================================================\n","#Train subjects: 7\n","Global class set: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","============================================================\n","2. Inner CV splitting (GroupKFold)\n","============================================================\n","  Inner fold 0: train 6 subs, val 1 subs\n","  Inner fold 1: train 6 subs, val 1 subs\n","  Inner fold 2: train 6 subs, val 1 subs\n","  Inner fold 3: train 5 subs, val 2 subs\n","  Inner fold 4: train 5 subs, val 2 subs\n","✓ Disjointness check passed\n","✓ Saved: tuning/fold_06/inner_cv_splits.json\n","\n","============================================================\n","5. Tune RandomForest\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4453 ± 0.0680 (5.33s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4389 ± 0.0493 (5.47s)\n","\n","Trial 3/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4519 ± 0.0527 (5.66s)\n","  ✓ New best!\n","\n","Trial 4/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4510 ± 0.0424 (5.69s)\n","\n","Trial 5/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4461 ± 0.0532 (10.09s)\n","\n","Trial 6/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4425 ± 0.0526 (10.07s)\n","\n","Trial 7/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4523 ± 0.0507 (10.78s)\n","  ✓ New best!\n","\n","Trial 8/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4480 ± 0.0428 (10.76s)\n","\n","✓ Best RandomForest: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 2} (F1=0.4523)\n","\n","============================================================\n","6. Tune KNN\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.4773 ± 0.0492 (0.44s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.4787 ± 0.0581 (1.78s)\n","  ✓ New best!\n","\n","Trial 3/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.4798 ± 0.0442 (0.32s)\n","  ✓ New best!\n","\n","Trial 4/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.4854 ± 0.0563 (1.41s)\n","  ✓ New best!\n","\n","Trial 5/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.4777 ± 0.0566 (0.28s)\n","\n","Trial 6/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.4731 ± 0.0590 (1.83s)\n","\n","Trial 7/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.4778 ± 0.0588 (0.31s)\n","\n","Trial 8/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.4813 ± 0.0641 (1.50s)\n","\n","✓ Best KNN: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan'} (F1=0.4854)\n","\n","============================================================\n","7. Tune InceptionTime\n","============================================================\n","#Trials: 2\n","\n","Trial 1/2: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5505 ± 0.0272 (96.07s)\n","  ✓ New best!\n","\n","Trial 2/2: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5516 ± 0.0511 (66.68s)\n","  ✓ New best!\n","\n","✓ Best InceptionTime: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6} (F1=0.5516)\n","\n","============================================================\n","8. Save config\n","============================================================\n","✓ Saved: tuning/fold_06/tuning_config.yaml\n","\n","============================================================\n","Step 14 complete for outer fold 6 (fold_06)\n","============================================================\n","Output dir: tuning/fold_06/\n","  - inner_cv_splits.json\n","  - rf_trials.csv, rf_best_params.json\n","  - knn_trials.csv, knn_best_params.json\n","  - inception_trials.csv, inception_best_params.json\n","  - tuning_config.yaml\n","\n","============================================================\n","Outer fold: FOLD_ID=7 (fold_07)\n","============================================================\n","BF16 mixed precision: True\n","Scalers dir: data/lara/mbientlab/proc/scalers/fold_07\n","Tuning dir:  tuning/fold_07\n","\n","============================================================\n","1. Load global class set\n","============================================================\n","#Train subjects: 7\n","Global class set: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","============================================================\n","2. Inner CV splitting (GroupKFold)\n","============================================================\n","  Inner fold 0: train 6 subs, val 1 subs\n","  Inner fold 1: train 6 subs, val 1 subs\n","  Inner fold 2: train 6 subs, val 1 subs\n","  Inner fold 3: train 5 subs, val 2 subs\n","  Inner fold 4: train 5 subs, val 2 subs\n","✓ Disjointness check passed\n","✓ Saved: tuning/fold_07/inner_cv_splits.json\n","\n","============================================================\n","5. Tune RandomForest\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4439 ± 0.0608 (5.51s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4395 ± 0.0574 (5.51s)\n","\n","Trial 3/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4651 ± 0.0529 (5.88s)\n","  ✓ New best!\n","\n","Trial 4/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4344 ± 0.0623 (5.73s)\n","\n","Trial 5/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4408 ± 0.0557 (10.41s)\n","\n","Trial 6/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4377 ± 0.0567 (10.34s)\n","\n","Trial 7/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4374 ± 0.0547 (11.16s)\n","\n","Trial 8/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4354 ± 0.0537 (10.97s)\n","\n","✓ Best RandomForest: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2} (F1=0.4651)\n","\n","============================================================\n","6. Tune KNN\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.5024 ± 0.0459 (0.45s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.4845 ± 0.0381 (1.82s)\n","\n","Trial 3/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.5113 ± 0.0480 (0.27s)\n","  ✓ New best!\n","\n","Trial 4/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.5100 ± 0.0531 (1.45s)\n","\n","Trial 5/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.4950 ± 0.0362 (0.28s)\n","\n","Trial 6/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.4857 ± 0.0462 (1.82s)\n","\n","Trial 7/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.5045 ± 0.0349 (0.30s)\n","\n","Trial 8/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.5080 ± 0.0554 (1.52s)\n","\n","✓ Best KNN: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'} (F1=0.5113)\n","\n","============================================================\n","7. Tune InceptionTime\n","============================================================\n","#Trials: 2\n","\n","Trial 1/2: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5745 ± 0.0411 (85.22s)\n","  ✓ New best!\n","\n","Trial 2/2: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5589 ± 0.0266 (74.28s)\n","\n","✓ Best InceptionTime: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6} (F1=0.5745)\n","\n","============================================================\n","8. Save config\n","============================================================\n","✓ Saved: tuning/fold_07/tuning_config.yaml\n","\n","============================================================\n","Step 14 complete for outer fold 7 (fold_07)\n","============================================================\n","Output dir: tuning/fold_07/\n","  - inner_cv_splits.json\n","  - rf_trials.csv, rf_best_params.json\n","  - knn_trials.csv, knn_best_params.json\n","  - inception_trials.csv, inception_best_params.json\n","  - tuning_config.yaml\n","\n","============================================================\n","Step 14 complete for all outer folds\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 15: Training & Inference (top-conf/journal grade, multi-fold)\n","Fit on the full training fold with best hyperparameters, run inference on the test subject,\n","and save per-window predictions for all outer folds.\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import json\n","import yaml\n","import os\n","import pickle\n","import time\n","import shutil\n","from datetime import datetime, timezone\n","\n","# PyTorch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.amp import autocast\n","\n","# Classical models\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.preprocessing import StandardScaler\n","\n","# ========== Config ==========\n","RANDOM_SEED = 42\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","INFERENCE_CONFIG = {\n","    \"batch_size\": 128,\n","    \"num_workers\": 4,\n","}\n","\n","USE_AMP_BF16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n","\n","print(\"=\" * 60)\n","print(\"Step 15: Training & Inference (multi-fold)\")\n","print(\"=\" * 60)\n","\n","# Path config\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","configs_dir.mkdir(parents=True, exist_ok=True)\n","\n","models_root = Path(\"models\")\n","predictions_root = Path(\"predictions\")\n","models_root.mkdir(parents=True, exist_ok=True)\n","predictions_root.mkdir(parents=True, exist_ok=True)\n","\n","print(f\"\\nDevice: {DEVICE}\")\n","print(f\"BF16 mixed precision: {USE_AMP_BF16}\")\n","print(f\"Inference batch size: {INFERENCE_CONFIG['batch_size']}\")\n","print(f\"Num workers: {INFERENCE_CONFIG['num_workers']}\")\n","\n","# Set seeds\n","torch.manual_seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(RANDOM_SEED)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# ========== Determine outer folds from splits.json ==========\n","splits_path = configs_dir / \"splits.json\"\n","if splits_path.exists():\n","    with open(splits_path, \"r\", encoding=\"utf-8\") as f:\n","        splits_cfg = json.load(f)\n","    outer_fold_ids = sorted(int(k) for k in splits_cfg.keys())\n","else:\n","    raise RuntimeError(\"splits.json not found; Step 15 expects defined LOSO folds.\")\n","\n","print(f\"\\nOuter folds detected: {outer_fold_ids}\")\n","\n","# ========== InceptionTime model definition ==========\n","class InceptionModule(nn.Module):\n","    def __init__(self, in_channels, n_filters, kernel_sizes, bottleneck_channels):\n","        super().__init__()\n","        self.bottleneck = nn.Conv1d(in_channels, bottleneck_channels, 1, bias=False)\n","        self.conv_list = nn.ModuleList([\n","            nn.Conv1d(bottleneck_channels, n_filters, k, padding=k // 2, bias=False)\n","            for k in kernel_sizes\n","        ])\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool1d(3, stride=1, padding=1),\n","            nn.Conv1d(in_channels, n_filters, 1, bias=False),\n","        )\n","        out_channels = n_filters * (len(kernel_sizes) + 1)\n","        self.bn = nn.BatchNorm1d(out_channels)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        bottleneck = self.bottleneck(x)\n","        conv_outputs = [conv(bottleneck) for conv in self.conv_list]\n","        maxpool_output = self.maxpool_conv(x)\n","        out = torch.cat([*conv_outputs, maxpool_output], dim=1)\n","        return self.relu(self.bn(out))\n","\n","\n","class InceptionTime(nn.Module):\n","    def __init__(self, n_channels, n_classes, n_filters, depth):\n","        super().__init__()\n","        kernel_sizes = [9, 19, 39]\n","        bottleneck_channels = 32\n","        self.inception_modules = nn.ModuleList()\n","        in_ch = n_channels\n","        out_ch = n_filters * (len(kernel_sizes) + 1)\n","        for _ in range(depth):\n","            self.inception_modules.append(\n","                InceptionModule(in_ch, n_filters, kernel_sizes, bottleneck_channels)\n","            )\n","            in_ch = out_ch\n","        self.gap = nn.AdaptiveAvgPool1d(1)\n","        self.fc = nn.Linear(out_ch, n_classes)\n","\n","    def forward(self, x):\n","        x = x.transpose(1, 2).contiguous()\n","        for inception in self.inception_modules:\n","            x = inception(x)\n","        x = self.gap(x).squeeze(-1)\n","        return self.fc(x)\n","\n","\n","class WindowDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = torch.FloatTensor(X)\n","        self.y = torch.LongTensor(y)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","\n","# For cross-fold summary\n","fold_summaries = []\n","\n","# ========== Loop over all outer folds ==========\n","for FOLD_ID in outer_fold_ids:\n","    fold_tag = f\"fold_{FOLD_ID:02d}\"\n","    tuning_dir = Path(\"tuning\") / fold_tag\n","    models_dir = models_root / fold_tag\n","    predictions_dir = predictions_root / fold_tag\n","\n","    models_dir.mkdir(parents=True, exist_ok=True)\n","    predictions_dir.mkdir(parents=True, exist_ok=True)\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(f\"Outer fold: FOLD_ID={FOLD_ID} ({fold_tag})\")\n","    print(\"=\" * 60)\n","\n","    # ========== 1. Load best hyperparameters ==========\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"1. Load best hyperparameters\")\n","    print(\"=\" * 60)\n","\n","    best_params = {}\n","\n","    rf_params_file = tuning_dir / \"rf_best_params.json\"\n","    if rf_params_file.exists():\n","        with open(rf_params_file, \"r\") as f:\n","            best_params[\"rf\"] = json.load(f)\n","        print(f\"✓ RF: {best_params['rf']['params']}\")\n","\n","    knn_params_file = tuning_dir / \"knn_best_params.json\"\n","    if knn_params_file.exists():\n","        with open(knn_params_file, \"r\") as f:\n","            best_params[\"knn\"] = json.load(f)\n","        print(f\"✓ KNN: {best_params['knn']['params']}\")\n","\n","    inception_params_file = tuning_dir / \"inception_best_params.json\"\n","    if inception_params_file.exists():\n","        with open(inception_params_file, \"r\") as f:\n","            best_params[\"inception\"] = json.load(f)\n","        print(f\"✓ InceptionTime: {best_params['inception']['params']}\")\n","\n","    if not best_params:\n","        print(f\"⚠️ No best hyperparameters found for {fold_tag}; skipping this fold.\")\n","        continue\n","\n","    # ========== 2. Load data and build label mapping ==========\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"2. Load data and label mapping\")\n","    print(\"=\" * 60)\n","\n","    scalers_dir = proc_dir / \"scalers\" / fold_tag\n","    features_dir = proc_dir / \"features\" / fold_tag\n","\n","    # Deep meta & labels\n","    train_meta_file = scalers_dir / \"train_meta.parquet\"\n","    df_train_meta = pd.read_parquet(train_meta_file)\n","    all_train_y = np.load(scalers_dir / \"y_train.npy\")\n","\n","    CLASS_LIST = sorted(np.unique(all_train_y).tolist())\n","    N_CLASSES = len(CLASS_LIST)\n","    CLASS_TO_INDEX = {c: i for i, c in enumerate(CLASS_LIST)}\n","    INDEX_TO_CLASS = {i: c for c, i in CLASS_TO_INDEX.items()}\n","\n","    print(f\"#classes: {N_CLASSES}\")\n","    print(f\"Class mapping: {CLASS_LIST} -> {list(range(N_CLASSES))}\")\n","\n","    # Meta for test sets\n","    df_test_meta_deep = pd.read_parquet(scalers_dir / \"test_meta.parquet\")\n","    df_test_meta_feat = pd.read_parquet(features_dir / \"test_meta.parquet\")\n","\n","    # Deep model data\n","    X_train_deep = np.load(scalers_dir / \"X_train_scaled.npy\")\n","    y_train_deep = np.array([CLASS_TO_INDEX[int(y)] for y in all_train_y], dtype=np.int64)\n","\n","    X_test_deep = np.load(scalers_dir / \"X_test_scaled.npy\")\n","    y_test_raw = np.load(scalers_dir / \"y_test.npy\")\n","    y_test_deep = np.array([CLASS_TO_INDEX[int(y)] for y in y_test_raw], dtype=np.int64)\n","\n","    assert len(df_test_meta_deep) == X_test_deep.shape[0], \\\n","        f\"[{fold_tag}] Deep-model metadata and data are misaligned\"\n","\n","    print(f\"\\nDeep-model data ({fold_tag}):\")\n","    print(f\"  Train: {X_train_deep.shape}\")\n","    print(f\"  Test:  {X_test_deep.shape}\")\n","\n","    # Feature-based data\n","    df_train_X = pd.read_parquet(features_dir / \"train_X.parquet\")\n","    df_train_y = pd.read_parquet(features_dir / \"train_y.parquet\")\n","    df_test_X = pd.read_parquet(features_dir / \"test_X.parquet\")\n","    df_test_y = pd.read_parquet(features_dir / \"test_y.parquet\")\n","\n","    X_train_feat = df_train_X.values\n","    y_train_feat = np.array(\n","        [CLASS_TO_INDEX[int(y)] for y in df_train_y[\"label\"].values],\n","        dtype=np.int64,\n","    )\n","    X_test_feat = df_test_X.values\n","    y_test_feat = np.array(\n","        [CLASS_TO_INDEX[int(y)] for y in df_test_y[\"label\"].values],\n","        dtype=np.int64,\n","    )\n","\n","    assert len(df_test_meta_feat) == X_test_feat.shape[0], \\\n","        f\"[{fold_tag}] Feature-model metadata and data are misaligned\"\n","\n","    print(f\"\\nFeature-model data ({fold_tag}):\")\n","    print(f\"  Train: {X_train_feat.shape}\")\n","    print(f\"  Test:  {X_test_feat.shape}\")\n","\n","    # Feature standardization (prefer per-fold scaler, fallback to features dir)\n","    scaler_candidates = [\n","        scalers_dir / \"feature_scaler.pkl\",   # per-fold\n","        features_dir / \"scaler.pkl\",          # from Step 11\n","    ]\n","    feat_scaler = None\n","    for p in scaler_candidates:\n","        if p.exists():\n","            with open(p, \"rb\") as f:\n","                feat_scaler = pickle.load(f)\n","            if hasattr(feat_scaler, \"feature_names_in_\"):\n","                delattr(feat_scaler, \"feature_names_in_\")\n","            if p != scaler_candidates[0]:\n","                shutil.copy(p, scaler_candidates[0])\n","            print(f\"\\n✓ Loaded feature scaler: {p}\")\n","            break\n","\n","    if feat_scaler is None:\n","        print(\"\\n⚠️ Feature scaler not found; refitting on this fold's training set\")\n","        feat_scaler = StandardScaler().fit(X_train_feat)\n","        with open(scalers_dir / \"feature_scaler.pkl\", \"wb\") as f:\n","            pickle.dump(feat_scaler, f)\n","\n","    X_train_feat = feat_scaler.transform(X_train_feat)\n","    X_test_feat = feat_scaler.transform(X_test_feat)\n","    print(\"  Features standardized (consistent with Step 14)\")\n","\n","    # Per-fold summary metrics\n","    rf_acc = rf_f1 = None\n","    knn_acc = knn_f1 = None\n","    inception_acc = inception_f1 = None\n","\n","    # ========== 3. Train RandomForest ==========\n","    if \"rf\" in best_params:\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"3. Train RandomForest (full training set)\")\n","        print(\"=\" * 60)\n","\n","        rf_params = best_params[\"rf\"][\"params\"]\n","        print(f\"Hyperparameters: {rf_params}\")\n","\n","        rf_start = time.time()\n","        rf_model = RandomForestClassifier(\n","            random_state=RANDOM_SEED,\n","            n_jobs=-1,\n","            **rf_params,\n","        )\n","        rf_model.fit(X_train_feat, y_train_feat)\n","        rf_train_time = time.time() - rf_start\n","\n","        print(\"\\nInferencing (RF)...\")\n","        rf_test_proba = rf_model.predict_proba(X_test_feat)\n","        rf_test_pred = rf_model.predict(X_test_feat)\n","\n","        assert list(rf_model.classes_) == list(range(N_CLASSES)), \\\n","            f\"[{fold_tag}] RF classes order mismatch: {rf_model.classes_} != {list(range(N_CLASSES))}\"\n","\n","        with open(models_dir / \"rf_final.pkl\", \"wb\") as f:\n","            pickle.dump(rf_model, f)\n","\n","        df_rf_pred = pd.DataFrame({\n","            \"window_id\": df_test_meta_feat[\"window_id\"].values,\n","            \"subject_id\": df_test_meta_feat[\"subject_id\"].values,\n","            \"pred_label\": [INDEX_TO_CLASS[int(p)] for p in rf_test_pred],\n","            \"true_label\": [INDEX_TO_CLASS[int(y)] for y in y_test_feat],\n","        })\n","        for i in range(N_CLASSES):\n","            df_rf_pred[f\"proba_class_{CLASS_LIST[i]}\"] = rf_test_proba[:, i]\n","\n","        df_rf_pred.to_parquet(predictions_dir / \"rf_predictions.parquet\", index=False)\n","\n","        rf_acc = accuracy_score(y_test_feat, rf_test_pred)\n","        rf_f1 = f1_score(y_test_feat, rf_test_pred, average=\"macro\", zero_division=0)\n","\n","        print(f\"\\n✓ RF training complete ({fold_tag}):\")\n","        print(f\"  Train time: {rf_train_time:.2f}s\")\n","        print(f\"  Test Accuracy: {rf_acc:.4f}\")\n","        print(f\"  Test F1:       {rf_f1:.4f}\")\n","        print(\"  Saved: rf_final.pkl, rf_predictions.parquet\")\n","\n","    # ========== 4. Train KNN ==========\n","    if \"knn\" in best_params:\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"4. Train KNN (full training set)\")\n","        print(\"=\" * 60)\n","\n","        knn_params = best_params[\"knn\"][\"params\"]\n","        print(f\"Hyperparameters: {knn_params}\")\n","\n","        knn_start = time.time()\n","        knn_model = KNeighborsClassifier(n_jobs=-1, **knn_params)\n","        knn_model.fit(X_train_feat, y_train_feat)\n","        knn_train_time = time.time() - knn_start\n","\n","        print(\"\\nInferencing (KNN)...\")\n","        knn_test_proba = knn_model.predict_proba(X_test_feat)\n","        knn_test_pred = knn_model.predict(X_test_feat)\n","\n","        assert list(knn_model.classes_) == list(range(N_CLASSES)), \\\n","            f\"[{fold_tag}] KNN classes order mismatch: {knn_model.classes_} != {list(range(N_CLASSES))}\"\n","\n","        with open(models_dir / \"knn_final.pkl\", \"wb\") as f:\n","            pickle.dump(knn_model, f)\n","\n","        df_knn_pred = pd.DataFrame({\n","            \"window_id\": df_test_meta_feat[\"window_id\"].values,\n","            \"subject_id\": df_test_meta_feat[\"subject_id\"].values,\n","            \"pred_label\": [INDEX_TO_CLASS[int(p)] for p in knn_test_pred],\n","            \"true_label\": [INDEX_TO_CLASS[int(y)] for y in y_test_feat],\n","        })\n","        for i in range(N_CLASSES):\n","            df_knn_pred[f\"proba_class_{CLASS_LIST[i]}\"] = knn_test_proba[:, i]\n","\n","        df_knn_pred.to_parquet(predictions_dir / \"knn_predictions.parquet\", index=False)\n","\n","        knn_acc = accuracy_score(y_test_feat, knn_test_pred)\n","        knn_f1 = f1_score(y_test_feat, knn_test_pred, average=\"macro\", zero_division=0)\n","\n","        print(f\"\\n✓ KNN training complete ({fold_tag}):\")\n","        print(f\"  Train time: {knn_train_time:.2f}s\")\n","        print(f\"  Test Accuracy: {knn_acc:.4f}\")\n","        print(f\"  Test F1:       {knn_f1:.4f}\")\n","        print(\"  Saved: knn_final.pkl, knn_predictions.parquet\")\n","\n","    # ========== 5. Train InceptionTime ==========\n","    if \"inception\" in best_params:\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"5. Train InceptionTime (full training set)\")\n","        print(\"=\" * 60)\n","\n","        inception_params = best_params[\"inception\"][\"params\"]\n","        print(f\"Hyperparameters: {inception_params}\")\n","\n","        pin = torch.cuda.is_available()\n","        train_loader = DataLoader(\n","            WindowDataset(X_train_deep, y_train_deep),\n","            batch_size=INFERENCE_CONFIG[\"batch_size\"],\n","            shuffle=True,\n","            num_workers=INFERENCE_CONFIG[\"num_workers\"],\n","            pin_memory=pin,\n","        )\n","        test_loader = DataLoader(\n","            WindowDataset(X_test_deep, y_test_deep),\n","            batch_size=INFERENCE_CONFIG[\"batch_size\"],\n","            shuffle=False,\n","            num_workers=INFERENCE_CONFIG[\"num_workers\"],\n","            pin_memory=pin,\n","        )\n","\n","        n_channels = X_train_deep.shape[2]\n","        model = InceptionTime(\n","            n_channels=n_channels,\n","            n_classes=N_CLASSES,\n","            n_filters=inception_params[\"n_filters\"],\n","            depth=inception_params[\"depth\"],\n","        ).to(DEVICE)\n","\n","        optimizer = torch.optim.Adam(\n","            model.parameters(),\n","            lr=inception_params[\"learning_rate\"],\n","            weight_decay=1e-4,\n","        )\n","        criterion = nn.CrossEntropyLoss()\n","\n","        print(\"\\nStart training (InceptionTime)...\")\n","        inception_start = time.time()\n","        max_epochs = 50\n","        patience = 10\n","        best_train_loss = float(\"inf\")\n","        patience_counter = 0\n","\n","        for epoch in range(max_epochs):\n","            model.train()\n","            total_loss = 0.0\n","            correct = 0\n","            total = 0\n","\n","            for X_batch, y_batch in train_loader:\n","                X_batch = X_batch.to(DEVICE, non_blocking=True)\n","                y_batch = y_batch.to(DEVICE, non_blocking=True)\n","\n","                optimizer.zero_grad(set_to_none=True)\n","\n","                if USE_AMP_BF16:\n","                    with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n","                        outputs = model(X_batch)\n","                        loss = criterion(outputs, y_batch)\n","                else:\n","                    outputs = model(X_batch)\n","                    loss = criterion(outputs, y_batch)\n","\n","                loss.backward()\n","                optimizer.step()\n","\n","                total_loss += loss.item() * X_batch.size(0)\n","                _, preds = outputs.max(1)\n","                total += y_batch.size(0)\n","                correct += preds.eq(y_batch).sum().item()\n","\n","            train_loss = total_loss / total\n","            train_acc = correct / total\n","\n","            print(f\"Epoch {epoch+1}/{max_epochs}: Loss={train_loss:.4f}, Acc={train_acc:.4f}\")\n","\n","            if train_loss < best_train_loss - 1e-4:\n","                best_train_loss = train_loss\n","                patience_counter = 0\n","                torch.save(model.state_dict(), models_dir / \"inception_final.pt\")\n","            else:\n","                patience_counter += 1\n","                if patience_counter >= patience:\n","                    print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n","                    break\n","\n","        inception_train_time = time.time() - inception_start\n","\n","        # Load best model\n","        model.load_state_dict(torch.load(models_dir / \"inception_final.pt\", map_location=DEVICE))\n","\n","        # Inference\n","        print(\"\\nInferencing (InceptionTime)...\")\n","        model.eval()\n","        all_logits = []\n","        all_preds = []\n","        all_labels = []\n","\n","        with torch.no_grad():\n","            for X_batch, y_batch in test_loader:\n","                X_batch = X_batch.to(DEVICE, non_blocking=True)\n","\n","                if USE_AMP_BF16:\n","                    with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n","                        outputs = model(X_batch)\n","                else:\n","                    outputs = model(X_batch)\n","\n","                all_logits.append(outputs.float().cpu().numpy())\n","                _, preds = outputs.max(1)\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(y_batch.cpu().numpy())\n","\n","        all_logits = np.vstack(all_logits)\n","        all_proba = torch.softmax(torch.FloatTensor(all_logits), dim=1).numpy()\n","        all_preds = np.array(all_preds)\n","        all_labels = np.array(all_labels)\n","\n","        df_inception_pred = pd.DataFrame({\n","            \"window_id\": df_test_meta_deep[\"window_id\"].values,\n","            \"subject_id\": df_test_meta_deep[\"subject_id\"].values,\n","            \"pred_label\": [INDEX_TO_CLASS[int(p)] for p in all_preds],\n","            \"true_label\": [INDEX_TO_CLASS[int(y)] for y in all_labels],\n","        })\n","        for i in range(N_CLASSES):\n","            df_inception_pred[f\"logit_class_{CLASS_LIST[i]}\"] = all_logits[:, i]\n","        for i in range(N_CLASSES):\n","            df_inception_pred[f\"proba_class_{CLASS_LIST[i]}\"] = all_proba[:, i]\n","\n","        df_inception_pred.to_parquet(predictions_dir / \"inception_predictions.parquet\", index=False)\n","\n","        inception_acc = accuracy_score(all_labels, all_preds)\n","        inception_f1 = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n","\n","        print(f\"\\n✓ InceptionTime training complete ({fold_tag}):\")\n","        print(f\"  Train time: {inception_train_time:.2f}s\")\n","        print(f\"  Test Accuracy: {inception_acc:.4f}\")\n","        print(f\"  Test F1:       {inception_f1:.4f}\")\n","        print(\"  Saved: inception_final.pt, inception_predictions.parquet\")\n","\n","    # ========== 6. Save per-fold training config/results ==========\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"6. Save training config/results\")\n","    print(\"=\" * 60)\n","\n","    final_results = {\n","        \"fold_id\": FOLD_ID,\n","        \"fold_tag\": fold_tag,\n","        \"random_seed\": RANDOM_SEED,\n","        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n","        \"inference_config\": INFERENCE_CONFIG,\n","        \"class_mapping\": {\n","            \"original_classes\": CLASS_LIST,\n","            \"mapped_indices\": list(range(N_CLASSES)),\n","            \"class_to_index\": CLASS_TO_INDEX,\n","            \"index_to_class\": INDEX_TO_CLASS,\n","        },\n","        \"models\": {},\n","        \"notes\": [\n","            \"Train final models with best hyperparameters from Step 14\",\n","            \"Fit on the full training set for each outer fold\",\n","            \"Run full inference on the test set\",\n","            \"Save per-window predictions (window_id → logits/proba)\",\n","            \"RF/KNN use metadata from the features directory (ensures alignment)\",\n","            \"Deep model uses metadata from the scalers directory\",\n","            \"Feature standardization consistent with Step 14 (fit on full train)\",\n","            \"Added alignment assertions (prevent order mismatch)\",\n","            \"Probability column order assertions for RF/KNN\",\n","            \"BF16 logits cast to float32 before NumPy\",\n","        ],\n","    }\n","\n","    if \"rf\" in best_params:\n","        final_results[\"models\"][\"rf\"] = {\n","            \"params\": best_params[\"rf\"][\"params\"],\n","            \"test_accuracy\": float(rf_acc),\n","            \"test_f1\": float(rf_f1),\n","        }\n","\n","    if \"knn\" in best_params:\n","        final_results[\"models\"][\"knn\"] = {\n","            \"params\": best_params[\"knn\"][\"params\"],\n","            \"test_accuracy\": float(knn_acc),\n","            \"test_f1\": float(knn_f1),\n","        }\n","\n","    if \"inception\" in best_params:\n","        final_results[\"models\"][\"inception\"] = {\n","            \"params\": best_params[\"inception\"][\"params\"],\n","            \"test_accuracy\": float(inception_acc),\n","            \"test_f1\": float(inception_f1),\n","        }\n","\n","    with open(predictions_dir / \"final_results.yaml\", \"w\", encoding=\"utf-8\") as f:\n","        yaml.dump(final_results, f, default_flow_style=False, allow_unicode=True)\n","\n","    with open(predictions_dir / \"final_results.json\", \"w\", encoding=\"utf-8\") as f:\n","        json.dump(final_results, f, indent=2)\n","\n","    print(f\"✓ Saved: {predictions_dir / 'final_results.yaml'}\")\n","    print(f\"✓ Saved: {predictions_dir / 'final_results.json'}\")\n","\n","    # Fold-level summary\n","    fold_summaries.append({\n","        \"fold_id\": FOLD_ID,\n","        \"fold_tag\": fold_tag,\n","        \"rf_acc\": rf_acc,\n","        \"rf_f1\": rf_f1,\n","        \"knn_acc\": knn_acc,\n","        \"knn_f1\": knn_f1,\n","        \"inception_acc\": inception_acc,\n","        \"inception_f1\": inception_f1,\n","    })\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(f\"Fold {fold_tag} summary\")\n","    print(\"=\" * 60)\n","    if rf_acc is not None:\n","        print(f\"  RF:         Acc={rf_acc:.4f}, F1={rf_f1:.4f}\")\n","    if knn_acc is not None:\n","        print(f\"  KNN:        Acc={knn_acc:.4f}, F1={knn_f1:.4f}\")\n","    if inception_acc is not None:\n","        print(f\"  Inception:  Acc={inception_acc:.4f}, F1={inception_f1:.4f}\")\n","    print(f\"  Predictions dir: {predictions_dir}/\")\n","\n","# ========== Global summary across folds ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"Step 15 complete — Training & Inference for all folds\")\n","print(\"=\" * 60)\n","\n","if fold_summaries:\n","    df_summary = pd.DataFrame(fold_summaries).sort_values(\"fold_id\")\n","    print(\"\\nPer-fold test performance:\")\n","    print(df_summary)\n","\n","    # Deep model summary if available\n","    if df_summary[\"inception_acc\"].notna().any():\n","        print(\"\\nInceptionTime cross-fold mean ± std:\")\n","        print(\n","            f\"  Acc: {df_summary['inception_acc'].mean():.4f} \"\n","            f\"± {df_summary['inception_acc'].std():.4f}\"\n","        )\n","        print(\n","            f\"  F1 : {df_summary['inception_f1'].mean():.4f} \"\n","            f\"± {df_summary['inception_f1'].std():.4f}\"\n","        )\n","else:\n","    print(\"No folds were processed. Please check previous steps and tuning files.\")\n","\n","print(\"\\nNext steps:\")\n","print(\"  - Use predictions/*/ to build confusion matrices and calibration plots\")\n","print(\"  - Aggregate per-window predictions to subject/session-level metrics if needed\")\n","print(\"  - Feed final_results.json into Step 16 / evaluation scripts\")\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HFeeyo9c_Rws","executionInfo":{"status":"ok","timestamp":1763151301300,"user_tz":0,"elapsed":305507,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"6f831233-43c8-4b1e-f421-031f86969948"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 15: Training & Inference (multi-fold)\n","============================================================\n","\n","Device: cuda\n","BF16 mixed precision: True\n","Inference batch size: 128\n","Num workers: 4\n","\n","Outer folds detected: [0, 1, 2, 3, 4, 5, 6, 7]\n","\n","============================================================\n","Outer fold: FOLD_ID=0 (fold_00)\n","============================================================\n","\n","============================================================\n","1. Load best hyperparameters\n","============================================================\n","✓ RF: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5}\n","✓ KNN: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'}\n","✓ InceptionTime: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6}\n","\n","============================================================\n","2. Load data and label mapping\n","============================================================\n","#classes: 6\n","Class mapping: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","Deep-model data (fold_00):\n","  Train: (4965, 150, 8)\n","  Test:  (766, 150, 8)\n","\n","Feature-model data (fold_00):\n","  Train: (4965, 220)\n","  Test:  (766, 220)\n","\n","✓ Loaded feature scaler: data/lara/mbientlab/proc/features/fold_00/scaler.pkl\n","  Features standardized (consistent with Step 14)\n","\n","============================================================\n","3. Train RandomForest (full training set)\n","============================================================\n","Hyperparameters: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5}\n","\n","Inferencing (RF)...\n","\n","✓ RF training complete (fold_00):\n","  Train time: 3.19s\n","  Test Accuracy: 0.6292\n","  Test F1:       0.4154\n","  Saved: rf_final.pkl, rf_predictions.parquet\n","\n","============================================================\n","4. Train KNN (full training set)\n","============================================================\n","Hyperparameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'}\n","\n","Inferencing (KNN)...\n","\n","✓ KNN training complete (fold_00):\n","  Train time: 0.01s\n","  Test Accuracy: 0.5953\n","  Test F1:       0.4628\n","  Saved: knn_final.pkl, knn_predictions.parquet\n","\n","============================================================\n","5. Train InceptionTime (full training set)\n","============================================================\n","Hyperparameters: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6}\n","\n","Start training (InceptionTime)...\n","Epoch 1/50: Loss=1.2104, Acc=0.5885\n","Epoch 2/50: Loss=0.9726, Acc=0.6665\n","Epoch 3/50: Loss=0.9141, Acc=0.6757\n","Epoch 4/50: Loss=0.8508, Acc=0.6993\n","Epoch 5/50: Loss=0.7927, Acc=0.7223\n","Epoch 6/50: Loss=0.7581, Acc=0.7309\n","Epoch 7/50: Loss=0.7050, Acc=0.7509\n","Epoch 8/50: Loss=0.6802, Acc=0.7617\n","Epoch 9/50: Loss=0.6147, Acc=0.7793\n","Epoch 10/50: Loss=0.5685, Acc=0.7996\n","Epoch 11/50: Loss=0.5216, Acc=0.8097\n","Epoch 12/50: Loss=0.4793, Acc=0.8294\n","Epoch 13/50: Loss=0.4102, Acc=0.8592\n","Epoch 14/50: Loss=0.3531, Acc=0.8785\n","Epoch 15/50: Loss=0.3286, Acc=0.8882\n","Epoch 16/50: Loss=0.3032, Acc=0.8985\n","Epoch 17/50: Loss=0.2525, Acc=0.9162\n","Epoch 18/50: Loss=0.2023, Acc=0.9329\n","Epoch 19/50: Loss=0.2190, Acc=0.9261\n","Epoch 20/50: Loss=0.1788, Acc=0.9416\n","Epoch 21/50: Loss=0.1786, Acc=0.9412\n","Epoch 22/50: Loss=0.1425, Acc=0.9557\n","Epoch 23/50: Loss=0.1143, Acc=0.9619\n","Epoch 24/50: Loss=0.0969, Acc=0.9702\n","Epoch 25/50: Loss=0.1025, Acc=0.9666\n","Epoch 26/50: Loss=0.0904, Acc=0.9738\n","Epoch 27/50: Loss=0.0923, Acc=0.9708\n","Epoch 28/50: Loss=0.0972, Acc=0.9664\n","Epoch 29/50: Loss=0.0848, Acc=0.9706\n","Epoch 30/50: Loss=0.0840, Acc=0.9734\n","Epoch 31/50: Loss=0.0749, Acc=0.9756\n","Epoch 32/50: Loss=0.0701, Acc=0.9789\n","Epoch 33/50: Loss=0.0740, Acc=0.9736\n","Epoch 34/50: Loss=0.0719, Acc=0.9772\n","Epoch 35/50: Loss=0.0630, Acc=0.9795\n","Epoch 36/50: Loss=0.0428, Acc=0.9873\n","Epoch 37/50: Loss=0.0326, Acc=0.9903\n","Epoch 38/50: Loss=0.0491, Acc=0.9845\n","Epoch 39/50: Loss=0.0566, Acc=0.9833\n","Epoch 40/50: Loss=0.0537, Acc=0.9837\n","Epoch 41/50: Loss=0.0611, Acc=0.9797\n","Epoch 42/50: Loss=0.0598, Acc=0.9778\n","Epoch 43/50: Loss=0.0772, Acc=0.9738\n","Epoch 44/50: Loss=0.0824, Acc=0.9730\n","Epoch 45/50: Loss=0.0582, Acc=0.9789\n","Epoch 46/50: Loss=0.0452, Acc=0.9865\n","Epoch 47/50: Loss=0.0338, Acc=0.9899\n","\n","Early stopping triggered at epoch 47\n","\n","Inferencing (InceptionTime)...\n","\n","✓ InceptionTime training complete (fold_00):\n","  Train time: 35.00s\n","  Test Accuracy: 0.6514\n","  Test F1:       0.5822\n","  Saved: inception_final.pt, inception_predictions.parquet\n","\n","============================================================\n","6. Save training config/results\n","============================================================\n","✓ Saved: predictions/fold_00/final_results.yaml\n","✓ Saved: predictions/fold_00/final_results.json\n","\n","============================================================\n","Fold fold_00 summary\n","============================================================\n","  RF:         Acc=0.6292, F1=0.4154\n","  KNN:        Acc=0.5953, F1=0.4628\n","  Inception:  Acc=0.6514, F1=0.5822\n","  Predictions dir: predictions/fold_00/\n","\n","============================================================\n","Outer fold: FOLD_ID=1 (fold_01)\n","============================================================\n","\n","============================================================\n","1. Load best hyperparameters\n","============================================================\n","✓ RF: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5}\n","✓ KNN: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan'}\n","✓ InceptionTime: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6}\n","\n","============================================================\n","2. Load data and label mapping\n","============================================================\n","#classes: 6\n","Class mapping: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","Deep-model data (fold_01):\n","  Train: (5072, 150, 8)\n","  Test:  (659, 150, 8)\n","\n","Feature-model data (fold_01):\n","  Train: (5072, 220)\n","  Test:  (659, 220)\n","\n","✓ Loaded feature scaler: data/lara/mbientlab/proc/features/fold_01/scaler.pkl\n","  Features standardized (consistent with Step 14)\n","\n","============================================================\n","3. Train RandomForest (full training set)\n","============================================================\n","Hyperparameters: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5}\n","\n","Inferencing (RF)...\n","\n","✓ RF training complete (fold_01):\n","  Train time: 2.72s\n","  Test Accuracy: 0.6282\n","  Test F1:       0.3355\n","  Saved: rf_final.pkl, rf_predictions.parquet\n","\n","============================================================\n","4. Train KNN (full training set)\n","============================================================\n","Hyperparameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan'}\n","\n","Inferencing (KNN)...\n","\n","✓ KNN training complete (fold_01):\n","  Train time: 0.00s\n","  Test Accuracy: 0.5903\n","  Test F1:       0.3615\n","  Saved: knn_final.pkl, knn_predictions.parquet\n","\n","============================================================\n","5. Train InceptionTime (full training set)\n","============================================================\n","Hyperparameters: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6}\n","\n","Start training (InceptionTime)...\n","Epoch 1/50: Loss=1.1885, Acc=0.5868\n","Epoch 2/50: Loss=0.9310, Acc=0.6721\n","Epoch 3/50: Loss=0.8514, Acc=0.7039\n","Epoch 4/50: Loss=0.7851, Acc=0.7224\n","Epoch 5/50: Loss=0.7562, Acc=0.7366\n","Epoch 6/50: Loss=0.7059, Acc=0.7543\n","Epoch 7/50: Loss=0.6603, Acc=0.7668\n","Epoch 8/50: Loss=0.6154, Acc=0.7865\n","Epoch 9/50: Loss=0.5752, Acc=0.8001\n","Epoch 10/50: Loss=0.5153, Acc=0.8172\n","Epoch 11/50: Loss=0.5145, Acc=0.8149\n","Epoch 12/50: Loss=0.4436, Acc=0.8425\n","Epoch 13/50: Loss=0.3837, Acc=0.8695\n","Epoch 14/50: Loss=0.3664, Acc=0.8738\n","Epoch 15/50: Loss=0.3332, Acc=0.8823\n","Epoch 16/50: Loss=0.3036, Acc=0.8963\n","Epoch 17/50: Loss=0.2493, Acc=0.9134\n","Epoch 18/50: Loss=0.2297, Acc=0.9286\n","Epoch 19/50: Loss=0.2090, Acc=0.9322\n","Epoch 20/50: Loss=0.1834, Acc=0.9440\n","Epoch 21/50: Loss=0.1722, Acc=0.9412\n","Epoch 22/50: Loss=0.1670, Acc=0.9440\n","Epoch 23/50: Loss=0.1654, Acc=0.9444\n","Epoch 24/50: Loss=0.1495, Acc=0.9472\n","Epoch 25/50: Loss=0.1339, Acc=0.9596\n","Epoch 26/50: Loss=0.1140, Acc=0.9639\n","Epoch 27/50: Loss=0.1202, Acc=0.9604\n","Epoch 28/50: Loss=0.0880, Acc=0.9718\n","Epoch 29/50: Loss=0.0609, Acc=0.9817\n","Epoch 30/50: Loss=0.0565, Acc=0.9815\n","Epoch 31/50: Loss=0.0645, Acc=0.9807\n","Epoch 32/50: Loss=0.0932, Acc=0.9698\n","Epoch 33/50: Loss=0.1203, Acc=0.9547\n","Epoch 34/50: Loss=0.1503, Acc=0.9485\n","Epoch 35/50: Loss=0.1308, Acc=0.9527\n","Epoch 36/50: Loss=0.1020, Acc=0.9655\n","Epoch 37/50: Loss=0.0681, Acc=0.9767\n","Epoch 38/50: Loss=0.0552, Acc=0.9836\n","Epoch 39/50: Loss=0.0459, Acc=0.9858\n","Epoch 40/50: Loss=0.0426, Acc=0.9854\n","Epoch 41/50: Loss=0.0479, Acc=0.9836\n","Epoch 42/50: Loss=0.0381, Acc=0.9872\n","Epoch 43/50: Loss=0.0547, Acc=0.9817\n","Epoch 44/50: Loss=0.0492, Acc=0.9836\n","Epoch 45/50: Loss=0.0407, Acc=0.9858\n","Epoch 46/50: Loss=0.0539, Acc=0.9811\n","Epoch 47/50: Loss=0.0962, Acc=0.9667\n","Epoch 48/50: Loss=0.1192, Acc=0.9572\n","Epoch 49/50: Loss=0.1186, Acc=0.9564\n","Epoch 50/50: Loss=0.0822, Acc=0.9712\n","\n","Inferencing (InceptionTime)...\n","\n","✓ InceptionTime training complete (fold_01):\n","  Train time: 36.72s\n","  Test Accuracy: 0.6282\n","  Test F1:       0.3816\n","  Saved: inception_final.pt, inception_predictions.parquet\n","\n","============================================================\n","6. Save training config/results\n","============================================================\n","✓ Saved: predictions/fold_01/final_results.yaml\n","✓ Saved: predictions/fold_01/final_results.json\n","\n","============================================================\n","Fold fold_01 summary\n","============================================================\n","  RF:         Acc=0.6282, F1=0.3355\n","  KNN:        Acc=0.5903, F1=0.3615\n","  Inception:  Acc=0.6282, F1=0.3816\n","  Predictions dir: predictions/fold_01/\n","\n","============================================================\n","Outer fold: FOLD_ID=2 (fold_02)\n","============================================================\n","\n","============================================================\n","1. Load best hyperparameters\n","============================================================\n","✓ RF: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5}\n","✓ KNN: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'}\n","✓ InceptionTime: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","\n","============================================================\n","2. Load data and label mapping\n","============================================================\n","#classes: 6\n","Class mapping: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","Deep-model data (fold_02):\n","  Train: (4960, 150, 8)\n","  Test:  (771, 150, 8)\n","\n","Feature-model data (fold_02):\n","  Train: (4960, 220)\n","  Test:  (771, 220)\n","\n","✓ Loaded feature scaler: data/lara/mbientlab/proc/features/fold_02/scaler.pkl\n","  Features standardized (consistent with Step 14)\n","\n","============================================================\n","3. Train RandomForest (full training set)\n","============================================================\n","Hyperparameters: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5}\n","\n","Inferencing (RF)...\n","\n","✓ RF training complete (fold_02):\n","  Train time: 1.25s\n","  Test Accuracy: 0.7198\n","  Test F1:       0.4687\n","  Saved: rf_final.pkl, rf_predictions.parquet\n","\n","============================================================\n","4. Train KNN (full training set)\n","============================================================\n","Hyperparameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'}\n","\n","Inferencing (KNN)...\n","\n","✓ KNN training complete (fold_02):\n","  Train time: 0.00s\n","  Test Accuracy: 0.6550\n","  Test F1:       0.4824\n","  Saved: knn_final.pkl, knn_predictions.parquet\n","\n","============================================================\n","5. Train InceptionTime (full training set)\n","============================================================\n","Hyperparameters: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","\n","Start training (InceptionTime)...\n","Epoch 1/50: Loss=1.2361, Acc=0.5770\n","Epoch 2/50: Loss=0.9895, Acc=0.6571\n","Epoch 3/50: Loss=0.8696, Acc=0.6970\n","Epoch 4/50: Loss=0.7942, Acc=0.7230\n","Epoch 5/50: Loss=0.7338, Acc=0.7369\n","Epoch 6/50: Loss=0.6609, Acc=0.7639\n","Epoch 7/50: Loss=0.6058, Acc=0.7837\n","Epoch 8/50: Loss=0.5118, Acc=0.8185\n","Epoch 9/50: Loss=0.4772, Acc=0.8284\n","Epoch 10/50: Loss=0.4279, Acc=0.8496\n","Epoch 11/50: Loss=0.3722, Acc=0.8758\n","Epoch 12/50: Loss=0.2951, Acc=0.9034\n","Epoch 13/50: Loss=0.2317, Acc=0.9331\n","Epoch 14/50: Loss=0.2062, Acc=0.9377\n","Epoch 15/50: Loss=0.2220, Acc=0.9274\n","Epoch 16/50: Loss=0.1968, Acc=0.9341\n","Epoch 17/50: Loss=0.1487, Acc=0.9558\n","Epoch 18/50: Loss=0.1407, Acc=0.9573\n","Epoch 19/50: Loss=0.1239, Acc=0.9653\n","Epoch 20/50: Loss=0.0987, Acc=0.9698\n","Epoch 21/50: Loss=0.0780, Acc=0.9796\n","Epoch 22/50: Loss=0.0841, Acc=0.9736\n","Epoch 23/50: Loss=0.0674, Acc=0.9806\n","Epoch 24/50: Loss=0.0457, Acc=0.9883\n","Epoch 25/50: Loss=0.0374, Acc=0.9913\n","Epoch 26/50: Loss=0.0405, Acc=0.9895\n","Epoch 27/50: Loss=0.0602, Acc=0.9808\n","Epoch 28/50: Loss=0.1093, Acc=0.9665\n","Epoch 29/50: Loss=0.1432, Acc=0.9528\n","Epoch 30/50: Loss=0.1451, Acc=0.9504\n","Epoch 31/50: Loss=0.1191, Acc=0.9613\n","Epoch 32/50: Loss=0.0943, Acc=0.9688\n","Epoch 33/50: Loss=0.0592, Acc=0.9835\n","Epoch 34/50: Loss=0.0380, Acc=0.9897\n","Epoch 35/50: Loss=0.0267, Acc=0.9942\n","Epoch 36/50: Loss=0.0243, Acc=0.9938\n","Epoch 37/50: Loss=0.0263, Acc=0.9917\n","Epoch 38/50: Loss=0.0273, Acc=0.9917\n","Epoch 39/50: Loss=0.0183, Acc=0.9950\n","Epoch 40/50: Loss=0.0169, Acc=0.9960\n","Epoch 41/50: Loss=0.0288, Acc=0.9911\n","Epoch 42/50: Loss=0.0350, Acc=0.9913\n","Epoch 43/50: Loss=0.0375, Acc=0.9881\n","Epoch 44/50: Loss=0.0369, Acc=0.9883\n","Epoch 45/50: Loss=0.0665, Acc=0.9790\n","Epoch 46/50: Loss=0.1263, Acc=0.9542\n","Epoch 47/50: Loss=0.1241, Acc=0.9540\n","Epoch 48/50: Loss=0.0980, Acc=0.9696\n","Epoch 49/50: Loss=0.0537, Acc=0.9843\n","Epoch 50/50: Loss=0.0355, Acc=0.9915\n","\n","Early stopping triggered at epoch 50\n","\n","Inferencing (InceptionTime)...\n","\n","✓ InceptionTime training complete (fold_02):\n","  Train time: 36.24s\n","  Test Accuracy: 0.5772\n","  Test F1:       0.5265\n","  Saved: inception_final.pt, inception_predictions.parquet\n","\n","============================================================\n","6. Save training config/results\n","============================================================\n","✓ Saved: predictions/fold_02/final_results.yaml\n","✓ Saved: predictions/fold_02/final_results.json\n","\n","============================================================\n","Fold fold_02 summary\n","============================================================\n","  RF:         Acc=0.7198, F1=0.4687\n","  KNN:        Acc=0.6550, F1=0.4824\n","  Inception:  Acc=0.5772, F1=0.5265\n","  Predictions dir: predictions/fold_02/\n","\n","============================================================\n","Outer fold: FOLD_ID=3 (fold_03)\n","============================================================\n","\n","============================================================\n","1. Load best hyperparameters\n","============================================================\n","✓ RF: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5}\n","✓ KNN: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n","✓ InceptionTime: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","\n","============================================================\n","2. Load data and label mapping\n","============================================================\n","#classes: 6\n","Class mapping: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","Deep-model data (fold_03):\n","  Train: (4858, 150, 8)\n","  Test:  (873, 150, 8)\n","\n","Feature-model data (fold_03):\n","  Train: (4858, 220)\n","  Test:  (873, 220)\n","\n","✓ Loaded feature scaler: data/lara/mbientlab/proc/features/fold_03/scaler.pkl\n","  Features standardized (consistent with Step 14)\n","\n","============================================================\n","3. Train RandomForest (full training set)\n","============================================================\n","Hyperparameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5}\n","\n","Inferencing (RF)...\n","\n","✓ RF training complete (fold_03):\n","  Train time: 1.44s\n","  Test Accuracy: 0.5968\n","  Test F1:       0.4685\n","  Saved: rf_final.pkl, rf_predictions.parquet\n","\n","============================================================\n","4. Train KNN (full training set)\n","============================================================\n","Hyperparameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n","\n","Inferencing (KNN)...\n","\n","✓ KNN training complete (fold_03):\n","  Train time: 0.00s\n","  Test Accuracy: 0.5578\n","  Test F1:       0.4852\n","  Saved: knn_final.pkl, knn_predictions.parquet\n","\n","============================================================\n","5. Train InceptionTime (full training set)\n","============================================================\n","Hyperparameters: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","\n","Start training (InceptionTime)...\n","Epoch 1/50: Loss=1.2423, Acc=0.5776\n","Epoch 2/50: Loss=0.9566, Acc=0.6756\n","Epoch 3/50: Loss=0.8576, Acc=0.7104\n","Epoch 4/50: Loss=0.7624, Acc=0.7468\n","Epoch 5/50: Loss=0.7006, Acc=0.7620\n","Epoch 6/50: Loss=0.6365, Acc=0.7853\n","Epoch 7/50: Loss=0.5856, Acc=0.8049\n","Epoch 8/50: Loss=0.4989, Acc=0.8361\n","Epoch 9/50: Loss=0.4348, Acc=0.8553\n","Epoch 10/50: Loss=0.3947, Acc=0.8709\n","Epoch 11/50: Loss=0.3364, Acc=0.8919\n","Epoch 12/50: Loss=0.2902, Acc=0.9059\n","Epoch 13/50: Loss=0.2628, Acc=0.9164\n","Epoch 14/50: Loss=0.2339, Acc=0.9261\n","Epoch 15/50: Loss=0.1870, Acc=0.9422\n","Epoch 16/50: Loss=0.1754, Acc=0.9442\n","Epoch 17/50: Loss=0.1375, Acc=0.9607\n","Epoch 18/50: Loss=0.1064, Acc=0.9708\n","Epoch 19/50: Loss=0.0858, Acc=0.9782\n","Epoch 20/50: Loss=0.0887, Acc=0.9753\n","Epoch 21/50: Loss=0.0809, Acc=0.9809\n","Epoch 22/50: Loss=0.0790, Acc=0.9788\n","Epoch 23/50: Loss=0.0826, Acc=0.9765\n","Epoch 24/50: Loss=0.0864, Acc=0.9763\n","Epoch 25/50: Loss=0.1038, Acc=0.9667\n","Epoch 26/50: Loss=0.0913, Acc=0.9720\n","Epoch 27/50: Loss=0.0644, Acc=0.9823\n","Epoch 28/50: Loss=0.0729, Acc=0.9784\n","Epoch 29/50: Loss=0.0725, Acc=0.9747\n","Epoch 30/50: Loss=0.0797, Acc=0.9755\n","Epoch 31/50: Loss=0.0828, Acc=0.9751\n","Epoch 32/50: Loss=0.0647, Acc=0.9811\n","Epoch 33/50: Loss=0.0516, Acc=0.9864\n","Epoch 34/50: Loss=0.0411, Acc=0.9883\n","Epoch 35/50: Loss=0.0343, Acc=0.9905\n","Epoch 36/50: Loss=0.0344, Acc=0.9897\n","Epoch 37/50: Loss=0.0355, Acc=0.9905\n","Epoch 38/50: Loss=0.0350, Acc=0.9901\n","Epoch 39/50: Loss=0.0343, Acc=0.9903\n","Epoch 40/50: Loss=0.0456, Acc=0.9854\n","Epoch 41/50: Loss=0.0749, Acc=0.9755\n","Epoch 42/50: Loss=0.0956, Acc=0.9675\n","Epoch 43/50: Loss=0.0881, Acc=0.9710\n","Epoch 44/50: Loss=0.1177, Acc=0.9555\n","Epoch 45/50: Loss=0.0876, Acc=0.9710\n","\n","Early stopping triggered at epoch 45\n","\n","Inferencing (InceptionTime)...\n","\n","✓ InceptionTime training complete (fold_03):\n","  Train time: 31.99s\n","  Test Accuracy: 0.5521\n","  Test F1:       0.5692\n","  Saved: inception_final.pt, inception_predictions.parquet\n","\n","============================================================\n","6. Save training config/results\n","============================================================\n","✓ Saved: predictions/fold_03/final_results.yaml\n","✓ Saved: predictions/fold_03/final_results.json\n","\n","============================================================\n","Fold fold_03 summary\n","============================================================\n","  RF:         Acc=0.5968, F1=0.4685\n","  KNN:        Acc=0.5578, F1=0.4852\n","  Inception:  Acc=0.5521, F1=0.5692\n","  Predictions dir: predictions/fold_03/\n","\n","============================================================\n","Outer fold: FOLD_ID=4 (fold_04)\n","============================================================\n","\n","============================================================\n","1. Load best hyperparameters\n","============================================================\n","✓ RF: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5}\n","✓ KNN: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n","✓ InceptionTime: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","\n","============================================================\n","2. Load data and label mapping\n","============================================================\n","#classes: 6\n","Class mapping: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","Deep-model data (fold_04):\n","  Train: (4985, 150, 8)\n","  Test:  (746, 150, 8)\n","\n","Feature-model data (fold_04):\n","  Train: (4985, 220)\n","  Test:  (746, 220)\n","\n","✓ Loaded feature scaler: data/lara/mbientlab/proc/features/fold_04/scaler.pkl\n","  Features standardized (consistent with Step 14)\n","\n","============================================================\n","3. Train RandomForest (full training set)\n","============================================================\n","Hyperparameters: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5}\n","\n","Inferencing (RF)...\n","\n","✓ RF training complete (fold_04):\n","  Train time: 2.85s\n","  Test Accuracy: 0.7252\n","  Test F1:       0.5514\n","  Saved: rf_final.pkl, rf_predictions.parquet\n","\n","============================================================\n","4. Train KNN (full training set)\n","============================================================\n","Hyperparameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n","\n","Inferencing (KNN)...\n","\n","✓ KNN training complete (fold_04):\n","  Train time: 0.00s\n","  Test Accuracy: 0.6622\n","  Test F1:       0.5330\n","  Saved: knn_final.pkl, knn_predictions.parquet\n","\n","============================================================\n","5. Train InceptionTime (full training set)\n","============================================================\n","Hyperparameters: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","\n","Start training (InceptionTime)...\n","Epoch 1/50: Loss=1.2430, Acc=0.5749\n","Epoch 2/50: Loss=0.9843, Acc=0.6604\n","Epoch 3/50: Loss=0.8830, Acc=0.6963\n","Epoch 4/50: Loss=0.8134, Acc=0.7147\n","Epoch 5/50: Loss=0.7453, Acc=0.7404\n","Epoch 6/50: Loss=0.6822, Acc=0.7609\n","Epoch 7/50: Loss=0.6054, Acc=0.7924\n","Epoch 8/50: Loss=0.5484, Acc=0.8096\n","Epoch 9/50: Loss=0.4834, Acc=0.8375\n","Epoch 10/50: Loss=0.4255, Acc=0.8594\n","Epoch 11/50: Loss=0.3690, Acc=0.8849\n","Epoch 12/50: Loss=0.3109, Acc=0.9027\n","Epoch 13/50: Loss=0.2872, Acc=0.9045\n","Epoch 14/50: Loss=0.2317, Acc=0.9330\n","Epoch 15/50: Loss=0.1959, Acc=0.9436\n","Epoch 16/50: Loss=0.1676, Acc=0.9476\n","Epoch 17/50: Loss=0.1558, Acc=0.9559\n","Epoch 18/50: Loss=0.1274, Acc=0.9631\n","Epoch 19/50: Loss=0.1218, Acc=0.9651\n","Epoch 20/50: Loss=0.1348, Acc=0.9571\n","Epoch 21/50: Loss=0.1127, Acc=0.9681\n","Epoch 22/50: Loss=0.0959, Acc=0.9715\n","Epoch 23/50: Loss=0.0919, Acc=0.9761\n","Epoch 24/50: Loss=0.0780, Acc=0.9751\n","Epoch 25/50: Loss=0.0765, Acc=0.9785\n","Epoch 26/50: Loss=0.0695, Acc=0.9803\n","Epoch 27/50: Loss=0.0728, Acc=0.9805\n","Epoch 28/50: Loss=0.0592, Acc=0.9842\n","Epoch 29/50: Loss=0.0613, Acc=0.9827\n","Epoch 30/50: Loss=0.0507, Acc=0.9868\n","Epoch 31/50: Loss=0.0506, Acc=0.9880\n","Epoch 32/50: Loss=0.0683, Acc=0.9785\n","Epoch 33/50: Loss=0.0702, Acc=0.9793\n","Epoch 34/50: Loss=0.0729, Acc=0.9757\n","Epoch 35/50: Loss=0.0806, Acc=0.9739\n","Epoch 36/50: Loss=0.0903, Acc=0.9701\n","Epoch 37/50: Loss=0.0835, Acc=0.9703\n","Epoch 38/50: Loss=0.0588, Acc=0.9815\n","Epoch 39/50: Loss=0.0465, Acc=0.9856\n","Epoch 40/50: Loss=0.0423, Acc=0.9874\n","Epoch 41/50: Loss=0.0251, Acc=0.9932\n","Epoch 42/50: Loss=0.0263, Acc=0.9924\n","Epoch 43/50: Loss=0.0187, Acc=0.9954\n","Epoch 44/50: Loss=0.0167, Acc=0.9950\n","Epoch 45/50: Loss=0.0196, Acc=0.9946\n","Epoch 46/50: Loss=0.0208, Acc=0.9930\n","Epoch 47/50: Loss=0.0298, Acc=0.9920\n","Epoch 48/50: Loss=0.0190, Acc=0.9952\n","Epoch 49/50: Loss=0.0243, Acc=0.9932\n","Epoch 50/50: Loss=0.0434, Acc=0.9856\n","\n","Inferencing (InceptionTime)...\n","\n","✓ InceptionTime training complete (fold_04):\n","  Train time: 36.11s\n","  Test Accuracy: 0.6850\n","  Test F1:       0.5936\n","  Saved: inception_final.pt, inception_predictions.parquet\n","\n","============================================================\n","6. Save training config/results\n","============================================================\n","✓ Saved: predictions/fold_04/final_results.yaml\n","✓ Saved: predictions/fold_04/final_results.json\n","\n","============================================================\n","Fold fold_04 summary\n","============================================================\n","  RF:         Acc=0.7252, F1=0.5514\n","  KNN:        Acc=0.6622, F1=0.5330\n","  Inception:  Acc=0.6850, F1=0.5936\n","  Predictions dir: predictions/fold_04/\n","\n","============================================================\n","Outer fold: FOLD_ID=5 (fold_05)\n","============================================================\n","\n","============================================================\n","1. Load best hyperparameters\n","============================================================\n","✓ RF: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 2}\n","✓ KNN: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'}\n","✓ InceptionTime: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6}\n","\n","============================================================\n","2. Load data and label mapping\n","============================================================\n","#classes: 6\n","Class mapping: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","Deep-model data (fold_05):\n","  Train: (5442, 150, 8)\n","  Test:  (289, 150, 8)\n","\n","Feature-model data (fold_05):\n","  Train: (5442, 220)\n","  Test:  (289, 220)\n","\n","✓ Loaded feature scaler: data/lara/mbientlab/proc/features/fold_05/scaler.pkl\n","  Features standardized (consistent with Step 14)\n","\n","============================================================\n","3. Train RandomForest (full training set)\n","============================================================\n","Hyperparameters: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 2}\n","\n","Inferencing (RF)...\n","\n","✓ RF training complete (fold_05):\n","  Train time: 2.97s\n","  Test Accuracy: 0.4498\n","  Test F1:       0.2966\n","  Saved: rf_final.pkl, rf_predictions.parquet\n","\n","============================================================\n","4. Train KNN (full training set)\n","============================================================\n","Hyperparameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'}\n","\n","Inferencing (KNN)...\n","\n","✓ KNN training complete (fold_05):\n","  Train time: 0.01s\n","  Test Accuracy: 0.4464\n","  Test F1:       0.3207\n","  Saved: knn_final.pkl, knn_predictions.parquet\n","\n","============================================================\n","5. Train InceptionTime (full training set)\n","============================================================\n","Hyperparameters: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6}\n","\n","Start training (InceptionTime)...\n","Epoch 1/50: Loss=1.1828, Acc=0.6038\n","Epoch 2/50: Loss=0.9245, Acc=0.6790\n","Epoch 3/50: Loss=0.8576, Acc=0.6986\n","Epoch 4/50: Loss=0.7997, Acc=0.7179\n","Epoch 5/50: Loss=0.7505, Acc=0.7286\n","Epoch 6/50: Loss=0.7069, Acc=0.7481\n","Epoch 7/50: Loss=0.6838, Acc=0.7569\n","Epoch 8/50: Loss=0.6344, Acc=0.7729\n","Epoch 9/50: Loss=0.5827, Acc=0.7925\n","Epoch 10/50: Loss=0.5871, Acc=0.7902\n","Epoch 11/50: Loss=0.5108, Acc=0.8251\n","Epoch 12/50: Loss=0.4814, Acc=0.8256\n","Epoch 13/50: Loss=0.4328, Acc=0.8440\n","Epoch 14/50: Loss=0.3876, Acc=0.8695\n","Epoch 15/50: Loss=0.3393, Acc=0.8844\n","Epoch 16/50: Loss=0.3345, Acc=0.8822\n","Epoch 17/50: Loss=0.2733, Acc=0.9059\n","Epoch 18/50: Loss=0.2588, Acc=0.9116\n","Epoch 19/50: Loss=0.2601, Acc=0.9116\n","Epoch 20/50: Loss=0.2134, Acc=0.9250\n","Epoch 21/50: Loss=0.2098, Acc=0.9250\n","Epoch 22/50: Loss=0.1659, Acc=0.9423\n","Epoch 23/50: Loss=0.1723, Acc=0.9410\n","Epoch 24/50: Loss=0.1248, Acc=0.9585\n","Epoch 25/50: Loss=0.1106, Acc=0.9655\n","Epoch 26/50: Loss=0.0926, Acc=0.9691\n","Epoch 27/50: Loss=0.0824, Acc=0.9745\n","Epoch 28/50: Loss=0.1150, Acc=0.9599\n","Epoch 29/50: Loss=0.1761, Acc=0.9355\n","Epoch 30/50: Loss=0.1378, Acc=0.9509\n","Epoch 31/50: Loss=0.1020, Acc=0.9662\n","Epoch 32/50: Loss=0.0925, Acc=0.9695\n","Epoch 33/50: Loss=0.0797, Acc=0.9768\n","Epoch 34/50: Loss=0.0538, Acc=0.9847\n","Epoch 35/50: Loss=0.0457, Acc=0.9851\n","Epoch 36/50: Loss=0.0557, Acc=0.9824\n","Epoch 37/50: Loss=0.0455, Acc=0.9870\n","Epoch 38/50: Loss=0.0449, Acc=0.9851\n","Epoch 39/50: Loss=0.0617, Acc=0.9794\n","Epoch 40/50: Loss=0.0853, Acc=0.9678\n","Epoch 41/50: Loss=0.0999, Acc=0.9678\n","Epoch 42/50: Loss=0.0940, Acc=0.9678\n","Epoch 43/50: Loss=0.0829, Acc=0.9708\n","Epoch 44/50: Loss=0.0861, Acc=0.9682\n","Epoch 45/50: Loss=0.0744, Acc=0.9737\n","Epoch 46/50: Loss=0.0550, Acc=0.9825\n","Epoch 47/50: Loss=0.0443, Acc=0.9835\n","Epoch 48/50: Loss=0.0448, Acc=0.9868\n","Epoch 49/50: Loss=0.0284, Acc=0.9919\n","Epoch 50/50: Loss=0.0250, Acc=0.9925\n","\n","Inferencing (InceptionTime)...\n","\n","✓ InceptionTime training complete (fold_05):\n","  Train time: 38.81s\n","  Test Accuracy: 0.4325\n","  Test F1:       0.3661\n","  Saved: inception_final.pt, inception_predictions.parquet\n","\n","============================================================\n","6. Save training config/results\n","============================================================\n","✓ Saved: predictions/fold_05/final_results.yaml\n","✓ Saved: predictions/fold_05/final_results.json\n","\n","============================================================\n","Fold fold_05 summary\n","============================================================\n","  RF:         Acc=0.4498, F1=0.2966\n","  KNN:        Acc=0.4464, F1=0.3207\n","  Inception:  Acc=0.4325, F1=0.3661\n","  Predictions dir: predictions/fold_05/\n","\n","============================================================\n","Outer fold: FOLD_ID=6 (fold_06)\n","============================================================\n","\n","============================================================\n","1. Load best hyperparameters\n","============================================================\n","✓ RF: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 2}\n","✓ KNN: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan'}\n","✓ InceptionTime: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","\n","============================================================\n","2. Load data and label mapping\n","============================================================\n","#classes: 6\n","Class mapping: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","Deep-model data (fold_06):\n","  Train: (4853, 150, 8)\n","  Test:  (878, 150, 8)\n","\n","Feature-model data (fold_06):\n","  Train: (4853, 220)\n","  Test:  (878, 220)\n","\n","✓ Loaded feature scaler: data/lara/mbientlab/proc/features/fold_06/scaler.pkl\n","  Features standardized (consistent with Step 14)\n","\n","============================================================\n","3. Train RandomForest (full training set)\n","============================================================\n","Hyperparameters: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 2}\n","\n","Inferencing (RF)...\n","\n","✓ RF training complete (fold_06):\n","  Train time: 2.75s\n","  Test Accuracy: 0.7130\n","  Test F1:       0.3959\n","  Saved: rf_final.pkl, rf_predictions.parquet\n","\n","============================================================\n","4. Train KNN (full training set)\n","============================================================\n","Hyperparameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan'}\n","\n","Inferencing (KNN)...\n","\n","✓ KNN training complete (fold_06):\n","  Train time: 0.00s\n","  Test Accuracy: 0.6515\n","  Test F1:       0.5125\n","  Saved: knn_final.pkl, knn_predictions.parquet\n","\n","============================================================\n","5. Train InceptionTime (full training set)\n","============================================================\n","Hyperparameters: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","\n","Start training (InceptionTime)...\n","Epoch 1/50: Loss=1.2186, Acc=0.6042\n","Epoch 2/50: Loss=0.9504, Acc=0.6767\n","Epoch 3/50: Loss=0.8464, Acc=0.7134\n","Epoch 4/50: Loss=0.7624, Acc=0.7367\n","Epoch 5/50: Loss=0.7018, Acc=0.7515\n","Epoch 6/50: Loss=0.6164, Acc=0.7830\n","Epoch 7/50: Loss=0.5549, Acc=0.8071\n","Epoch 8/50: Loss=0.4627, Acc=0.8459\n","Epoch 9/50: Loss=0.3977, Acc=0.8714\n","Epoch 10/50: Loss=0.3633, Acc=0.8792\n","Epoch 11/50: Loss=0.3103, Acc=0.8972\n","Epoch 12/50: Loss=0.2409, Acc=0.9312\n","Epoch 13/50: Loss=0.1935, Acc=0.9448\n","Epoch 14/50: Loss=0.1531, Acc=0.9563\n","Epoch 15/50: Loss=0.1659, Acc=0.9514\n","Epoch 16/50: Loss=0.1542, Acc=0.9545\n","Epoch 17/50: Loss=0.1344, Acc=0.9594\n","Epoch 18/50: Loss=0.1199, Acc=0.9672\n","Epoch 19/50: Loss=0.0882, Acc=0.9755\n","Epoch 20/50: Loss=0.0662, Acc=0.9823\n","Epoch 21/50: Loss=0.0516, Acc=0.9885\n","Epoch 22/50: Loss=0.0456, Acc=0.9905\n","Epoch 23/50: Loss=0.0612, Acc=0.9841\n","Epoch 24/50: Loss=0.0643, Acc=0.9831\n","Epoch 25/50: Loss=0.0487, Acc=0.9862\n","Epoch 26/50: Loss=0.0417, Acc=0.9887\n","Epoch 27/50: Loss=0.0659, Acc=0.9788\n","Epoch 28/50: Loss=0.0662, Acc=0.9788\n","Epoch 29/50: Loss=0.0692, Acc=0.9794\n","Epoch 30/50: Loss=0.1306, Acc=0.9561\n","Epoch 31/50: Loss=0.1032, Acc=0.9666\n","Epoch 32/50: Loss=0.0680, Acc=0.9810\n","Epoch 33/50: Loss=0.0552, Acc=0.9827\n","Epoch 34/50: Loss=0.0591, Acc=0.9823\n","Epoch 35/50: Loss=0.0382, Acc=0.9903\n","Epoch 36/50: Loss=0.0378, Acc=0.9883\n","Epoch 37/50: Loss=0.0280, Acc=0.9926\n","Epoch 38/50: Loss=0.0174, Acc=0.9957\n","Epoch 39/50: Loss=0.0130, Acc=0.9969\n","Epoch 40/50: Loss=0.0147, Acc=0.9959\n","Epoch 41/50: Loss=0.0115, Acc=0.9973\n","Epoch 42/50: Loss=0.0157, Acc=0.9959\n","Epoch 43/50: Loss=0.0098, Acc=0.9990\n","Epoch 44/50: Loss=0.0072, Acc=0.9981\n","Epoch 45/50: Loss=0.0048, Acc=0.9990\n","Epoch 46/50: Loss=0.0038, Acc=0.9994\n","Epoch 47/50: Loss=0.0051, Acc=0.9988\n","Epoch 48/50: Loss=0.0052, Acc=0.9990\n","Epoch 49/50: Loss=0.0059, Acc=0.9988\n","Epoch 50/50: Loss=0.0041, Acc=0.9990\n","\n","Inferencing (InceptionTime)...\n","\n","✓ InceptionTime training complete (fold_06):\n","  Train time: 35.32s\n","  Test Accuracy: 0.6515\n","  Test F1:       0.5968\n","  Saved: inception_final.pt, inception_predictions.parquet\n","\n","============================================================\n","6. Save training config/results\n","============================================================\n","✓ Saved: predictions/fold_06/final_results.yaml\n","✓ Saved: predictions/fold_06/final_results.json\n","\n","============================================================\n","Fold fold_06 summary\n","============================================================\n","  RF:         Acc=0.7130, F1=0.3959\n","  KNN:        Acc=0.6515, F1=0.5125\n","  Inception:  Acc=0.6515, F1=0.5968\n","  Predictions dir: predictions/fold_06/\n","\n","============================================================\n","Outer fold: FOLD_ID=7 (fold_07)\n","============================================================\n","\n","============================================================\n","1. Load best hyperparameters\n","============================================================\n","✓ RF: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2}\n","✓ KNN: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n","✓ InceptionTime: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6}\n","\n","============================================================\n","2. Load data and label mapping\n","============================================================\n","#classes: 6\n","Class mapping: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","Deep-model data (fold_07):\n","  Train: (4982, 150, 8)\n","  Test:  (749, 150, 8)\n","\n","Feature-model data (fold_07):\n","  Train: (4982, 220)\n","  Test:  (749, 220)\n","\n","✓ Loaded feature scaler: data/lara/mbientlab/proc/features/fold_07/scaler.pkl\n","  Features standardized (consistent with Step 14)\n","\n","============================================================\n","3. Train RandomForest (full training set)\n","============================================================\n","Hyperparameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2}\n","\n","Inferencing (RF)...\n","\n","✓ RF training complete (fold_07):\n","  Train time: 1.37s\n","  Test Accuracy: 0.6182\n","  Test F1:       0.4720\n","  Saved: rf_final.pkl, rf_predictions.parquet\n","\n","============================================================\n","4. Train KNN (full training set)\n","============================================================\n","Hyperparameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n","\n","Inferencing (KNN)...\n","\n","✓ KNN training complete (fold_07):\n","  Train time: 0.00s\n","  Test Accuracy: 0.5127\n","  Test F1:       0.4268\n","  Saved: knn_final.pkl, knn_predictions.parquet\n","\n","============================================================\n","5. Train InceptionTime (full training set)\n","============================================================\n","Hyperparameters: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6}\n","\n","Start training (InceptionTime)...\n","Epoch 1/50: Loss=1.1695, Acc=0.6032\n","Epoch 2/50: Loss=0.9557, Acc=0.6654\n","Epoch 3/50: Loss=0.8652, Acc=0.6913\n","Epoch 4/50: Loss=0.8008, Acc=0.7196\n","Epoch 5/50: Loss=0.7716, Acc=0.7188\n","Epoch 6/50: Loss=0.7029, Acc=0.7503\n","Epoch 7/50: Loss=0.6565, Acc=0.7617\n","Epoch 8/50: Loss=0.6150, Acc=0.7824\n","Epoch 9/50: Loss=0.5513, Acc=0.8067\n","Epoch 10/50: Loss=0.4853, Acc=0.8326\n","Epoch 11/50: Loss=0.4486, Acc=0.8440\n","Epoch 12/50: Loss=0.3958, Acc=0.8639\n","Epoch 13/50: Loss=0.3675, Acc=0.8709\n","Epoch 14/50: Loss=0.3069, Acc=0.8940\n","Epoch 15/50: Loss=0.2894, Acc=0.8970\n","Epoch 16/50: Loss=0.2643, Acc=0.9075\n","Epoch 17/50: Loss=0.2167, Acc=0.9289\n","Epoch 18/50: Loss=0.1930, Acc=0.9362\n","Epoch 19/50: Loss=0.1859, Acc=0.9362\n","Epoch 20/50: Loss=0.1557, Acc=0.9446\n","Epoch 21/50: Loss=0.1440, Acc=0.9550\n","Epoch 22/50: Loss=0.1579, Acc=0.9464\n","Epoch 23/50: Loss=0.1147, Acc=0.9631\n","Epoch 24/50: Loss=0.1442, Acc=0.9486\n","Epoch 25/50: Loss=0.1199, Acc=0.9601\n","Epoch 26/50: Loss=0.1097, Acc=0.9661\n","Epoch 27/50: Loss=0.1003, Acc=0.9689\n","Epoch 28/50: Loss=0.0747, Acc=0.9767\n","Epoch 29/50: Loss=0.0519, Acc=0.9857\n","Epoch 30/50: Loss=0.0414, Acc=0.9896\n","Epoch 31/50: Loss=0.0354, Acc=0.9908\n","Epoch 32/50: Loss=0.0438, Acc=0.9878\n","Epoch 33/50: Loss=0.0591, Acc=0.9815\n","Epoch 34/50: Loss=0.0861, Acc=0.9725\n","Epoch 35/50: Loss=0.1892, Acc=0.9324\n","Epoch 36/50: Loss=0.1489, Acc=0.9496\n","Epoch 37/50: Loss=0.1065, Acc=0.9657\n","Epoch 38/50: Loss=0.0655, Acc=0.9793\n","Epoch 39/50: Loss=0.0481, Acc=0.9855\n","Epoch 40/50: Loss=0.0436, Acc=0.9874\n","Epoch 41/50: Loss=0.0403, Acc=0.9866\n","\n","Early stopping triggered at epoch 41\n","\n","Inferencing (InceptionTime)...\n","\n","✓ InceptionTime training complete (fold_07):\n","  Train time: 29.55s\n","  Test Accuracy: 0.5287\n","  Test F1:       0.5433\n","  Saved: inception_final.pt, inception_predictions.parquet\n","\n","============================================================\n","6. Save training config/results\n","============================================================\n","✓ Saved: predictions/fold_07/final_results.yaml\n","✓ Saved: predictions/fold_07/final_results.json\n","\n","============================================================\n","Fold fold_07 summary\n","============================================================\n","  RF:         Acc=0.6182, F1=0.4720\n","  KNN:        Acc=0.5127, F1=0.4268\n","  Inception:  Acc=0.5287, F1=0.5433\n","  Predictions dir: predictions/fold_07/\n","\n","============================================================\n","Step 15 complete — Training & Inference for all folds\n","============================================================\n","\n","Per-fold test performance:\n","   fold_id fold_tag    rf_acc     rf_f1   knn_acc    knn_f1  inception_acc  \\\n","0        0  fold_00  0.629243  0.415378  0.595300  0.462762       0.651436   \n","1        1  fold_01  0.628225  0.335458  0.590288  0.361471       0.628225   \n","2        2  fold_02  0.719844  0.468676  0.654994  0.482432       0.577173   \n","3        3  fold_03  0.596793  0.468515  0.557847  0.485213       0.552119   \n","4        4  fold_04  0.725201  0.551397  0.662198  0.533049       0.684987   \n","5        5  fold_05  0.449827  0.296618  0.446367  0.320723       0.432526   \n","6        6  fold_06  0.712984  0.395919  0.651481  0.512474       0.651481   \n","7        7  fold_07  0.618158  0.472046  0.512684  0.426787       0.528705   \n","\n","   inception_f1  \n","0      0.582203  \n","1      0.381570  \n","2      0.526461  \n","3      0.569168  \n","4      0.593586  \n","5      0.366110  \n","6      0.596830  \n","7      0.543310  \n","\n","InceptionTime cross-fold mean ± std:\n","  Acc: 0.5883 ± 0.0830\n","  F1 : 0.5199 ± 0.0934\n","\n","Next steps:\n","  - Use predictions/*/ to build confusion matrices and calibration plots\n","  - Aggregate per-window predictions to subject/session-level metrics if needed\n","  - Feed final_results.json into Step 16 / evaluation scripts\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 16: Evaluation (Primary: Macro-F1, multi-fold)\n","Compute Macro-F1, Balanced Acc, Per-class F1, Confusion Matrix\n","BCa bootstrap 95% CI (1,000 iterations, window-level sampling)\n","For all outer folds found under predictions/fold_xx/.\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import json\n","import os\n","from collections import defaultdict\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import (\n","    f1_score, balanced_accuracy_score, confusion_matrix\n",")\n","from scipy import stats\n","\n","# ========== Config ==========\n","RANDOM_SEED = 42\n","N_BOOTSTRAP = 1000\n","\n","print(\"=\" * 60)\n","print(\"Step 16: Evaluation (Primary: Macro-F1, multi-fold)\")\n","print(\"=\" * 60)\n","\n","# Path config\n","predictions_dir = Path(\"predictions\")\n","metrics_dir = Path(\"metrics\")\n","metrics_dir.mkdir(parents=True, exist_ok=True)\n","\n","np.random.seed(RANDOM_SEED)\n","\n","# Detect available folds from predictions directory\n","fold_dirs = sorted(\n","    d for d in predictions_dir.glob(\"fold_*\")\n","    if d.is_dir()\n",")\n","\n","if not fold_dirs:\n","    raise RuntimeError(\"No prediction folders found under 'predictions/fold_*'.\")\n","\n","outer_folds = []\n","for d in fold_dirs:\n","    tag = d.name  # e.g., \"fold_00\"\n","    try:\n","        fid = int(tag.split(\"_\")[1])\n","    except (IndexError, ValueError):\n","        continue\n","    outer_folds.append((fid, tag))\n","\n","outer_folds = sorted(outer_folds, key=lambda x: x[0])\n","print(f\"\\nDetected folds from predictions/: {[tag for _, tag in outer_folds]}\")\n","print(f\"Bootstrap: {N_BOOTSTRAP} iterations, window-level sampling\")\n","\n","# ========== 1. Helper functions ==========\n","def mean_ci_t(values, alpha=0.05):\n","    \"\"\"\n","    Fold-level t-interval 95% CI for the mean (more stable for small n)\n","\n","    Returns: (mean, ci_lower, ci_upper)\n","    \"\"\"\n","    arr = np.asarray(values, dtype=float)\n","    n = len(arr)\n","    mean = float(np.mean(arr))\n","    if n < 2:\n","        return mean, None, None\n","    se = float(np.std(arr, ddof=1)) / np.sqrt(n)\n","    tcrit = stats.t.ppf(1 - alpha / 2, df=n - 1)\n","    ci_lower = mean - tcrit * se\n","    ci_upper = mean + tcrit * se\n","    return mean, ci_lower, ci_upper\n","\n","# ========== 2. BCa Bootstrap function ==========\n","def bootstrap_ci(y_true, y_pred, metric_func, n_bootstrap=1000, alpha=0.05):\n","    \"\"\"\n","    BCa Bootstrap 95% CI\n","    Sample at window level (with replacement)\n","\n","    Returns: (mean, lower, upper)\n","    \"\"\"\n","    y_true = np.asarray(y_true)\n","    y_pred = np.asarray(y_pred)\n","    n_samples = len(y_true)\n","    bootstrap_scores = []\n","\n","    # Bootstrap sampling\n","    for _ in range(n_bootstrap):\n","        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n","        y_true_boot = y_true[indices]\n","        y_pred_boot = y_pred[indices]\n","        score = metric_func(y_true_boot, y_pred_boot)\n","        bootstrap_scores.append(score)\n","\n","    bootstrap_scores = np.array(bootstrap_scores)\n","\n","    # Original score\n","    original_score = metric_func(y_true, y_pred)\n","\n","    # BCa correction\n","    # 1. bias correction (z0)\n","    n_less = np.sum(bootstrap_scores < original_score)\n","    p_less = n_less / n_bootstrap\n","    z0 = stats.norm.ppf(max(min(p_less, 0.9999), 0.0001))\n","\n","    # 2. acceleration (a) - jackknife\n","    jackknife_scores = []\n","    for i in range(n_samples):\n","        mask = np.ones(n_samples, dtype=bool)\n","        mask[i] = False\n","        jack_score = metric_func(y_true[mask], y_pred[mask])\n","        jackknife_scores.append(jack_score)\n","\n","    jackknife_scores = np.array(jackknife_scores)\n","    jack_mean = jackknife_scores.mean()\n","    numerator = np.sum((jack_mean - jackknife_scores) ** 3)\n","    denominator = 6 * (np.sum((jack_mean - jackknife_scores) ** 2) ** 1.5)\n","    a = numerator / denominator if denominator != 0 else 0.0\n","\n","    # 3. adjusted percentiles\n","    z_alpha_lower = stats.norm.ppf(alpha / 2)\n","    z_alpha_upper = stats.norm.ppf(1 - alpha / 2)\n","\n","    p_lower = stats.norm.cdf(z0 + (z0 + z_alpha_lower) / (1 - a * (z0 + z_alpha_lower)))\n","    p_upper = stats.norm.cdf(z0 + (z0 + z_alpha_upper) / (1 - a * (z0 + z_alpha_upper)))\n","\n","    p_lower = max(min(p_lower, 0.9999), 0.0001)\n","    p_upper = max(min(p_upper, 0.9999), 0.0001)\n","\n","    lower = np.percentile(bootstrap_scores, p_lower * 100)\n","    upper = np.percentile(bootstrap_scores, p_upper * 100)\n","\n","    return original_score, lower, upper\n","\n","# ========== 3. Evaluate a single model for a single fold ==========\n","def evaluate_model(pred_file, model_name, fold_id, fold_tag):\n","    \"\"\"Evaluate a single model for a single fold.\"\"\"\n","    print(f\"\\n{'=' * 60}\")\n","    print(f\"Evaluate {model_name} (fold {fold_id})\")\n","    print(f\"{'=' * 60}\")\n","\n","    # Load predictions\n","    df_pred = pd.read_parquet(pred_file)\n","    print(f\"Loaded predictions: {len(df_pred)} windows\")\n","\n","    # Extract ground truth and predictions\n","    y_true = df_pred[\"true_label\"].values\n","    y_pred = df_pred[\"pred_label\"].values\n","\n","    # Unique classes\n","    unique_labels = sorted(set(y_true) | set(y_pred))\n","    n_classes = len(unique_labels)\n","    print(f\"#classes: {n_classes}\")\n","    print(f\"classes: {unique_labels}\")\n","\n","    # ========== 3.1 Basic metrics ==========\n","    print(\"\\nBasic metrics:\")\n","\n","    # Macro-F1\n","    macro_f1 = f1_score(\n","        y_true, y_pred,\n","        average=\"macro\", labels=unique_labels, zero_division=0\n","    )\n","    print(f\"  Macro-F1: {macro_f1:.4f}\")\n","\n","    # Balanced Accuracy\n","    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n","    print(f\"  Balanced Acc: {balanced_acc:.4f}\")\n","\n","    # Per-class F1\n","    per_class_f1 = f1_score(\n","        y_true, y_pred,\n","        average=None, labels=unique_labels, zero_division=0\n","    )\n","    print(\"\\n  Per-class F1:\")\n","    for label, f1_val in zip(unique_labels, per_class_f1):\n","        print(f\"    Class {label}: {f1_val:.4f}\")\n","\n","    # ========== 3.2 Bootstrap 95% CI ==========\n","    print(f\"\\nBootstrap 95% CI ({N_BOOTSTRAP} iterations):\")\n","\n","    macro_f1_func = lambda yt, yp: f1_score(\n","        yt, yp, average=\"macro\", labels=unique_labels, zero_division=0\n","    )\n","    macro_f1_mean, macro_f1_lower, macro_f1_upper = bootstrap_ci(\n","        y_true, y_pred, macro_f1_func, N_BOOTSTRAP\n","    )\n","    print(f\"  Macro-F1: {macro_f1_mean:.4f} [{macro_f1_lower:.4f}, {macro_f1_upper:.4f}]\")\n","\n","    bal_acc_func = lambda yt, yp: balanced_accuracy_score(yt, yp)\n","    bal_acc_mean, bal_acc_lower, bal_acc_upper = bootstrap_ci(\n","        y_true, y_pred, bal_acc_func, N_BOOTSTRAP\n","    )\n","    print(f\"  Balanced Acc: {bal_acc_mean:.4f} [{bal_acc_lower:.4f}, {bal_acc_upper:.4f}]\")\n","\n","    # ========== 3.3 Confusion matrix ==========\n","    cm = confusion_matrix(y_true, y_pred, labels=unique_labels)\n","\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(\n","        cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n","        xticklabels=unique_labels, yticklabels=unique_labels\n","    )\n","    plt.title(f\"{model_name} - Confusion Matrix (Fold {fold_id})\")\n","    plt.ylabel(\"True Label\")\n","    plt.xlabel(\"Predicted Label\")\n","    plt.tight_layout()\n","\n","    cm_file = metrics_dir / f\"confusion_{model_name.lower()}_{fold_tag}.png\"\n","    plt.savefig(cm_file, dpi=150, bbox_inches=\"tight\")\n","    plt.close()\n","    print(f\"\\n✓ Saved confusion matrix: {cm_file.name}\")\n","\n","    # ========== 3.4 Subject-level Macro-F1 ==========\n","    print(\"\\nSubject-level evaluation:\")\n","    subject_f1_scores = []\n","\n","    for subject_id in df_pred[\"subject_id\"].unique():\n","        mask = df_pred[\"subject_id\"] == subject_id\n","        y_true_subj = df_pred.loc[mask, \"true_label\"].values\n","        y_pred_subj = df_pred.loc[mask, \"pred_label\"].values\n","\n","        if len(y_true_subj) > 0:\n","            f1_subj = f1_score(\n","                y_true_subj, y_pred_subj,\n","                average=\"macro\", labels=unique_labels, zero_division=0\n","            )\n","            subject_f1_scores.append(f1_subj)\n","\n","    subject_macro_f1_mean = float(np.mean(subject_f1_scores)) if subject_f1_scores else 0.0\n","    subject_macro_f1_std = float(np.std(subject_f1_scores)) if subject_f1_scores else 0.0\n","    print(f\"  Subject-level Macro-F1: {subject_macro_f1_mean:.4f} ± {subject_macro_f1_std:.4f}\")\n","    print(f\"  #test subjects: {len(subject_f1_scores)}\")\n","\n","    # ========== 3.5 Aggregate results ==========\n","    results = {\n","        \"model\": model_name,\n","        \"fold_id\": fold_id,\n","        \"fold_tag\": fold_tag,\n","        \"n_windows\": int(len(df_pred)),\n","        \"n_classes\": n_classes,\n","        \"classes\": [int(c) for c in unique_labels],\n","        \"window_level\": {\n","            \"macro_f1\": {\n","                \"value\": float(macro_f1),\n","                \"ci_lower\": float(macro_f1_lower),\n","                \"ci_upper\": float(macro_f1_upper),\n","            },\n","            \"balanced_accuracy\": {\n","                \"value\": float(balanced_acc),\n","                \"ci_lower\": float(bal_acc_lower),\n","                \"ci_upper\": float(bal_acc_upper),\n","            },\n","            \"per_class_f1\": {\n","                int(label): float(f1_val)\n","                for label, f1_val in zip(unique_labels, per_class_f1)\n","            },\n","        },\n","        \"subject_level\": {\n","            \"macro_f1_mean\": subject_macro_f1_mean,\n","            \"macro_f1_std\": subject_macro_f1_std,\n","            \"n_subjects\": len(subject_f1_scores),\n","        },\n","        \"confusion_matrix\": cm.tolist(),\n","        \"bootstrap\": {\n","            \"n_iterations\": N_BOOTSTRAP,\n","            \"method\": \"BCa\",\n","            \"sampling\": \"window-level with replacement\",\n","        },\n","    }\n","\n","    return results\n","\n","# ========== 4. Evaluate all models for each fold ==========\n","all_results_all_folds = {}  # {fold_tag: {model_name: results}}\n","\n","for fold_id, fold_tag in outer_folds:\n","    print(f\"\\n{'=' * 60}\")\n","    print(f\"Evaluate models for fold {fold_id} ({fold_tag})\")\n","    print(f\"{'=' * 60}\")\n","\n","    pred_fold_dir = predictions_dir / fold_tag\n","\n","    models_to_eval = {\n","        \"RF\": pred_fold_dir / \"rf_predictions.parquet\",\n","        \"KNN\": pred_fold_dir / \"knn_predictions.parquet\",\n","        \"InceptionTime\": pred_fold_dir / \"inception_predictions.parquet\",\n","    }\n","\n","    fold_results = {}\n","    for model_name, pred_file in models_to_eval.items():\n","        if not pred_file.exists():\n","            print(f\"\\n⚠️ Skip {model_name} on {fold_tag}: file not found {pred_file}\")\n","            continue\n","\n","        results = evaluate_model(pred_file, model_name, fold_id, fold_tag)\n","        fold_results[model_name] = results\n","\n","        # Save per-model results\n","        model_metrics_file = metrics_dir / f\"{model_name.lower()}_{fold_tag}.json\"\n","        with open(model_metrics_file, \"w\") as f:\n","            json.dump(results, f, indent=2)\n","        print(f\"✓ Saved metrics: {model_metrics_file.name}\")\n","\n","    all_results_all_folds[fold_tag] = fold_results\n","\n","    # Save aggregated fold summary\n","    summary = {\n","        \"fold_id\": fold_id,\n","        \"fold_tag\": fold_tag,\n","        \"models\": list(fold_results.keys()),\n","        \"results\": fold_results,\n","        \"summary_table\": {},\n","    }\n","\n","    for model_name, results in fold_results.items():\n","        wl = results[\"window_level\"]\n","        sl = results[\"subject_level\"]\n","        summary[\"summary_table\"][model_name] = {\n","            \"window_macro_f1\": (\n","                f\"{wl['macro_f1']['value']:.4f} \"\n","                f\"[{wl['macro_f1']['ci_lower']:.4f}, {wl['macro_f1']['ci_upper']:.4f}]\"\n","            ),\n","            \"window_balanced_acc\": (\n","                f\"{wl['balanced_accuracy']['value']:.4f} \"\n","                f\"[{wl['balanced_accuracy']['ci_lower']:.4f}, {wl['balanced_accuracy']['ci_upper']:.4f}]\"\n","            ),\n","            \"subject_macro_f1\": (\n","                f\"{sl['macro_f1_mean']:.4f} ± {sl['macro_f1_std']:.4f}\"\n","            ),\n","        }\n","\n","    summary_file = metrics_dir / f\"summary_{fold_tag}.json\"\n","    with open(summary_file, \"w\") as f:\n","        json.dump(summary, f, indent=2)\n","    print(f\"✓ Saved summary: {summary_file.name}\")\n","\n","# ========== 5. Cross-fold aggregation ==========\n","print(f\"\\n{'=' * 60}\")\n","print(\"Cross-fold aggregation\")\n","print(f\"{'=' * 60}\")\n","\n","# Collect all fold results\n","all_folds_results = defaultdict(lambda: defaultdict(list))\n","\n","for fold_file in sorted(metrics_dir.glob(\"summary_fold_*.json\")):\n","    with open(fold_file, \"r\") as f:\n","        fold_data = json.load(f)\n","\n","    for model_name, model_results in fold_data[\"results\"].items():\n","        wl = model_results[\"window_level\"]\n","        sl = model_results[\"subject_level\"]\n","\n","        all_folds_results[model_name][\"window_macro_f1\"].append(\n","            wl[\"macro_f1\"][\"value\"]\n","        )\n","        all_folds_results[model_name][\"balanced_acc\"].append(\n","            wl[\"balanced_accuracy\"][\"value\"]\n","        )\n","        all_folds_results[model_name][\"subject_macro_f1\"].append(\n","            sl[\"macro_f1_mean\"]\n","        )\n","\n","if all_folds_results:\n","    print(\n","        f\"\\nCross-fold summary \"\n","        f\"({len(list(metrics_dir.glob('summary_fold_*.json')))} folds):\"\n","    )\n","    print(\n","        f\"\\n{'Model':<15} {'Window Macro-F1 [95% CI]':<35} \"\n","        f\"{'Subject Macro-F1 [95% CI]':<35} {'Balanced Acc [95% CI]':<35}\"\n","    )\n","    print(\"-\" * 120)\n","\n","    cross_fold_summary = {}\n","    for model_name, metrics in all_folds_results.items():\n","        w_mean, w_lo, w_hi = mean_ci_t(metrics[\"window_macro_f1\"])\n","        s_mean, s_lo, s_hi = mean_ci_t(metrics[\"subject_macro_f1\"])\n","        b_mean, b_lo, b_hi = mean_ci_t(metrics[\"balanced_acc\"])\n","\n","        w_str = (\n","            f\"{w_mean:.4f} [{w_lo:.4f}, {w_hi:.4f}]\"\n","            if w_lo is not None else f\"{w_mean:.4f}\"\n","        )\n","        s_str = (\n","            f\"{s_mean:.4f} [{s_lo:.4f}, {s_hi:.4f}]\"\n","            if s_lo is not None else f\"{s_mean:.4f}\"\n","        )\n","        b_str = (\n","            f\"{b_mean:.4f} [{b_lo:.4f}, {b_hi:.4f}]\"\n","            if b_lo is not None else f\"{b_mean:.4f}\"\n","        )\n","\n","        print(f\"{model_name:<15} {w_str:<35} {s_str:<35} {b_str:<35}\")\n","\n","        cross_fold_summary[model_name] = {\n","            \"window_macro_f1\": {\n","                \"mean\": w_mean,\n","                \"ci_lower\": w_lo,\n","                \"ci_upper\": w_hi,\n","            },\n","            \"subject_macro_f1\": {\n","                \"mean\": s_mean,\n","                \"ci_lower\": s_lo,\n","                \"ci_upper\": s_hi,\n","            },\n","            \"balanced_acc\": {\n","                \"mean\": b_mean,\n","                \"ci_lower\": b_lo,\n","                \"ci_upper\": b_hi,\n","            },\n","            \"n_folds\": len(metrics[\"window_macro_f1\"]),\n","            \"ci_method\": \"t-interval (fold-level)\",\n","        }\n","\n","    cross_fold_file = metrics_dir / \"cross_fold_summary.json\"\n","    with open(cross_fold_file, \"w\") as f:\n","        json.dump(cross_fold_summary, f, indent=2)\n","    print(f\"\\n✓ Saved cross-fold summary: {cross_fold_file.name}\")\n","else:\n","    print(\"\\nOnly single-fold results available; run prediction for more folds to produce cross-fold summary.\")\n","\n","# ========== 6. Final summary ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"Step 16 complete — Evaluation for all folds\")\n","print(\"=\" * 60 + \"\\n\")\n","\n","print(f\"Evaluated folds: {[tag for _, tag in outer_folds]}\")\n","print(f\"Bootstrap: {N_BOOTSTRAP} iterations (BCa)\")\n","\n","print(\"\\nOutput files:\")\n","print(f\"  {metrics_dir}/\")\n","for _, fold_tag in outer_folds:\n","    for model_name in [\"RF\", \"KNN\", \"InceptionTime\"]:\n","        per_model_file = metrics_dir / f\"{model_name.lower()}_{fold_tag}.json\"\n","        cm_file = metrics_dir / f\"confusion_{model_name.lower()}_{fold_tag}.png\"\n","        if per_model_file.exists():\n","            print(f\"    - {per_model_file.name}\")\n","        if cm_file.exists():\n","            print(f\"    - {cm_file.name}\")\n","    summary_file = metrics_dir / f\"summary_{fold_tag}.json\"\n","    if summary_file.exists():\n","        print(f\"    - {summary_file.name}\")\n","if (metrics_dir / \"cross_fold_summary.json\").exists():\n","    print(f\"    - cross_fold_summary.json\")\n","\n","print(\"\\nEvaluation metrics:\")\n","print(\"  ✓ Window-level Macro-F1 (95% CI - BCa bootstrap)\")\n","print(\"  ✓ Balanced Accuracy (95% CI - BCa bootstrap)\")\n","print(\"  ✓ Per-class F1\")\n","print(\"  ✓ Subject-level Macro-F1 (mean ± std)\")\n","print(\"  ✓ Confusion Matrix\")\n","print(\"  ✓ Cross-fold means (95% CI - t-distribution)\")\n","\n","print(\"\\nNext steps:\")\n","print(\"  - Use cross_fold_summary.json in your report/tables\")\n","print(\"  - Generate paper-ready figures (ROC/PR curves, etc.)\")\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e21WIoH9BLqD","executionInfo":{"status":"ok","timestamp":1763151617720,"user_tz":0,"elapsed":122181,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"f28a3776-9bb7-40dd-d134-2acaf9aa0b61"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 16: Evaluation (Primary: Macro-F1, multi-fold)\n","============================================================\n","\n","Detected folds from predictions/: ['fold_00', 'fold_01', 'fold_02', 'fold_03', 'fold_04', 'fold_05', 'fold_06', 'fold_07']\n","Bootstrap: 1000 iterations, window-level sampling\n","\n","============================================================\n","Evaluate models for fold 0 (fold_00)\n","============================================================\n","\n","============================================================\n","Evaluate RF (fold 0)\n","============================================================\n","Loaded predictions: 766 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.4154\n","  Balanced Acc: 0.4269\n","\n","  Per-class F1:\n","    Class 1: 0.0588\n","    Class 2: 0.8393\n","    Class 3: 0.0000\n","    Class 4: 0.7742\n","    Class 5: 0.7223\n","    Class 6: 0.0976\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.4154 [0.3790, 0.4506]\n","  Balanced Acc: 0.4269 [0.3853, 0.4621]\n","\n","✓ Saved confusion matrix: confusion_rf_fold_00.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.4154 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: rf_fold_00.json\n","\n","============================================================\n","Evaluate KNN (fold 0)\n","============================================================\n","Loaded predictions: 766 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.4628\n","  Balanced Acc: 0.4567\n","\n","  Per-class F1:\n","    Class 1: 0.1167\n","    Class 2: 0.8205\n","    Class 3: 0.0857\n","    Class 4: 0.8485\n","    Class 5: 0.6883\n","    Class 6: 0.2169\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.4628 [0.4240, 0.4996]\n","  Balanced Acc: 0.4567 [0.4087, 0.4907]\n","\n","✓ Saved confusion matrix: confusion_knn_fold_00.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.4628 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: knn_fold_00.json\n","\n","============================================================\n","Evaluate InceptionTime (fold 0)\n","============================================================\n","Loaded predictions: 766 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.5822\n","  Balanced Acc: 0.5686\n","\n","  Per-class F1:\n","    Class 1: 0.4324\n","    Class 2: 0.8424\n","    Class 3: 0.1538\n","    Class 4: 0.9189\n","    Class 5: 0.7188\n","    Class 6: 0.4268\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.5822 [0.5461, 0.6176]\n","  Balanced Acc: 0.5686 [0.5262, 0.6031]\n","\n","✓ Saved confusion matrix: confusion_inceptiontime_fold_00.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.5822 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: inceptiontime_fold_00.json\n","✓ Saved summary: summary_fold_00.json\n","\n","============================================================\n","Evaluate models for fold 1 (fold_01)\n","============================================================\n","\n","============================================================\n","Evaluate RF (fold 1)\n","============================================================\n","Loaded predictions: 659 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.3355\n","  Balanced Acc: 0.3325\n","\n","  Per-class F1:\n","    Class 1: 0.4588\n","    Class 2: 0.7778\n","    Class 3: 0.0000\n","    Class 4: 0.0000\n","    Class 5: 0.7221\n","    Class 6: 0.0541\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.3355 [0.3128, 0.3671]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n"]},{"output_type":"stream","name":"stdout","text":["  Balanced Acc: 0.3325 [0.2936, 0.3962]\n","\n","✓ Saved confusion matrix: confusion_rf_fold_01.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.3355 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: rf_fold_01.json\n","\n","============================================================\n","Evaluate KNN (fold 1)\n","============================================================\n","Loaded predictions: 659 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.3615\n","  Balanced Acc: 0.3534\n","\n","  Per-class F1:\n","    Class 1: 0.4873\n","    Class 2: 0.7725\n","    Class 3: 0.0877\n","    Class 4: 0.0000\n","    Class 5: 0.6834\n","    Class 6: 0.1379\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.3615 [0.3293, 0.3976]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n"]},{"output_type":"stream","name":"stdout","text":["  Balanced Acc: 0.3534 [0.3044, 0.4214]\n","\n","✓ Saved confusion matrix: confusion_knn_fold_01.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.3615 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: knn_fold_01.json\n","\n","============================================================\n","Evaluate InceptionTime (fold 1)\n","============================================================\n","Loaded predictions: 659 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.3816\n","  Balanced Acc: 0.3791\n","\n","  Per-class F1:\n","    Class 1: 0.4027\n","    Class 2: 0.7717\n","    Class 3: 0.0396\n","    Class 4: 0.0000\n","    Class 5: 0.7374\n","    Class 6: 0.3380\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.3816 [0.3503, 0.4217]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n"]},{"output_type":"stream","name":"stdout","text":["  Balanced Acc: 0.3791 [0.3310, 0.4482]\n","\n","✓ Saved confusion matrix: confusion_inceptiontime_fold_01.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.3816 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: inceptiontime_fold_01.json\n","✓ Saved summary: summary_fold_01.json\n","\n","============================================================\n","Evaluate models for fold 2 (fold_02)\n","============================================================\n","\n","============================================================\n","Evaluate RF (fold 2)\n","============================================================\n","Loaded predictions: 771 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.4687\n","  Balanced Acc: 0.5216\n","\n","  Per-class F1:\n","    Class 1: 0.4976\n","    Class 2: 0.7651\n","    Class 3: 0.0000\n","    Class 4: 0.6667\n","    Class 5: 0.8137\n","    Class 6: 0.0690\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.4687 [0.4122, 0.5187]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n"]},{"output_type":"stream","name":"stdout","text":["  Balanced Acc: 0.5216 [0.5000, 0.5513]\n","\n","✓ Saved confusion matrix: confusion_rf_fold_02.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.4687 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: rf_fold_02.json\n","\n","============================================================\n","Evaluate KNN (fold 2)\n","============================================================\n","Loaded predictions: 771 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.4824\n","  Balanced Acc: 0.5243\n","\n","  Per-class F1:\n","    Class 1: 0.4465\n","    Class 2: 0.7205\n","    Class 3: 0.1263\n","    Class 4: 0.7500\n","    Class 5: 0.7624\n","    Class 6: 0.0889\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.4824 [0.4254, 0.5311]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n"]},{"output_type":"stream","name":"stdout","text":["  Balanced Acc: 0.5243 [0.4951, 0.5595]\n","\n","✓ Saved confusion matrix: confusion_knn_fold_02.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.4824 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: knn_fold_02.json\n","\n","============================================================\n","Evaluate InceptionTime (fold 2)\n","============================================================\n","Loaded predictions: 771 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.5265\n","  Balanced Acc: 0.5860\n","\n","  Per-class F1:\n","    Class 1: 0.5200\n","    Class 2: 0.7179\n","    Class 3: 0.1067\n","    Class 4: 0.8571\n","    Class 5: 0.6785\n","    Class 6: 0.2785\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.5265 [0.4679, 0.5676]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n","  warnings.warn(\"y_pred contains classes not in y_true\")\n"]},{"output_type":"stream","name":"stdout","text":["  Balanced Acc: 0.5860 [0.5436, 0.6288]\n","\n","✓ Saved confusion matrix: confusion_inceptiontime_fold_02.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.5265 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: inceptiontime_fold_02.json\n","✓ Saved summary: summary_fold_02.json\n","\n","============================================================\n","Evaluate models for fold 3 (fold_03)\n","============================================================\n","\n","============================================================\n","Evaluate RF (fold 3)\n","============================================================\n","Loaded predictions: 873 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.4685\n","  Balanced Acc: 0.4769\n","\n","  Per-class F1:\n","    Class 1: 0.4526\n","    Class 2: 0.8256\n","    Class 3: 0.0000\n","    Class 4: 0.8421\n","    Class 5: 0.6907\n","    Class 6: 0.0000\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.4685 [0.4199, 0.4983]\n","  Balanced Acc: 0.4769 [0.4078, 0.5156]\n","\n","✓ Saved confusion matrix: confusion_rf_fold_03.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.4685 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: rf_fold_03.json\n","\n","============================================================\n","Evaluate KNN (fold 3)\n","============================================================\n","Loaded predictions: 873 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.4852\n","  Balanced Acc: 0.5082\n","\n","  Per-class F1:\n","    Class 1: 0.3968\n","    Class 2: 0.8176\n","    Class 3: 0.0317\n","    Class 4: 0.9524\n","    Class 5: 0.6608\n","    Class 6: 0.0519\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.4852 [0.4562, 0.5111]\n","  Balanced Acc: 0.5082 [0.4495, 0.5336]\n","\n","✓ Saved confusion matrix: confusion_knn_fold_03.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.4852 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: knn_fold_03.json\n","\n","============================================================\n","Evaluate InceptionTime (fold 3)\n","============================================================\n","Loaded predictions: 873 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.5692\n","  Balanced Acc: 0.5791\n","\n","  Per-class F1:\n","    Class 1: 0.3351\n","    Class 2: 0.8201\n","    Class 3: 0.1798\n","    Class 4: 1.0000\n","    Class 5: 0.6425\n","    Class 6: 0.4375\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.5692 [0.5428, 0.5985]\n","  Balanced Acc: 0.5791 [0.5484, 0.6106]\n","\n","✓ Saved confusion matrix: confusion_inceptiontime_fold_03.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.5692 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: inceptiontime_fold_03.json\n","✓ Saved summary: summary_fold_03.json\n","\n","============================================================\n","Evaluate models for fold 4 (fold_04)\n","============================================================\n","\n","============================================================\n","Evaluate RF (fold 4)\n","============================================================\n","Loaded predictions: 746 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.5514\n","  Balanced Acc: 0.5534\n","\n","  Per-class F1:\n","    Class 1: 0.6780\n","    Class 2: 0.9322\n","    Class 3: 0.0000\n","    Class 4: 0.9167\n","    Class 5: 0.7471\n","    Class 6: 0.0345\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.5514 [0.5198, 0.5783]\n","  Balanced Acc: 0.5534 [0.5016, 0.5817]\n","\n","✓ Saved confusion matrix: confusion_rf_fold_04.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.5514 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: rf_fold_04.json\n","\n","============================================================\n","Evaluate KNN (fold 4)\n","============================================================\n","Loaded predictions: 746 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.5330\n","  Balanced Acc: 0.5370\n","\n","  Per-class F1:\n","    Class 1: 0.5560\n","    Class 2: 0.8933\n","    Class 3: 0.1053\n","    Class 4: 0.9167\n","    Class 5: 0.6954\n","    Class 6: 0.0317\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.5330 [0.4981, 0.5641]\n","  Balanced Acc: 0.5370 [0.4867, 0.5693]\n","\n","✓ Saved confusion matrix: confusion_knn_fold_04.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.5330 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: knn_fold_04.json\n","\n","============================================================\n","Evaluate InceptionTime (fold 4)\n","============================================================\n","Loaded predictions: 746 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.5936\n","  Balanced Acc: 0.5816\n","\n","  Per-class F1:\n","    Class 1: 0.6423\n","    Class 2: 0.8864\n","    Class 3: 0.0706\n","    Class 4: 0.9600\n","    Class 5: 0.7009\n","    Class 6: 0.3014\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.5936 [0.5600, 0.6341]\n","  Balanced Acc: 0.5816 [0.5411, 0.6132]\n","\n","✓ Saved confusion matrix: confusion_inceptiontime_fold_04.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.5936 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: inceptiontime_fold_04.json\n","✓ Saved summary: summary_fold_04.json\n","\n","============================================================\n","Evaluate models for fold 5 (fold_05)\n","============================================================\n","\n","============================================================\n","Evaluate RF (fold 5)\n","============================================================\n","Loaded predictions: 289 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.2966\n","  Balanced Acc: 0.3056\n","\n","  Per-class F1:\n","    Class 1: 0.1782\n","    Class 2: 0.5814\n","    Class 3: 0.0000\n","    Class 4: 0.4615\n","    Class 5: 0.5586\n","    Class 6: 0.0000\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.2966 [0.2311, 0.3643]\n","  Balanced Acc: 0.3056 [0.2511, 0.3726]\n","\n","✓ Saved confusion matrix: confusion_rf_fold_05.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.2966 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: rf_fold_05.json\n","\n","============================================================\n","Evaluate KNN (fold 5)\n","============================================================\n","Loaded predictions: 289 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.3207\n","  Balanced Acc: 0.3130\n","\n","  Per-class F1:\n","    Class 1: 0.2407\n","    Class 2: 0.5870\n","    Class 3: 0.0000\n","    Class 4: 0.3333\n","    Class 5: 0.5490\n","    Class 6: 0.2143\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.3207 [0.2581, 0.4174]\n","  Balanced Acc: 0.3130 [0.2582, 0.3907]\n","\n","✓ Saved confusion matrix: confusion_knn_fold_05.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.3207 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: knn_fold_05.json\n","\n","============================================================\n","Evaluate InceptionTime (fold 5)\n","============================================================\n","Loaded predictions: 289 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.3661\n","  Balanced Acc: 0.3376\n","\n","  Per-class F1:\n","    Class 1: 0.3942\n","    Class 2: 0.6207\n","    Class 3: 0.0615\n","    Class 4: 0.3333\n","    Class 5: 0.5167\n","    Class 6: 0.2703\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.3661 [0.2999, 0.4532]\n","  Balanced Acc: 0.3376 [0.2795, 0.4188]\n","\n","✓ Saved confusion matrix: confusion_inceptiontime_fold_05.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.3661 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: inceptiontime_fold_05.json\n","✓ Saved summary: summary_fold_05.json\n","\n","============================================================\n","Evaluate models for fold 6 (fold_06)\n","============================================================\n","\n","============================================================\n","Evaluate RF (fold 6)\n","============================================================\n","Loaded predictions: 878 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.3959\n","  Balanced Acc: 0.3866\n","\n","  Per-class F1:\n","    Class 1: 0.5246\n","    Class 2: 0.8718\n","    Class 3: 0.0408\n","    Class 4: 0.0000\n","    Class 5: 0.7783\n","    Class 6: 0.1600\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.3959 [0.3687, 0.4331]\n","  Balanced Acc: 0.3866 [0.3634, 0.4714]\n","\n","✓ Saved confusion matrix: confusion_rf_fold_06.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.3959 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: rf_fold_06.json\n","\n","============================================================\n","Evaluate KNN (fold 6)\n","============================================================\n","Loaded predictions: 878 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.5125\n","  Balanced Acc: 0.4867\n","\n","  Per-class F1:\n","    Class 1: 0.6161\n","    Class 2: 0.8812\n","    Class 3: 0.0725\n","    Class 4: 0.6667\n","    Class 5: 0.7123\n","    Class 6: 0.1261\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.5125 [0.3936, 0.5986]\n","  Balanced Acc: 0.4867 [0.3857, 0.5838]\n","\n","✓ Saved confusion matrix: confusion_knn_fold_06.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.5125 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: knn_fold_06.json\n","\n","============================================================\n","Evaluate InceptionTime (fold 6)\n","============================================================\n","Loaded predictions: 878 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.5968\n","  Balanced Acc: 0.6111\n","\n","  Per-class F1:\n","    Class 1: 0.6063\n","    Class 2: 0.8557\n","    Class 3: 0.0946\n","    Class 4: 1.0000\n","    Class 5: 0.7096\n","    Class 6: 0.3148\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.5968 [0.4287, 0.6321]\n","  Balanced Acc: 0.6111 [0.5253, 0.6482]\n","\n","✓ Saved confusion matrix: confusion_inceptiontime_fold_06.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.5968 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: inceptiontime_fold_06.json\n","✓ Saved summary: summary_fold_06.json\n","\n","============================================================\n","Evaluate models for fold 7 (fold_07)\n","============================================================\n","\n","============================================================\n","Evaluate RF (fold 7)\n","============================================================\n","Loaded predictions: 749 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.4720\n","  Balanced Acc: 0.4974\n","\n","  Per-class F1:\n","    Class 1: 0.4094\n","    Class 2: 0.7376\n","    Class 3: 0.0000\n","    Class 4: 0.8889\n","    Class 5: 0.7424\n","    Class 6: 0.0541\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.4720 [0.3403, 0.5038]\n","  Balanced Acc: 0.4974 [0.3680, 0.5368]\n","\n","✓ Saved confusion matrix: confusion_rf_fold_07.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.4720 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: rf_fold_07.json\n","\n","============================================================\n","Evaluate KNN (fold 7)\n","============================================================\n","Loaded predictions: 749 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.4268\n","  Balanced Acc: 0.4365\n","\n","  Per-class F1:\n","    Class 1: 0.3493\n","    Class 2: 0.6983\n","    Class 3: 0.0357\n","    Class 4: 0.7500\n","    Class 5: 0.6336\n","    Class 6: 0.0938\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.4268 [0.3081, 0.4813]\n","  Balanced Acc: 0.4365 [0.3341, 0.5062]\n","\n","✓ Saved confusion matrix: confusion_knn_fold_07.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.4268 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: knn_fold_07.json\n","\n","============================================================\n","Evaluate InceptionTime (fold 7)\n","============================================================\n","Loaded predictions: 749 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.5433\n","  Balanced Acc: 0.5457\n","\n","  Per-class F1:\n","    Class 1: 0.5978\n","    Class 2: 0.8031\n","    Class 3: 0.1126\n","    Class 4: 0.8889\n","    Class 5: 0.6003\n","    Class 6: 0.2571\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.5433 [0.3978, 0.5791]\n","  Balanced Acc: 0.5457 [0.4047, 0.5927]\n","\n","✓ Saved confusion matrix: confusion_inceptiontime_fold_07.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.5433 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: inceptiontime_fold_07.json\n","✓ Saved summary: summary_fold_07.json\n","\n","============================================================\n","Cross-fold aggregation\n","============================================================\n","\n","Cross-fold summary (8 folds):\n","\n","Model           Window Macro-F1 [95% CI]            Subject Macro-F1 [95% CI]           Balanced Acc [95% CI]              \n","------------------------------------------------------------------------------------------------------------------------\n","RF              0.4255 [0.3567, 0.4943]             0.4255 [0.3567, 0.4943]             0.4376 [0.3623, 0.5129]            \n","KNN             0.4481 [0.3863, 0.5100]             0.4481 [0.3863, 0.5100]             0.4520 [0.3841, 0.5198]            \n","InceptionTime   0.5199 [0.4418, 0.5980]             0.5199 [0.4418, 0.5980]             0.5236 [0.4365, 0.6107]            \n","\n","✓ Saved cross-fold summary: cross_fold_summary.json\n","\n","============================================================\n","Step 16 complete — Evaluation for all folds\n","============================================================\n","\n","Evaluated folds: ['fold_00', 'fold_01', 'fold_02', 'fold_03', 'fold_04', 'fold_05', 'fold_06', 'fold_07']\n","Bootstrap: 1000 iterations (BCa)\n","\n","Output files:\n","  metrics/\n","    - rf_fold_00.json\n","    - confusion_rf_fold_00.png\n","    - knn_fold_00.json\n","    - confusion_knn_fold_00.png\n","    - inceptiontime_fold_00.json\n","    - confusion_inceptiontime_fold_00.png\n","    - summary_fold_00.json\n","    - rf_fold_01.json\n","    - confusion_rf_fold_01.png\n","    - knn_fold_01.json\n","    - confusion_knn_fold_01.png\n","    - inceptiontime_fold_01.json\n","    - confusion_inceptiontime_fold_01.png\n","    - summary_fold_01.json\n","    - rf_fold_02.json\n","    - confusion_rf_fold_02.png\n","    - knn_fold_02.json\n","    - confusion_knn_fold_02.png\n","    - inceptiontime_fold_02.json\n","    - confusion_inceptiontime_fold_02.png\n","    - summary_fold_02.json\n","    - rf_fold_03.json\n","    - confusion_rf_fold_03.png\n","    - knn_fold_03.json\n","    - confusion_knn_fold_03.png\n","    - inceptiontime_fold_03.json\n","    - confusion_inceptiontime_fold_03.png\n","    - summary_fold_03.json\n","    - rf_fold_04.json\n","    - confusion_rf_fold_04.png\n","    - knn_fold_04.json\n","    - confusion_knn_fold_04.png\n","    - inceptiontime_fold_04.json\n","    - confusion_inceptiontime_fold_04.png\n","    - summary_fold_04.json\n","    - rf_fold_05.json\n","    - confusion_rf_fold_05.png\n","    - knn_fold_05.json\n","    - confusion_knn_fold_05.png\n","    - inceptiontime_fold_05.json\n","    - confusion_inceptiontime_fold_05.png\n","    - summary_fold_05.json\n","    - rf_fold_06.json\n","    - confusion_rf_fold_06.png\n","    - knn_fold_06.json\n","    - confusion_knn_fold_06.png\n","    - inceptiontime_fold_06.json\n","    - confusion_inceptiontime_fold_06.png\n","    - summary_fold_06.json\n","    - rf_fold_07.json\n","    - confusion_rf_fold_07.png\n","    - knn_fold_07.json\n","    - confusion_knn_fold_07.png\n","    - inceptiontime_fold_07.json\n","    - confusion_inceptiontime_fold_07.png\n","    - summary_fold_07.json\n","    - cross_fold_summary.json\n","\n","Evaluation metrics:\n","  ✓ Window-level Macro-F1 (95% CI - BCa bootstrap)\n","  ✓ Balanced Accuracy (95% CI - BCa bootstrap)\n","  ✓ Per-class F1\n","  ✓ Subject-level Macro-F1 (mean ± std)\n","  ✓ Confusion Matrix\n","  ✓ Cross-fold means (95% CI - t-distribution)\n","\n","Next steps:\n","  - Use cross_fold_summary.json in your report/tables\n","  - Generate paper-ready figures (ROC/PR curves, etc.)\n","============================================================\n"]}]}]}