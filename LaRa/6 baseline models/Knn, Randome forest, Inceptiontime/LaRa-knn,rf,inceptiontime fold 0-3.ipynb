{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyNErQRPDIH9rL951ODDf/gz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SwsPBAUflPLF","executionInfo":{"status":"ok","timestamp":1763111216942,"user_tz":0,"elapsed":6683,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"ba77ba22-9ece-4c6f-e535-1038a93be2db"},"outputs":[{"output_type":"stream","name":"stdout","text":["⚠️ Note: In Jupyter/Colab, PYTHONHASHSEED must be set before the kernel starts\n","   Suggestion: After setting environment variables, restart the runtime, then run the main code\n","\n","Configured random seeds: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","Generating SEEDS.yaml...\n","Generating requirements.txt...\n","Generating env.txt...\n","Generating environment.yml...\n","Collecting hardware information...\n","Collecting Git information...\n","Saving PyTorch build information...\n","Generating data checksums...\n","  data/ directory does not exist; skipping checksums\n","Computing environment hashes...\n","\n","============================================================\n","Step 0 complete - Reproducible environment configuration (top-conf/journal grade)\n","============================================================\n","Output directory: artifacts/env/\n","  ✓ SEEDS.yaml (seeds: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","  ✓ requirements.txt\n","  ✓ env.txt (with system summary)\n","  ✓ environment.yml\n","  ✓ hardware_log.json\n","  ✓ git_info.json (dirty=False)\n","  ✓ torch_build.txt\n","  ✓ ENV.SHA256 (covers all key files)\n","\n","Strict determinism configuration:\n","  - torch.use_deterministic_algorithms: True (warn_only=False)\n","  - cudnn.deterministic: True\n","  - cudnn.benchmark: False\n","  - TF32 disabled: False\n","  - Environment variables set:\n","    PYTHONHASHSEED: 0\n","    CUBLAS_WORKSPACE_CONFIG: :4096:8\n","    Thread control: OMP/MKL/OPENBLAS/NUMEXPR=1\n","============================================================\n"]}],"source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 0: Reproducible Environment (Colab/Jupyter adapted - top-conf/journal grade)\n","Generate a complete reproducible environment configuration\n","\"\"\"\n","\n","# ===== Set environment variables directly (Colab/Jupyter env) =====\n","import os\n","import sys\n","\n","os.environ[\"PYTHONHASHSEED\"] = \"0\"\n","os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n","os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n","os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n","os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n","os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n","\n","print(\"⚠️ Note: In Jupyter/Colab, PYTHONHASHSEED must be set before the kernel starts\")\n","print(\"   Suggestion: After setting environment variables, restart the runtime, then run the main code\\n\")\n","\n","# ===== Environment variables set; continue normal flow =====\n","import json\n","import hashlib\n","import subprocess\n","from pathlib import Path\n","from datetime import datetime, timezone\n","from contextlib import redirect_stdout\n","import io\n","\n","# Check Python version\n","assert sys.version_info >= (3, 10), f\"Require Python ≥ 3.10, current: {sys.version}\"\n","\n","# Create output directory\n","output_dir = Path(\"artifacts/env\")\n","output_dir.mkdir(parents=True, exist_ok=True)\n","\n","# 1. Multiple random seeds (0–9)\n","SEEDS = list(range(10))\n","print(f\"Configured random seeds: {SEEDS}\")\n","\n","# Import and configure\n","import random\n","import numpy as np\n","import torch\n","\n","# Initialize with the first seed\n","random.seed(SEEDS[0])\n","np.random.seed(SEEDS[0])\n","torch.manual_seed(SEEDS[0])\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEEDS[0])\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # Disable TF32\n","    torch.backends.cuda.matmul.allow_tf32 = False\n","    torch.backends.cudnn.allow_tf32 = False\n","\n","# Enable strict deterministic algorithms (not using warn_only)\n","torch.use_deterministic_algorithms(True)\n","\n","# Set matmul precision\n","if hasattr(torch, 'set_float32_matmul_precision'):\n","    torch.set_float32_matmul_precision(\"high\")\n","\n","# 2. Generate SEEDS.yaml (with fallback)\n","print(\"Generating SEEDS.yaml...\")\n","seeds_config = {\n","    \"seeds\": SEEDS,\n","    \"default_seed\": SEEDS[0],\n","    \"description\": \"Random seeds for python, numpy, torch, sklearn\"\n","}\n","try:\n","    import yaml\n","    with open(output_dir / \"SEEDS.yaml\", \"w\") as f:\n","        yaml.dump(seeds_config, f, default_flow_style=False)\n","except ImportError:\n","    # Fallback if PyYAML is not installed\n","    yaml_content = f\"\"\"seeds: {SEEDS}\n","default_seed: {SEEDS[0]}\n","description: Random seeds for python, numpy, torch, sklearn\n","\"\"\"\n","    with open(output_dir / \"SEEDS.yaml\", \"w\") as f:\n","        f.write(yaml_content)\n","\n","# 3. Generate requirements.txt (frozen versions)\n","print(\"Generating requirements.txt...\")\n","result = subprocess.run(\n","    [sys.executable, \"-m\", \"pip\", \"freeze\"],\n","    capture_output=True, text=True\n",")\n","requirements = result.stdout\n","with open(output_dir / \"requirements.txt\", \"w\") as f:\n","    f.write(requirements)\n","\n","# 4. Collect system info (for env.txt header)\n","import platform\n","system_info = []\n","system_info.append(\"=\"*60)\n","system_info.append(\"Environment Snapshot - System Overview\")\n","system_info.append(\"=\"*60)\n","system_info.append(f\"Time (UTC): {datetime.now(timezone.utc).isoformat()}\")\n","system_info.append(f\"Python: {sys.version}\")\n","system_info.append(f\"Platform: {platform.system()} {platform.release()} ({platform.machine()})\")\n","\n","try:\n","    import psutil\n","    system_info.append(f\"CPU: {psutil.cpu_count(logical=False)} cores / {psutil.cpu_count(logical=True)} threads\")\n","    system_info.append(f\"Memory: {round(psutil.virtual_memory().total / (1024**3), 2)} GB\")\n","except ImportError:\n","    pass\n","\n","system_info.append(f\"PyTorch: {torch.__version__}\")\n","if torch.cuda.is_available():\n","    system_info.append(f\"CUDA: {torch.version.cuda}\")\n","    system_info.append(f\"cuDNN: {torch.backends.cudnn.version()}\")\n","    try:\n","        out = subprocess.run(\n","            [\"nvidia-smi\", \"--query-gpu=driver_version\", \"--format=csv,noheader\"],\n","            capture_output=True, text=True\n","        )\n","        if out.returncode == 0 and out.stdout.strip():\n","            system_info.append(f\"NVIDIA driver: {out.stdout.strip().splitlines()[0]}\")\n","    except:\n","        pass\n","\n","system_info.append(\"\\nEnvironment variables:\")\n","for key in [\"PYTHONHASHSEED\", \"CUBLAS_WORKSPACE_CONFIG\", \"OMP_NUM_THREADS\",\n","            \"MKL_NUM_THREADS\", \"OPENBLAS_NUM_THREADS\", \"NUMEXPR_NUM_THREADS\"]:\n","    system_info.append(f\"  {key}={os.environ.get(key, 'N/A')}\")\n","\n","system_info.append(\"\\n\" + \"=\"*60)\n","system_info.append(\"Installed packages list\")\n","system_info.append(\"=\"*60 + \"\\n\")\n","\n","# 5. Generate env.txt (human-readable + system summary)\n","print(\"Generating env.txt...\")\n","result = subprocess.run(\n","    [sys.executable, \"-m\", \"pip\", \"list\"],\n","    capture_output=True, text=True\n",")\n","with open(output_dir / \"env.txt\", \"w\") as f:\n","    f.write(\"\\n\".join(system_info))\n","    f.write(result.stdout)\n","\n","# 6. Generate environment.yml\n","print(\"Generating environment.yml...\")\n","env_yml = f\"\"\"name: har_lara\n","channels:\n","  - defaults\n","  - conda-forge\n","dependencies:\n","  - python={sys.version_info.major}.{sys.version_info.minor}\n","  - pip\n","  - pip:\n","\"\"\"\n","for line in requirements.strip().split(\"\\n\"):\n","    if line and not line.startswith(\"#\"):\n","        env_yml += f\"      - {line}\\n\"\n","\n","with open(output_dir / \"environment.yml\", \"w\") as f:\n","    f.write(env_yml)\n","\n","# 7. Collect complete hardware information\n","print(\"Collecting hardware information...\")\n","hardware_info = {\n","    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n","    \"python_version\": sys.version,\n","    \"python_executable\": sys.executable,\n","    \"platform\": sys.platform,\n","    \"os\": platform.system(),\n","    \"os_release\": platform.release(),\n","    \"os_version\": platform.version(),\n","    \"machine\": platform.machine(),\n","    \"processor\": platform.processor(),\n","}\n","\n","try:\n","    import psutil\n","    hardware_info[\"cpu_count_physical\"] = psutil.cpu_count(logical=False)\n","    hardware_info[\"cpu_count_logical\"] = psutil.cpu_count(logical=True)\n","    hardware_info[\"memory_total_gb\"] = round(psutil.virtual_memory().total / (1024**3), 2)\n","except ImportError:\n","    pass\n","\n","hardware_info[\"torch_version\"] = torch.__version__\n","\n","if torch.cuda.is_available():\n","    hardware_info[\"gpu_available\"] = True\n","    hardware_info[\"gpu_count\"] = torch.cuda.device_count()\n","    hardware_info[\"gpu_names\"] = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n","    hardware_info[\"cuda_version\"] = torch.version.cuda\n","    hardware_info[\"cudnn_version\"] = torch.backends.cudnn.version()\n","\n","    gpu_details = []\n","    for i in range(torch.cuda.device_count()):\n","        props = torch.cuda.get_device_properties(i)\n","        gpu_details.append({\n","            \"id\": i,\n","            \"name\": props.name,\n","            \"compute_capability\": f\"{props.major}.{props.minor}\",\n","            \"total_memory_gb\": round(props.total_memory / (1024**3), 2),\n","            \"multi_processor_count\": props.multi_processor_count\n","        })\n","    hardware_info[\"gpu_details\"] = gpu_details\n","\n","    try:\n","        out = subprocess.run(\n","            [\"nvidia-smi\", \"--query-gpu=driver_version\", \"--format=csv,noheader\"],\n","            capture_output=True, text=True\n","        )\n","        if out.returncode == 0 and out.stdout.strip():\n","            hardware_info[\"nvidia_driver_version\"] = out.stdout.strip().splitlines()[0]\n","    except:\n","        pass\n","else:\n","    hardware_info[\"gpu_available\"] = False\n","\n","hardware_info[\"deterministic_config\"] = {\n","    \"cudnn_deterministic\": torch.backends.cudnn.deterministic,\n","    \"cudnn_benchmark\": torch.backends.cudnn.benchmark,\n","    \"use_deterministic_algorithms\": True,\n","    \"warn_only\": False,\n","    \"tf32_disabled\": not torch.backends.cuda.matmul.allow_tf32 if torch.cuda.is_available() else \"N/A\",\n","    \"float32_matmul_precision\": \"high\" if hasattr(torch, 'set_float32_matmul_precision') else \"N/A\",\n","    \"PYTHONHASHSEED\": os.environ.get(\"PYTHONHASHSEED\"),\n","    \"CUBLAS_WORKSPACE_CONFIG\": os.environ.get(\"CUBLAS_WORKSPACE_CONFIG\"),\n","    \"OMP_NUM_THREADS\": os.environ.get(\"OMP_NUM_THREADS\"),\n","    \"MKL_NUM_THREADS\": os.environ.get(\"MKL_NUM_THREADS\"),\n","    \"OPENBLAS_NUM_THREADS\": os.environ.get(\"OPENBLAS_NUM_THREADS\"),\n","    \"NUMEXPR_NUM_THREADS\": os.environ.get(\"NUMEXPR_NUM_THREADS\"),\n","}\n","\n","with open(output_dir / \"hardware_log.json\", \"w\") as f:\n","    json.dump(hardware_info, f, indent=2)\n","\n","# 8. Git commit + dirty flag\n","print(\"Collecting Git information...\")\n","git_info = {}\n","try:\n","    git_commit = subprocess.run(\n","        [\"git\", \"rev-parse\", \"HEAD\"],\n","        capture_output=True, text=True, check=True\n","    ).stdout.strip()\n","    git_info[\"commit\"] = git_commit\n","\n","    git_branch = subprocess.run(\n","        [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n","        capture_output=True, text=True, check=True\n","    ).stdout.strip()\n","    git_info[\"branch\"] = git_branch\n","\n","    dirty = subprocess.run(\n","        [\"git\", \"status\", \"--porcelain\"],\n","        capture_output=True, text=True\n","    ).stdout.strip()\n","    git_info[\"dirty\"] = bool(dirty)\n","except:\n","    git_info[\"commit\"] = \"N/A (not a git repo)\"\n","    git_info[\"dirty\"] = False\n","\n","with open(output_dir / \"git_info.json\", \"w\") as f:\n","    json.dump(git_info, f, indent=2)\n","\n","# 9. PyTorch build information\n","print(\"Saving PyTorch build information...\")\n","try:\n","    buf = io.StringIO()\n","    with redirect_stdout(buf):\n","        torch.__config__.show()\n","    (output_dir / \"torch_build.txt\").write_text(buf.getvalue(), encoding=\"utf-8\")\n","except:\n","    pass\n","\n","# 10. Data checksums (only original archives)\n","print(\"Generating data checksums...\")\n","data_dir = Path(\"data\")\n","if data_dir.exists():\n","    sha256sums = []\n","    archive_exts = {'.zip', '.tar', '.gz', '.tgz', '.bz2', '.xz', '.7z', '.rar'}\n","    for file_path in sorted(data_dir.rglob(\"*\")):\n","        if file_path.is_file() and file_path.suffix.lower() in archive_exts:\n","            sha256 = hashlib.sha256()\n","            with open(file_path, \"rb\") as f:\n","                for chunk in iter(lambda: f.read(65536), b\"\"):\n","                    sha256.update(chunk)\n","            rel_path = file_path.relative_to(data_dir)\n","            sha256sums.append(f\"{sha256.hexdigest()}  {rel_path}\")\n","\n","    if sha256sums:\n","        with open(output_dir / \"data_SHA256SUMS.txt\", \"w\") as f:\n","            f.write(\"\\n\".join(sha256sums))\n","        print(f\"  Generated checksums for {len(sha256sums)} archives\")\n","    else:\n","        print(\"  No archives in data/ directory; skipping checksums\")\n","else:\n","    print(\"  data/ directory does not exist; skipping checksums\")\n","\n","# 11. Compute environment hashes of all key files\n","print(\"Computing environment hashes...\")\n","env_files = [\n","    \"requirements.txt\",\n","    \"environment.yml\",\n","    \"env.txt\",\n","    \"SEEDS.yaml\",\n","    \"hardware_log.json\",\n","    \"git_info.json\"\n","]\n","sha256_lines = []\n","for filename in env_files:\n","    filepath = output_dir / filename\n","    if filepath.exists():\n","        sha256 = hashlib.sha256()\n","        with open(filepath, \"rb\") as f:\n","            sha256.update(f.read())\n","        sha256_lines.append(f\"{sha256.hexdigest()}  {filename}\")\n","\n","with open(output_dir / \"ENV.SHA256\", \"w\") as f:\n","    f.write(\"\\n\".join(sha256_lines))\n","\n","# Output summary\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 0 complete - Reproducible environment configuration (top-conf/journal grade)\")\n","print(\"=\"*60)\n","print(f\"Output directory: {output_dir}/\")\n","print(f\"  ✓ SEEDS.yaml (seeds: {SEEDS})\")\n","print(f\"  ✓ requirements.txt\")\n","print(f\"  ✓ env.txt (with system summary)\")\n","print(f\"  ✓ environment.yml\")\n","print(f\"  ✓ hardware_log.json\")\n","print(f\"  ✓ git_info.json (dirty={git_info.get('dirty', False)})\")\n","print(f\"  ✓ torch_build.txt\")\n","print(f\"  ✓ ENV.SHA256 (covers all key files)\")\n","if (output_dir / \"data_SHA256SUMS.txt\").exists():\n","    print(f\"  ✓ data_SHA256SUMS.txt (archives only)\")\n","\n","print(f\"\\nStrict determinism configuration:\")\n","print(f\"  - torch.use_deterministic_algorithms: True (warn_only=False)\")\n","print(f\"  - cudnn.deterministic: {torch.backends.cudnn.deterministic}\")\n","print(f\"  - cudnn.benchmark: {torch.backends.cudnn.benchmark}\")\n","if torch.cuda.is_available():\n","    print(f\"  - TF32 disabled: {not torch.backends.cuda.matmul.allow_tf32}\")\n","print(f\"  - Environment variables set:\")\n","print(f\"    PYTHONHASHSEED: {os.environ.get('PYTHONHASHSEED')}\")\n","print(f\"    CUBLAS_WORKSPACE_CONFIG: {os.environ.get('CUBLAS_WORKSPACE_CONFIG')}\")\n","print(f\"    Thread control: OMP/MKL/OPENBLAS/NUMEXPR=1\")\n","print(\"=\"*60)"]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Steps 1–2: Data Acquisition & Unpack Standardization (top-conf/journal grade)\n","Process the uploaded LARa MbientLab IMU archive\n","\"\"\"\n","\n","import os\n","import hashlib\n","import zipfile\n","import shutil\n","import json\n","import re\n","import numpy as np\n","from pathlib import Path\n","from datetime import datetime, timezone\n","import pandas as pd\n","\n","# ========== Helper functions ==========\n","def read_any_csv(path, nrows=None):\n","    \"\"\"CSV reader with auto delimiter detection\"\"\"\n","    try:\n","        return pd.read_csv(path, nrows=nrows, sep=None, engine=\"python\")\n","    except Exception:\n","        return pd.read_csv(path, nrows=nrows)\n","\n","def infer_sampling_rate(df):\n","    \"\"\"Infer sampling rate; auto-handle ns/μs/ms/s time units\"\"\"\n","    cols = [c.lower() for c in df.columns]\n","    time_cols = [c for c in df.columns if re.search(r\"(time|timestamp|epoch)\", c.lower())]\n","    if not time_cols:\n","        return None\n","\n","    c = time_cols[0]\n","    t = pd.to_numeric(df[c], errors=\"coerce\").dropna().to_numpy()\n","    if t.size < 3:\n","        return None\n","\n","    # Infer time unit by magnitude\n","    max_val = np.nanmax(np.abs(t[:1000])) if t.size else 0\n","    if max_val >= 1e12:      # nanoseconds\n","        scale = 1e-9\n","    elif max_val >= 1e9:     # nanoseconds\n","        scale = 1e-9\n","    elif max_val >= 1e6:     # microseconds\n","        scale = 1e-6\n","    elif max_val >= 1e3:     # milliseconds\n","        scale = 1e-3\n","    else:                    # seconds\n","        scale = 1.0\n","\n","    t_sec = t * scale\n","    dt = np.diff(t_sec)\n","    dt = dt[dt > 0]\n","    if dt.size == 0:\n","        return None\n","\n","    # Use median for robustness\n","    return float(np.round(1.0 / np.median(dt), 3))\n","\n","def infer_sensor_type(cols_lower, filename):\n","    \"\"\"Infer sensor type\"\"\"\n","    if 'label' in filename.lower() or 'activity' in filename.lower():\n","        return \"labels\"\n","\n","    sensors = []\n","    if any((\"acc\" in c) or (\"accelerom\" in c) for c in cols_lower):\n","        sensors.append(\"acc\")\n","    if any((\"gyro\" in c) or re.search(r\"\\bgyr\", c) for c in cols_lower):\n","        sensors.append(\"gyro\")\n","    if any((\"mag\" in c) or (\"magnetom\" in c) for c in cols_lower):\n","        sensors.append(\"mag\")\n","\n","    return \"+\".join(sensors) if sensors else \"unknown\"\n","\n","# LARa placement mapping (per official docs)\n","PLACEMENT_MAP = {\n","    \"L01\": \"lwrist\",      # Left wrist\n","    \"L02\": \"rwrist\",      # Right wrist\n","    \"L03\": \"chest\",       # Chest\n","    \"L04\": \"belt\",        # Belt\n","    \"L05\": \"lankle\",      # Left ankle\n","    \"L06\": \"pocket\",      # Pocket\n","    \"L07\": \"lforearm\",    # Left forearm\n","    \"L08\": \"lupperarm\",   # Left upper arm\n","}\n","\n","# ========== Step 1: Acquire & verify ==========\n","print(\"=\"*60)\n","print(\"Step 1: Data acquisition & verification\")\n","print(\"=\"*60)\n","\n","# Create directory structure\n","raw_dir = Path(\"data/lara/mbientlab/raw\")\n","raw_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Find uploaded zip files (prefer annotated versions)\n","uploaded_files = list(Path(\".\").glob(\"*annotated*MbientLab*.zip\"))\n","if not uploaded_files:\n","    uploaded_files = list(Path(\".\").glob(\"*MbientLab*.zip\"))\n","if not uploaded_files:\n","    uploaded_files = list(Path(\".\").glob(\"*.zip\"))\n","\n","if not uploaded_files:\n","    raise FileNotFoundError(\"No MbientLab data archive found; please upload a zip file first\")\n","\n","if len(uploaded_files) > 1:\n","    print(f\"Warning: found multiple candidate files: {[f.name for f in uploaded_files]}\")\n","    print(f\"Using the first: {uploaded_files[0].name}\")\n","\n","zip_file = uploaded_files[0]\n","print(f\"Found archive: {zip_file}\")\n","\n","# Move to raw data directory\n","target_zip = raw_dir / zip_file.name\n","if not target_zip.exists():\n","    shutil.copy2(zip_file, target_zip)\n","    print(f\"Copied to: {target_zip}\")\n","else:\n","    print(f\"File already exists: {target_zip}\")\n","\n","# Compute SHA256 checksum\n","print(\"Computing SHA256 checksum...\")\n","sha256_hash = hashlib.sha256()\n","with open(target_zip, \"rb\") as f:\n","    for chunk in iter(lambda: f.read(65536), b\"\"):\n","        sha256_hash.update(chunk)\n","\n","checksum = sha256_hash.hexdigest()\n","print(f\"SHA256: {checksum}\")\n","\n","# Save checksum\n","sha256_file = raw_dir / \"SHA256SUMS.txt\"\n","with open(sha256_file, \"w\") as f:\n","    f.write(f\"{checksum}  {target_zip.name}\\n\")\n","print(f\"Saved checksum: {sha256_file}\")\n","\n","# Record provenance (traceability)\n","provenance = {\n","    \"dataset\": \"LARa IMU-only / MbientLab\",\n","    \"origin\": \"manual-upload\",\n","    \"official_url\": \"https://sensor.informatik.uni-mannheim.de/#dataset_lara\",\n","    \"retrieved_at_utc\": datetime.now(timezone.utc).isoformat(),\n","    \"archive\": target_zip.name,\n","    \"sha256\": checksum\n","}\n","(raw_dir / \"PROVENANCE.json\").write_text(\n","    json.dumps(provenance, indent=2, ensure_ascii=False),\n","    encoding=\"utf-8\"\n",")\n","print(f\"Recorded provenance info: {raw_dir / 'PROVENANCE.json'}\")\n","\n","# Set raw archive to read-only\n","os.chmod(target_zip, 0o444)\n","print(f\"Set read-only permission: {target_zip}\")\n","\n","# ========== Step 2: Unpack & directory standardization ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 2: Unpack & directory standardization\")\n","print(\"=\"*60)\n","\n","# Extract to temp directory\n","temp_extract = raw_dir / \"temp_extract\"\n","temp_extract.mkdir(exist_ok=True)\n","\n","print(f\"Extracting {target_zip.name}...\")\n","with zipfile.ZipFile(target_zip, 'r') as zip_ref:\n","    zip_ref.extractall(temp_extract)\n","\n","# Scan extracted files and normalize\n","file_records = []\n","problems = []  # record files that failed to parse\n","\n","# Recursively scan all CSV/TSV files\n","for file_path in temp_extract.rglob(\"*\"):\n","    if not file_path.is_file():\n","        continue\n","\n","    # Process only data files\n","    if file_path.suffix.lower() not in ['.csv', '.tsv', '.txt']:\n","        continue\n","\n","    # Parse filename: LARa pattern L01_S07_R01.csv\n","    filename = file_path.stem\n","\n","    # Extract L01/L02/L03 (placement)\n","    placement_match = re.search(r'L(\\d+)', filename)\n","    placement_raw = f\"L{placement_match.group(1).zfill(2)}\" if placement_match else \"L00\"\n","    placement = PLACEMENT_MAP.get(placement_raw, placement_raw)\n","\n","    # Extract S07 (subject)\n","    subject_match = re.search(r'S(\\d+)', filename)\n","    subject_id = f\"S{subject_match.group(1).zfill(2)}\" if subject_match else \"S00\"\n","\n","    # Extract R01 (session)\n","    session_match = re.search(r'R(\\d+)', filename)\n","    session_id = f\"R{session_match.group(1).zfill(2)}\" if session_match else \"R01\"\n","\n","    # Detect parse failures (avoid LOSO leakage)\n","    if subject_id == \"S00\" or session_id == \"R01\":\n","        if not re.search(r'R01', filename):  # exclude real R01\n","            problems.append(str(file_path.relative_to(temp_extract)))\n","\n","    # Create standardized directory structure\n","    std_dir = raw_dir / subject_id / session_id / placement\n","    std_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # Standardized filename (lowercase, underscores)\n","    std_filename = file_path.name.lower().replace(' ', '_').replace('-', '_')\n","    std_path = std_dir / std_filename\n","\n","    # Copy to standardized location\n","    if not std_path.exists():\n","        shutil.copy2(file_path, std_path)\n","\n","    # Get file info\n","    file_size = file_path.stat().st_size\n","    num_rows = 0\n","    sampling_rate = None\n","    duration = None\n","    sensor_type = \"unknown\"\n","\n","    try:\n","        # Read sample\n","        df_sample = read_any_csv(file_path, nrows=2000)\n","        columns_lower = [c.lower() for c in df_sample.columns]\n","\n","        # Infer sensor type\n","        sensor_type = infer_sensor_type(columns_lower, filename)\n","\n","        # Infer sampling rate (skip for labels)\n","        if sensor_type != \"labels\":\n","            sampling_rate = infer_sampling_rate(df_sample)\n","\n","        # Count total rows (streaming to avoid loading big files)\n","        with open(file_path, \"rb\") as fh:\n","            num_rows = sum(1 for _ in fh) - 1  # minus header\n","\n","        # Compute duration\n","        if sampling_rate and num_rows > 0:\n","            duration = round(num_rows / sampling_rate, 2)\n","\n","    except Exception:\n","        pass  # silently skip files that cannot be parsed\n","\n","    # Record file info\n","    file_records.append({\n","        \"subject_id\": subject_id,\n","        \"session_id\": session_id,\n","        \"placement\": placement,\n","        \"placement_raw\": placement_raw,\n","        \"sensor_type\": sensor_type,\n","        \"original_path\": str(file_path.relative_to(temp_extract)),\n","        \"standardized_path\": str(std_path.relative_to(raw_dir)),\n","        \"filename\": std_filename,\n","        \"file_size_bytes\": file_size,\n","        \"num_rows\": num_rows,\n","        \"sampling_rate_hz\": sampling_rate,\n","        \"duration_sec\": duration,\n","    })\n","\n","print(f\"Processed {len(file_records)} files\")\n","\n","# Check parse failures\n","if problems:\n","    problems_file = raw_dir / \"PROBLEMS.log\"\n","    problems_file.write_text(\n","        \"The following files could not parse subject/session (would break LOSO):\\n\" +\n","        \"\\n\".join(problems) + \"\\n\",\n","        encoding=\"utf-8\"\n","    )\n","    raise RuntimeError(\n","        f\"Found {len(problems)} files with unparsed subject/session; \"\n","        f\"please check {problems_file} and fix\"\n","    )\n","\n","# Remove temp extraction directory\n","shutil.rmtree(temp_extract)\n","print(\"Removed temporary files\")\n","\n","# Generate file_index (Parquet preferred; fallback to CSV)\n","if file_records:\n","    file_index = pd.DataFrame(file_records)\n","\n","    # Sort\n","    file_index = file_index.sort_values(\n","        ['subject_id', 'session_id', 'placement', 'sensor_type']\n","    )\n","\n","    # Save index\n","    index_file = raw_dir / \"file_index.parquet\"\n","    try:\n","        file_index.to_parquet(index_file, index=False)\n","        saved_index = index_file\n","        print(f\"\\nGenerated file index: {saved_index}\")\n","    except Exception as e:\n","        print(f\"Warning: Parquet write failed ({e}); falling back to CSV\")\n","        index_file_csv = raw_dir / \"file_index.csv\"\n","        file_index.to_csv(index_file_csv, index=False)\n","        saved_index = index_file_csv\n","        print(f\"Generated file index: {saved_index}\")\n","\n","    # Show dataset statistics\n","    print(\"\\nDataset statistics:\")\n","    print(f\"  Number of subjects: {file_index['subject_id'].nunique()}\")\n","    print(f\"  Number of sessions: {file_index.groupby('subject_id')['session_id'].nunique().sum()}\")\n","    print(f\"  Placements: {sorted(file_index['placement'].unique().tolist())}\")\n","    print(f\"  Sensor types: {sorted(file_index['sensor_type'].unique().tolist())}\")\n","    print(f\"  Total files: {len(file_index)}\")\n","\n","    # Sampling rate stats\n","    sensor_files = file_index[file_index['sensor_type'] != 'labels']\n","    if not sensor_files.empty:\n","        rates = sensor_files['sampling_rate_hz'].dropna()\n","        if not rates.empty:\n","            print(f\"  Sampling rate range: {rates.min():.1f} - {rates.max():.1f} Hz\")\n","            print(f\"  Median sampling rate: {rates.median():.1f} Hz\")\n","\n","    # Preview first records\n","    print(\"\\nFile index preview:\")\n","    print(file_index.head(10).to_string())\n","else:\n","    print(\"Warning: No data files found\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Steps 1–2 complete (top-conf/journal grade)\")\n","print(\"=\"*60)\n","print(f\"Raw data: {raw_dir}/\")\n","print(f\"Checksum: {sha256_file}\")\n","print(f\"Provenance record: {raw_dir / 'PROVENANCE.json'}\")\n","print(f\"File index: {saved_index}\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y-AFdP_Pm2Gp","executionInfo":{"status":"ok","timestamp":1763111246088,"user_tz":0,"elapsed":29143,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"059ab926-c603-4feb-8eaa-fa6388aacba2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 1: Data acquisition & verification\n","============================================================\n","Found archive: IMU data (annotated) _ MbientLab.zip\n","Copied to: data/lara/mbientlab/raw/IMU data (annotated) _ MbientLab.zip\n","Computing SHA256 checksum...\n","SHA256: 70968b6b8874375e96671af67e31c27ccb63793f31191f86e732d40f24ac3106\n","Saved checksum: data/lara/mbientlab/raw/SHA256SUMS.txt\n","Recorded provenance info: data/lara/mbientlab/raw/PROVENANCE.json\n","Set read-only permission: data/lara/mbientlab/raw/IMU data (annotated) _ MbientLab.zip\n","\n","============================================================\n","Step 2: Unpack & directory standardization\n","============================================================\n","Extracting IMU data (annotated) _ MbientLab.zip...\n","Processed 386 files\n","Removed temporary files\n","\n","Generated file index: data/lara/mbientlab/raw/file_index.parquet\n","\n","Dataset statistics:\n","  Number of subjects: 8\n","  Number of sessions: 193\n","  Placements: ['chest', 'lwrist', 'rwrist']\n","  Sensor types: ['acc+gyro', 'labels']\n","  Total files: 386\n","\n","File index preview:\n","    subject_id session_id placement placement_raw sensor_type                                                original_path                      standardized_path                filename  file_size_bytes  num_rows sampling_rate_hz duration_sec\n","265        S07        R01    lwrist           L01    acc+gyro         IMU data (annotated) _ MbientLab/S07/L01_S07_R01.csv         S07/R01/lwrist/l01_s07_r01.csv         l01_s07_r01.csv          7227132     11824             None         None\n","248        S07        R01    lwrist           L01      labels  IMU data (annotated) _ MbientLab/S07/L01_S07_R01_labels.csv  S07/R01/lwrist/l01_s07_r01_labels.csv  l01_s07_r01_labels.csv           485055     11824             None         None\n","273        S07        R02    lwrist           L01    acc+gyro         IMU data (annotated) _ MbientLab/S07/L01_S07_R02.csv         S07/R02/lwrist/l01_s07_r02.csv         l01_s07_r02.csv          7196022     11776             None         None\n","278        S07        R02    lwrist           L01      labels  IMU data (annotated) _ MbientLab/S07/L01_S07_R02_labels.csv  S07/R02/lwrist/l01_s07_r02_labels.csv  l01_s07_r02_labels.csv           483087     11776             None         None\n","260        S07        R03    rwrist           L02    acc+gyro         IMU data (annotated) _ MbientLab/S07/L02_S07_R03.csv         S07/R03/rwrist/l02_s07_r03.csv         l02_s07_r03.csv          7193816     11758             None         None\n","253        S07        R03    rwrist           L02      labels  IMU data (annotated) _ MbientLab/S07/L02_S07_R03_labels.csv  S07/R03/rwrist/l02_s07_r03_labels.csv  l02_s07_r03_labels.csv           482349     11758             None         None\n","269        S07        R05    rwrist           L02    acc+gyro         IMU data (annotated) _ MbientLab/S07/L02_S07_R05.csv         S07/R05/rwrist/l02_s07_r05.csv         l02_s07_r05.csv          7197459     11766             None         None\n","286        S07        R05    rwrist           L02      labels  IMU data (annotated) _ MbientLab/S07/L02_S07_R05_labels.csv  S07/R05/rwrist/l02_s07_r05_labels.csv  l02_s07_r05_labels.csv           482677     11766             None         None\n","258        S07        R06    rwrist           L02    acc+gyro         IMU data (annotated) _ MbientLab/S07/L02_S07_R06.csv         S07/R06/rwrist/l02_s07_r06.csv         l02_s07_r06.csv          7238567     11838             None         None\n","281        S07        R06    rwrist           L02      labels  IMU data (annotated) _ MbientLab/S07/L02_S07_R06_labels.csv  S07/R06/rwrist/l02_s07_r06_labels.csv  l02_s07_r06_labels.csv           485629     11838             None         None\n","\n","============================================================\n","Steps 1–2 complete (top-conf/journal grade)\n","============================================================\n","Raw data: data/lara/mbientlab/raw/\n","Checksum: data/lara/mbientlab/raw/SHA256SUMS.txt\n","Provenance record: data/lara/mbientlab/raw/PROVENANCE.json\n","File index: data/lara/mbientlab/raw/file_index.parquet\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 3: Metadata & Quality Audit (top-conf/journal grade - final)\n","Parse subjects, activity set, sampling rate, placement, session time; empty-window cleanup\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","from datetime import datetime, timezone\n","import json\n","import re\n","\n","# ========== Config ==========\n","MISSING_THRESHOLD = 0.05      # Missing-rate threshold 5%\n","GAP_THRESHOLD = 2.0           # Single-gap absolute threshold (seconds)\n","GAP_RATIO_THRESHOLD = 0.05    # Gap ratio threshold 5%\n","\n","print(\"=\"*60)\n","print(\"Step 3: Metadata & Quality Audit\")\n","print(\"=\"*60)\n","\n","# Load file index\n","raw_dir = Path(\"data/lara/mbientlab/raw\")\n","index_file = raw_dir / \"file_index.parquet\"\n","if not index_file.exists():\n","    index_file = raw_dir / \"file_index.csv\"\n","\n","print(f\"Loading file index: {index_file}\")\n","file_index = pd.read_parquet(index_file) if index_file.suffix == '.parquet' else pd.read_csv(index_file)\n","\n","# Initialize variables (avoid undefined in edge cases)\n","subject_agg = pd.DataFrame()\n","meta_subjects_file = None\n","meta_sessions_file = None\n","keep_sessions_file = None\n","\n","# ========== Helper functions ==========\n","def pick_scale(med_raw, sr_hint=None):\n","    \"\"\"Smartly pick time unit (s/ms/μs/ns → seconds)\"\"\"\n","    cands = [1.0, 1e-3, 1e-6, 1e-9]\n","\n","    if sr_hint and sr_hint > 0:\n","        target_dt = 1.0 / sr_hint\n","        return min(cands, key=lambda s: abs(med_raw * s - target_dt))\n","\n","    # Without hint: prefer median interval mapping into 5-400 Hz, bias toward ~50 Hz\n","    best, err = 1.0, float(\"inf\")\n","    for s in cands:\n","        dt = med_raw * s\n","        if dt <= 0:\n","            continue\n","        sr = 1.0 / dt\n","        score = 0 if 5 <= sr <= 400 else abs(sr - 50) * 10\n","        if score < err:\n","            best, err = s, score\n","    return best\n","\n","def extract_time_range_and_gaps(file_path, sampling_rate_hint=None, head_rows=20000, chunksize=200000):\n","    \"\"\"Read time column in chunks; extract range and gaps (incl. inter-chunk gaps, memory-friendly)\"\"\"\n","    try:\n","        # Infer time column & unit from a small sample\n","        df_head = pd.read_csv(file_path, nrows=head_rows, sep=None, engine=\"python\")\n","        time_cols = [c for c in df_head.columns if re.search(r\"(time|timestamp|epoch|ts)\", c, re.I)]\n","        if not time_cols:\n","            return None, None, 0.0, 0.0, 0.0\n","\n","        c = time_cols[0]\n","        s = pd.to_numeric(df_head[c], errors=\"coerce\").dropna().to_numpy()\n","\n","        # Numeric timestamp branch\n","        if s.size >= 3:\n","            diffs = np.diff(s)\n","            diffs = diffs[np.isfinite(diffs) & (diffs > 0)]\n","            if diffs.size > 0:\n","                med = float(np.median(diffs))\n","                scale = pick_scale(med, sampling_rate_hint)\n","                expected = (1.0 / sampling_rate_hint) if (sampling_rate_hint and sampling_rate_hint > 0) else (med * scale)\n","\n","                # OR logic: two independent thresholds\n","                rel_threshold = 10.0 * expected  # Relative threshold: 10× expected interval\n","                abs_threshold = GAP_THRESHOLD    # Absolute threshold: 2 s\n","\n","                first = None\n","                last = None\n","                prev = None\n","                gap_sec = 0.0\n","                max_gap = 0.0\n","\n","                for chunk in pd.read_csv(file_path, usecols=[c], sep=None, engine=\"python\", chunksize=chunksize):\n","                    v = pd.to_numeric(chunk[c], errors=\"coerce\").dropna().to_numpy()\n","                    if v.size == 0:\n","                        continue\n","\n","                    if first is None:\n","                        first = v[0]\n","\n","                    # Inter-chunk gaps (fix: use max as baseline)\n","                    if prev is not None:\n","                        delta = (v[0] - prev) * scale\n","                        cond_rel = delta > rel_threshold\n","                        cond_abs = delta > abs_threshold\n","\n","                        if cond_rel or cond_abs:\n","                            # If both trigger, use max (more lenient); if only one, use that one\n","                            if cond_rel and cond_abs:\n","                                base = max(rel_threshold, abs_threshold)\n","                            elif cond_rel:\n","                                base = rel_threshold\n","                            else:\n","                                base = abs_threshold\n","\n","                            gap_this = delta - base\n","                            gap_sec += gap_this\n","                            max_gap = max(max_gap, gap_this)\n","\n","                    # Intra-chunk gaps (fix: shape + baseline)\n","                    d = np.diff(v) * scale\n","                    mask_rel = d > rel_threshold\n","                    mask_abs = d > abs_threshold\n","                    mask = mask_rel | mask_abs\n","\n","                    if mask.any():\n","                        # Vectorized: choose the threshold triggered by each gap (use max if both)\n","                        both_triggered = mask_rel & mask_abs\n","                        thr_used = np.where(\n","                            both_triggered,\n","                            max(rel_threshold, abs_threshold),\n","                            np.where(mask_rel, rel_threshold, abs_threshold)\n","                        )\n","                        gaps = d[mask] - thr_used[mask]  # Fix: also index thr_used\n","                        gap_sec += float(gaps.sum())\n","                        max_gap = max(max_gap, float(gaps.max()))\n","\n","                    prev = v[-1]\n","                    last = v[-1]\n","\n","                if first is not None and last is not None:\n","                    start_sec = float(first * scale)\n","                    end_sec = float(last * scale)\n","                    total = end_sec - start_sec\n","                    ratio = float(gap_sec / total) if total > 0 else 0.0\n","                    return start_sec, end_sec, float(round(gap_sec, 2)), float(round(ratio, 4)), float(round(max_gap, 2))\n","\n","        # Fallback branch: datetime strings\n","        t_head = pd.to_datetime(df_head[c], utc=True, errors=\"coerce\").dropna()\n","        if t_head.size >= 3:\n","            med = float(t_head.diff().dt.total_seconds().dropna().median())\n","            if med > 0:\n","                expected = (1.0 / sampling_rate_hint) if (sampling_rate_hint and sampling_rate_hint > 0) else med\n","\n","                # OR logic\n","                rel_threshold = 10.0 * expected\n","                abs_threshold = GAP_THRESHOLD\n","\n","                first = None\n","                last = None\n","                prev = None\n","                gap_sec = 0.0\n","                max_gap = 0.0\n","\n","                for chunk in pd.read_csv(file_path, usecols=[c], sep=None, engine=\"python\", chunksize=chunksize):\n","                    tt = pd.to_datetime(chunk[c], utc=True, errors=\"coerce\").dropna()\n","                    if tt.empty:\n","                        continue\n","\n","                    if first is None:\n","                        first = tt.iloc[0]\n","\n","                    # Inter-chunk gaps (fix: use max as baseline)\n","                    if prev is not None:\n","                        delta = (tt.iloc[0] - prev).total_seconds()\n","                        cond_rel = delta > rel_threshold\n","                        cond_abs = delta > abs_threshold\n","\n","                        if cond_rel or cond_abs:\n","                            if cond_rel and cond_abs:\n","                                base = max(rel_threshold, abs_threshold)\n","                            elif cond_rel:\n","                                base = rel_threshold\n","                            else:\n","                                base = abs_threshold\n","\n","                            gap_this = delta - base\n","                            gap_sec += gap_this\n","                            max_gap = max(max_gap, gap_this)\n","\n","                    # Intra-chunk gaps (fix: shape + baseline)\n","                    d = tt.diff().dt.total_seconds().dropna()\n","                    mask_rel = d > rel_threshold\n","                    mask_abs = d > abs_threshold\n","                    mask = mask_rel | mask_abs\n","\n","                    if not mask.empty and mask.any():\n","                        both_triggered = mask_rel & mask_abs\n","                        thr_used = np.where(\n","                            both_triggered,\n","                            max(rel_threshold, abs_threshold),\n","                            np.where(mask_rel, rel_threshold, abs_threshold)\n","                        )\n","                        gaps = d[mask].values - thr_used[mask]\n","                        gap_sec += float(gaps.sum())\n","                        max_gap = max(max_gap, float(gaps.max()))\n","\n","                    prev = tt.iloc[-1]\n","                    last = tt.iloc[-1]\n","\n","                if first is not None and last is not None:\n","                    total = (last - first).total_seconds()\n","                    ratio = float(gap_sec / total) if total > 0 else 0.0\n","                    return first.timestamp(), last.timestamp(), float(round(gap_sec, 2)), float(round(ratio, 4)), float(round(max_gap, 2))\n","\n","        return None, None, 0.0, 0.0, 0.0\n","\n","    except Exception:\n","        return None, None, 0.0, 0.0, 0.0\n","\n","def safe_float(x, default=0.0):\n","    \"\"\"Safely cast to float, handling NaN/Inf\"\"\"\n","    try:\n","        if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))):\n","            return default\n","        return float(x)\n","    except:\n","        return default\n","\n","# ========== 1. Parse sensor data metadata ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Parse sensor data metadata\")\n","print(\"=\"*60)\n","\n","# Determine label files directly from filenames (more reliable)\n","label_files = file_index[\n","    file_index['filename'].str.contains('label', case=False, na=False)\n","].copy()\n","sensor_files = file_index[\n","    ~file_index['filename'].str.contains('label', case=False, na=False)\n","].copy()\n","\n","print(f\"Sensor files: {len(sensor_files)}\")\n","print(f\"Label files: {len(label_files)}\")\n","\n","# Extract time ranges for sensor files (receive 5 return values)\n","print(\"Extracting time spans and gap statistics (chunked)...\")\n","time_records = []\n","for idx, row in sensor_files.iterrows():\n","    file_path = raw_dir / row['standardized_path']\n","    start, end, gap_sec, gap_ratio, max_gap = extract_time_range_and_gaps(\n","        file_path,\n","        row['sampling_rate_hz']\n","    )\n","    time_records.append({\n","        'subject_id': row['subject_id'],\n","        'session_id': row['session_id'],\n","        'placement': row['placement'],\n","        'start_time': start,\n","        'end_time': end,\n","        'gap_seconds': gap_sec,\n","        'gap_ratio': gap_ratio,\n","        'max_gap_seconds': max_gap,\n","    })\n","\n","df_time_ranges = pd.DataFrame(time_records)\n","\n","# Aggregate time ranges by session (includes max_gap)\n","session_time_agg = df_time_ranges.groupby(['subject_id', 'session_id']).agg({\n","    'start_time': 'min',\n","    'end_time': 'max',\n","    'gap_seconds': 'sum',\n","    'max_gap_seconds': 'max',\n","}).reset_index()\n","\n","session_time_agg['session_duration_sec'] = (\n","    session_time_agg['end_time'] - session_time_agg['start_time']\n",")\n","session_time_agg['gap_ratio'] = (\n","    session_time_agg['gap_seconds'] / session_time_agg['session_duration_sec']\n",").fillna(0.0).infer_objects(copy=False)\n","\n","session_time_agg.rename(columns={\n","    'start_time': 'session_start_time',\n","    'end_time': 'session_end_time'\n","}, inplace=True)\n","\n","# Add ISO8601 (human-readable) times\n","def to_iso(x):\n","    try:\n","        if pd.notna(x):\n","            return datetime.fromtimestamp(float(x), tz=timezone.utc).isoformat()\n","    except:\n","        pass\n","    return None\n","\n","session_time_agg['session_start_utc'] = session_time_agg['session_start_time'].apply(to_iso)\n","session_time_agg['session_end_utc'] = session_time_agg['session_end_time'].apply(to_iso)\n","\n","print(f\"Extracted time spans for {len(session_time_agg)} sessions\")\n","\n","# ========== 2. Parse labels & activity statistics ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Parse labels & activity statistics\")\n","print(\"=\"*60)\n","\n","activity_stats = []\n","session_records = []\n","\n","for idx, label_row in label_files.iterrows():\n","    label_path = raw_dir / label_row['standardized_path']\n","\n","    if not label_path.exists():\n","        continue\n","\n","    try:\n","        # Read label file\n","        df_label = pd.read_csv(label_path, sep=None, engine='python')\n","\n","        # Find label column (LARa dataset uses 'Class')\n","        if 'Class' in df_label.columns:\n","            label_col = 'Class'\n","        elif 'class' in df_label.columns:\n","            label_col = 'class'\n","        else:\n","            label_cols = [c for c in df_label.columns if 'label' in c.lower() or 'activity' in c.lower()]\n","            if not label_cols:\n","                print(f\"  No label column ({df_label.columns.tolist()}): {label_path.name}\")\n","                continue\n","            label_col = label_cols[0]\n","\n","        # Count activity distribution\n","        activity_counts = df_label[label_col].value_counts()\n","        total_samples = len(df_label)\n","\n","        # Check missing\n","        missing_count = df_label[label_col].isna().sum()\n","        missing_rate = missing_count / total_samples if total_samples > 0 else 0\n","\n","        # Record session info\n","        session_info = {\n","            'subject_id': label_row['subject_id'],\n","            'session_id': label_row['session_id'],\n","            'placement': label_row['placement'],\n","            'total_samples': total_samples,\n","            'missing_samples': missing_count,\n","            'missing_rate': round(missing_rate, 4),\n","            'num_activities': len(activity_counts),\n","        }\n","\n","        # Add per-activity stats\n","        for activity, count in activity_counts.items():\n","            activity_stats.append({\n","                'subject_id': label_row['subject_id'],\n","                'session_id': label_row['session_id'],\n","                'placement': label_row['placement'],\n","                'activity': str(activity),\n","                'count': int(count),\n","                'percentage': round(count / total_samples * 100, 2)\n","            })\n","\n","        session_records.append(session_info)\n","\n","    except Exception as e:\n","        print(f\"  Warning: failed to parse {label_path.name}: {e}\")\n","        continue\n","\n","print(f\"Parsed {len(session_records)} sessions\")\n","\n","# ========== 2.1 Orphan session check ==========\n","print(\"\\nChecking orphan sessions...\")\n","sess_from_sensors = set(zip(sensor_files['subject_id'], sensor_files['session_id']))\n","sess_from_labels = set(zip(label_files['subject_id'], label_files['session_id']))\n","orphans = sess_from_sensors - sess_from_labels\n","\n","if orphans:\n","    orphan_file = raw_dir / \"QA_ISSUES.log\"\n","    with open(orphan_file, \"a\", encoding=\"utf-8\") as f:\n","        f.write(\"\\nSessions with sensors but no labels (orphan sessions):\\n\")\n","        for s, r in sorted(orphans):\n","            f.write(f\"  {s}-{r}\\n\")\n","    print(f\"⚠️  Found {len(orphans)} orphan sessions; logged to QA_ISSUES.log\")\n","\n","# ========== 3. Merge session metadata ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Merge session metadata\")\n","print(\"=\"*60)\n","\n","df_sessions = pd.DataFrame(session_records)\n","df_activities = pd.DataFrame(activity_stats)\n","\n","# Merge time info\n","if not df_sessions.empty and not session_time_agg.empty:\n","    df_sessions = df_sessions.merge(\n","        session_time_agg,\n","        on=['subject_id', 'session_id'],\n","        how='left'\n","    )\n","    print(f\"Merged time span info\")\n","\n","# ========== 4. Data quality checks & empty-window cleanup ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Data quality checks & empty-window cleanup\")\n","print(\"=\"*60)\n","\n","if not df_sessions.empty:\n","    # Generate keep flag\n","    df_sessions['keep'] = True\n","    df_sessions['reject_reason'] = ''\n","\n","    # Check missing-rate exceeds threshold\n","    high_missing_mask = df_sessions['missing_rate'] > MISSING_THRESHOLD\n","    if high_missing_mask.any():\n","        df_sessions.loc[high_missing_mask, 'keep'] = False\n","        df_sessions.loc[high_missing_mask, 'reject_reason'] = 'high_missing_rate'\n","        print(f\"⚠️  {high_missing_mask.sum()} sessions marked not kept due to high missing rate\")\n","\n","    # Check time-gap ratio exceeds threshold\n","    if 'gap_ratio' in df_sessions.columns:\n","        high_gap_mask = df_sessions['gap_ratio'] > GAP_RATIO_THRESHOLD\n","        if high_gap_mask.any():\n","            # Append reason if already rejected; otherwise mark alone\n","            for idx in df_sessions[high_gap_mask].index:\n","                if df_sessions.loc[idx, 'keep']:\n","                    df_sessions.loc[idx, 'keep'] = False\n","                    df_sessions.loc[idx, 'reject_reason'] = 'high_gap_ratio'\n","                else:\n","                    df_sessions.loc[idx, 'reject_reason'] += '+high_gap_ratio'\n","            print(f\"⚠️  {high_gap_mask.sum()} sessions marked not kept due to high gap ratio\")\n","\n","    # Summary\n","    keep_count = df_sessions['keep'].sum()\n","    reject_count = (~df_sessions['keep']).sum()\n","    print(f\"✓ QC result: keep {keep_count} sessions, reject {reject_count} sessions\")\n","\n","    # Save keep list\n","    keep_sessions_file = raw_dir / \"qa_keep_sessions.csv\"\n","    df_sessions[['subject_id', 'session_id', 'placement', 'keep', 'reject_reason',\n","                 'missing_rate', 'gap_ratio']].to_csv(keep_sessions_file, index=False)\n","    print(f\"✓ Saved: {keep_sessions_file}\")\n","\n","    # Log rejection details\n","    if reject_count > 0:\n","        rejected = df_sessions[~df_sessions['keep']]\n","        qa_issues = raw_dir / \"QA_ISSUES.log\"\n","        with open(qa_issues, \"a\") as f:\n","            f.write(f\"\\nSessions rejected by QC (total {reject_count}):\\n\\n\")\n","            f.write(rejected[['subject_id', 'session_id', 'placement', 'reject_reason',\n","                             'missing_rate', 'gap_ratio']].to_string(index=False))\n","        print(f\"  Details logged to: {qa_issues}\")\n","\n","# ========== 4.1 Generate file-level empty-window list ==========\n","print(\"\\nGenerating file-level empty-window list...\")\n","if not df_time_ranges.empty:\n","    empty_segments = df_time_ranges[\n","        df_time_ranges['gap_ratio'].notna() &\n","        (df_time_ranges['gap_ratio'] > GAP_RATIO_THRESHOLD)\n","    ].copy()\n","\n","    if not empty_segments.empty:\n","        empty_todo_file = raw_dir / \"EMPTY_SEGMENTS_TODO.csv\"\n","        empty_segments[['subject_id', 'session_id', 'placement',\n","                       'gap_seconds', 'gap_ratio', 'max_gap_seconds']].to_csv(empty_todo_file, index=False)\n","        print(f\"⚠️  Generated empty-segment list: {empty_todo_file} ({len(empty_segments)} files)\")\n","\n","# ========== 5. Generate subject-level metadata ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Generate subject-level metadata\")\n","print(\"=\"*60)\n","\n","if not df_sessions.empty:\n","    # Only count kept sessions\n","    df_keep = df_sessions[df_sessions['keep']]\n","\n","    if not df_keep.empty:\n","        # Aggregate by subject\n","        subject_agg = df_keep.groupby('subject_id').agg({\n","            'session_id': 'nunique',\n","            'total_samples': 'sum',\n","            'missing_samples': 'sum',\n","            'session_duration_sec': 'sum',\n","            'num_activities': 'sum',\n","        }).reset_index()\n","\n","        subject_agg.columns = ['subject_id', 'num_sessions', 'total_samples',\n","                               'total_missing', 'total_duration_sec', 'total_activities']\n","\n","        # Compute overall missing rate\n","        subject_agg['overall_missing_rate'] = (\n","            subject_agg['total_missing'] / subject_agg['total_samples']\n","        ).round(4)\n","\n","        # Add placement coverage\n","        placement_coverage = df_keep.groupby('subject_id')['placement'].apply(\n","            lambda x: ','.join(sorted(set(x)))\n","        ).reset_index()\n","        placement_coverage.columns = ['subject_id', 'placements']\n","\n","        subject_agg = subject_agg.merge(placement_coverage, on='subject_id')\n","\n","        # Save subject metadata\n","        meta_subjects_file = raw_dir / \"meta_subjects.csv\"\n","        subject_agg.to_csv(meta_subjects_file, index=False)\n","        print(f\"✓ Saved: {meta_subjects_file}\")\n","        print(f\"  Number of subjects: {len(subject_agg)}\")\n","\n","# ========== 6. Generate session-level metadata ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Generate session-level metadata\")\n","print(\"=\"*60)\n","\n","if not df_sessions.empty:\n","    # Add activity list\n","    if not df_activities.empty:\n","        activity_list = df_activities.groupby(['subject_id', 'session_id'])['activity'].apply(\n","            lambda x: ','.join(sorted(set(x)))\n","        ).reset_index()\n","        activity_list.columns = ['subject_id', 'session_id', 'activities']\n","\n","        df_sessions_full = df_sessions.merge(\n","            activity_list,\n","            on=['subject_id', 'session_id'],\n","            how='left'\n","        )\n","    else:\n","        df_sessions_full = df_sessions\n","\n","    # Save session metadata\n","    meta_sessions_file = raw_dir / \"meta_sessions.csv\"\n","    df_sessions_full.to_csv(meta_sessions_file, index=False)\n","    print(f\"✓ Saved: {meta_sessions_file}\")\n","    print(f\"  Number of sessions: {len(df_sessions_full)}\")\n","\n","# ========== 7. Generate quality audit report ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"7. Generate quality audit report\")\n","print(\"=\"*60)\n","\n","qa_report = []\n","qa_report.append(\"=\"*70)\n","qa_report.append(\"LARa MbientLab IMU Dataset - Quality Audit Report\")\n","qa_report.append(\"=\"*70)\n","qa_report.append(f\"Generated at: {datetime.now(timezone.utc).isoformat()}\")\n","qa_report.append(f\"Data path: {raw_dir}\")\n","qa_report.append(\"\")\n","\n","# Overall stats\n","qa_report.append(\"[1. Dataset overview]\")\n","qa_report.append(\"-\"*70)\n","if not subject_agg.empty:\n","    total_hours = safe_float(subject_agg['total_duration_sec'].sum() / 3600)\n","    qa_report.append(f\"Number of subjects: {len(subject_agg)}\")\n","    qa_report.append(f\"Total sessions: {subject_agg['num_sessions'].sum()}\")\n","    qa_report.append(f\"Total duration: {total_hours:.2f} hours\")\n","    qa_report.append(f\"Total samples: {subject_agg['total_samples'].sum():,}\")\n","qa_report.append(\"\")\n","\n","# Sampling rate stats\n","qa_report.append(\"[2. Sampling rate statistics]\")\n","qa_report.append(\"-\"*70)\n","if not sensor_files.empty:\n","    rates = sensor_files['sampling_rate_hz'].dropna()\n","    if not rates.empty:\n","        qa_report.append(f\"Sampling rate range: {rates.min():.2f} - {rates.max():.2f} Hz\")\n","        qa_report.append(f\"Median sampling rate: {rates.median():.2f} Hz\")\n","        qa_report.append(f\"Mode sampling rate: {rates.mode().values[0]:.2f} Hz\")\n","qa_report.append(\"\")\n","\n","# Placement coverage\n","qa_report.append(\"[3. Sensor placement coverage]\")\n","qa_report.append(\"-\"*70)\n","if not df_sessions.empty:\n","    df_keep = df_sessions[df_sessions['keep']]\n","    if not df_keep.empty:\n","        placement_dist = df_keep['placement'].value_counts()\n","        for placement, count in placement_dist.items():\n","            percentage = count / len(df_keep) * 100\n","            qa_report.append(f\"  {placement:15s}: {count:3d} sessions ({percentage:5.1f}%)\")\n","qa_report.append(\"\")\n","\n","# Activity distribution\n","qa_report.append(\"[4. Activity distribution]\")\n","qa_report.append(\"-\"*70)\n","if not df_activities.empty:\n","    activity_total = df_activities.groupby('activity').agg({\n","        'count': 'sum',\n","    }).sort_values('count', ascending=False)\n","\n","    total_count = activity_total['count'].sum()\n","    qa_report.append(f\"Number of activity classes: {len(activity_total)}\")\n","    qa_report.append(f\"Total samples: {total_count:,}\")\n","    qa_report.append(\"\")\n","    qa_report.append(\"Per-activity share:\")\n","    for activity, row in activity_total.iterrows():\n","        percentage = row['count'] / total_count * 100\n","        qa_report.append(f\"  {str(activity):30s}: {row['count']:8,} ({percentage:5.2f}%)\")\n","qa_report.append(\"\")\n","\n","# Data quality (incl. max_gap stats)\n","qa_report.append(\"[5. Data quality assessment]\")\n","qa_report.append(\"-\"*70)\n","if not df_sessions.empty:\n","    qa_report.append(f\"Missing-rate threshold: {MISSING_THRESHOLD*100}%\")\n","    qa_report.append(f\"Gap absolute threshold: {GAP_THRESHOLD} s\")\n","    qa_report.append(f\"Gap relative threshold: 10× expected interval\")\n","    qa_report.append(f\"Gap ratio threshold: {GAP_RATIO_THRESHOLD*100}%\")\n","\n","    avg_miss = safe_float(df_sessions['missing_rate'].mean())\n","    max_miss = safe_float(df_sessions['missing_rate'].max())\n","    med_miss = safe_float(df_sessions['missing_rate'].median())\n","\n","    qa_report.append(f\"Overall average missing rate: {avg_miss*100:.2f}%\")\n","    qa_report.append(f\"Max missing rate: {max_miss*100:.2f}%\")\n","    qa_report.append(f\"Median missing rate: {med_miss*100:.2f}%\")\n","\n","    if 'gap_ratio' in df_sessions.columns:\n","        avg_gap = safe_float(df_sessions['gap_ratio'].mean())\n","        max_gap_ratio = safe_float(df_sessions['gap_ratio'].max())\n","        qa_report.append(f\"Average gap ratio: {avg_gap*100:.2f}%\")\n","        qa_report.append(f\"Max gap ratio: {max_gap_ratio*100:.2f}%\")\n","\n","    if 'max_gap_seconds' in df_sessions.columns:\n","        max_single_gap = safe_float(df_sessions['max_gap_seconds'].max())\n","        qa_report.append(f\"Max single gap: {max_single_gap:.2f} s\")\n","\n","    keep_count = df_sessions['keep'].sum()\n","    total_count = len(df_sessions)\n","    pass_rate = keep_count / total_count * 100 if total_count > 0 else 0\n","    qa_report.append(f\"\")\n","    qa_report.append(f\"Sessions passing QC: {keep_count}/{total_count} ({pass_rate:.1f}%)\")\n","\n","if (raw_dir / \"EMPTY_SEGMENTS_TODO.csv\").exists():\n","    qa_report.append(\"\")\n","    qa_report.append(\"[Note] Empty/abnormal segments found; see: EMPTY_SEGMENTS_TODO.csv (exclude during later sliding-window segmentation)\")\n","\n","qa_report.append(\"\")\n","\n","# Per-subject details\n","qa_report.append(\"[6. Subject-level details]\")\n","qa_report.append(\"-\"*70)\n","if not subject_agg.empty:\n","    for _, subj in subject_agg.iterrows():\n","        qa_report.append(f\"Subject {subj['subject_id']}:\")\n","        qa_report.append(f\"  # sessions: {subj['num_sessions']}\")\n","        qa_report.append(f\"  Total duration: {subj['total_duration_sec']/60:.1f} minutes\")\n","        qa_report.append(f\"  Total samples: {subj['total_samples']:,}\")\n","        qa_report.append(f\"  Missing rate: {subj['overall_missing_rate']*100:.2f}%\")\n","        qa_report.append(f\"  Placements: {subj['placements']}\")\n","        qa_report.append(\"\")\n","\n","qa_report.append(\"=\"*70)\n","qa_report.append(\"End of report\")\n","qa_report.append(\"=\"*70)\n","\n","# Save QA report\n","qa_report_file = raw_dir / \"QA_REPORT.txt\"\n","with open(qa_report_file, \"w\", encoding=\"utf-8\") as f:\n","    f.write(\"\\n\".join(qa_report))\n","\n","print(f\"✓ Saved quality report: {qa_report_file}\")\n","\n","# Also print to console\n","print(\"\\n\" + \"\\n\".join(qa_report))\n","\n","# ========== 8. Generate summary JSON ==========\n","summary = {\n","    \"generated_at_utc\": datetime.now(timezone.utc).isoformat(),\n","    \"num_subjects\": int(len(subject_agg)) if not subject_agg.empty else 0,\n","    \"num_sessions_total\": len(df_sessions) if not df_sessions.empty else 0,\n","    \"num_sessions_keep\": int(df_sessions['keep'].sum()) if not df_sessions.empty else 0,\n","    \"total_duration_hours\": safe_float(subject_agg['total_duration_sec'].sum() / 3600) if not subject_agg.empty else 0.0,\n","    \"missing_threshold\": MISSING_THRESHOLD,\n","    \"gap_threshold_sec\": GAP_THRESHOLD,\n","    \"gap_ratio_threshold\": GAP_RATIO_THRESHOLD,\n","    \"avg_missing_rate\": safe_float(df_sessions['missing_rate'].mean()) if not df_sessions.empty else 0.0,\n","    \"avg_gap_ratio\": safe_float(df_sessions['gap_ratio'].mean()) if not df_sessions.empty and 'gap_ratio' in df_sessions.columns else 0.0,\n","    \"max_single_gap_seconds\": safe_float(df_sessions['max_gap_seconds'].max()) if not df_sessions.empty and 'max_gap_seconds' in df_sessions.columns else 0.0,\n","    \"num_activities\": int(len(activity_total)) if not df_activities.empty else 0,\n","    \"placements\": sorted(df_sessions[df_sessions['keep']]['placement'].unique().tolist()) if not df_sessions.empty and df_sessions['keep'].any() else [],\n","}\n","\n","summary_file = raw_dir / \"qa_summary.json\"\n","with open(summary_file, \"w\") as f:\n","    json.dump(summary, f, indent=2)\n","\n","print(f\"\\n✓ Saved summary: {summary_file}\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 3 complete - Metadata & Quality Audit (top-conf/journal grade)\")\n","print(\"=\"*60)\n","print(f\"Output files:\")\n","if meta_subjects_file:\n","    print(f\"  - {meta_subjects_file}\")\n","if meta_sessions_file:\n","    print(f\"  - {meta_sessions_file}\")\n","if keep_sessions_file:\n","    print(f\"  - {keep_sessions_file}\")\n","print(f\"  - {qa_report_file}\")\n","print(f\"  - {summary_file}\")\n","if (raw_dir / \"EMPTY_SEGMENTS_TODO.csv\").exists():\n","    print(f\"  - {raw_dir / 'EMPTY_SEGMENTS_TODO.csv'} (file-level empty-window list)\")\n","if (raw_dir / \"QA_ISSUES.log\").exists():\n","    print(f\"  - {raw_dir / 'QA_ISSUES.log'} (quality issue details)\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PmR06GALnJkE","executionInfo":{"status":"ok","timestamp":1763111371567,"user_tz":0,"elapsed":125435,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"c5e822d5-7395-4319-f8c2-1fb73d77ffe9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 3: Metadata & Quality Audit\n","============================================================\n","Loading file index: data/lara/mbientlab/raw/file_index.parquet\n","\n","============================================================\n","1. Parse sensor data metadata\n","============================================================\n","Sensor files: 193\n","Label files: 193\n","Extracting time spans and gap statistics (chunked)...\n","Extracted time spans for 193 sessions\n","\n","============================================================\n","2. Parse labels & activity statistics\n","============================================================\n","Parsed 193 sessions\n","\n","Checking orphan sessions...\n","\n","============================================================\n","3. Merge session metadata\n","============================================================\n","Merged time span info\n","\n","============================================================\n","4. Data quality checks & empty-window cleanup\n","============================================================\n","✓ QC result: keep 193 sessions, reject 0 sessions\n","✓ Saved: data/lara/mbientlab/raw/qa_keep_sessions.csv\n","\n","Generating file-level empty-window list...\n","\n","============================================================\n","5. Generate subject-level metadata\n","============================================================\n","✓ Saved: data/lara/mbientlab/raw/meta_subjects.csv\n","  Number of subjects: 8\n","\n","============================================================\n","6. Generate session-level metadata\n","============================================================\n","✓ Saved: data/lara/mbientlab/raw/meta_sessions.csv\n","  Number of sessions: 193\n","\n","============================================================\n","7. Generate quality audit report\n","============================================================\n","✓ Saved quality report: data/lara/mbientlab/raw/QA_REPORT.txt\n","\n","======================================================================\n","LARa MbientLab IMU Dataset - Quality Audit Report\n","======================================================================\n","Generated at: 2025-11-14T09:09:33.887260+00:00\n","Data path: data/lara/mbientlab/raw\n","\n","[1. Dataset overview]\n","----------------------------------------------------------------------\n","Number of subjects: 8\n","Total sessions: 193\n","Total duration: 6.18 hours\n","Total samples: 2,224,452\n","\n","[2. Sampling rate statistics]\n","----------------------------------------------------------------------\n","\n","[3. Sensor placement coverage]\n","----------------------------------------------------------------------\n","  rwrist         :  96 sessions ( 49.7%)\n","  chest          :  85 sessions ( 44.0%)\n","  lwrist         :  12 sessions (  6.2%)\n","\n","[4. Activity distribution]\n","----------------------------------------------------------------------\n","Number of activity classes: 8\n","Total samples: 2,224,452\n","\n","Per-activity share:\n","  4                             : 1,198,354 (53.87%)\n","  0                             :  231,084 (10.39%)\n","  2                             :  197,087 ( 8.86%)\n","  1                             :  182,687 ( 8.21%)\n","  3                             :  150,685 ( 6.77%)\n","  5                             :  112,818 ( 5.07%)\n","  7                             :  107,298 ( 4.82%)\n","  6                             :   44,439 ( 2.00%)\n","\n","[5. Data quality assessment]\n","----------------------------------------------------------------------\n","Missing-rate threshold: 5.0%\n","Gap absolute threshold: 2.0 s\n","Gap relative threshold: 10× expected interval\n","Gap ratio threshold: 5.0%\n","Overall average missing rate: 0.00%\n","Max missing rate: 0.00%\n","Median missing rate: 0.00%\n","Average gap ratio: 0.00%\n","Max gap ratio: 0.00%\n","Max single gap: 0.00 s\n","\n","Sessions passing QC: 193/193 (100.0%)\n","\n","[6. Subject-level details]\n","----------------------------------------------------------------------\n","Subject S07:\n","  # sessions: 29\n","  Total duration: 57.1 minutes\n","  Total samples: 342,376\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","Subject S08:\n","  # sessions: 24\n","  Total duration: 47.4 minutes\n","  Total samples: 284,329\n","  Missing rate: 0.00%\n","  Placements: chest,rwrist\n","\n","Subject S09:\n","  # sessions: 29\n","  Total duration: 54.5 minutes\n","  Total samples: 326,680\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","Subject S10:\n","  # sessions: 23\n","  Total duration: 43.2 minutes\n","  Total samples: 259,019\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","Subject S11:\n","  # sessions: 14\n","  Total duration: 27.7 minutes\n","  Total samples: 165,980\n","  Missing rate: 0.00%\n","  Placements: lwrist,rwrist\n","\n","Subject S12:\n","  # sessions: 17\n","  Total duration: 30.5 minutes\n","  Total samples: 183,024\n","  Missing rate: 0.00%\n","  Placements: chest,rwrist\n","\n","Subject S13:\n","  # sessions: 29\n","  Total duration: 56.7 minutes\n","  Total samples: 340,084\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","Subject S14:\n","  # sessions: 28\n","  Total duration: 53.8 minutes\n","  Total samples: 322,960\n","  Missing rate: 0.00%\n","  Placements: chest,lwrist,rwrist\n","\n","======================================================================\n","End of report\n","======================================================================\n","\n","✓ Saved summary: data/lara/mbientlab/raw/qa_summary.json\n","\n","============================================================\n","Step 3 complete - Metadata & Quality Audit (top-conf/journal grade)\n","============================================================\n","Output files:\n","  - data/lara/mbientlab/raw/meta_subjects.csv\n","  - data/lara/mbientlab/raw/meta_sessions.csv\n","  - data/lara/mbientlab/raw/qa_keep_sessions.csv\n","  - data/lara/mbientlab/raw/QA_REPORT.txt\n","  - data/lara/mbientlab/raw/qa_summary.json\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 4: Channel & Placement Strategy Selection (top-conf/journal grade)\n","Select placement, raw channels, derived channels; generate config file\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import re\n","\n","print(\"=\"*60)\n","print(\"Step 4: Channel & Placement Strategy Selection\")\n","print(\"=\"*60)\n","\n","# ========== Placement → Prefix allowlist (eradicate cross-placement leakage) ==========\n","PREFIX_ALLOWLIST = {\n","    \"rwrist\": [\"RA_\"],\n","    \"lwrist\": [\"LA_\"],\n","    \"chest\":  [\"N_\"],\n","    # Extensible: \"rleg\": [\"RL_\"], \"lleg\": [\"LL_\"]\n","}\n","\n","REQ_SUFFIX = {\n","    \"ax\": \"AccelerometerX\", \"ay\": \"AccelerometerY\", \"az\": \"AccelerometerZ\",\n","    \"gx\": \"GyroscopeX\",     \"gy\": \"GyroscopeY\",     \"gz\": \"GyroscopeZ\",\n","}\n","\n","# Coverage threshold: required column presence ratio across files (1.0=100%, 0.95=95%)\n","MIN_COVERAGE = 1.0\n","\n","# Load metadata\n","raw_dir = Path(\"data/lara/mbientlab/raw\")\n","configs_dir = Path(\"configs\")\n","configs_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Load subject metadata\n","meta_subjects = pd.read_csv(raw_dir / \"meta_subjects.csv\")\n","print(f\"\\nLoaded subject metadata: {len(meta_subjects)} subjects\")\n","\n","# Load file index\n","index_file = raw_dir / \"file_index.parquet\"\n","if not index_file.exists():\n","    index_file = raw_dir / \"file_index.csv\"\n","file_index = pd.read_parquet(index_file) if index_file.suffix == '.parquet' else pd.read_csv(index_file)\n","\n","# Keep only sensor files (more robust: filter by sensor_type and filename)\n","if 'sensor_type' in file_index.columns:\n","    sensor_files = file_index[\n","        (file_index['sensor_type'].isin(['acc+gyro', 'acc', 'gyro'])) &\n","        ~file_index['filename'].str.contains('label', case=False, na=False)\n","    ].copy()\n","else:\n","    sensor_files = file_index[\n","        ~file_index['filename'].str.contains('label', case=False, na=False)\n","    ].copy()\n","\n","print(f\"Number of sensor files: {len(sensor_files)}\")\n","\n","# ========== 1. Analyze placement coverage ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Analyze placement coverage\")\n","print(\"=\"*60)\n","\n","# Count data volume per placement\n","placement_stats = sensor_files.groupby('placement').agg({\n","    'subject_id': 'nunique',\n","    'session_id': 'nunique',\n","    'file_size_bytes': 'sum',\n","    'num_rows': 'sum',\n","}).reset_index()\n","placement_stats.columns = ['placement', 'num_subjects', 'num_sessions', 'total_bytes', 'total_samples']\n","placement_stats = placement_stats.sort_values('total_samples', ascending=False)\n","\n","print(\"\\nPlacement statistics (sorted by sample count):\")\n","print(placement_stats.to_string(index=False))\n","\n","# Fix selection to right wrist (this round)\n","selected_placement = \"rwrist\"\n","print(f\"\\nFixed placement for this round: {selected_placement}\")\n","\n","# Check whether placement exists\n","if selected_placement not in placement_stats['placement'].values:\n","    raise ValueError(f\"Specified placement '{selected_placement}' does not exist in the data\")\n","\n","# Check which subjects have that placement\n","subjects_with_selected = sensor_files[sensor_files['placement'] == selected_placement]['subject_id'].unique()\n","print(f\"Subjects with {selected_placement} data: {len(subjects_with_selected)}/{len(meta_subjects)}\")\n","\n","# ========== 2. Allowlist validation & channel check ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Allowlist validation & channel check\")\n","print(\"=\"*60)\n","\n","# Read only from files of selected placement\n","placement_files = sensor_files[sensor_files['placement'] == selected_placement]\n","print(f\"Number of files for selected placement '{selected_placement}': {len(placement_files)}\")\n","\n","# Get allowlist prefixes\n","allowed_prefixes = PREFIX_ALLOWLIST.get(selected_placement, [])\n","assert allowed_prefixes, f\"Prefix allowlist for '{selected_placement}' not configured; please add it in PREFIX_ALLOWLIST\"\n","print(f\"\\nUsing placement→prefix allowlist: {selected_placement} → {allowed_prefixes}\")\n","\n","# Robust header-reading function\n","def read_cols(fp):\n","    \"\"\"Read column names (with fallback)\"\"\"\n","    try:\n","        return pd.read_csv(fp, nrows=5, sep=None, engine='python').columns.tolist()\n","    except Exception:\n","        return pd.read_csv(fp, nrows=5, sep=\",\").columns.tolist()\n","\n","# Read headers of all files\n","print(f\"\\nRead headers of all {len(placement_files)} files to check consistency...\")\n","all_columns_by_file = []\n","\n","for _, row in placement_files.iterrows():\n","    fp = raw_dir / row['standardized_path']\n","    cols = read_cols(fp)\n","    data_cols = [c for c in cols if not re.search(r'(time|timestamp|epoch|index|id|class|label)', c, re.I)]\n","    all_columns_by_file.append(data_cols)\n","\n","# Assert all files were read successfully\n","assert len(all_columns_by_file) == len(placement_files), \\\n","    f\"{len(placement_files)-len(all_columns_by_file)} '{selected_placement}' files failed header reading; fix or exclude these files first\"\n","\n","print(f\"✓ Successfully read {len(all_columns_by_file)} files\")\n","\n","# Show columns of the first file as a reference\n","if all_columns_by_file:\n","    print(f\"\\nData columns of the first file:\")\n","    for col in all_columns_by_file[0]:\n","        print(f\"  {col}\")\n","\n","# ========== 3. Build strict channel mapping (allowlist + consistency assertions) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Build strict channel mapping (allowlist + consistency assertions)\")\n","print(\"=\"*60)\n","\n","def extract_prefix(col):\n","    \"\"\"Extract column prefix\"\"\"\n","    m = re.match(r'^([A-Z]{1,}_)', col)\n","    return m.group(1) if m else None\n","\n","def build_mapping_from_allowlist(allowed_prefixes, all_cols_by_file, min_coverage=1.0):\n","    \"\"\"Compose column names from allowlist × suffix and check coverage\"\"\"\n","    mapping = {}\n","    missing_files = {}\n","\n","    for std, suf in REQ_SUFFIX.items():\n","        chosen = None\n","        for pfx in allowed_prefixes:\n","            cand = f\"{pfx}{suf}\"\n","            # Count in how many files this column exists\n","            present_files = [i for i, cols in enumerate(all_cols_by_file) if cand in cols]\n","            coverage = len(present_files) / len(all_cols_by_file)\n","\n","            if coverage >= min_coverage:\n","                chosen = cand\n","                if coverage < 1.0:\n","                    # Record indices of files missing this column (for later inspection)\n","                    missing_idx = [i for i in range(len(all_cols_by_file)) if i not in present_files]\n","                    missing_files[std] = missing_idx\n","                break\n","\n","        if not chosen:\n","            raise RuntimeError(\n","                f\"[Consistency assertion failed] {std}: Under prefixes {allowed_prefixes}, no '{suf}' meets {min_coverage*100:.0f}% coverage. \"\n","                f\"Check raw column names or change placement/prefix allowlist.\"\n","            )\n","\n","        mapping[std] = chosen\n","\n","    # Prefix consistency check: all mapped columns must come from allowlist\n","    used_prefixes = {extract_prefix(v) for v in mapping.values()}\n","    if not used_prefixes.issubset(set(allowed_prefixes)):\n","        raise RuntimeError(\n","            f\"[Consistency assertion failed] Final mapping prefixes {used_prefixes} are not all within allowlist {allowed_prefixes}\"\n","        )\n","\n","    return mapping, used_prefixes, missing_files\n","\n","# Build mapping\n","final_mapping, used_prefixes, missing_files = build_mapping_from_allowlist(\n","    allowed_prefixes, all_columns_by_file, MIN_COVERAGE\n",")\n","\n","print(\"\\nFinal channel mapping (standard_name <- original_column):\")\n","for std, orig in sorted(final_mapping.items()):\n","    print(f\"  {std} <- {orig}\")\n","\n","# Explicit hard assertions\n","assert len(used_prefixes) == 1, f\"A single prefix should be used; got {used_prefixes}\"\n","assert list(used_prefixes)[0] in set(PREFIX_ALLOWLIST[selected_placement]), \\\n","    f\"Source prefix {used_prefixes} not in allowlist {PREFIX_ALLOWLIST[selected_placement]} for {selected_placement}\"\n","\n","print(f\"\\n✓ Consistency assertions passed:\")\n","print(f\"  - Using a single prefix: {sorted(used_prefixes)}\")\n","print(f\"  - Prefix is in the allowlist: {PREFIX_ALLOWLIST[selected_placement]}\")\n","print(f\"  - Number of files checked: {len(all_columns_by_file)}\")\n","print(f\"  - Coverage requirement: {MIN_COVERAGE*100:.0f}%\")\n","\n","# If there are missing, print warnings\n","if missing_files:\n","    print(f\"\\n⚠️  The following channels are missing in some files (coverage threshold set to {MIN_COVERAGE*100:.0f}%):\")\n","    for std, idx_list in missing_files.items():\n","        print(f\"  {std}: missing in {len(idx_list)} files\")\n","\n","# ========== 4. Generate channel & placement config ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Generate channel & placement config\")\n","print(\"=\"*60)\n","\n","# Config content\n","config = {\n","    'dataset': 'LARa_MbientLab_IMU',\n","    'strategy': 'single_placement_baseline',\n","\n","    # Placement configuration\n","    'placements': {\n","        'selected': [selected_placement],\n","        'available': placement_stats['placement'].tolist(),\n","        'rationale': f'Fixed selection {selected_placement}, covering {len(subjects_with_selected)} subjects',\n","    },\n","\n","    # Raw channel configuration\n","    'channels': {\n","        'raw': ['ax', 'ay', 'az', 'gx', 'gy', 'gz'],\n","        'mapping': final_mapping,\n","        'prefix_allowlist': PREFIX_ALLOWLIST,\n","        'source_prefix': sorted(used_prefixes)[0],\n","        'min_coverage': MIN_COVERAGE,\n","        'description': {\n","            'ax': 'Accelerometer X axis (m/s² or g)',\n","            'ay': 'Accelerometer Y axis (m/s² or g)',\n","            'az': 'Accelerometer Z axis (m/s² or g)',\n","            'gx': 'Gyroscope X axis (rad/s or deg/s)',\n","            'gy': 'Gyroscope Y axis (rad/s or deg/s)',\n","            'gz': 'Gyroscope Z axis (rad/s or deg/s)',\n","        }\n","    },\n","\n","    # Derived channel configuration\n","    'derived_channels': {\n","        'acc_mag': {\n","            'formula': 'sqrt(ax^2 + ay^2 + az^2)',\n","            'description': 'Accelerometer vector magnitude',\n","        },\n","        'gyr_mag': {\n","            'formula': 'sqrt(gx^2 + gy^2 + gz^2)',\n","            'description': 'Gyroscope vector magnitude',\n","        }\n","    },\n","\n","    # Final channel order\n","    'final_channels': ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag'],\n","\n","    # Multi-placement fusion (reserved; currently disabled)\n","    'multi_placement_fusion': {\n","        'enabled': False,\n","        'strategy': None,\n","        'warning': 'If enabling multi-placement fusion, you must select the fusion strategy independently within each training fold to avoid cross-fold leakage',\n","    },\n","\n","    # Rigor notes\n","    'notes': [\n","        'Single-placement baseline: avoid cross-placement information leakage',\n","        'Channel mapping uses \"placement→prefix allowlist + consistency assertions\"; no cross-prefix voting',\n","        f'Consistency checked over all {len(all_columns_by_file)} {selected_placement} files',\n","        f'Coverage requirement: {MIN_COVERAGE*100:.0f}% (tunable tolerance)',\n","        'Derived channels are computed at feature-extraction stage to preserve raw data integrity',\n","        'Any multi-placement fusion must be chosen & validated within each LOSO fold',\n","    ]\n","}\n","\n","# Save config\n","config_file = configs_dir / \"channels.yaml\"\n","with open(config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","print(f\"✓ Saved config: {config_file}\")\n","\n","# ========== 5. Validate config (random multi-file sampling) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Validate config\")\n","print(\"=\"*60)\n","\n","# Verify coverage of selected placement across all sensor files\n","files_with_placement = sensor_files[sensor_files['placement'] == selected_placement]\n","\n","print(f\"\\nValidate placement '{selected_placement}':\")\n","print(f\"  Files: {len(files_with_placement)}\")\n","print(f\"  Subjects: {files_with_placement['subject_id'].nunique()}\")\n","print(f\"  Sessions: {files_with_placement['session_id'].nunique()}\")\n","\n","# Validate channel mapping: randomly sample multiple files\n","verify_sample_size = min(5, len(files_with_placement))\n","verify_df = files_with_placement.sample(n=verify_sample_size, random_state=0)\n","\n","print(f\"\\nValidate channel mapping (random sample of {verify_sample_size} files):\")\n","for idx, sample_file in verify_df.iterrows():\n","    sample_path = raw_dir / sample_file['standardized_path']\n","    try:\n","        df_verify = pd.read_csv(sample_path, nrows=100, sep=None, engine='python')\n","\n","        print(f\"\\nFile: {sample_file['filename']}\")\n","        all_found = True\n","        for std_name in ['ax', 'ay', 'az', 'gx', 'gy', 'gz']:\n","            if std_name in final_mapping:\n","                orig_name = final_mapping[std_name]\n","                if orig_name in df_verify.columns:\n","                    sample_val = df_verify[orig_name].iloc[0]\n","                    print(f\"  ✓ {std_name} <- {orig_name} (sample value: {sample_val:.4f})\")\n","                else:\n","                    print(f\"  ✗ {std_name} <- {orig_name} (column not found)\")\n","                    all_found = False\n","            else:\n","                print(f\"  ✗ {std_name} (not mapped)\")\n","                all_found = False\n","\n","        if not all_found:\n","            print(f\"  ⚠️  This file failed validation\")\n","\n","    except Exception as e:\n","        print(f\"\\nFile: {sample_file['filename']}\")\n","        print(f\"  ✗ Error during validation: {e}\")\n","\n","# Compute derived-channel examples on the first successfully validated file\n","for idx, sample_file in verify_df.iterrows():\n","    sample_path = raw_dir / sample_file['standardized_path']\n","    try:\n","        df_verify = pd.read_csv(sample_path, nrows=100, sep=None, engine='python')\n","        if all(final_mapping[ch] in df_verify.columns for ch in ['ax', 'ay', 'az', 'gx', 'gy', 'gz']):\n","            acc_mag = np.sqrt(\n","                df_verify[final_mapping['ax']].values**2 +\n","                df_verify[final_mapping['ay']].values**2 +\n","                df_verify[final_mapping['az']].values**2\n","            )\n","            gyr_mag = np.sqrt(\n","                df_verify[final_mapping['gx']].values**2 +\n","                df_verify[final_mapping['gy']].values**2 +\n","                df_verify[final_mapping['gz']].values**2\n","            )\n","\n","            print(f\"\\nDerived-channel example values (file: {sample_file['filename']}):\")\n","            print(f\"  acc_mag: min={acc_mag.min():.4f}, max={acc_mag.max():.4f}, mean={acc_mag.mean():.4f}\")\n","            print(f\"  gyr_mag: min={gyr_mag.min():.4f}, max={gyr_mag.max():.4f}, mean={gyr_mag.mean():.4f}\")\n","            break\n","    except:\n","        continue\n","\n","# ========== 6. Fuse check (reload config for verification) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Fuse check (reload config for verification)\")\n","print(\"=\"*60)\n","\n","with open(config_file, \"r\", encoding=\"utf-8\") as f:\n","    cfg = yaml.safe_load(f)\n","\n","# Extract prefixes of all mapped columns\n","srcs = list(cfg[\"channels\"][\"mapping\"].values())\n","pfxs = {re.match(r'^([A-Za-z]+_)', s).group(1) for s in srcs if re.match(r'^([A-Za-z]+_)', s)}\n","\n","# Assertion: all channels use the same prefix\n","assert len(pfxs) == 1, f\"ax..gz not using a single prefix: {pfxs}\"\n","\n","# Assertion: prefix in allowlist\n","sel = cfg[\"placements\"][\"selected\"][0]\n","allow = set(cfg[\"channels\"][\"prefix_allowlist\"][sel])\n","assert list(pfxs)[0] in allow, f\"Prefix {pfxs} not in {sel} allowlist {allow}\"\n","\n","print(f\"✓ Config fuse check passed:\")\n","print(f\"  - Reloaded config: {config_file}\")\n","print(f\"  - All channels use a single prefix: {pfxs}\")\n","print(f\"  - Prefix is in {sel} allowlist: {allow}\")\n","\n","# ========== 7. Summary ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 4 complete - Channels & Placement Strategy\")\n","print(\"=\"*60)\n","print(f\"\\nConfig summary:\")\n","print(f\"  Strategy: single-placement baseline\")\n","print(f\"  Fixed placement: {config['placements']['selected']}\")\n","print(f\"  Raw channels: {config['channels']['raw']}\")\n","print(f\"  Derived channels: {list(config['derived_channels'].keys())}\")\n","print(f\"  Final number of channels: {len(config['final_channels'])}\")\n","print(f\"  Prefix used: {sorted(used_prefixes)}\")\n","print(f\"  Coverage requirement: {MIN_COVERAGE*100:.0f}%\")\n","print(f\"\\nConfig file: {config_file}\")\n","print(f\"\\nRigor guarantees:\")\n","print(f\"  1. ✓ Use placement→prefix allowlist (hard-coded)\")\n","print(f\"  2. ✓ Consistency assertions across all files ({len(all_columns_by_file)} files)\")\n","print(f\"  3. ✓ No cross-prefix voting; avoid mis-selection\")\n","print(f\"  4. ✓ Error out if column names don't match allowlist\")\n","print(f\"  5. ✓ Explicit assertions: single prefix + within allowlist\")\n","print(f\"  6. ✓ Abort if header reading fails\")\n","print(f\"  7. ✓ Randomly sample {verify_sample_size} files to validate mapping\")\n","print(f\"  8. ✓ Fuse check: reload config and verify prefix\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VnK8B2-6nYNO","executionInfo":{"status":"ok","timestamp":1763111371822,"user_tz":0,"elapsed":252,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"fe4d5b11-2ef4-4bc7-cf36-c614fca4a753"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 4: Channel & Placement Strategy Selection\n","============================================================\n","\n","Loaded subject metadata: 8 subjects\n","Number of sensor files: 193\n","\n","============================================================\n","1. Analyze placement coverage\n","============================================================\n","\n","Placement statistics (sorted by sample count):\n","placement  num_subjects  num_sessions  total_bytes  total_samples\n","   rwrist             8            14    685467785        1120045\n","    chest             7            14    595662725         972496\n","   lwrist             6             2     80626579         131911\n","\n","Fixed placement for this round: rwrist\n","Subjects with rwrist data: 8/8\n","\n","============================================================\n","2. Allowlist validation & channel check\n","============================================================\n","Number of files for selected placement 'rwrist': 96\n","\n","Using placement→prefix allowlist: rwrist → ['RA_']\n","\n","Read headers of all 96 files to check consistency...\n","✓ Successfully read 96 files\n","\n","Data columns of the first file:\n","  LA_AccelerometerX\n","  LA_AccelerometerY\n","  LA_AccelerometerZ\n","  LA_GyroscopeX\n","  LA_GyroscopeY\n","  LA_GyroscopeZ\n","  LL_AccelerometerX\n","  LL_AccelerometerY\n","  LL_AccelerometerZ\n","  LL_GyroscopeX\n","  LL_GyroscopeY\n","  LL_GyroscopeZ\n","  N_AccelerometerX\n","  N_AccelerometerY\n","  N_AccelerometerZ\n","  N_GyroscopeX\n","  N_GyroscopeY\n","  N_GyroscopeZ\n","  RA_AccelerometerX\n","  RA_AccelerometerY\n","  RA_AccelerometerZ\n","  RA_GyroscopeX\n","  RA_GyroscopeY\n","  RA_GyroscopeZ\n","  RL_AccelerometerX\n","  RL_AccelerometerY\n","  RL_AccelerometerZ\n","  RL_GyroscopeX\n","  RL_GyroscopeY\n","  RL_GyroscopeZ\n","\n","============================================================\n","3. Build strict channel mapping (allowlist + consistency assertions)\n","============================================================\n","\n","Final channel mapping (standard_name <- original_column):\n","  ax <- RA_AccelerometerX\n","  ay <- RA_AccelerometerY\n","  az <- RA_AccelerometerZ\n","  gx <- RA_GyroscopeX\n","  gy <- RA_GyroscopeY\n","  gz <- RA_GyroscopeZ\n","\n","✓ Consistency assertions passed:\n","  - Using a single prefix: ['RA_']\n","  - Prefix is in the allowlist: ['RA_']\n","  - Number of files checked: 96\n","  - Coverage requirement: 100%\n","\n","============================================================\n","4. Generate channel & placement config\n","============================================================\n","✓ Saved config: configs/channels.yaml\n","\n","============================================================\n","5. Validate config\n","============================================================\n","\n","Validate placement 'rwrist':\n","  Files: 96\n","  Subjects: 8\n","  Sessions: 14\n","\n","Validate channel mapping (random sample of 5 files):\n","\n","File: l02_s09_r05.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.3215)\n","  ✓ ay <- RA_AccelerometerY (sample value: -0.9254)\n","  ✓ az <- RA_AccelerometerZ (sample value: 0.2765)\n","  ✓ gx <- RA_GyroscopeX (sample value: -1.1439)\n","  ✓ gy <- RA_GyroscopeY (sample value: -1.3566)\n","  ✓ gz <- RA_GyroscopeZ (sample value: -0.7625)\n","\n","File: l02_s14_r03.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.2813)\n","  ✓ ay <- RA_AccelerometerY (sample value: -0.9499)\n","  ✓ az <- RA_AccelerometerZ (sample value: 0.1393)\n","  ✓ gx <- RA_GyroscopeX (sample value: 1.2430)\n","  ✓ gy <- RA_GyroscopeY (sample value: 12.4323)\n","  ✓ gz <- RA_GyroscopeZ (sample value: 4.8893)\n","\n","File: l02_s07_r06.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.1674)\n","  ✓ ay <- RA_AccelerometerY (sample value: -0.9645)\n","  ✓ az <- RA_AccelerometerZ (sample value: -0.0895)\n","  ✓ gx <- RA_GyroscopeX (sample value: -1.0865)\n","  ✓ gy <- RA_GyroscopeY (sample value: 9.2328)\n","  ✓ gz <- RA_GyroscopeZ (sample value: 55.7498)\n","\n","File: l02_s11_r06.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.5316)\n","  ✓ ay <- RA_AccelerometerY (sample value: -0.8636)\n","  ✓ az <- RA_AccelerometerZ (sample value: 0.1911)\n","  ✓ gx <- RA_GyroscopeX (sample value: 8.0598)\n","  ✓ gy <- RA_GyroscopeY (sample value: 32.2562)\n","  ✓ gz <- RA_GyroscopeZ (sample value: -1.3813)\n","\n","File: l02_s12_r15.csv\n","  ✓ ax <- RA_AccelerometerX (sample value: -0.9424)\n","  ✓ ay <- RA_AccelerometerY (sample value: -1.0324)\n","  ✓ az <- RA_AccelerometerZ (sample value: 0.9479)\n","  ✓ gx <- RA_GyroscopeX (sample value: 59.7400)\n","  ✓ gy <- RA_GyroscopeY (sample value: 145.9488)\n","  ✓ gz <- RA_GyroscopeZ (sample value: -34.0791)\n","\n","Derived-channel example values (file: l02_s09_r05.csv):\n","  acc_mag: min=0.9377, max=1.0473, mean=1.0126\n","  gyr_mag: min=0.4164, max=9.8511, mean=3.7085\n","\n","============================================================\n","6. Fuse check (reload config for verification)\n","============================================================\n","✓ Config fuse check passed:\n","  - Reloaded config: configs/channels.yaml\n","  - All channels use a single prefix: {'RA_'}\n","  - Prefix is in rwrist allowlist: {'RA_'}\n","\n","============================================================\n","Step 4 complete - Channels & Placement Strategy\n","============================================================\n","\n","Config summary:\n","  Strategy: single-placement baseline\n","  Fixed placement: ['rwrist']\n","  Raw channels: ['ax', 'ay', 'az', 'gx', 'gy', 'gz']\n","  Derived channels: ['acc_mag', 'gyr_mag']\n","  Final number of channels: 8\n","  Prefix used: ['RA_']\n","  Coverage requirement: 100%\n","\n","Config file: configs/channels.yaml\n","\n","Rigor guarantees:\n","  1. ✓ Use placement→prefix allowlist (hard-coded)\n","  2. ✓ Consistency assertions across all files (96 files)\n","  3. ✓ No cross-prefix voting; avoid mis-selection\n","  4. ✓ Error out if column names don't match allowlist\n","  5. ✓ Explicit assertions: single prefix + within allowlist\n","  6. ✓ Abort if header reading fails\n","  7. ✓ Randomly sample 5 files to validate mapping\n","  8. ✓ Fuse check: reload config and verify prefix\n","============================================================\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","\n","# ========== Manually set FOLD_ID (if the environment variable is not set) ==========\n","if \"FOLD_ID\" not in os.environ:\n","    print(\"⚠️ Environment variable FOLD_ID is not set; please specify it manually:\")\n","    print(\"Hint: If your LOSO has N folds, FOLD_ID should be 0 to N-1\")\n","    fold_input = input(\"Enter FOLD_ID (press Enter to default to 0): \").strip()\n","    os.environ[\"FOLD_ID\"] = fold_input if fold_input else \"0\"\n","    print(f\"✓ FOLD_ID set to {os.environ['FOLD_ID']}\")\n","\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","\"\"\"\n","Step 5: Timeline Unification & Resampling (top-conf/journal grade - flawless)\n","Unify to 50 Hz; linear interpolation/forward-fill; align start/end\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import re\n","import os\n","import json\n","from datetime import datetime, timezone\n","\n","# ========== Config ==========\n","TARGET_FREQ_HZ = 50.0           # Target sampling rate\n","MAX_INTERP_GAP_MS = 20.0        # Maximum interpolation gap (milliseconds)\n","MAX_INTERP_RATIO = 0.15         # Gap coverage threshold 15% (constant; applied globally)\n","\n","print(\"=\"*60)\n","print(\"Step 5: Timeline Unification & Resampling\")\n","print(\"=\"*60)\n","\n","# Load config and metadata\n","raw_dir = Path(\"data/lara/mbientlab/raw\")\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","proc_dir.mkdir(parents=True, exist_ok=True)\n","\n","configs_dir = Path(\"configs\")\n","with open(configs_dir / \"channels.yaml\", 'r', encoding='utf-8') as f:\n","    channel_config = yaml.safe_load(f)\n","\n","selected_placement = channel_config['placements']['selected'][0]\n","channel_mapping = channel_config['channels']['mapping']\n","print(f\"\\nTarget sampling rate: {TARGET_FREQ_HZ} Hz\")\n","print(f\"Selected placement: {selected_placement}\")\n","\n","# Load QC results (all kept sessions)\n","qa_keep = pd.read_csv(raw_dir / \"qa_keep_sessions.csv\")\n","keep_sessions = qa_keep[qa_keep['keep'] == True].copy()\n","keep_sessions = keep_sessions[keep_sessions['placement'] == selected_placement].copy()\n","\n","# Load train-fold markers (for statistics)\n","splits_path = configs_dir / \"splits.json\"\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","\n","train_subjects = set()\n","if splits_path.exists() and FOLD_ID >= 0:\n","    with open(splits_path, \"r\") as f:\n","        splits = json.load(f)\n","    train_subjects = set(splits[str(FOLD_ID)][\"train_subjects\"])\n","    print(f\"\\nTrain-fold markers: FOLD_ID={FOLD_ID}\")\n","    print(f\"  Train subjects: {len(train_subjects)}\")\n","    print(f\"  Total sessions: {len(keep_sessions)}\")\n","    print(f\"  Train sessions: {keep_sessions['subject_id'].isin(train_subjects).sum()}\")\n","    print(f\"  Test sessions: {(~keep_sessions['subject_id'].isin(train_subjects)).sum()}\")\n","else:\n","    print(f\"\\nTrain-fold markers not enabled (only logging; no pruning)\")\n","    print(f\"  Total sessions: {len(keep_sessions)}\")\n","\n","# Fix 1: Prune switch (enabled only when training fold exists)\n","APPLY_PRUNE = len(train_subjects) > 0\n","keep_sessions['is_train'] = (\n","    keep_sessions['subject_id'].isin(train_subjects) if len(train_subjects) > 0 else False\n",")\n","\n","print(f\"  Prune switch: {'ON' if APPLY_PRUNE else 'OFF'}\")\n","\n","# Load file index\n","index_file = raw_dir / \"file_index.parquet\"\n","if not index_file.exists():\n","    index_file = raw_dir / \"file_index.csv\"\n","file_index = pd.read_parquet(index_file) if index_file.suffix == '.parquet' else pd.read_csv(index_file)\n","\n","# ========== Helper functions ==========\n","def detect_time_column(df):\n","    \"\"\"Detect time column (avoid false positive matches on 'ts' substring)\"\"\"\n","    time_cols = [c for c in df.columns\n","                 if re.search(r'(^|_)(time|timestamp|epoch|ts)($|_)', c, re.I)]\n","    return time_cols[0] if time_cols else None\n","\n","def parse_time_to_seconds(time_series):\n","    \"\"\"Convert time to seconds (correctly infer Unix timestamp units)\"\"\"\n","    numeric = pd.to_numeric(time_series, errors='coerce')\n","    if numeric.notna().sum() > len(time_series) * 0.9:\n","        vals = numeric.dropna().values\n","        max_val = np.abs(vals[:1000]).max() if len(vals) else 0\n","\n","        # Infer by 2025 Unix timestamp magnitude\n","        if max_val > 1e17:      # nanoseconds\n","            return numeric * 1e-9\n","        elif max_val > 1e14:    # microseconds\n","            return numeric * 1e-6\n","        elif max_val > 1e11:    # milliseconds\n","            return numeric * 1e-3\n","        else:                   # seconds\n","            return numeric\n","\n","    dt = pd.to_datetime(time_series, utc=True, errors='coerce')\n","    if dt.notna().sum() > len(time_series) * 0.9:\n","        epoch = pd.Timestamp(\"1970-01-01\", tz='UTC')\n","        return (dt - epoch).dt.total_seconds()\n","\n","    return None\n","\n","def resample_sensor_data(df, time_col, data_cols, target_freq_hz=50.0, max_gap_ms=20.0):\n","    \"\"\"Resample sensor data (return cleaned time for labels)\"\"\"\n","    time_sec = parse_time_to_seconds(df[time_col])\n","    if time_sec is None:\n","        raise ValueError(\"Unable to parse time column\")\n","\n","    valid_mask = time_sec.notna() & df[data_cols].notna().all(axis=1)\n","    time_clean = time_sec[valid_mask].values\n","    data_clean = df.loc[valid_mask, data_cols].values\n","\n","    if len(time_clean) < 2:\n","        return None, 0.0, 0, 0.0, 0.0, None\n","\n","    # De-duplicate + sort\n","    unique_idx = np.unique(time_clean, return_index=True)[1]\n","    time_clean = time_clean[unique_idx]\n","    data_clean = data_clean[unique_idx]\n","\n","    order = np.argsort(time_clean)\n","    time_clean = time_clean[order]\n","    data_clean = data_clean[order]\n","\n","    # Original frequency\n","    dt_orig = np.median(np.diff(time_clean))\n","    orig_freq_hz = 1.0 / dt_orig if dt_orig > 0 else 0.0\n","\n","    # Build target timeline with integer number of samples\n","    dt = 1.0 / target_freq_hz\n","    t_start = time_clean[0]\n","    t_end = time_clean[-1]\n","    n_samples = int(np.round((t_end - t_start) / dt))\n","    target_time = t_start + np.arange(n_samples + 1) * dt\n","\n","    # Linear interpolation\n","    resampled_data = np.zeros((len(target_time), len(data_cols)))\n","    for i in range(len(data_cols)):\n","        resampled_data[:, i] = np.interp(target_time, time_clean, data_clean[:, i])\n","\n","    # Large-gap detection (account for jitter)\n","    max_gap_sec = max(max_gap_ms / 1000.0, 1.25 * dt)\n","    time_diffs = np.diff(time_clean)\n","    gap_mask = time_diffs > max_gap_sec\n","\n","    is_in_gap = np.zeros(len(target_time), dtype=int)\n","    is_forced_nan = np.zeros(len(target_time), dtype=int)\n","    actual_interp_count = 0\n","    total_gap_time = 0.0\n","\n","    if gap_mask.any():\n","        for i in range(len(time_clean) - 1):\n","            if gap_mask[i]:\n","                t_gap_start = time_clean[i]\n","                t_gap_end = time_clean[i + 1]\n","                gap_duration = t_gap_end - t_gap_start\n","                total_gap_time += gap_duration\n","\n","                idxs = np.where((target_time > t_gap_start) & (target_time < t_gap_end))[0]\n","\n","                if idxs.size > 0:\n","                    is_in_gap[idxs] = 1\n","                    actual_interp_count += 1\n","\n","                    if idxs.size > 1:\n","                        forced_nan_idxs = idxs[1:]\n","                        is_forced_nan[forced_nan_idxs] = 1\n","                        resampled_data[forced_nan_idxs, :] = np.nan\n","\n","    # Gap coverage\n","    gap_points = int(is_in_gap.sum())\n","    interp_ratio = gap_points / len(target_time) if len(target_time) > 0 else 0.0\n","\n","    # Gap time fraction\n","    total_duration = t_end - t_start\n","    gap_time_fraction = total_gap_time / total_duration if total_duration > 0 else 0.0\n","\n","    resampled_df = pd.DataFrame(resampled_data, columns=data_cols)\n","    resampled_df.insert(0, 'time_sec', target_time)\n","    resampled_df['is_in_gap'] = is_in_gap\n","    resampled_df['is_forced_nan'] = is_forced_nan\n","\n","    return resampled_df, interp_ratio, gap_points, gap_time_fraction, orig_freq_hz, time_clean\n","\n","def resample_labels(df_label, label_col, target_time, sensor_time_original=None, label_time_col=None):\n","    \"\"\"Resample labels (boundary NaN + sorting)\"\"\"\n","    if label_time_col is not None:\n","        time_sec = parse_time_to_seconds(df_label[label_time_col])\n","        if time_sec is None:\n","            raise ValueError(\"Unable to parse label time column\")\n","\n","        valid_mask = time_sec.notna() & df_label[label_col].notna()\n","        time_clean = time_sec[valid_mask].values\n","        labels_clean = df_label.loc[valid_mask, label_col].values\n","    else:\n","        if sensor_time_original is None:\n","            raise ValueError(\"Labels have no time column and no sensor time provided\")\n","\n","        min_len = min(len(df_label), len(sensor_time_original))\n","        if abs(len(df_label) - len(sensor_time_original)) > min_len * 0.01:\n","            raise ValueError(f\"Label rows ({len(df_label)}) differ too much from sensor rows ({len(sensor_time_original)})\")\n","\n","        time_clean = sensor_time_original[:min_len]\n","        labels_clean = df_label[label_col].iloc[:min_len].values\n","\n","        valid_mask = pd.notna(labels_clean)\n","        time_clean = time_clean[valid_mask]\n","        labels_clean = labels_clean[valid_mask]\n","\n","    if len(time_clean) == 0:\n","        return np.full(len(target_time), np.nan)\n","\n","    # Explicit sorting\n","    order = np.argsort(time_clean)\n","    time_clean = time_clean[order]\n","    labels_clean = labels_clean[order]\n","\n","    idx = np.searchsorted(time_clean, target_time, side='right') - 1\n","    idx = np.clip(idx, 0, len(time_clean) - 1)\n","\n","    labels = labels_clean[idx].copy()\n","\n","    # Fix: cast integers to float to allow NaN\n","    if labels.dtype.kind in ['i', 'u']:  # integer or unsigned integer\n","        labels = labels.astype('float64')\n","\n","    # Boundary NaNs\n","    mask_before = target_time < time_clean[0]\n","    mask_after = target_time > time_clean[-1]\n","    labels[mask_before | mask_after] = np.nan\n","\n","    return labels\n","\n","# ========== 1. Process all sessions ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Resampling\")\n","print(\"=\"*60)\n","\n","resampled_records = []\n","interp_stats = []\n","issues = []\n","\n","for idx, session in keep_sessions.iterrows():\n","    subject_id = session['subject_id']\n","    session_id = session['session_id']\n","    placement = session['placement']\n","    is_train = session['is_train']\n","\n","    print(f\"\\nProcessing {subject_id}/{session_id}/{placement} {'[TRAIN]' if is_train else '[TEST]'}...\")\n","\n","    sensor_file = file_index[\n","        (file_index['subject_id'] == subject_id) &\n","        (file_index['session_id'] == session_id) &\n","        (file_index['placement'] == placement) &\n","        (~file_index['filename'].str.contains('label', case=False, na=False))\n","    ]\n","\n","    label_file = file_index[\n","        (file_index['subject_id'] == subject_id) &\n","        (file_index['session_id'] == session_id) &\n","        (file_index['placement'] == placement) &\n","        (file_index['filename'].str.contains('label', case=False, na=False))\n","    ]\n","\n","    if sensor_file.empty or label_file.empty:\n","        print(f\"  Skip: missing files\")\n","        continue\n","\n","    sensor_path = raw_dir / sensor_file.iloc[0]['standardized_path']\n","    label_path = raw_dir / label_file.iloc[0]['standardized_path']\n","\n","    try:\n","        df_sensor = pd.read_csv(sensor_path, sep=None, engine='python')\n","        time_col = detect_time_column(df_sensor)\n","        if not time_col:\n","            print(f\"  Skip: no time column\")\n","            continue\n","\n","        data_cols = [channel_mapping[std] for std in ['ax', 'ay', 'az', 'gx', 'gy', 'gz']]\n","        missing_cols = [c for c in data_cols if c not in df_sensor.columns]\n","        if missing_cols:\n","            print(f\"  Skip: missing columns {missing_cols}\")\n","            continue\n","\n","        print(f\"  Resampling sensors ({len(df_sensor)} rows)...\")\n","        result = resample_sensor_data(\n","            df_sensor, time_col, data_cols, TARGET_FREQ_HZ, MAX_INTERP_GAP_MS\n","        )\n","\n","        if result[0] is None:\n","            print(f\"  Skip: resampling failed\")\n","            continue\n","\n","        # Fix 2: receive cleaned time for labels\n","        resampled_sensor, interp_ratio, gap_points, gap_time_frac, orig_freq, sensor_time_clean = result\n","\n","        valid_samples = resampled_sensor[data_cols].notna().all(axis=1).sum()\n","        nan_samples = len(resampled_sensor) - valid_samples\n","        forced_nan_points = int(resampled_sensor['is_forced_nan'].sum())\n","\n","        print(f\"  → {len(resampled_sensor)} rows, gap coverage: {interp_ratio*100:.2f}%, NaN: {nan_samples}\")\n","\n","        # Fix 1: prune based on switch\n","        if interp_ratio > MAX_INTERP_RATIO:\n","            msg = f\"Gap coverage too high ({interp_ratio*100:.1f}%)\"\n","            print(f\"  ⚠️  {msg}\")\n","            issues.append({\n","                'subject_id': subject_id,\n","                'session_id': session_id,\n","                'placement': placement,\n","                'is_train': is_train,\n","                'issue': 'high_gap_coverage',\n","                'gap_coverage': round(interp_ratio, 4),\n","            })\n","            if APPLY_PRUNE:\n","                continue\n","\n","        interp_stats.append({\n","            'subject_id': subject_id,\n","            'session_id': session_id,\n","            'placement': placement,\n","            'is_train': is_train,\n","            'original_samples': len(df_sensor),\n","            'original_freq_hz': round(orig_freq, 2),\n","            'resampled_samples': len(resampled_sensor),\n","            'valid_samples': valid_samples,\n","            'nan_samples': nan_samples,\n","            'gap_points': gap_points,\n","            'gap_coverage': round(interp_ratio, 4),\n","            'gap_time_fraction': round(gap_time_frac, 4),\n","            'forced_nan_points': forced_nan_points,\n","        })\n","\n","        df_label = pd.read_csv(label_path, sep=None, engine='python')\n","\n","        label_col = None\n","        for col_candidate in ['Class', 'class', 'label', 'Label', 'activity', 'Activity']:\n","            if col_candidate in df_label.columns:\n","                label_col = col_candidate\n","                break\n","\n","        if not label_col:\n","            for col in df_label.columns:\n","                if any(kw in col.lower() for kw in ['label', 'activity', 'class', 'action']):\n","                    label_col = col\n","                    break\n","\n","        if not label_col:\n","            print(f\"  Skip: no label column\")\n","            issues.append({\n","                'subject_id': subject_id,\n","                'session_id': session_id,\n","                'placement': placement,\n","                'is_train': is_train,\n","                'issue': 'no_label_column',\n","            })\n","            continue\n","\n","        label_time_col = detect_time_column(df_label)\n","        target_time = resampled_sensor['time_sec'].values\n","\n","        print(f\"  Resampling labels...\")\n","        try:\n","            if label_time_col:\n","                resampled_labels = resample_labels(\n","                    df_label, label_col, target_time,\n","                    label_time_col=label_time_col\n","                )\n","            else:\n","                # Fix 2: use cleaned sensor time (consistent basis)\n","                resampled_labels = resample_labels(\n","                    df_label, label_col, target_time,\n","                    sensor_time_original=sensor_time_clean\n","                )\n","\n","            resampled_sensor['label'] = resampled_labels\n","\n","        except Exception as e:\n","            print(f\"  Skip: label resampling failed - {e}\")\n","            issues.append({\n","                'subject_id': subject_id,\n","                'session_id': session_id,\n","                'placement': placement,\n","                'is_train': is_train,\n","                'issue': 'label_resample_error',\n","                'error': str(e),  # Fix 3: include error details\n","            })\n","            continue\n","\n","        resampled_sensor.rename(columns={\n","            channel_mapping['ax']: 'ax',\n","            channel_mapping['ay']: 'ay',\n","            channel_mapping['az']: 'az',\n","            channel_mapping['gx']: 'gx',\n","            channel_mapping['gy']: 'gy',\n","            channel_mapping['gz']: 'gz',\n","        }, inplace=True)\n","\n","        resampled_sensor.insert(0, 'subject_id', subject_id)\n","        resampled_sensor.insert(1, 'session_id', session_id)\n","        resampled_sensor.insert(2, 'placement', placement)\n","\n","        resampled_records.append(resampled_sensor)\n","        print(f\"  ✓ Done\")\n","\n","    except Exception as e:\n","        print(f\"  ✗ Error: {e}\")\n","        issues.append({\n","            'subject_id': subject_id,\n","            'session_id': session_id,\n","            'placement': placement,\n","            'is_train': is_train,\n","            'issue': 'processing_error',\n","            'error': str(e),  # Fix 3: include error details\n","        })\n","\n","print(f\"\\nSuccessfully processed: {len(resampled_records)} sessions\")\n","print(f\"Skipped/failed: {len(issues)} sessions\")\n","\n","# ========== 2. Combine & save ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Combine & Save\")\n","print(\"=\"*60)\n","\n","if resampled_records:\n","    df_all = pd.concat(resampled_records, ignore_index=True)\n","\n","    # Optimization: cast dtypes (reduce size)\n","    for c in ['ax', 'ay', 'az', 'gx', 'gy', 'gz']:\n","        df_all[c] = df_all[c].astype('float32')\n","    df_all['time_sec'] = df_all['time_sec'].astype('float64')  # Keep high precision for time\n","\n","    output_file = proc_dir / \"resampled.parquet\"\n","\n","    if output_file.exists():\n","        import shutil\n","        if output_file.is_dir():\n","            shutil.rmtree(output_file)\n","        else:\n","            output_file.unlink()\n","        print(f\"Removed old data: {output_file}\")\n","\n","    df_all.to_parquet(\n","        output_file,\n","        index=False,\n","        partition_cols=['subject_id', 'placement'],\n","        engine='pyarrow'\n","    )\n","    print(f\"✓ Saved: {output_file}\")\n","    print(f\"  Total rows: {len(df_all):,}\")\n","    print(f\"  # subjects: {df_all['subject_id'].nunique()}\")\n","    print(f\"  # sessions: {df_all.groupby(['subject_id', 'session_id']).ngroups}\")\n","\n","    valid_mask = df_all[['ax', 'ay', 'az', 'gx', 'gy', 'gz']].notna().all(axis=1)\n","    print(f\"  Valid samples: {valid_mask.sum():,} ({valid_mask.sum()/len(df_all)*100:.1f}%)\")\n","    print(f\"  Samples with NaN: {(~valid_mask).sum():,}\")\n","\n","    print(\"\\nData preview:\")\n","    print(df_all.head(10).to_string())\n","\n","    print(\"\\nNumeric column stats (valid samples):\")\n","    numeric_cols = ['ax', 'ay', 'az', 'gx', 'gy', 'gz']\n","    print(df_all.loc[valid_mask, numeric_cols].describe().round(4))\n","else:\n","    print(\"Warning: No data to save\")\n","\n","# ========== 3. Save statistics ==========\n","if interp_stats:\n","    df_interp = pd.DataFrame(interp_stats)\n","    interp_file = proc_dir / \"resample_stats.csv\"\n","    df_interp.to_csv(interp_file, index=False)\n","    print(f\"\\n✓ Saved stats: {interp_file}\")\n","\n","    if 'is_train' in df_interp.columns and df_interp['is_train'].any():\n","        train_stats = df_interp[df_interp['is_train']]\n","        print(f\"\\nGap statistics (train fold):\")\n","        print(f\"  Mean gap coverage: {train_stats['gap_coverage'].mean()*100:.2f}%\")\n","        print(f\"  Max gap coverage: {train_stats['gap_coverage'].max()*100:.2f}%\")\n","        print(f\"  Mean gap time fraction: {train_stats['gap_time_fraction'].mean()*100:.2f}%\")\n","\n","        print(f\"\\nGap statistics (overall):\")\n","        print(f\"  Mean gap coverage: {df_interp['gap_coverage'].mean()*100:.2f}%\")\n","        print(f\"  Max gap coverage: {df_interp['gap_coverage'].max()*100:.2f}%\")\n","    else:\n","        print(f\"\\nGap statistics:\")\n","        print(f\"  Mean gap coverage: {df_interp['gap_coverage'].mean()*100:.2f}%\")\n","        print(f\"  Max gap coverage: {df_interp['gap_coverage'].max()*100:.2f}%\")\n","\n","if issues:\n","    df_issues = pd.DataFrame(issues)\n","    issues_file = proc_dir / \"resample_issues.csv\"\n","    df_issues.to_csv(issues_file, index=False)\n","    print(f\"\\n⚠️  Saved issue records: {issues_file} ({len(issues)} items)\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 5 complete - Flawless version\")\n","print(\"=\"*60)\n","print(f\"\\nFinal fixes:\")\n","print(f\"  1. ✓ Prune switch (enabled only when training fold is set)\")\n","print(f\"  2. ✓ Label time harmonized (reuse cleaned time)\")\n","print(f\"  3. ✓ Complete error information (\\\"error\\\" field)\")\n","print(f\"  4. ✓ Comment fix (constant threshold 0.15)\")\n","print(f\"  5. ✓ Type optimization (float32/float64)\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rfRVvEUJrKiZ","executionInfo":{"status":"ok","timestamp":1763117589000,"user_tz":0,"elapsed":51797,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"76eda6c7-6dee-46f9-a92f-073c2ad81719"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["⚠️ Environment variable FOLD_ID is not set; please specify it manually:\n","Hint: If your LOSO has N folds, FOLD_ID should be 0 to N-1\n","Enter FOLD_ID (press Enter to default to 0): 3\n","✓ FOLD_ID set to 3\n","============================================================\n","Step 5: Timeline Unification & Resampling\n","============================================================\n","\n","Target sampling rate: 50.0 Hz\n","Selected placement: rwrist\n","\n","Train-fold markers: FOLD_ID=3\n","  Train subjects: 7\n","  Total sessions: 96\n","  Train sessions: 82\n","  Test sessions: 14\n","  Prune switch: ON\n","\n","============================================================\n","1. Resampling\n","============================================================\n","\n","Processing S07/R03/rwrist [TRAIN]...\n","  Resampling sensors (11758 rows)...\n","  → 5879 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R05/rwrist [TRAIN]...\n","  Resampling sensors (11766 rows)...\n","  → 5883 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R06/rwrist [TRAIN]...\n","  Resampling sensors (11838 rows)...\n","  → 5919 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R07/rwrist [TRAIN]...\n","  Resampling sensors (11795 rows)...\n","  → 5898 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R08/rwrist [TRAIN]...\n","  Resampling sensors (11804 rows)...\n","  → 5902 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R09/rwrist [TRAIN]...\n","  Resampling sensors (11779 rows)...\n","  → 5890 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R10/rwrist [TRAIN]...\n","  Resampling sensors (11777 rows)...\n","  → 5889 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R11/rwrist [TRAIN]...\n","  Resampling sensors (11775 rows)...\n","  → 5888 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R12/rwrist [TRAIN]...\n","  Resampling sensors (11814 rows)...\n","  → 5908 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R13/rwrist [TRAIN]...\n","  Resampling sensors (11842 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R14/rwrist [TRAIN]...\n","  Resampling sensors (11806 rows)...\n","  → 5903 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R15/rwrist [TRAIN]...\n","  Resampling sensors (11774 rows)...\n","  → 5888 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S07/R16/rwrist [TRAIN]...\n","  Resampling sensors (11863 rows)...\n","  → 5932 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R03/rwrist [TRAIN]...\n","  Resampling sensors (11868 rows)...\n","  → 5935 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R04/rwrist [TRAIN]...\n","  Resampling sensors (11843 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R06/rwrist [TRAIN]...\n","  Resampling sensors (11864 rows)...\n","  → 5932 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R07/rwrist [TRAIN]...\n","  Resampling sensors (11901 rows)...\n","  → 5951 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R08/rwrist [TRAIN]...\n","  Resampling sensors (11856 rows)...\n","  → 5929 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R10/rwrist [TRAIN]...\n","  Resampling sensors (11926 rows)...\n","  → 5963 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R11/rwrist [TRAIN]...\n","  Resampling sensors (11903 rows)...\n","  → 5952 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R12/rwrist [TRAIN]...\n","  Resampling sensors (11924 rows)...\n","  → 5963 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R13/rwrist [TRAIN]...\n","  Resampling sensors (11925 rows)...\n","  → 5963 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R15/rwrist [TRAIN]...\n","  Resampling sensors (11270 rows)...\n","  → 5636 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S08/R16/rwrist [TRAIN]...\n","  Resampling sensors (11540 rows)...\n","  → 5770 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R03/rwrist [TRAIN]...\n","  Resampling sensors (11866 rows)...\n","  → 5934 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R04/rwrist [TRAIN]...\n","  Resampling sensors (11907 rows)...\n","  → 5954 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R05/rwrist [TRAIN]...\n","  Resampling sensors (11895 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R06/rwrist [TRAIN]...\n","  Resampling sensors (11916 rows)...\n","  → 5959 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R07/rwrist [TRAIN]...\n","  Resampling sensors (11860 rows)...\n","  → 5931 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R08/rwrist [TRAIN]...\n","  Resampling sensors (11909 rows)...\n","  → 5955 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R09/rwrist [TRAIN]...\n","  Resampling sensors (11906 rows)...\n","  → 5953 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R10/rwrist [TRAIN]...\n","  Resampling sensors (11886 rows)...\n","  → 5944 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R11/rwrist [TRAIN]...\n","  Resampling sensors (11880 rows)...\n","  → 5941 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R12/rwrist [TRAIN]...\n","  Resampling sensors (11866 rows)...\n","  → 5933 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R13/rwrist [TRAIN]...\n","  Resampling sensors (11904 rows)...\n","  → 5953 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R14/rwrist [TRAIN]...\n","  Resampling sensors (4035 rows)...\n","  → 2018 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R15/rwrist [TRAIN]...\n","  Resampling sensors (11895 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S09/R16/rwrist [TRAIN]...\n","  Resampling sensors (11895 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R03/rwrist [TEST]...\n","  Resampling sensors (11795 rows)...\n","  → 5898 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R04/rwrist [TEST]...\n","  Resampling sensors (11806 rows)...\n","  → 5903 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R05/rwrist [TEST]...\n","  Resampling sensors (11815 rows)...\n","  → 5908 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R06/rwrist [TEST]...\n","  Resampling sensors (11767 rows)...\n","  → 5884 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R07/rwrist [TEST]...\n","  Resampling sensors (11869 rows)...\n","  → 5935 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R08/rwrist [TEST]...\n","  Resampling sensors (11794 rows)...\n","  → 5898 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R09/rwrist [TEST]...\n","  Resampling sensors (11791 rows)...\n","  → 5896 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R10/rwrist [TEST]...\n","  Resampling sensors (11863 rows)...\n","  → 5932 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R11/rwrist [TEST]...\n","  Resampling sensors (11822 rows)...\n","  → 5911 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R12/rwrist [TEST]...\n","  Resampling sensors (11845 rows)...\n","  → 5923 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R13/rwrist [TEST]...\n","  Resampling sensors (11792 rows)...\n","  → 5897 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R14/rwrist [TEST]...\n","  Resampling sensors (11798 rows)...\n","  → 5900 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R15/rwrist [TEST]...\n","  Resampling sensors (11803 rows)...\n","  → 5902 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S10/R16/rwrist [TEST]...\n","  Resampling sensors (11751 rows)...\n","  → 5876 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R03/rwrist [TRAIN]...\n","  Resampling sensors (11858 rows)...\n","  → 5929 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R04/rwrist [TRAIN]...\n","  Resampling sensors (11809 rows)...\n","  → 5905 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R05/rwrist [TRAIN]...\n","  Resampling sensors (11821 rows)...\n","  → 5911 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R06/rwrist [TRAIN]...\n","  Resampling sensors (11843 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R07/rwrist [TRAIN]...\n","  Resampling sensors (11897 rows)...\n","  → 5949 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R08/rwrist [TRAIN]...\n","  Resampling sensors (11889 rows)...\n","  → 5945 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R09/rwrist [TRAIN]...\n","  Resampling sensors (11906 rows)...\n","  → 5953 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R10/rwrist [TRAIN]...\n","  Resampling sensors (11880 rows)...\n","  → 5940 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R11/rwrist [TRAIN]...\n","  Resampling sensors (11859 rows)...\n","  → 5930 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R12/rwrist [TRAIN]...\n","  Resampling sensors (11885 rows)...\n","  → 5943 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R13/rwrist [TRAIN]...\n","  Resampling sensors (11841 rows)...\n","  → 5921 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S11/R15/rwrist [TRAIN]...\n","  Resampling sensors (11879 rows)...\n","  → 5940 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R11/rwrist [TRAIN]...\n","  Resampling sensors (11894 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R12/rwrist [TRAIN]...\n","  Resampling sensors (2817 rows)...\n","  → 1409 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R13/rwrist [TRAIN]...\n","  Resampling sensors (11905 rows)...\n","  → 5953 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R14/rwrist [TRAIN]...\n","  Resampling sensors (11489 rows)...\n","  → 5745 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R15/rwrist [TRAIN]...\n","  Resampling sensors (11927 rows)...\n","  → 5964 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S12/R16/rwrist [TRAIN]...\n","  Resampling sensors (11843 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R03/rwrist [TRAIN]...\n","  Resampling sensors (11845 rows)...\n","  → 5923 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R04/rwrist [TRAIN]...\n","  Resampling sensors (11878 rows)...\n","  → 5940 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R05/rwrist [TRAIN]...\n","  Resampling sensors (11910 rows)...\n","  → 5956 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R06/rwrist [TRAIN]...\n","  Resampling sensors (11902 rows)...\n","  → 5951 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R07/rwrist [TRAIN]...\n","  Resampling sensors (11903 rows)...\n","  → 5952 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R08/rwrist [TRAIN]...\n","  Resampling sensors (11896 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R09/rwrist [TRAIN]...\n","  Resampling sensors (11910 rows)...\n","  → 5955 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R10/rwrist [TRAIN]...\n","  Resampling sensors (11918 rows)...\n","  → 5959 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R11/rwrist [TRAIN]...\n","  Resampling sensors (11861 rows)...\n","  → 5931 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R12/rwrist [TRAIN]...\n","  Resampling sensors (11912 rows)...\n","  → 5956 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R13/rwrist [TRAIN]...\n","  Resampling sensors (11894 rows)...\n","  → 5948 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R14/rwrist [TRAIN]...\n","  Resampling sensors (11884 rows)...\n","  → 5943 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R15/rwrist [TRAIN]...\n","  Resampling sensors (11892 rows)...\n","  → 5946 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S13/R16/rwrist [TRAIN]...\n","  Resampling sensors (11870 rows)...\n","  → 5935 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R03/rwrist [TRAIN]...\n","  Resampling sensors (11849 rows)...\n","  → 5925 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R04/rwrist [TRAIN]...\n","  Resampling sensors (11760 rows)...\n","  → 5881 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R06/rwrist [TRAIN]...\n","  Resampling sensors (11856 rows)...\n","  → 5928 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R07/rwrist [TRAIN]...\n","  Resampling sensors (11863 rows)...\n","  → 5932 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R08/rwrist [TRAIN]...\n","  Resampling sensors (11921 rows)...\n","  → 5961 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R09/rwrist [TRAIN]...\n","  Resampling sensors (11871 rows)...\n","  → 5936 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R10/rwrist [TRAIN]...\n","  Resampling sensors (11911 rows)...\n","  → 5956 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R11/rwrist [TRAIN]...\n","  Resampling sensors (11843 rows)...\n","  → 5922 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R12/rwrist [TRAIN]...\n","  Resampling sensors (11787 rows)...\n","  → 5894 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R14/rwrist [TRAIN]...\n","  Resampling sensors (11851 rows)...\n","  → 5926 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R15/rwrist [TRAIN]...\n","  Resampling sensors (11823 rows)...\n","  → 5912 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Processing S14/R16/rwrist [TRAIN]...\n","  Resampling sensors (11851 rows)...\n","  → 5926 rows, gap coverage: 0.00%, NaN: 0\n","  Resampling labels...\n","  ✓ Done\n","\n","Successfully processed: 96 sessions\n","Skipped/failed: 0 sessions\n","\n","============================================================\n","2. Combine & Save\n","============================================================\n","Removed old data: data/lara/mbientlab/proc/resampled.parquet\n","✓ Saved: data/lara/mbientlab/proc/resampled.parquet\n","  Total rows: 560,070\n","  # subjects: 8\n","  # sessions: 96\n","  Valid samples: 560,070 (100.0%)\n","  Samples with NaN: 0\n","\n","Data preview:\n","  subject_id session_id placement      time_sec        ax        ay        az         gx         gy         gz  is_in_gap  is_forced_nan  label\n","0        S07        R03    rwrist  1.564739e+09 -0.281438 -0.924541  0.325248   1.414270  24.960403  -9.089332          0              0    6.0\n","1        S07        R03    rwrist  1.564739e+09 -0.298464 -0.935255  0.334803   3.294115  27.049623 -12.293148          0              0    6.0\n","2        S07        R03    rwrist  1.564739e+09 -0.263180 -1.002837  0.456556   2.767826  14.701869 -19.894005          0              0    6.0\n","3        S07        R03    rwrist  1.564739e+09 -0.174738 -1.060583  0.545962   5.506332   5.940687 -22.905399          0              0    6.0\n","4        S07        R03    rwrist  1.564739e+09 -0.125629 -1.083030  0.620053  16.071184  10.412006 -25.808487          0              0    6.0\n","5        S07        R03    rwrist  1.564739e+09 -0.148535 -1.093232  0.639439  25.785658  20.021729 -29.624060          0              0    6.0\n","6        S07        R03    rwrist  1.564739e+09 -0.210257 -1.095191  0.675926  40.849041  25.070871 -35.659191          0              0    6.0\n","7        S07        R03    rwrist  1.564739e+09 -0.222465 -1.095854  0.701550  45.951775  22.461361 -37.891891          0              0    6.0\n","8        S07        R03    rwrist  1.564739e+09 -0.214872 -1.125343  0.729511  57.344635  10.825806 -42.137077          0              0    6.0\n","9        S07        R03    rwrist  1.564739e+09 -0.218806 -1.212497  0.748468  68.151352   9.044179 -45.126099          0              0    6.0\n","\n","Numeric column stats (valid samples):\n","                ax           ay           az           gx           gy  \\\n","count  560070.0000  560070.0000  560070.0000  560070.0000  560070.0000   \n","mean       -0.6569      -0.1522       0.3342      -1.0943       0.6134   \n","std         0.4212       0.5260       0.4697      77.1153      72.2905   \n","min       -11.1498      -9.2003     -27.0872   -4185.1846   -1426.9553   \n","25%        -0.9339      -0.5590       0.1043     -19.9812     -22.0506   \n","50%        -0.7474      -0.0536       0.3487       0.1252      -0.3429   \n","75%        -0.4270       0.2280       0.6074      20.5185      21.7400   \n","max        32.7536      11.5694      14.6606    1110.7982    1697.6891   \n","\n","                gz  \n","count  560070.0000  \n","mean        0.5667  \n","std        72.9628  \n","min     -3227.1677  \n","25%       -19.0527  \n","50%         0.5039  \n","75%        22.0198  \n","max       736.3111  \n","\n","✓ Saved stats: data/lara/mbientlab/proc/resample_stats.csv\n","\n","Gap statistics (train fold):\n","  Mean gap coverage: 0.00%\n","  Max gap coverage: 0.00%\n","  Mean gap time fraction: 0.00%\n","\n","Gap statistics (overall):\n","  Mean gap coverage: 0.00%\n","  Max gap coverage: 0.00%\n","\n","============================================================\n","Step 5 complete - Flawless version\n","============================================================\n","\n","Final fixes:\n","  1. ✓ Prune switch (enabled only when training fold is set)\n","  2. ✓ Label time harmonized (reuse cleaned time)\n","  3. ✓ Complete error information (\"error\" field)\n","  4. ✓ Comment fix (constant threshold 0.15)\n","  5. ✓ Type optimization (float32/float64)\n","============================================================\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","\n","# ========== Manually set FOLD_ID (if the environment variable is not set) ==========\n","if \"FOLD_ID\" not in os.environ:\n","    print(\"⚠️ Environment variable FOLD_ID is not set; please specify it manually:\")\n","    print(\"Hint: if your LOSO has N folds, FOLD_ID should be 0 to N-1\")\n","    fold_input = input(\"Please enter FOLD_ID (press Enter to default to 0): \").strip()\n","    os.environ[\"FOLD_ID\"] = fold_input if fold_input else \"0\"\n","    print(f\"✓ FOLD_ID has been set to {os.environ['FOLD_ID']}\")\n","\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","\"\"\"\n","Step 6: Sensor Preprocessing (top-conf/journal grade - final fixed version)\n","Accelerometer high-pass to remove gravity; gyroscope denoising; adaptive ±Nσ clipping (target 1%)\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import json\n","import os\n","from scipy import signal\n","\n","# ========== Config ==========\n","# Accelerometer high-pass (remove gravity)\n","ACC_HPF_CUTOFF_HZ = 0.3      # Cutoff frequency\n","ACC_HPF_ORDER = 2            # Filter order\n","\n","# Gyroscope low-pass (denoise)\n","GYR_LPF_CUTOFF_HZ = 20.0     # Cutoff frequency\n","GYR_LPF_ORDER = 2            # Filter order\n","\n","# Fix: adaptive clipping threshold (auto-tuned to target clipping rate)\n","TARGET_CLIP_RATE = 0.01      # Target clipping rate 1% (sum of both tails on the train fold)\n","\n","# Sampling rate (from Step 5)\n","SAMPLING_RATE_HZ = 50.0\n","\n","# Unit conversions\n","DEG2RAD = np.pi / 180.0\n","G_TO_MS2 = 9.80665\n","\n","print(\"=\"*60)\n","print(\"Step 6: Sensor Preprocessing\")\n","print(\"=\"*60)\n","\n","# Load data\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","\n","print(f\"\\nLoading resampled data: {proc_dir / 'resampled.parquet'}\")\n","df = pd.read_parquet(proc_dir / \"resampled.parquet\")\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Number of subjects: {df['subject_id'].nunique()}\")\n","print(f\"Number of sessions: {df.groupby(['subject_id', 'session_id'], observed=True).ngroups}\")\n","\n","# ========== 0. Unit normalization ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"0. Unit normalization\")\n","print(\"=\"*60)\n","\n","acc_channels = ['ax', 'ay', 'az']\n","print(f\"\\nAccelerometer unit conversion: g → m/s²\")\n","for ch in acc_channels:\n","    if ch in df.columns:\n","        mask = df[ch].notna()\n","        df.loc[mask, ch] = df.loc[mask, ch] * G_TO_MS2\n","\n","print(f\"✓ Conversion factor: {G_TO_MS2:.5f}\")\n","\n","gyr_channels = ['gx', 'gy', 'gz']\n","print(f\"\\nGyroscope unit conversion: deg/s → rad/s\")\n","for ch in gyr_channels:\n","    if ch in df.columns:\n","        mask = df[ch].notna()\n","        df.loc[mask, ch] = df.loc[mask, ch] * DEG2RAD\n","\n","print(f\"✓ Conversion factor: π/180 = {DEG2RAD:.6f}\")\n","\n","# ========== Helper functions ==========\n","def design_highpass_filter(cutoff_hz, fs_hz, order=2):\n","    \"\"\"Design a high-pass Butterworth filter\"\"\"\n","    nyq = 0.5 * fs_hz\n","    normal_cutoff = cutoff_hz / nyq\n","    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n","    return b, a\n","\n","def design_lowpass_filter(cutoff_hz, fs_hz, order=2):\n","    \"\"\"Design a low-pass Butterworth filter\"\"\"\n","    nyq = 0.5 * fs_hz\n","    normal_cutoff = cutoff_hz / nyq\n","    b, a = signal.butter(order, normal_cutoff, btype='low', analog=False)\n","    return b, a\n","\n","def filtfilt_nan_safe(x, b, a):\n","    \"\"\"Zero-phase filtering tolerant to NaN (filter each contiguous non-NaN run)\"\"\"\n","    y = x.copy()\n","    good = np.isfinite(x)\n","\n","    if not good.any():\n","        return x\n","\n","    idx = np.where(good)[0]\n","    cuts = np.where(np.diff(idx) > 1)[0] + 1\n","    runs = np.split(idx, cuts)\n","\n","    padlen = 3 * (max(len(a), len(b)) - 1)\n","\n","    for run in runs:\n","        seg = x[run]\n","\n","        if len(seg) > padlen:\n","            y[run] = signal.filtfilt(b, a, seg, method=\"pad\")\n","        else:\n","            tmp = signal.lfilter(b, a, seg)\n","            y[run] = signal.lfilter(b, a, tmp[::-1])[::-1]\n","\n","    return y\n","\n","def apply_filter_by_session(df, channels, b, a):\n","    \"\"\"Apply zero-phase filtering grouped by session (polish: add placement grouping + sorting)\"\"\"\n","    filtered_data = []\n","\n","    # Polish 1: include placement grouping; sort by time_sec\n","    for (subj, sess, plc), group in df.groupby(['subject_id', 'session_id', 'placement'], observed=True):\n","        group = group.sort_values('time_sec').copy()\n","\n","        for ch in channels:\n","            if ch not in group.columns:\n","                continue\n","\n","            data = group[ch].values\n","            filtered = filtfilt_nan_safe(data, b, a)\n","            group[ch] = filtered\n","\n","        filtered_data.append(group)\n","\n","    return pd.concat(filtered_data, ignore_index=True)\n","\n","def compute_clip_thresholds_target(df, channels, target_rate=0.01, use_robust=True):\n","    \"\"\"Fix: adapt thresholds to a target clipping rate (Scheme A)\n","\n","    Args:\n","        target_rate: target total clipping rate for both tails (e.g., 0.01 = 1%)\n","        use_robust: if True, use Median±MAD; otherwise Mean±Std\n","    \"\"\"\n","    eps = 1e-6\n","    thresholds = {}\n","\n","    for ch in channels:\n","        if ch not in df.columns:\n","            continue\n","\n","        x = df[ch].dropna().values\n","        if x.size == 0:\n","            continue\n","\n","        if use_robust:\n","            # Robust estimate: Median ± k·(1.4826·MAD)\n","            median = np.median(x)\n","            mad = np.median(np.abs(x - median))\n","            robust_std = max(1.4826 * mad, eps)\n","\n","            # deviations already absolute (two-sided combined); use 1 - target_rate\n","            deviations = np.abs(x - median) / robust_std\n","            k = np.quantile(deviations, 1 - target_rate)\n","\n","            lower = median - k * robust_std\n","            upper = median + k * robust_std\n","\n","            thresholds[ch] = {\n","                'center': float(median),\n","                'scale': float(robust_std),\n","                'k': float(k),\n","                'lower': float(lower),\n","                'upper': float(upper),\n","                'method': f'Median±k·MAD (k={k:.3f}, both tails total {target_rate*100:.1f}%)',\n","            }\n","        else:\n","            # Conventional estimate: Mean ± k·Std\n","            mean = np.mean(x)\n","            std = max(np.std(x), eps)\n","\n","            deviations = np.abs(x - mean) / std\n","            k = np.quantile(deviations, 1 - target_rate)\n","\n","            lower = mean - k * std\n","            upper = mean + k * std\n","\n","            thresholds[ch] = {\n","                'center': float(mean),\n","                'scale': float(std),\n","                'k': float(k),\n","                'lower': float(lower),\n","                'upper': float(upper),\n","                'method': f'Mean±k·Std (k={k:.3f}, both tails total {target_rate*100:.1f}%)',\n","            }\n","\n","    return thresholds\n","\n","def apply_clip(df, channels, thresholds):\n","    \"\"\"Apply clipping and compute actual clipping rate\"\"\"\n","    df_clipped = df.copy()\n","    clip_stats = {}\n","\n","    for ch in channels:\n","        if ch not in df_clipped.columns or ch not in thresholds:\n","            continue\n","\n","        lower = thresholds[ch]['lower']\n","        upper = thresholds[ch]['upper']\n","\n","        mask = df_clipped[ch].notna()\n","        total = mask.sum()\n","\n","        if total > 0:\n","            outliers = ((df_clipped.loc[mask, ch] < lower) | (df_clipped.loc[mask, ch] > upper)).sum()\n","            clip_rate = outliers / total\n","            clip_stats[ch] = {\n","                'outliers': int(outliers),\n","                'total': int(total),\n","                'rate': float(clip_rate),\n","            }\n","\n","        df_clipped.loc[mask, ch] = df_clipped.loc[mask, ch].clip(lower, upper)\n","\n","    return df_clipped, clip_stats\n","\n","# ========== 1. Design filters ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Design filters\")\n","print(\"=\"*60)\n","\n","print(f\"\\nAccelerometer high-pass filter:\")\n","print(f\"  Cutoff frequency: {ACC_HPF_CUTOFF_HZ} Hz\")\n","print(f\"  Order: {ACC_HPF_ORDER}\")\n","acc_b, acc_a = design_highpass_filter(ACC_HPF_CUTOFF_HZ, SAMPLING_RATE_HZ, ACC_HPF_ORDER)\n","\n","print(f\"\\nGyroscope low-pass filter:\")\n","print(f\"  Cutoff frequency: {GYR_LPF_CUTOFF_HZ} Hz\")\n","print(f\"  Order: {GYR_LPF_ORDER}\")\n","gyr_b, gyr_a = design_lowpass_filter(GYR_LPF_CUTOFF_HZ, SAMPLING_RATE_HZ, GYR_LPF_ORDER)\n","\n","# ========== 2. Apply filters (by session + placement) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Apply filters (by session + placement, zero-phase)\")\n","print(\"=\"*60)\n","\n","print(\"\\nApplying accelerometer high-pass (remove gravity)...\")\n","acc_channels = ['ax', 'ay', 'az']\n","df_filtered = apply_filter_by_session(df, acc_channels, acc_b, acc_a)\n","print(\"✓ Done\")\n","\n","print(\"\\nApplying gyroscope low-pass (denoise)...\")\n","df_filtered = apply_filter_by_session(df_filtered, gyr_channels, gyr_b, gyr_a)\n","print(\"✓ Done\")\n","\n","# ========== 3. Compute clipping thresholds (adaptive to target rate) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Compute adaptive clipping thresholds (fix: target clipping rate)\")\n","print(\"=\"*60)\n","\n","splits_path = configs_dir / \"splits.json\"\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","\n","if splits_path.exists() and FOLD_ID >= 0:\n","    with open(splits_path, \"r\") as f:\n","        splits = json.load(f)\n","    train_subjects = set(splits[str(FOLD_ID)][\"train_subjects\"])\n","    df_for_stats = df_filtered[df_filtered[\"subject_id\"].isin(train_subjects)]\n","    print(f\"Estimate clipping thresholds on training fold: FOLD_ID={FOLD_ID}\")\n","    print(f\"  Number of train subjects: {len(train_subjects)}\")\n","    print(f\"  Target clip rate: {TARGET_CLIP_RATE*100:.1f}%\")\n","else:\n","    df_for_stats = df_filtered\n","    print(\"Estimate clipping thresholds on all data\")\n","    print(f\"  Target clip rate: {TARGET_CLIP_RATE*100:.1f}%\")\n","\n","all_channels = acc_channels + gyr_channels\n","clip_thresholds = compute_clip_thresholds_target(\n","    df_for_stats, all_channels, TARGET_CLIP_RATE, use_robust=True\n",")\n","\n","print(f\"\\nClipping thresholds (adaptive robust estimation):\")\n","for ch, thresh in clip_thresholds.items():\n","    print(f\"  {ch}:\")\n","    print(f\"    center: {thresh['center']:.4f}\")\n","    print(f\"    scale: {thresh['scale']:.4f}\")\n","    print(f\"    k: {thresh['k']:.3f}\")\n","    print(f\"    range: [{thresh['lower']:.4f}, {thresh['upper']:.4f}]\")\n","\n","# ========== 4. Apply clipping ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Apply adaptive clipping\")\n","print(\"=\"*60)\n","\n","df_clipped, clip_stats = apply_clip(df_filtered, all_channels, clip_thresholds)\n","\n","print(\"\\nActual clipping statistics:\")\n","for ch, stats in clip_stats.items():\n","    print(f\"  {ch}: {stats['outliers']:,} / {stats['total']:,} ({stats['rate']*100:.2f}%)\")\n","\n","# ========== 5. Cast to float32 to save memory ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Data type optimization\")\n","print(\"=\"*60)\n","\n","numeric_cols = ['ax', 'ay', 'az', 'gx', 'gy', 'gz']\n","for col in numeric_cols:\n","    if col in df_clipped.columns:\n","        df_clipped[col] = df_clipped[col].astype('float32')\n","\n","print(f\"✓ Sensor columns cast to float32\")\n","print(f\"✓ time_sec kept as float64\")\n","\n","# ========== 6. Save results ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Save results\")\n","print(\"=\"*60)\n","\n","output_file = proc_dir / \"filtered.parquet\"\n","\n","if output_file.exists():\n","    import shutil\n","    if output_file.is_dir():\n","        shutil.rmtree(output_file)\n","    else:\n","        output_file.unlink()\n","    print(f\"Removed old data: {output_file}\")\n","\n","df_clipped.to_parquet(\n","    output_file,\n","    index=False,\n","    partition_cols=['subject_id', 'placement'],\n","    engine='pyarrow'\n",")\n","print(f\"✓ Saved: {output_file}\")\n","print(f\"  Data shape: {df_clipped.shape}\")\n","\n","print(\"\\nData preview:\")\n","print(df_clipped.head(10).to_string())\n","\n","print(\"\\nPost-filter numeric column stats:\")\n","valid_mask = df_clipped[['ax', 'ay', 'az', 'gx', 'gy', 'gz']].notna().all(axis=1)\n","print(df_clipped.loc[valid_mask, ['ax', 'ay', 'az', 'gx', 'gy', 'gz']].describe().round(4))\n","\n","# ========== 7. Save filter configuration ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"7. Save filter configuration\")\n","print(\"=\"*60)\n","\n","filter_config = {\n","    'sampling_rate_hz': SAMPLING_RATE_HZ,\n","\n","    'units': {\n","        'accelerometer': 'm/s² (converted from g)',\n","        'gyroscope': 'rad/s (converted from deg/s)',\n","        'conversion': {\n","            'accelerometer_g_to_ms2': G_TO_MS2,\n","            'gyroscope_deg_to_rad': DEG2RAD,\n","        }\n","    },\n","\n","    'dtypes': {\n","        'sensor_channels': 'float32',\n","        'time_sec': 'float64',\n","    },\n","\n","    'accelerometer': {\n","        'filter_type': 'highpass',\n","        'purpose': 'detrend (remove gravity)',\n","        'method': 'Butterworth',\n","        'cutoff_hz': ACC_HPF_CUTOFF_HZ,\n","        'order': ACC_HPF_ORDER,\n","        'coefficients': {\n","            'b': acc_b.tolist(),\n","            'a': acc_a.tolist(),\n","        },\n","        'zero_phase': True,\n","    },\n","\n","    'gyroscope': {\n","        'filter_type': 'lowpass',\n","        'purpose': 'denoise',\n","        'method': 'Butterworth',\n","        'cutoff_hz': GYR_LPF_CUTOFF_HZ,\n","        'order': GYR_LPF_ORDER,\n","        'coefficients': {\n","            'b': gyr_b.tolist(),\n","            'a': gyr_a.tolist(),\n","        },\n","        'zero_phase': True,\n","    },\n","\n","    'clipping': {\n","        'method': 'Adaptive robust estimation (Median±k·MAD, Scheme A)',\n","        'target_clip_rate': TARGET_CLIP_RATE,\n","        'estimated_on': 'train_fold' if FOLD_ID >= 0 else 'all_data',\n","        'fold_id': FOLD_ID if FOLD_ID >= 0 else None,\n","        'thresholds': clip_thresholds,\n","        'actual_clip_stats': clip_stats,  # Polish 2: record actual clipping rate\n","        'rationale': f'Auto-adjust k so the training-fold clipping rate reaches the target {TARGET_CLIP_RATE*100:.1f}%',\n","    },\n","\n","    'notes': [\n","        'All filters use filtfilt for zero phase',\n","        'Filtering is grouped by session + placement, sorted by time_sec; avoid crossing session boundaries',\n","        'filtfilt_nan_safe filters each contiguous non-NaN run separately',\n","        'Accelerometer converted from g to m/s² (×9.80665)',\n","        'Gyroscope converted from deg/s to rad/s (×π/180)',\n","        f'Adaptive clipping thresholds: determine k on the training fold so clipping ≈ {TARGET_CLIP_RATE*100:.1f}%, then apply consistently to all data',\n","        'NaNs remain unchanged',\n","        'Sensor columns are float32; time_sec is float64',\n","    ]\n","}\n","\n","filter_config_file = configs_dir / \"filter.yaml\"\n","with open(filter_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(filter_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","print(f\"✓ Saved filter configuration: {filter_config_file}\")\n","\n","filter_config_json = configs_dir / \"filter.json\"\n","with open(filter_config_json, 'w', encoding='utf-8') as f:\n","    json.dump(filter_config, f, indent=2)\n","\n","print(f\"✓ Saved filter configuration: {filter_config_json}\")\n","\n","# ========== 8. Summary ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 6 complete - Sensor preprocessing (final fixed version)\")\n","print(\"=\"*60)\n","print(f\"\\nConfig:\")\n","print(f\"  Units: Acc g→m/s², Gyro deg/s→rad/s\")\n","print(f\"  Accelerometer: high-pass {ACC_HPF_CUTOFF_HZ} Hz (remove gravity)\")\n","print(f\"  Gyroscope: low-pass {GYR_LPF_CUTOFF_HZ} Hz (denoise)\")\n","print(f\"  Clipping: adaptive ±k·MAD (target {TARGET_CLIP_RATE*100:.1f}%)\")\n","if FOLD_ID >= 0:\n","    print(f\"  Clipping thresholds: based on training fold (FOLD_ID={FOLD_ID})\")\n","print(f\"\\nResults:\")\n","print(f\"  Output file: {output_file}\")\n","print(f\"  Config file: {filter_config_file}\")\n","print(f\"  Data shape: {df_clipped.shape}\")\n","print(\"\\nFinal fixes:\")\n","print(f\"  ✓ Adaptive clipping thresholds (Scheme A)\")\n","print(f\"  ✓ Target clipping rate {TARGET_CLIP_RATE*100:.1f}%, auto-solve k\")\n","print(f\"  ✓ Polish 1: group by placement + sort by time_sec\")\n","print(f\"  ✓ Polish 2: write actual clipping rate into config\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mkcnMDnBrQpu","executionInfo":{"status":"ok","timestamp":1763117591133,"user_tz":0,"elapsed":2111,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"391b3dfd-7528-4155-8778-cbd97a7f4221"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 6: Sensor Preprocessing\n","============================================================\n","\n","Loading resampled data: data/lara/mbientlab/proc/resampled.parquet\n","Data shape: (560070, 13)\n","Number of subjects: 8\n","Number of sessions: 96\n","\n","============================================================\n","0. Unit normalization\n","============================================================\n","\n","Accelerometer unit conversion: g → m/s²\n","✓ Conversion factor: 9.80665\n","\n","Gyroscope unit conversion: deg/s → rad/s\n","✓ Conversion factor: π/180 = 0.017453\n","\n","============================================================\n","1. Design filters\n","============================================================\n","\n","Accelerometer high-pass filter:\n","  Cutoff frequency: 0.3 Hz\n","  Order: 2\n","\n","Gyroscope low-pass filter:\n","  Cutoff frequency: 20.0 Hz\n","  Order: 2\n","\n","============================================================\n","2. Apply filters (by session + placement, zero-phase)\n","============================================================\n","\n","Applying accelerometer high-pass (remove gravity)...\n","✓ Done\n","\n","Applying gyroscope low-pass (denoise)...\n","✓ Done\n","\n","============================================================\n","3. Compute adaptive clipping thresholds (fix: target clipping rate)\n","============================================================\n","Estimate clipping thresholds on training fold: FOLD_ID=3\n","  Number of train subjects: 7\n","  Target clip rate: 1.0%\n","\n","Clipping thresholds (adaptive robust estimation):\n","  ax:\n","    center: 0.0118\n","    scale: 1.3587\n","    k: 6.825\n","    range: [-9.2622, 9.2858]\n","  ay:\n","    center: 0.0147\n","    scale: 1.3815\n","    k: 5.866\n","    range: [-8.0895, 8.1189]\n","  az:\n","    center: 0.0010\n","    scale: 1.3351\n","    k: 6.062\n","    range: [-8.0917, 8.0937]\n","  gx:\n","    center: 0.0018\n","    scale: 0.5216\n","    k: 6.583\n","    range: [-3.4318, 3.4354]\n","  gy:\n","    center: -0.0076\n","    scale: 0.5632\n","    k: 8.532\n","    range: [-4.8128, 4.7975]\n","  gz:\n","    center: 0.0081\n","    scale: 0.5192\n","    k: 7.778\n","    range: [-4.0302, 4.0464]\n","\n","============================================================\n","4. Apply adaptive clipping\n","============================================================\n","\n","Actual clipping statistics:\n","  ax: 5,239 / 560,070 (0.94%)\n","  ay: 5,285 / 560,070 (0.94%)\n","  az: 5,325 / 560,070 (0.95%)\n","  gx: 5,337 / 560,070 (0.95%)\n","  gy: 5,433 / 560,070 (0.97%)\n","  gz: 5,139 / 560,070 (0.92%)\n","\n","============================================================\n","5. Data type optimization\n","============================================================\n","✓ Sensor columns cast to float32\n","✓ time_sec kept as float64\n","\n","============================================================\n","6. Save results\n","============================================================\n","Removed old data: data/lara/mbientlab/proc/filtered.parquet\n","✓ Saved: data/lara/mbientlab/proc/filtered.parquet\n","  Data shape: (560070, 13)\n","\n","Data preview:\n","  session_id      time_sec        ax        ay        az        gx        gy        gz  is_in_gap  is_forced_nan  label subject_id placement\n","0        R03  1.564739e+09  0.891079 -2.660401  0.238257  0.025291  0.435949 -0.158735          0              0    6.0        S07    rwrist\n","1        R03  1.564739e+09  0.715069 -2.840439  0.239453  0.059089  0.468073 -0.219117          0              0    6.0        S07    rwrist\n","2        R03  1.564739e+09  1.051161 -3.582292  1.341616  0.042692  0.260707 -0.339956          0              0    6.0        S07    rwrist\n","3        R03  1.564739e+09  1.907651 -4.231893  2.127376  0.107476  0.100950 -0.407764          0              0    6.0        S07    rwrist\n","4        R03  1.564739e+09  2.377473 -4.539616  2.763897  0.263063  0.184827 -0.442223          0              0    6.0        S07    rwrist\n","5        R03  1.564739e+09  2.140075 -4.731589  2.865023  0.472918  0.345074 -0.525765          0              0    6.0        S07    rwrist\n","6        R03  1.564739e+09  1.521002 -4.847136  3.135042  0.686333  0.442941 -0.613507          0              0    6.0        S07    rwrist\n","7        R03  1.564739e+09  1.386402 -4.954428  3.299844  0.829848  0.386559 -0.669225          0              0    6.0        S07    rwrist\n","8        R03  1.564739e+09  1.444855 -5.348888  3.488989  0.973541  0.191755 -0.729564          0              0    6.0        S07    rwrist\n","9        R03  1.564739e+09  1.389077 -6.313359  3.591348  1.214971  0.159118 -0.790919          0              0    6.0        S07    rwrist\n","\n","Post-filter numeric column stats:\n","                ax           ay           az           gx           gy  \\\n","count  560070.0000  560070.0000  560070.0000  560070.0000  560070.0000   \n","mean       -0.0088       0.0068       0.0014      -0.0024       0.0101   \n","std         2.2816       2.1497       2.0919       0.8876       1.1620   \n","min        -9.2622      -8.0895      -8.0917      -3.4318      -4.8128   \n","25%        -0.9098      -0.9143      -0.9075      -0.3482      -0.3829   \n","50%         0.0152       0.0141       0.0019       0.0021      -0.0064   \n","75%         0.9153       0.9493       0.9042       0.3572       0.3771   \n","max         9.2858       8.1189       8.0937       3.4354       4.7975   \n","\n","                gz  \n","count  560070.0000  \n","mean        0.0232  \n","std         0.9906  \n","min        -4.0302  \n","25%        -0.3321  \n","50%         0.0088  \n","75%         0.3837  \n","max         4.0464  \n","\n","============================================================\n","7. Save filter configuration\n","============================================================\n","✓ Saved filter configuration: configs/filter.yaml\n","✓ Saved filter configuration: configs/filter.json\n","\n","============================================================\n","Step 6 complete - Sensor preprocessing (final fixed version)\n","============================================================\n","\n","Config:\n","  Units: Acc g→m/s², Gyro deg/s→rad/s\n","  Accelerometer: high-pass 0.3 Hz (remove gravity)\n","  Gyroscope: low-pass 20.0 Hz (denoise)\n","  Clipping: adaptive ±k·MAD (target 1.0%)\n","  Clipping thresholds: based on training fold (FOLD_ID=3)\n","\n","Results:\n","  Output file: data/lara/mbientlab/proc/filtered.parquet\n","  Config file: configs/filter.yaml\n","  Data shape: (560070, 13)\n","\n","Final fixes:\n","  ✓ Adaptive clipping thresholds (Scheme A)\n","  ✓ Target clipping rate 1.0%, auto-solve k\n","  ✓ Polish 1: group by placement + sort by time_sec\n","  ✓ Polish 2: write actual clipping rate into config\n","============================================================\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","\n","# ========== Manually set FOLD_ID (if the environment variable is not set) ==========\n","if \"FOLD_ID\" not in os.environ:\n","    print(\"⚠️ Environment variable FOLD_ID is not set; please specify it manually:\")\n","    print(\"Hint: If your LOSO has N folds, FOLD_ID should be 0 to N-1\")\n","    fold_input = input(\"Please enter FOLD_ID (press Enter to default to 0): \").strip()\n","    os.environ[\"FOLD_ID\"] = fold_input if fold_input else \"0\"\n","    print(f\"✓ FOLD_ID has been set to {os.environ['FOLD_ID']}\")\n","\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","\"\"\"\n","Step 7: Coordinate/Magnitude Normalization (top-conf/journal grade)\n","Compute magnitude channels; z-score standardization (train-only statistics)\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import json\n","import os\n","import pickle\n","\n","# ========== Config ==========\n","EPSILON = 1e-8  # Prevent division by zero\n","\n","print(\"=\"*60)\n","print(\"Step 7: Coordinate/Magnitude Normalization\")\n","print(\"=\"*60)\n","\n","# Load data\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","\n","print(f\"\\nLoading filtered data: {proc_dir / 'filtered.parquet'}\")\n","df = pd.read_parquet(proc_dir / \"filtered.parquet\")\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Number of subjects: {df['subject_id'].nunique()}\")\n","print(f\"Number of sessions: {df.groupby(['subject_id', 'session_id'], observed=True).ngroups}\")\n","\n","# ========== 1. Compute derived channels (magnitude) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Compute derived channels (magnitude)\")\n","print(\"=\"*60)\n","\n","# Accelerometer magnitude\n","print(\"\\nComputing acc_mag = sqrt(ax² + ay² + az²)...\")\n","df['acc_mag'] = np.sqrt(\n","    df['ax'].values**2 +\n","    df['ay'].values**2 +\n","    df['az'].values**2\n",").astype('float32')\n","\n","# Gyroscope magnitude\n","print(\"Computing gyr_mag = sqrt(gx² + gy² + gz²)...\")\n","df['gyr_mag'] = np.sqrt(\n","    df['gx'].values**2 +\n","    df['gy'].values**2 +\n","    df['gz'].values**2\n",").astype('float32')\n","\n","print(f\"✓ Added derived channels: acc_mag, gyr_mag\")\n","\n","# Show derived-channel stats\n","print(\"\\nDerived channel statistics (post-filter):\")\n","for col in ['acc_mag', 'gyr_mag']:\n","    valid_data = df[col].dropna()\n","    if len(valid_data) > 0:\n","        print(f\"  {col}:\")\n","        print(f\"    Mean: {valid_data.mean():.4f}\")\n","        print(f\"    Std: {valid_data.std():.4f}\")\n","        print(f\"    Range: [{valid_data.min():.4f}, {valid_data.max():.4f}]\")\n","\n","# ========== 2. Determine training set ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Determine training set\")\n","print(\"=\"*60)\n","\n","# Check whether to standardize per fold\n","splits_path = configs_dir / \"splits.json\"\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","\n","if splits_path.exists() and FOLD_ID >= 0:\n","    with open(splits_path, \"r\") as f:\n","        splits = json.load(f)\n","    train_subjects = set(splits[str(FOLD_ID)][\"train_subjects\"])\n","    test_subjects = set(splits[str(FOLD_ID)][\"test_subjects\"])\n","\n","    train_mask = df[\"subject_id\"].isin(train_subjects)\n","    df_train = df[train_mask]\n","\n","    print(f\"Compute statistics on training fold: FOLD_ID={FOLD_ID}\")\n","    print(f\"  Number of train subjects: {len(train_subjects)}\")\n","    print(f\"  Number of test subjects: {len(test_subjects)}\")\n","    print(f\"  Train samples: {len(df_train):,}\")\n","    print(f\"  Total samples: {len(df):,}\")\n","else:\n","    df_train = df\n","    train_subjects = set(df['subject_id'].unique())\n","    test_subjects = set()\n","    FOLD_ID = -1\n","    print(\"Compute statistics on all data (no fold split)\")\n","    print(f\"  Samples: {len(df):,}\")\n","\n","# ========== 3. Compute z-score parameters (train-only) ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Compute z-score parameters (train-only)\")\n","print(\"=\"*60)\n","\n","# Channels to standardize\n","channels_to_normalize = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","# Compute mean and std (train-only valid data)\n","scaler_params = {}\n","\n","print(\"\\nz-score parameters (train set):\")\n","for ch in channels_to_normalize:\n","    if ch not in df_train.columns:\n","        continue\n","\n","    # Use non-NaN values only\n","    valid_data = df_train[ch].dropna().values\n","\n","    if len(valid_data) > 0:\n","        mean = float(np.mean(valid_data))\n","        std = float(np.std(valid_data))\n","\n","        # Guard against zero std\n","        if std < EPSILON:\n","            std = 1.0\n","\n","        scaler_params[ch] = {\n","            'mean': mean,\n","            'std': std,\n","        }\n","\n","        print(f\"  {ch}:\")\n","        print(f\"    Mean: {mean:.6f}\")\n","        print(f\"    Std: {std:.6f}\")\n","\n","# ========== 4. Apply z-score standardization ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Apply z-score standardization\")\n","print(\"=\"*60)\n","\n","df_normalized = df.copy()\n","\n","for ch in channels_to_normalize:\n","    if ch not in scaler_params:\n","        continue\n","\n","    mean = scaler_params[ch]['mean']\n","    std = scaler_params[ch]['std']\n","\n","    # Standardize non-NaN values only; cast to float32 to avoid warnings\n","    mask = df_normalized[ch].notna()\n","    normalized_values = ((df_normalized.loc[mask, ch] - mean) / (std + EPSILON)).astype('float32')\n","    df_normalized.loc[mask, ch] = normalized_values\n","\n","print(f\"✓ Standardized {len(scaler_params)} channels\")\n","\n","# Show post-standardization stats\n","print(\"\\nPost-standardization stats (train set):\")\n","for ch in channels_to_normalize:\n","    if ch not in scaler_params:\n","        continue\n","\n","    if FOLD_ID >= 0:\n","        valid_data = df_normalized.loc[train_mask, ch].dropna()\n","    else:\n","        valid_data = df_normalized[ch].dropna()\n","\n","    if len(valid_data) > 0:\n","        print(f\"  {ch}:\")\n","        print(f\"    Mean: {valid_data.mean():.6f} (should be near 0)\")\n","        print(f\"    Std: {valid_data.std():.6f} (should be near 1)\")\n","\n","# ========== 5. Save results ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Save results\")\n","print(\"=\"*60)\n","\n","# Save normalized data\n","output_file = proc_dir / \"normalized.parquet\"\n","\n","# Delete existing directory (avoid duplicate appends)\n","if output_file.exists():\n","    import shutil\n","    shutil.rmtree(output_file)\n","    print(f\"Removed old data: {output_file}\")\n","\n","df_normalized.to_parquet(\n","    output_file,\n","    index=False,\n","    partition_cols=['subject_id', 'placement'],\n","    engine='pyarrow'\n",")\n","print(f\"✓ Saved: {output_file}\")\n","print(f\"  Data shape: {df_normalized.shape}\")\n","\n","# Show data preview\n","print(\"\\nData preview:\")\n","display_cols = ['subject_id', 'session_id', 'ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag', 'label']\n","available_cols = [c for c in display_cols if c in df_normalized.columns]\n","print(df_normalized[available_cols].head(10).to_string())\n","\n","# Post-standardization numeric stats (overall)\n","print(\"\\nPost-standardization numeric column stats (overall):\")\n","numeric_cols = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","valid_mask = df_normalized[numeric_cols].notna().all(axis=1)\n","print(df_normalized.loc[valid_mask, numeric_cols].describe().round(4))\n","\n","# ========== 6. Save scaler parameters ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Save scaler parameters\")\n","print(\"=\"*60)\n","\n","scaler_info = {\n","    'fold_id': FOLD_ID,\n","    'epsilon': EPSILON,\n","    'train_subjects': sorted(list(train_subjects)),\n","    'test_subjects': sorted(list(test_subjects)) if test_subjects else None,\n","    'channels': channels_to_normalize,\n","    'params': scaler_params,\n","    'notes': [\n","        'z-score standardization: (x - mean) / (std + ε)',\n","        'Mean and std computed from the training set only',\n","        'If std < ε, set std = 1.0 to avoid divide-by-zero',\n","        'NaN values are excluded from stats and remain NaN after normalization',\n","    ]\n","}\n","\n","# Save as pickle (per-fold supported)\n","if FOLD_ID >= 0:\n","    scaler_file = proc_dir / f\"standardization_fold{FOLD_ID}.pkl\"\n","else:\n","    scaler_file = proc_dir / \"standardization.pkl\"\n","\n","with open(scaler_file, 'wb') as f:\n","    pickle.dump(scaler_info, f)\n","\n","print(f\"✓ Saved scaler: {scaler_file}\")\n","\n","# Also save as JSON (human-readable)\n","if FOLD_ID >= 0:\n","    scaler_json = proc_dir / f\"standardization_fold{FOLD_ID}.json\"\n","else:\n","    scaler_json = proc_dir / \"standardization.json\"\n","\n","with open(scaler_json, 'w') as f:\n","    json.dump(scaler_info, f, indent=2)\n","\n","print(f\"✓ Saved scaler: {scaler_json}\")\n","\n","# ========== 7. Validate standardization ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"7. Validate standardization\")\n","print(\"=\"*60)\n","\n","# Check whether train set is ~0 mean and ~1 std\n","if FOLD_ID >= 0:\n","    print(\"\\nTrain set validation:\")\n","    for ch in channels_to_normalize[:3]:  # check first 3 channels only\n","        if ch in scaler_params:\n","            valid_data = df_normalized.loc[train_mask, ch].dropna()\n","            if len(valid_data) > 0:\n","                mean_check = valid_data.mean()\n","                std_check = valid_data.std()\n","                print(f\"  {ch}: mean={mean_check:.6f}, std={std_check:.6f}\")\n","\n","    # Check that test set uses train-set statistics (should not be 0/1)\n","    print(\"\\nTest set validation (should use train-set statistics; not 0/1):\")\n","    test_mask = df[\"subject_id\"].isin(test_subjects)\n","    for ch in channels_to_normalize[:3]:\n","        if ch in scaler_params:\n","            valid_data = df_normalized.loc[test_mask, ch].dropna()\n","            if len(valid_data) > 0:\n","                mean_check = valid_data.mean()\n","                std_check = valid_data.std()\n","                print(f\"  {ch}: mean={mean_check:.6f}, std={std_check:.6f}\")\n","\n","# ========== 8. Summary ==========\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 7 complete - Coordinate/Magnitude Normalization\")\n","print(\"=\"*60)\n","print(f\"\\nConfig:\")\n","print(f\"  Method: z-score standardization\")\n","print(f\"  ε (avoid divide-by-zero): {EPSILON}\")\n","print(f\"  Standardized channels: {len(scaler_params)}\")\n","if FOLD_ID >= 0:\n","    print(f\"  Train fold: FOLD_ID={FOLD_ID}\")\n","    print(f\"  Train subjects: {len(train_subjects)}\")\n","    print(f\"  Test subjects: {len(test_subjects)}\")\n","print(f\"\\nResults:\")\n","print(f\"  Output data: {output_file}\")\n","print(f\"  Scaler (pkl): {scaler_file}\")\n","print(f\"  Scaler (json): {scaler_json}\")\n","print(f\"  Data shape: {df_normalized.shape}\")\n","print(f\"  New columns: acc_mag, gyr_mag\")\n","print(\"\\nRigor guarantees:\")\n","print(\"  1. ✓ Mean/std computed from training set only\")\n","print(\"  2. ✓ Test set standardized using training-set statistics\")\n","print(\"  3. ✓ ε={} prevents divide-by-zero\".format(EPSILON))\n","print(\"  4. ✓ NaNs remain unchanged\")\n","print(\"  5. ✓ Scaler saved independently per fold\")\n","print(\"  6. ✓ Derived channels acc_mag, gyr_mag\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C84KFeVPrTg_","executionInfo":{"status":"ok","timestamp":1763117591818,"user_tz":0,"elapsed":675,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"e7af4f38-1caf-457d-886e-c5a6f7bbda9f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 7: Coordinate/Magnitude Normalization\n","============================================================\n","\n","Loading filtered data: data/lara/mbientlab/proc/filtered.parquet\n","Data shape: (560070, 13)\n","Number of subjects: 8\n","Number of sessions: 96\n","\n","============================================================\n","1. Compute derived channels (magnitude)\n","============================================================\n","\n","Computing acc_mag = sqrt(ax² + ay² + az²)...\n","Computing gyr_mag = sqrt(gx² + gy² + gz²)...\n","✓ Added derived channels: acc_mag, gyr_mag\n","\n","Derived channel statistics (post-filter):\n","  acc_mag:\n","    Mean: 2.9184\n","    Std: 2.3858\n","    Range: [0.0073, 14.7530]\n","  gyr_mag:\n","    Mean: 1.3136\n","    Std: 1.1815\n","    Range: [0.0015, 7.1651]\n","\n","============================================================\n","2. Determine training set\n","============================================================\n","Compute statistics on training fold: FOLD_ID=3\n","  Number of train subjects: 7\n","  Number of test subjects: 1\n","  Train samples: 477,407\n","  Total samples: 560,070\n","\n","============================================================\n","3. Compute z-score parameters (train-only)\n","============================================================\n","\n","z-score parameters (train set):\n","  ax:\n","    Mean: -0.009468\n","    Std: 2.317761\n","  ay:\n","    Mean: 0.008701\n","    Std: 2.177679\n","  az:\n","    Mean: 0.000053\n","    Std: 2.101021\n","  gx:\n","    Mean: -0.003394\n","    Std: 0.901434\n","  gy:\n","    Mean: 0.009096\n","    Std: 1.170688\n","  gz:\n","    Mean: 0.023848\n","    Std: 1.001829\n","  acc_mag:\n","    Mean: 2.943414\n","    Std: 2.421790\n","  gyr_mag:\n","    Mean: 1.321598\n","    Std: 1.200333\n","\n","============================================================\n","4. Apply z-score standardization\n","============================================================\n","✓ Standardized 8 channels\n","\n","Post-standardization stats (train set):\n","  ax:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999807 (should be near 1)\n","  ay:\n","    Mean: -0.000000 (should be near 0)\n","    Std: 0.999854 (should be near 1)\n","  az:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999846 (should be near 1)\n","  gx:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999826 (should be near 1)\n","  gy:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999756 (should be near 1)\n","  gz:\n","    Mean: -0.000000 (should be near 0)\n","    Std: 0.999795 (should be near 1)\n","  acc_mag:\n","    Mean: -0.000000 (should be near 0)\n","    Std: 0.999934 (should be near 1)\n","  gyr_mag:\n","    Mean: 0.000000 (should be near 0)\n","    Std: 0.999931 (should be near 1)\n","\n","============================================================\n","5. Save results\n","============================================================\n","Removed old data: data/lara/mbientlab/proc/normalized.parquet\n","✓ Saved: data/lara/mbientlab/proc/normalized.parquet\n","  Data shape: (560070, 15)\n","\n","Data preview:\n","  subject_id session_id        ax        ay        az        gx        gy        gz   acc_mag   gyr_mag  label\n","0        S07        R03  0.388542 -1.225664  0.113375  0.031822  0.364617 -0.182250 -0.052710 -0.713936    6.0\n","1        S07        R03  0.312602 -1.308338  0.113944  0.069315  0.392058 -0.242521 -0.001890 -0.667656    6.0\n","2        S07        R03  0.457609 -1.649001  0.638529  0.051125  0.214926 -0.363140  0.422688 -0.742346    6.0\n","3        S07        R03  0.827143 -1.947300  1.012519  0.122993  0.078462 -0.430824  0.893073 -0.739789    6.0\n","4        S07        R03  1.029848 -2.088608  1.315477  0.295592  0.150109 -0.465220  1.188758 -0.645536    6.0\n","5        S07        R03  0.927422 -2.176763  1.363608  0.528394  0.286992 -0.548609  1.233608 -0.445487    6.0\n","6        S07        R03  0.660323 -2.229823  1.492126  0.765144  0.370590 -0.636191  1.249584 -0.249939    6.0\n","7        S07        R03  0.602249 -2.279091  1.570565  0.924352  0.322429 -0.691808  1.308395 -0.156295    6.0\n","8        S07        R03  0.627469 -2.460230  1.660591  1.083756  0.156027 -0.752037  1.488238 -0.074985    6.0\n","9        S07        R03  0.603404 -2.903119  1.709309  1.351585  0.128149 -0.813280  1.838133  0.113998    6.0\n","\n","Post-standardization numeric column stats (overall):\n","                ax           ay           az           gx           gy  \\\n","count  560070.0000  560070.0000  560070.0000  560070.0000  560070.0000   \n","mean        0.0003      -0.0009       0.0007       0.0012       0.0009   \n","std         0.9844       0.9872       0.9957       0.9846       0.9925   \n","min        -3.9921      -3.7187      -3.8514      -3.8033      -4.1188   \n","25%        -0.3884      -0.4238      -0.4320      -0.3825      -0.3349   \n","50%         0.0106       0.0025       0.0009       0.0061      -0.0132   \n","75%         0.3990       0.4319       0.4303       0.4000       0.3144   \n","max         4.0105       3.7243       3.8523       3.8148       4.0903   \n","\n","                gz      acc_mag      gyr_mag  \n","count  560070.0000  560070.0000  560070.0000  \n","mean       -0.0006      -0.0103      -0.0067  \n","std         0.9888       0.9852       0.9843  \n","min        -4.0466      -1.2124      -1.0998  \n","25%        -0.3553      -0.7140      -0.7220  \n","50%        -0.0150      -0.3013      -0.3263  \n","75%         0.3592       0.3925       0.3937  \n","max         4.0152       4.8764       4.8682  \n","\n","============================================================\n","6. Save scaler parameters\n","============================================================\n","✓ Saved scaler: data/lara/mbientlab/proc/standardization_fold3.pkl\n","✓ Saved scaler: data/lara/mbientlab/proc/standardization_fold3.json\n","\n","============================================================\n","7. Validate standardization\n","============================================================\n","\n","Train set validation:\n","  ax: mean=0.000000, std=0.999807\n","  ay: mean=-0.000000, std=0.999854\n","  az: mean=0.000000, std=0.999846\n","\n","Test set validation (should use train-set statistics; not 0/1):\n","  ax: mean=0.001908, std=0.890628\n","  ay: mean=-0.005810, std=0.911284\n","  az: mean=0.004463, std=0.971900\n","\n","============================================================\n","Step 7 complete - Coordinate/Magnitude Normalization\n","============================================================\n","\n","Config:\n","  Method: z-score standardization\n","  ε (avoid divide-by-zero): 1e-08\n","  Standardized channels: 8\n","  Train fold: FOLD_ID=3\n","  Train subjects: 7\n","  Test subjects: 1\n","\n","Results:\n","  Output data: data/lara/mbientlab/proc/normalized.parquet\n","  Scaler (pkl): data/lara/mbientlab/proc/standardization_fold3.pkl\n","  Scaler (json): data/lara/mbientlab/proc/standardization_fold3.json\n","  Data shape: (560070, 15)\n","  New columns: acc_mag, gyr_mag\n","\n","Rigor guarantees:\n","  1. ✓ Mean/std computed from training set only\n","  2. ✓ Test set standardized using training-set statistics\n","  3. ✓ ε=1e-08 prevents divide-by-zero\n","  4. ✓ NaNs remain unchanged\n","  5. ✓ Scaler saved independently per fold\n","  6. ✓ Derived channels acc_mag, gyr_mag\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\n","\"\"\"\n","Step 8: Label Alignment & Cleaning (top-conf/journal grade - revised)\n","Clean NULL/transition, unify to a standard label set, and record mappings\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import json\n","from collections import Counter\n","\n","# ========== Config ==========\n","\n","# Label cleaning strategy\n","NULL_STRATEGY = \"remove\"  # \"remove\" or \"merge_to_transition\"\n","TRANSITION_STRATEGY = \"merge_to_nearest\"  # \"remove\" or \"merge_to_nearest\"\n","\n","# Unmapped label threshold (abort if exceeded)\n","UNMAPPED_THRESHOLD = 0.01  # 1%\n","\n","print(\"=\"*60)\n","print(\"Step 8: Label Alignment & Cleaning\")\n","print(\"=\"*60)\n","\n","# Create directories\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","reports_dir = Path(\"reports\")\n","reports_dir.mkdir(parents=True, exist_ok=True)\n","\n","print(f\"\\nLoading normalized data: {proc_dir / 'normalized.parquet'}\")\n","df = pd.read_parquet(proc_dir / \"normalized.parquet\")\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Number of subjects: {df['subject_id'].nunique()}\")\n","\n","# ========== 1. Analyze original label distribution ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Analyze original label distribution\")\n","print(\"=\"*60)\n","\n","# Count all labels\n","label_counts = df['label'].value_counts(dropna=False)\n","total_samples = len(df)\n","null_count = df['label'].isna().sum()\n","\n","print(f\"\\nOriginal label stats:\")\n","print(f\"  Total samples: {total_samples:,}\")\n","print(f\"  NULL samples: {null_count:,} ({null_count/total_samples*100:.2f}%)\")\n","print(f\"  Number of label classes: {df['label'].nunique(dropna=True)}\")\n","\n","print(f\"\\nLabel distribution (top 20):\")\n","for label, count in label_counts.head(20).items():\n","    pct = count / total_samples * 100\n","    print(f\"  {str(label):30s}: {count:8,} ({pct:5.2f}%)\")\n","\n","# ========== 2. Define label mapping rules ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Define label mapping rules\")\n","print(\"=\"*60)\n","\n","# Map LARa dataset labels to a cross-dataset unified label superset\n","# Covers LARa / RealWorld / SHL\n","LABEL_MAPPING = {\n","    # Basic activities (shared by RealWorld + LARa)\n","    1: {\"original\": \"walking\", \"mapped\": \"walking\", \"category\": \"locomotion\"},\n","    2: {\"original\": \"running\", \"mapped\": \"running\", \"category\": \"locomotion\"},\n","    3: {\"original\": \"shuffling\", \"mapped\": \"walking\", \"category\": \"locomotion\"},  # merge into walking\n","    4: {\"original\": \"stairs (ascending)\", \"mapped\": \"upstairs\", \"category\": \"locomotion\"},\n","    5: {\"original\": \"stairs (descending)\", \"mapped\": \"downstairs\", \"category\": \"locomotion\"},\n","    6: {\"original\": \"standing\", \"mapped\": \"standing\", \"category\": \"static\"},\n","    7: {\"original\": \"sitting\", \"mapped\": \"sitting\", \"category\": \"static\"},\n","    8: {\"original\": \"lying\", \"mapped\": \"lying\", \"category\": \"static\"},\n","\n","    # Transport (specific to LARa; not in RealWorld)\n","    13: {\"original\": \"cycling (sit)\", \"mapped\": \"cycling\", \"category\": \"transport\"},\n","    14: {\"original\": \"cycling (stand)\", \"mapped\": \"cycling\", \"category\": \"transport\"},\n","    130: {\"original\": \"cycling\", \"mapped\": \"cycling\", \"category\": \"transport\"},\n","\n","    17: {\"original\": \"car\", \"mapped\": \"car\", \"category\": \"transport\"},\n","    18: {\"original\": \"bus\", \"mapped\": \"bus\", \"category\": \"transport\"},\n","    19: {\"original\": \"train\", \"mapped\": \"train\", \"category\": \"transport\"},\n","    20: {\"original\": \"subway\", \"mapped\": \"subway\", \"category\": \"transport\"},\n","\n","    # Transition label\n","    0: {\"original\": \"transition\", \"mapped\": \"transition\", \"category\": \"transition\"},\n","}\n","\n","# Cross-dataset unified label superset (LARa + RealWorld + SHL)\n","UNIFIED_LABELS = {\n","    \"walking\": 1,\n","    \"running\": 2,\n","    \"sitting\": 3,\n","    \"standing\": 4,\n","    \"upstairs\": 5,\n","    \"downstairs\": 6,\n","    \"lying\": 7,\n","    \"cycling\": 8,\n","    \"car\": 9,\n","    \"bus\": 10,\n","    \"train\": 11,\n","    \"subway\": 12,\n","    \"transition\": 0,  # kept or cleaned\n","}\n","\n","print(f\"\\nDefined mapping rules: {len(LABEL_MAPPING)} original labels\")\n","print(f\"Unified label set: {len(UNIFIED_LABELS)} labels (cross-dataset superset)\")\n","\n","print(f\"\\nMapping examples:\")\n","for orig_id, info in list(LABEL_MAPPING.items())[:10]:\n","    print(f\"  {orig_id} ({info['original']}) -> {info['mapped']}\")\n","\n","# ========== 3. Audit assertion: check unmapped labels ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Audit assertion: check unmapped labels\")\n","print(\"=\"*60)\n","\n","# Find all original label IDs (excluding NULL)\n","orig_ids = set(df['label'].dropna().astype(int).unique())\n","covered_ids = set(LABEL_MAPPING.keys())\n","unmapped_ids = sorted(orig_ids - covered_ids)\n","\n","if unmapped_ids:\n","    # Count samples for unmapped labels\n","    unmapped_counts = []\n","    for uid in unmapped_ids:\n","        count = (df['label'] == uid).sum()\n","        pct = count / total_samples\n","        unmapped_counts.append({\n","            'original_label_id': uid,\n","            'sample_count': count,\n","            'percentage': round(pct * 100, 4),\n","        })\n","\n","    df_unmapped = pd.DataFrame(unmapped_counts)\n","    total_unmapped = df_unmapped['sample_count'].sum()\n","    unmapped_ratio = total_unmapped / total_samples\n","\n","    # Save list of unmapped labels\n","    unmapped_file = reports_dir / \"unmapped_labels.csv\"\n","    df_unmapped.to_csv(unmapped_file, index=False)\n","\n","    print(f\"\\n⚠️ Found unmapped labels: {len(unmapped_ids)}\")\n","    print(f\"  Unmapped sample count: {total_unmapped:,} ({unmapped_ratio*100:.2f}%)\")\n","    print(f\"  Details saved to: {unmapped_file}\")\n","    print(f\"\\nList of unmapped labels:\")\n","    print(df_unmapped.to_string(index=False))\n","\n","    # Abort if threshold exceeded\n","    if unmapped_ratio > UNMAPPED_THRESHOLD:\n","        raise RuntimeError(\n","            f\"Unmapped label ratio {unmapped_ratio*100:.2f}% exceeds threshold {UNMAPPED_THRESHOLD*100}%. \"\n","            f\"Please check {unmapped_file} and extend LABEL_MAPPING.\"\n","        )\n","    else:\n","        print(f\"\\n✓ Unmapped label ratio does not exceed threshold {UNMAPPED_THRESHOLD*100}%; continuing (will mark as NULL)\")\n","else:\n","    print(f\"\\n✓ All original labels are covered\")\n","\n","# ========== 4. Apply label mapping ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Apply label mapping\")\n","print(\"=\"*60)\n","\n","df_mapped = df.copy()\n","\n","# Keep a copy of original labels (nullable integer)\n","df_mapped['label_original'] = df_mapped['label'].astype('Int32')\n","\n","# Apply mapping\n","def map_label(label):\n","    \"\"\"Map a single label\"\"\"\n","    if pd.isna(label):\n","        return np.nan\n","\n","    label = int(label)\n","    if label in LABEL_MAPPING:\n","        mapped_name = LABEL_MAPPING[label]['mapped']\n","        return UNIFIED_LABELS[mapped_name]\n","    else:\n","        # Unknown labels marked as NaN\n","        return np.nan\n","\n","df_mapped['label'] = df_mapped['label_original'].apply(map_label)\n","\n","# Stats after mapping\n","mapped_label_counts = df_mapped['label'].value_counts(dropna=False)\n","null_after_mapping = df_mapped['label'].isna().sum()\n","\n","print(f\"\\nPost-mapping label stats:\")\n","print(f\"  NULL samples: {null_after_mapping:,} ({null_after_mapping/total_samples*100:.2f}%)\")\n","print(f\"  Number of valid label classes: {df_mapped['label'].nunique(dropna=True)}\")\n","\n","print(f\"\\nPost-mapping distribution:\")\n","for label, count in mapped_label_counts.head(15).items():\n","    pct = count / total_samples * 100\n","    # find label name\n","    label_name = \"NULL\"\n","    if not pd.isna(label):\n","        label_name = [k for k, v in UNIFIED_LABELS.items() if v == int(label)][0]\n","    print(f\"  {label_name:15s} ({str(label):2s}): {count:8,} ({pct:5.2f}%)\")\n","\n","# ========== 5. Clean NULL and transition labels (true nearest neighbor) ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Clean NULL and transition labels (true nearest neighbor)\")\n","print(\"=\"*60)\n","\n","df_cleaned = df_mapped.copy()\n","\n","# Handle NULL labels\n","if NULL_STRATEGY == \"remove\":\n","    null_mask = df_cleaned['label'].isna()\n","    removed_null = null_mask.sum()\n","    df_cleaned = df_cleaned[~null_mask].copy()\n","    print(f\"\\nNULL handling: removed {removed_null:,} samples\")\n","elif NULL_STRATEGY == \"merge_to_transition\":\n","    null_mask = df_cleaned['label'].isna()\n","    df_cleaned.loc[null_mask, 'label'] = UNIFIED_LABELS['transition']\n","    print(f\"\\nNULL handling: merged into transition ({null_mask.sum():,} samples)\")\n","\n","# Detect time column\n","time_col = None\n","for candidate in ['time_sec', 'timestamp', 'timestamp_ms', 'time', 'epoch_ms']:\n","    if candidate in df_cleaned.columns:\n","        time_col = candidate\n","        break\n","\n","if time_col:\n","    print(f\"\\nDetected time column: {time_col}\")\n","else:\n","    print(f\"\\nNo time column detected; will process by index order\")\n","\n","# Handle transition label (true nearest neighbor)\n","transition_value = UNIFIED_LABELS['transition']\n","if TRANSITION_STRATEGY == \"remove\":\n","    trans_mask = df_cleaned['label'] == transition_value\n","    removed_trans = trans_mask.sum()\n","    df_cleaned = df_cleaned[~trans_mask].copy()\n","    print(f\"Transition handling: removed {removed_trans:,} samples\")\n","\n","elif TRANSITION_STRATEGY == \"merge_to_nearest\":\n","    trans_mask = df_cleaned['label'] == transition_value\n","    trans_count = trans_mask.sum()\n","\n","    if trans_count > 0:\n","        print(f\"Transition handling: merge {trans_count:,} samples using nearest-neighbor interpolation\")\n","\n","        # Sort by time (ensure nearest-neighbor semantics)\n","        if time_col:\n","            df_cleaned = df_cleaned.sort_values(\n","                ['subject_id', 'session_id', 'placement', time_col],\n","                kind='stable'\n","            ).copy()\n","            print(f\"  ✓ Sorted by [{time_col}]\")\n","        else:\n","            df_cleaned = df_cleaned.sort_index(kind='stable').copy()\n","            print(f\"  ⚠️ Sorted by index (no time column)\")\n","\n","        # True nearest-neighbor merge\n","        merged_count = 0\n","        for (subj, sess, plc), group in df_cleaned.groupby(\n","            ['subject_id', 'session_id', 'placement'], observed=True\n","        ):\n","            idx = group.index\n","            labels = df_cleaned.loc[idx, 'label'].copy()\n","\n","            # Replace transition with NaN\n","            labels_with_nan = labels.replace(transition_value, np.nan).astype('float')\n","\n","            if labels_with_nan.isna().any():\n","                # Use nearest interpolation (true nearest neighbor)\n","                labels_filled = labels_with_nan.interpolate(\n","                    method='nearest',\n","                    limit_direction='both'\n","                )\n","\n","                # Count successfully merged items\n","                was_trans = (labels == transition_value)\n","                now_filled = labels_filled.notna()\n","                merged_this_group = (was_trans & now_filled).sum()\n","                merged_count += merged_this_group\n","\n","                # Update labels (round then cast to int)\n","                df_cleaned.loc[idx, 'label'] = labels_filled.round()\n","\n","        print(f\"  ✓ Successfully merged {merged_count:,} transition samples to nearest labels\")\n","\n","        # Remove transitions that could not be merged (entire segments are transition)\n","        remaining_trans = (df_cleaned['label'] == transition_value).sum()\n","        if remaining_trans > 0:\n","            df_cleaned = df_cleaned[df_cleaned['label'] != transition_value].copy()\n","            print(f\"  ✓ Removed remaining {remaining_trans:,} transition samples that could not be merged\")\n","\n","# Remove remaining NaNs\n","final_nan = df_cleaned['label'].isna().sum()\n","if final_nan > 0:\n","    df_cleaned = df_cleaned[df_cleaned['label'].notna()].copy()\n","    print(f\"\\nRemoved final residual NaN samples: {final_nan:,}\")\n","\n","# Cast to int32\n","df_cleaned['label'] = df_cleaned['label'].astype('int32')\n","\n","# Reset index\n","df_cleaned = df_cleaned.reset_index(drop=True)\n","\n","print(f\"\\nData after cleaning:\")\n","print(f\"  Samples: {len(df_cleaned):,}\")\n","print(f\"  Number of label classes: {df_cleaned['label'].nunique()}\")\n","print(f\"  Retention rate: {len(df_cleaned)/total_samples*100:.2f}%\")\n","\n","# ========== 6. Audit assertion: verify final label set ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Audit assertion: verify final label set\")\n","print(\"=\"*60)\n","\n","# Determine allowed label set\n","allowed_labels = set(UNIFIED_LABELS.values())\n","if TRANSITION_STRATEGY == \"remove\":\n","    allowed_labels.discard(UNIFIED_LABELS['transition'])\n","\n","# Check actual label set\n","actual_labels = set(df_cleaned['label'].unique())\n","unexpected = sorted(actual_labels - allowed_labels)\n","\n","if unexpected:\n","    raise RuntimeError(\n","        f\"Illegal labels found after cleaning: {unexpected}\\n\"\n","        f\"Allowed labels: {sorted(allowed_labels)}\"\n","    )\n","else:\n","    print(f\"✓ Final label set validation passed\")\n","    print(f\"  Allowed labels: {sorted(allowed_labels)}\")\n","    print(f\"  Actual labels: {sorted(actual_labels)}\")\n","\n","# ========== 7. Final label distribution ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"7. Final label distribution\")\n","print(\"=\"*60)\n","\n","final_label_counts = df_cleaned['label'].value_counts()\n","\n","print(f\"\\nFinal label distribution:\")\n","for label_id, count in final_label_counts.items():\n","    pct = count / len(df_cleaned) * 100\n","    label_name = [k for k, v in UNIFIED_LABELS.items() if v == int(label_id)][0]\n","    print(f\"  {label_name:15s} ({int(label_id):2d}): {count:8,} ({pct:5.2f}%)\")\n","\n","# By-category statistics\n","category_stats = {}\n","for label_id, count in final_label_counts.items():\n","    label_name = [k for k, v in UNIFIED_LABELS.items() if v == int(label_id)][0]\n","    # Find category\n","    category = None\n","    for orig_id, info in LABEL_MAPPING.items():\n","        if info['mapped'] == label_name:\n","            category = info['category']\n","            break\n","\n","    if category:\n","        category_stats[category] = category_stats.get(category, 0) + count\n","\n","print(f\"\\nBy-category statistics:\")\n","for category, count in sorted(category_stats.items()):\n","    pct = count / len(df_cleaned) * 100\n","    print(f\"  {category:15s}: {count:8,} ({pct:5.2f}%)\")\n","\n","# ========== 8. Save results ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"8. Save results\")\n","print(\"=\"*60)\n","\n","# Save cleaned data (using directory layout)\n","output_dir = proc_dir / \"labeled\"\n","if output_dir.exists():\n","    import shutil\n","    shutil.rmtree(output_dir)\n","\n","df_cleaned.to_parquet(\n","    output_dir,\n","    index=False,\n","    partition_cols=['subject_id', 'placement'],\n","    engine='pyarrow'\n",")\n","\n","print(f\"✓ Saved: {output_dir}/\")\n","print(f\"  Data shape: {df_cleaned.shape}\")\n","print(f\"  Partitions: subject_id / placement\")\n","\n","# ========== 9. Save label mapping config (rich) ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"9. Save label mapping config (rich)\")\n","print(\"=\"*60)\n","\n","# Build labels_map with more info\n","labels_map_data = []\n","for label_name, label_id in sorted(UNIFIED_LABELS.items(), key=lambda x: x[1]):\n","    if label_name == \"transition\" and TRANSITION_STRATEGY == \"remove\":\n","        continue  # exclude removed transition\n","\n","    # Find original label IDs and names\n","    original_ids = []\n","    original_names = []\n","    category = None\n","\n","    for orig_id, info in LABEL_MAPPING.items():\n","        if info['mapped'] == label_name:\n","            original_ids.append(str(orig_id))\n","            original_names.append(info['original'])\n","            if category is None:\n","                category = info['category']\n","\n","    # Actual sample count\n","    sample_count = final_label_counts.get(label_id, 0)\n","\n","    labels_map_data.append({\n","        'label_id': label_id,\n","        'label_name': label_name,\n","        'category': category or 'unknown',\n","        'sample_count': int(sample_count),\n","        'percentage': round(sample_count / len(df_cleaned) * 100, 2) if len(df_cleaned) > 0 else 0.0,\n","        'original_label_ids': ','.join(original_ids) if original_ids else '',\n","        'original_label_names': '; '.join(original_names) if original_names else '',\n","        'source_dataset': 'LARa-MbientLab',\n","        'description': f\"{label_name} activity\",\n","    })\n","\n","df_labels_map = pd.DataFrame(labels_map_data)\n","labels_map_file = proc_dir / \"labels_map.csv\"\n","df_labels_map.to_csv(labels_map_file, index=False)\n","\n","print(f\"✓ Saved label mapping: {labels_map_file}\")\n","print(f\"\\nLabel mapping table:\")\n","print(df_labels_map.to_string(index=False))\n","\n","# Save detailed configuration\n","label_config = {\n","    'dataset': 'LARa-MbientLab',\n","    'label_system': 'Cross-dataset unified label superset (covers LARa/RealWorld/SHL)',\n","    'unified_labels': UNIFIED_LABELS,\n","    'label_mapping': LABEL_MAPPING,\n","    'cleaning_strategy': {\n","        'null_strategy': NULL_STRATEGY,\n","        'transition_strategy': TRANSITION_STRATEGY,\n","        'transition_method': 'nearest-neighbor interpolation (true nearest neighbor)' if TRANSITION_STRATEGY == 'merge_to_nearest' else 'remove',\n","        'time_sorted': time_col is not None,\n","        'time_column': time_col,\n","        'unmapped_threshold': UNMAPPED_THRESHOLD,\n","    },\n","    'statistics': {\n","        'original_samples': int(total_samples),\n","        'cleaned_samples': int(len(df_cleaned)),\n","        'removed_samples': int(total_samples - len(df_cleaned)),\n","        'removal_rate': float((total_samples - len(df_cleaned)) / total_samples),\n","        'original_label_count': int(df['label'].nunique(dropna=True)),\n","        'final_label_count': int(df_cleaned['label'].nunique()),\n","        'unmapped_label_count': len(unmapped_ids) if unmapped_ids else 0,\n","    },\n","    'label_distribution': {\n","        label_name: int(final_label_counts.get(label_id, 0))\n","        for label_name, label_id in UNIFIED_LABELS.items()\n","        if label_name != 'transition' or TRANSITION_STRATEGY != 'remove'\n","    },\n","    'notes': [\n","        'Label mapping based on cross-dataset unified label superset (LARa + RealWorld + SHL)',\n","        f'NULL label strategy: {NULL_STRATEGY}',\n","        f'Transition label strategy: {TRANSITION_STRATEGY} (true nearest-neighbor interpolation)',\n","        'Unmapped original labels are automatically marked as NULL',\n","        f'Unmapped label threshold: {UNMAPPED_THRESHOLD*100}%',\n","        f'Sorted by time column: {time_col if time_col else \"No (by index)\"}',\n","        'Mapping table saved at proc/labels_map.csv',\n","        'label_original column uses nullable integer Int32',\n","        'Includes audit assertions to ensure label set integrity',\n","    ]\n","}\n","\n","label_config_file = configs_dir / \"labels.yaml\"\n","with open(label_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(label_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","print(f\"✓ Saved config: {label_config_file}\")\n","\n","label_config_json = configs_dir / \"labels.json\"\n","with open(label_config_json, 'w', encoding='utf-8') as f:\n","    json.dump(label_config, f, indent=2)\n","\n","print(f\"✓ Saved config: {label_config_json}\")\n","\n","# ========== 10. Summary ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 8 complete - Label alignment & cleaning (top-tier revised)\")\n","print(\"=\"*60)\n","\n","print(f\"\\nConfig:\")\n","print(f\"  Label system: cross-dataset unified superset (LARa/RealWorld/SHL)\")\n","print(f\"  NULL strategy: {NULL_STRATEGY}\")\n","print(f\"  Transition strategy: {TRANSITION_STRATEGY} (true nearest neighbor)\")\n","print(f\"  Unmapped threshold: {UNMAPPED_THRESHOLD*100}%\")\n","print(f\"  Time column: {time_col if time_col else 'No (by index)'}\")\n","\n","print(f\"\\nResults:\")\n","print(f\"  Original samples: {total_samples:,}\")\n","print(f\"  Cleaned samples: {len(df_cleaned):,}\")\n","print(f\"  Removed samples: {total_samples - len(df_cleaned):,}\")\n","print(f\"  Retention rate: {len(df_cleaned)/total_samples*100:.2f}%\")\n","\n","print(f\"\\nLabel stats:\")\n","print(f\"  Original label classes: {df['label'].nunique(dropna=True)}\")\n","print(f\"  Final label classes: {df_cleaned['label'].nunique()}\")\n","print(f\"  Unmapped labels: {len(unmapped_ids) if unmapped_ids else 0}\")\n","\n","print(f\"\\nOutputs:\")\n","print(f\"  Data: {output_dir}/\")\n","print(f\"  Mapping table: {labels_map_file}\")\n","print(f\"  Config: {label_config_file}\")\n","if unmapped_ids:\n","    print(f\"  Unmapped list: {reports_dir / 'unmapped_labels.csv'}\")\n","\n","print(\"\\nKey fixes (top-tier):\")\n","print(\"  1. ✓ True nearest-neighbor merge (interpolate method='nearest')\")\n","print(\"  2. ✓ Sort by time before processing (correct semantics)\")\n","print(\"  3. ✓ Record unmapped labels to reports/unmapped_labels.csv\")\n","print(\"  4. ✓ label_original uses nullable Int32\")\n","print(\"  5. ✓ Removed irrelevant MAJORITY_VOTE_THRESHOLD\")\n","print(\"  6. ✓ Audit assertions (fail-fast)\")\n","print(\"  7. ✓ labels_map.csv includes original names and source\")\n","print(\"  8. ✓ Label system described as cross-dataset superset\")\n","print(\"  9. ✓ Output directory changed to labeled/\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ei0HO780rVfm","executionInfo":{"status":"ok","timestamp":1763117593358,"user_tz":0,"elapsed":1510,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"e37d64ee-ee2f-4130-d0f1-ac8c49669dd2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 8: Label Alignment & Cleaning\n","============================================================\n","\n","Loading normalized data: data/lara/mbientlab/proc/normalized.parquet\n","Data shape: (560070, 15)\n","Number of subjects: 8\n","\n","============================================================\n","1. Analyze original label distribution\n","============================================================\n","\n","Original label stats:\n","  Total samples: 560,070\n","  NULL samples: 28 (0.00%)\n","  Number of label classes: 8\n","\n","Label distribution (top 20):\n","  4.0                           :  215,988 (38.56%)\n","  2.0                           :   86,132 (15.38%)\n","  0.0                           :   73,180 (13.07%)\n","  7.0                           :   48,927 ( 8.74%)\n","  5.0                           :   43,966 ( 7.85%)\n","  1.0                           :   42,039 ( 7.51%)\n","  3.0                           :   38,224 ( 6.82%)\n","  6.0                           :   11,586 ( 2.07%)\n","  nan                           :       28 ( 0.00%)\n","\n","============================================================\n","2. Define label mapping rules\n","============================================================\n","\n","Defined mapping rules: 16 original labels\n","Unified label set: 13 labels (cross-dataset superset)\n","\n","Mapping examples:\n","  1 (walking) -> walking\n","  2 (running) -> running\n","  3 (shuffling) -> walking\n","  4 (stairs (ascending)) -> upstairs\n","  5 (stairs (descending)) -> downstairs\n","  6 (standing) -> standing\n","  7 (sitting) -> sitting\n","  8 (lying) -> lying\n","  13 (cycling (sit)) -> cycling\n","  14 (cycling (stand)) -> cycling\n","\n","============================================================\n","3. Audit assertion: check unmapped labels\n","============================================================\n","\n","✓ All original labels are covered\n","\n","============================================================\n","4. Apply label mapping\n","============================================================\n","\n","Post-mapping label stats:\n","  NULL samples: 28 (0.00%)\n","  Number of valid label classes: 7\n","\n","Post-mapping distribution:\n","  upstairs        (5.0):  215,988 (38.56%)\n","  running         (2.0):   86,132 (15.38%)\n","  walking         (1.0):   80,263 (14.33%)\n","  transition      (0.0):   73,180 (13.07%)\n","  sitting         (3.0):   48,927 ( 8.74%)\n","  downstairs      (6.0):   43,966 ( 7.85%)\n","  standing        (4.0):   11,586 ( 2.07%)\n","  NULL            (nan):       28 ( 0.00%)\n","\n","============================================================\n","5. Clean NULL and transition labels (true nearest neighbor)\n","============================================================\n","\n","NULL handling: removed 28 samples\n","\n","Detected time column: time_sec\n","Transition handling: merge 73,180 samples using nearest-neighbor interpolation\n","  ✓ Sorted by [time_sec]\n","  ✓ Successfully merged 69,642 transition samples to nearest labels\n","\n","Removed final residual NaN samples: 3,538\n","\n","Data after cleaning:\n","  Samples: 556,504\n","  Number of label classes: 6\n","  Retention rate: 99.36%\n","\n","============================================================\n","6. Audit assertion: verify final label set\n","============================================================\n","✓ Final label set validation passed\n","  Allowed labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n","  Actual labels: [np.int32(1), np.int32(2), np.int32(3), np.int32(4), np.int32(5), np.int32(6)]\n","\n","============================================================\n","7. Final label distribution\n","============================================================\n","\n","Final label distribution:\n","  upstairs        ( 5):  255,038 (45.83%)\n","  walking         ( 1):   96,957 (17.42%)\n","  running         ( 2):   89,302 (16.05%)\n","  sitting         ( 3):   51,931 ( 9.33%)\n","  downstairs      ( 6):   50,391 ( 9.05%)\n","  standing        ( 4):   12,885 ( 2.32%)\n","\n","By-category statistics:\n","  locomotion     :  491,688 (88.35%)\n","  static         :   64,816 (11.65%)\n","\n","============================================================\n","8. Save results\n","============================================================\n","✓ Saved: data/lara/mbientlab/proc/labeled/\n","  Data shape: (556504, 16)\n","  Partitions: subject_id / placement\n","\n","============================================================\n","9. Save label mapping config (rich)\n","============================================================\n","✓ Saved label mapping: data/lara/mbientlab/proc/labels_map.csv\n","\n","Label mapping table:\n"," label_id label_name   category  sample_count  percentage original_label_ids                    original_label_names source_dataset         description\n","        0 transition transition             0        0.00                  0                              transition LARa-MbientLab transition activity\n","        1    walking locomotion         96957       17.42                1,3                      walking; shuffling LARa-MbientLab    walking activity\n","        2    running locomotion         89302       16.05                  2                                 running LARa-MbientLab    running activity\n","        3    sitting     static         51931        9.33                  7                                 sitting LARa-MbientLab    sitting activity\n","        4   standing     static         12885        2.32                  6                                standing LARa-MbientLab   standing activity\n","        5   upstairs locomotion        255038       45.83                  4                      stairs (ascending) LARa-MbientLab   upstairs activity\n","        6 downstairs locomotion         50391        9.05                  5                     stairs (descending) LARa-MbientLab downstairs activity\n","        7      lying     static             0        0.00                  8                                   lying LARa-MbientLab      lying activity\n","        8    cycling  transport             0        0.00          13,14,130 cycling (sit); cycling (stand); cycling LARa-MbientLab    cycling activity\n","        9        car  transport             0        0.00                 17                                     car LARa-MbientLab        car activity\n","       10        bus  transport             0        0.00                 18                                     bus LARa-MbientLab        bus activity\n","       11      train  transport             0        0.00                 19                                   train LARa-MbientLab      train activity\n","       12     subway  transport             0        0.00                 20                                  subway LARa-MbientLab     subway activity\n","✓ Saved config: configs/labels.yaml\n","✓ Saved config: configs/labels.json\n","\n","============================================================\n","Step 8 complete - Label alignment & cleaning (top-tier revised)\n","============================================================\n","\n","Config:\n","  Label system: cross-dataset unified superset (LARa/RealWorld/SHL)\n","  NULL strategy: remove\n","  Transition strategy: merge_to_nearest (true nearest neighbor)\n","  Unmapped threshold: 1.0%\n","  Time column: time_sec\n","\n","Results:\n","  Original samples: 560,070\n","  Cleaned samples: 556,504\n","  Removed samples: 3,566\n","  Retention rate: 99.36%\n","\n","Label stats:\n","  Original label classes: 8\n","  Final label classes: 6\n","  Unmapped labels: 0\n","\n","Outputs:\n","  Data: data/lara/mbientlab/proc/labeled/\n","  Mapping table: data/lara/mbientlab/proc/labels_map.csv\n","  Config: configs/labels.yaml\n","\n","Key fixes (top-tier):\n","  1. ✓ True nearest-neighbor merge (interpolate method='nearest')\n","  2. ✓ Sort by time before processing (correct semantics)\n","  3. ✓ Record unmapped labels to reports/unmapped_labels.csv\n","  4. ✓ label_original uses nullable Int32\n","  5. ✓ Removed irrelevant MAJORITY_VOTE_THRESHOLD\n","  6. ✓ Audit assertions (fail-fast)\n","  7. ✓ labels_map.csv includes original names and source\n","  8. ✓ Label system described as cross-dataset superset\n","  9. ✓ Output directory changed to labeled/\n","============================================================\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","\n","# ========== Manually set FOLD_ID (if the environment variable is not set) ==========\n","if \"FOLD_ID\" not in os.environ:\n","    print(\"⚠️ Environment variable FOLD_ID is not set; please specify it manually:\")\n","    print(\"Hint: if your LOSO has N folds, FOLD_ID should be 0 to N-1\")\n","    fold_input = input(\"Please enter FOLD_ID (press Enter to default to 0): \").strip()\n","    os.environ[\"FOLD_ID\"] = fold_input if fold_input else \"0\"\n","    print(f\"✓ FOLD_ID has been set to {os.environ['FOLD_ID']}\")\n","\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","\n","\"\"\"\n","Step 9: Sliding-window Slicing (top-conf/journal grade - revised)\n","Slice with fixed window length/step; assign window label by majority label\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import yaml\n","import json\n","import os\n","from collections import Counter\n","\n","# ========== Config ==========\n","\n","# Sliding-window parameters\n","SAMPLING_RATE_HZ = 50.0\n","WINDOW_SIZE_SEC = 3.0\n","OVERLAP_RATIO = 0.5\n","\n","# Compute sample counts\n","WINDOW_SIZE = int(WINDOW_SIZE_SEC * SAMPLING_RATE_HZ)  # 150 samples\n","STEP_SIZE = int(WINDOW_SIZE * (1 - OVERLAP_RATIO))  # 75 samples\n","\n","# Majority label threshold\n","DOMINANT_THRESHOLD = 0.8\n","\n","# Feature columns (8 channels)\n","FEATURE_COLS = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","print(\"=\"*60)\n","print(\"Step 9: Sliding-window slicing\")\n","print(\"=\"*60)\n","\n","# Create output directory (persist per fold)\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","\n","# Create subdirectory based on FOLD_ID\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","fold_tag = f\"fold_{FOLD_ID:02d}\" if FOLD_ID >= 0 else \"all\"\n","windows_dir = proc_dir / \"windows\" / fold_tag\n","windows_dir.mkdir(parents=True, exist_ok=True)\n","\n","if FOLD_ID >= 0:\n","    print(f\"\\nUsing train fold: FOLD_ID={FOLD_ID}\")\n","    print(f\"Output directory: {windows_dir}\")\n","else:\n","    print(f\"\\nFOLD_ID not specified; using all data\")\n","    print(f\"Output directory: {windows_dir}\")\n","\n","print(f\"\\nSliding-window parameters:\")\n","print(f\"  Window length: {WINDOW_SIZE_SEC} s = {WINDOW_SIZE} samples @ {SAMPLING_RATE_HZ} Hz\")\n","print(f\"  Step size: {STEP_SIZE} samples (overlap {OVERLAP_RATIO*100:.0f}%)\")\n","print(f\"  Dominant label threshold: {DOMINANT_THRESHOLD*100:.0f}%\")\n","\n","# ========== 1. Load data ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Load cleaned data\")\n","print(\"=\"*60)\n","\n","labeled_dir = proc_dir / \"labeled\"\n","print(f\"Loading data: {labeled_dir}/\")\n","df = pd.read_parquet(labeled_dir)\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Number of subjects: {df['subject_id'].nunique()}\")\n","print(f\"Number of label classes: {df['label'].nunique()}\")\n","\n","# Check required columns\n","required_cols = ['subject_id', 'session_id', 'placement', 'label'] + FEATURE_COLS\n","missing_cols = [c for c in required_cols if c not in df.columns]\n","if missing_cols:\n","    raise ValueError(f\"Missing required columns: {missing_cols}\")\n","\n","print(f\"\\nFeature columns: {FEATURE_COLS}\")\n","\n","# Detect time column\n","time_col = 'time_sec' if 'time_sec' in df.columns else None\n","if time_col:\n","    print(f\"Time column: {time_col}\")\n","else:\n","    print(\"No time column detected; will sort by index\")\n","\n","# ========== 2. Load train/test split ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Load train/test split\")\n","print(\"=\"*60)\n","\n","splits_path = configs_dir / \"splits.json\"\n","\n","if splits_path.exists() and FOLD_ID >= 0:\n","    with open(splits_path, \"r\") as f:\n","        splits = json.load(f)\n","\n","    train_subjects = set(splits[str(FOLD_ID)][\"train_subjects\"])\n","    test_subjects = set(splits[str(FOLD_ID)][\"test_subjects\"])\n","\n","    print(f\"Train subjects: {len(train_subjects)}\")\n","    print(f\"Test subjects: {len(test_subjects)}\")\n","\n","    # Split data\n","    df_train = df[df['subject_id'].isin(train_subjects)].copy()\n","    df_test = df[df['subject_id'].isin(test_subjects)].copy()\n","\n","    print(f\"\\nTrain set: {len(df_train):,} samples\")\n","    print(f\"Test set: {len(df_test):,} samples\")\n","else:\n","    print(\"No train-fold config found; using all data\")\n","    df_train = df.copy()\n","    df_test = pd.DataFrame()\n","    train_subjects = set(df['subject_id'].unique())\n","    test_subjects = set()\n","\n","# ========== 3. Define sliding-window function (with time continuity check) ==========\n","\n","def sliding_window_extract(df_subset, window_size, step_size, dominant_threshold, time_col=None):\n","    \"\"\"\n","    Perform sliding-window slicing grouped by session.\n","\n","    Returns:\n","        windows_list: list of window feature arrays\n","        metadata_list: list of window metadata dicts\n","    \"\"\"\n","    windows_list = []\n","    metadata_list = []\n","    window_id = 0\n","\n","    # Group by session\n","    for (subj, sess, plc), group in df_subset.groupby(\n","        ['subject_id', 'session_id', 'placement'], observed=True\n","    ):\n","        # Sort by time column (preferred), otherwise by index\n","        if time_col and time_col in group.columns:\n","            group = group.sort_values(time_col, kind='stable').copy()\n","        else:\n","            group = group.sort_index(kind='stable').copy()\n","\n","        # Extract features and labels\n","        features = group[FEATURE_COLS].values\n","        labels = group['label'].values\n","\n","        # Extract timestamps (if any)\n","        if time_col and time_col in group.columns:\n","            timestamps = group[time_col].values\n","        else:\n","            timestamps = None\n","\n","        # Sliding-window slicing\n","        n_samples = len(group)\n","        for start_idx in range(0, n_samples - window_size + 1, step_size):\n","            end_idx = start_idx + window_size\n","\n","            # Extract window\n","            window_features = features[start_idx:end_idx]\n","            window_labels = labels[start_idx:end_idx]\n","\n","            # Check NaNs\n","            if np.isnan(window_features).any():\n","                continue\n","\n","            # Time continuity check (if timestamps exist)\n","            if timestamps is not None:\n","                expected_duration = (window_size - 1) / SAMPLING_RATE_HZ\n","                actual_duration = timestamps[end_idx - 1] - timestamps[start_idx]\n","                # Allow 10% jitter\n","                if abs(actual_duration - expected_duration) > 0.1 * expected_duration:\n","                    continue\n","\n","            # Compute dominant label\n","            label_counts = Counter(window_labels)\n","            dominant_label, dominant_count = label_counts.most_common(1)[0]\n","            dominant_ratio = dominant_count / window_size\n","\n","            # Keep only windows that meet the threshold\n","            if dominant_ratio < dominant_threshold:\n","                continue\n","\n","            # Extract time range\n","            if timestamps is not None:\n","                time_start = timestamps[start_idx]\n","                time_end = timestamps[end_idx - 1]\n","                time_range = f\"{time_start:.3f}-{time_end:.3f}\"\n","            else:\n","                time_range = f\"{start_idx}-{end_idx-1}\"\n","\n","            # Save window\n","            windows_list.append(window_features)\n","\n","            # Save metadata\n","            metadata_list.append({\n","                'window_id': window_id,\n","                'subject_id': subj,\n","                'session_id': sess,\n","                'placement': plc,\n","                'label': int(dominant_label),\n","                'label_purity': round(dominant_ratio, 4),\n","                'time_range': time_range,\n","                'start_idx': start_idx,\n","                'end_idx': end_idx,\n","            })\n","\n","            window_id += 1\n","\n","    return windows_list, metadata_list\n","\n","# ========== 4. Extract training-set windows ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Extract training-set windows\")\n","print(\"=\"*60)\n","\n","# Initialize variables (avoid undefined)\n","train_windows = []\n","df_train_meta = pd.DataFrame()\n","train_label_counts = pd.Series(dtype=int)\n","\n","if not df_train.empty:\n","    print(f\"Processing train set ({len(df_train):,} samples)...\")\n","\n","    train_windows, train_metadata = sliding_window_extract(\n","        df_train, WINDOW_SIZE, STEP_SIZE, DOMINANT_THRESHOLD, time_col\n","    )\n","\n","    print(f\"✓ Extracted training windows: {len(train_windows):,}\")\n","\n","    if train_windows:\n","        # To numpy array\n","        X_train = np.array(train_windows, dtype='float32')  # shape: (n_windows, window_size, n_features)\n","        df_train_meta = pd.DataFrame(train_metadata)\n","\n","        print(f\"  X_train shape: {X_train.shape}\")\n","        print(f\"  Feature dimensions: {X_train.shape[2]} channels × {X_train.shape[1]} timesteps\")\n","\n","        # Label distribution\n","        train_label_counts = df_train_meta['label'].value_counts().sort_index()\n","        print(f\"\\nTrain-set label distribution:\")\n","        for label, count in train_label_counts.items():\n","            pct = count / len(df_train_meta) * 100\n","            print(f\"  Label {label}: {count:6,} windows ({pct:5.2f}%)\")\n","\n","        # Label purity stats\n","        avg_purity = df_train_meta['label_purity'].mean()\n","        min_purity = df_train_meta['label_purity'].min()\n","        print(f\"\\nTrain-set label purity:\")\n","        print(f\"  Mean: {avg_purity*100:.2f}%\")\n","        print(f\"  Min: {min_purity*100:.2f}%\")\n","\n","        # Save train set\n","        print(f\"\\nSaving train set...\")\n","\n","        # Save features (numpy)\n","        X_train_npy_file = windows_dir / \"X_train.npy\"\n","        np.save(X_train_npy_file, X_train)\n","        print(f\"  ✓ {X_train_npy_file} (feature tensor)\")\n","\n","        # Save metadata (Parquet; filename aligned to X_train.parquet)\n","        X_train_meta_file = windows_dir / \"X_train.parquet\"\n","        df_train_meta[['window_id', 'subject_id', 'session_id', 'placement',\n","                       'label', 'label_purity', 'time_range', 'start_idx', 'end_idx']].to_parquet(\n","            X_train_meta_file, index=False\n","        )\n","        print(f\"  ✓ {X_train_meta_file} (metadata)\")\n","\n","        # Save label vector\n","        y_train = df_train_meta['label'].values.astype('int32')\n","        y_train_file = windows_dir / \"y_train.npy\"\n","        np.save(y_train_file, y_train)\n","        print(f\"  ✓ {y_train_file}\")\n","\n","        # Export label distribution snapshot (for audit)\n","        train_label_counts.to_csv(windows_dir / \"train_label_counts.csv\", header=['count'])\n","        print(f\"  ✓ train_label_counts.csv\")\n","    else:\n","        print(\"⚠️ No training windows extracted\")\n","else:\n","    print(\"Train set is empty; skipping\")\n","\n","# ========== 5. Extract test-set windows ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Extract test-set windows\")\n","print(\"=\"*60)\n","\n","# Initialize variables (avoid undefined)\n","test_windows = []\n","df_test_meta = pd.DataFrame()\n","test_label_counts = pd.Series(dtype=int)\n","\n","if not df_test.empty:\n","    print(f\"Processing test set ({len(df_test):,} samples)...\")\n","\n","    test_windows, test_metadata = sliding_window_extract(\n","        df_test, WINDOW_SIZE, STEP_SIZE, DOMINANT_THRESHOLD, time_col\n","    )\n","\n","    print(f\"✓ Extracted test windows: {len(test_windows):,}\")\n","\n","    if test_windows:\n","        # To numpy array\n","        X_test = np.array(test_windows, dtype='float32')\n","        df_test_meta = pd.DataFrame(test_metadata)\n","\n","        print(f\"  X_test shape: {X_test.shape}\")\n","\n","        # Label distribution\n","        test_label_counts = df_test_meta['label'].value_counts().sort_index()\n","        print(f\"\\nTest-set label distribution:\")\n","        for label, count in test_label_counts.items():\n","            pct = count / len(df_test_meta) * 100\n","            print(f\"  Label {label}: {count:6,} windows ({pct:5.2f}%)\")\n","\n","        # Label purity stats\n","        avg_purity = df_test_meta['label_purity'].mean()\n","        min_purity = df_test_meta['label_purity'].min()\n","        print(f\"\\nTest-set label purity:\")\n","        print(f\"  Mean: {avg_purity*100:.2f}%\")\n","        print(f\"  Min: {min_purity*100:.2f}%\")\n","\n","        # Save test set\n","        print(f\"\\nSaving test set...\")\n","\n","        # Save features (numpy)\n","        X_test_npy_file = windows_dir / \"X_test.npy\"\n","        np.save(X_test_npy_file, X_test)\n","        print(f\"  ✓ {X_test_npy_file} (feature tensor)\")\n","\n","        # Save metadata (Parquet; filename aligned to X_test.parquet)\n","        X_test_meta_file = windows_dir / \"X_test.parquet\"\n","        df_test_meta[['window_id', 'subject_id', 'session_id', 'placement',\n","                      'label', 'label_purity', 'time_range', 'start_idx', 'end_idx']].to_parquet(\n","            X_test_meta_file, index=False\n","        )\n","        print(f\"  ✓ {X_test_meta_file} (metadata)\")\n","\n","        # Save label vector\n","        y_test = df_test_meta['label'].values.astype('int32')\n","        y_test_file = windows_dir / \"y_test.npy\"\n","        np.save(y_test_file, y_test)\n","        print(f\"  ✓ {y_test_file}\")\n","\n","        # Export label distribution snapshot (for audit)\n","        test_label_counts.to_csv(windows_dir / \"test_label_counts.csv\", header=['count'])\n","        print(f\"  ✓ test_label_counts.csv\")\n","    else:\n","        print(\"⚠️ No test windows extracted\")\n","else:\n","    print(\"Test set is empty; skipping\")\n","\n","# ========== 6. Save window configuration ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Save window configuration\")\n","print(\"=\"*60)\n","\n","# Build config\n","window_config = {\n","    'window_parameters': {\n","        'sampling_rate_hz': SAMPLING_RATE_HZ,\n","        'window_size_sec': WINDOW_SIZE_SEC,\n","        'window_size_samples': WINDOW_SIZE,\n","        'overlap_ratio': OVERLAP_RATIO,\n","        'step_size_samples': STEP_SIZE,\n","        'dominant_threshold': DOMINANT_THRESHOLD,\n","    },\n","    'features': {\n","        'channels': FEATURE_COLS,\n","        'n_channels': len(FEATURE_COLS),\n","        'description': '8-channel IMU features (ax,ay,az,gx,gy,gz,acc_mag,gyr_mag)',\n","    },\n","    'dataset_split': {\n","        'fold_id': FOLD_ID if FOLD_ID >= 0 else None,\n","        'fold_tag': fold_tag,\n","        'train_subjects': sorted(list(train_subjects)),\n","        'test_subjects': sorted(list(test_subjects)) if test_subjects else None,\n","    },\n","    'statistics': {},\n","    'notes': [\n","        f'Window parameters: {WINDOW_SIZE_SEC}s @ {SAMPLING_RATE_HZ}Hz = {WINDOW_SIZE} samples',\n","        f'Step size: {STEP_SIZE} samples (overlap {OVERLAP_RATIO*100:.0f}%)',\n","        f'Dominant label threshold: {DOMINANT_THRESHOLD*100:.0f}% (discard windows below threshold)',\n","        'Features: 8 channels (3-axis accelerometer + 3-axis gyroscope + 2 magnitudes)',\n","        'Data formats: X_*.npy (float32 tensor), X_*.parquet (metadata), y_*.npy (int32)',\n","        'Metadata includes: window_id/time_range/label/label_purity, etc.',\n","        'Slice per session to ensure temporal continuity',\n","        f'Order by {time_col if time_col else \"index\"}',\n","        'Discard windows containing NaN',\n","        'Time continuity check (allow 10% jitter)',\n","        f'Persist by fold: windows/{fold_tag}/ (avoid overwrite)',\n","    ]\n","}\n","\n","# Add train stats\n","if train_windows:\n","    window_config['statistics']['train'] = {\n","        'n_windows': len(train_windows),\n","        'n_subjects': int(df_train_meta['subject_id'].nunique()),\n","        'n_sessions': int(df_train_meta.groupby(['subject_id', 'session_id']).ngroups),\n","        'label_distribution': {int(k): int(v) for k, v in train_label_counts.items()},\n","        'avg_label_purity': round(float(df_train_meta['label_purity'].mean()), 4),\n","        'min_label_purity': round(float(df_train_meta['label_purity'].min()), 4),\n","    }\n","\n","# Add test stats\n","if test_windows:\n","    window_config['statistics']['test'] = {\n","        'n_windows': len(test_windows),\n","        'n_subjects': int(df_test_meta['subject_id'].nunique()),\n","        'n_sessions': int(df_test_meta.groupby(['subject_id', 'session_id']).ngroups),\n","        'label_distribution': {int(k): int(v) for k, v in test_label_counts.items()},\n","        'avg_label_purity': round(float(df_test_meta['label_purity'].mean()), 4),\n","        'min_label_purity': round(float(df_test_meta['label_purity'].min()), 4),\n","    }\n","\n","# Save config\n","window_config_file = configs_dir / \"windows.yaml\"\n","with open(window_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(window_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","print(f\"✓ Saved config: {window_config_file}\")\n","\n","window_config_json = configs_dir / \"windows.json\"\n","with open(window_config_json, 'w', encoding='utf-8') as f:\n","    json.dump(window_config, f, indent=2)\n","\n","print(f\"✓ Saved config: {window_config_json}\")\n","\n","# ========== 7. Summary ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 9 complete - Sliding-window slicing (revised)\")\n","print(\"=\"*60)\n","\n","print(f\"\\nWindow parameters:\")\n","print(f\"  Window length: {WINDOW_SIZE_SEC} s = {WINDOW_SIZE} samples\")\n","print(f\"  Step size: {STEP_SIZE} samples (overlap {OVERLAP_RATIO*100:.0f}%)\")\n","print(f\"  Dominant threshold: {DOMINANT_THRESHOLD*100:.0f}%\")\n","print(f\"  Feature dimension: {len(FEATURE_COLS)} channels\")\n","print(f\"  Sort order: {time_col if time_col else 'index'}\")\n","\n","if train_windows:\n","    print(f\"\\nTrain set:\")\n","    print(f\"  # windows: {len(train_windows):,}\")\n","    print(f\"  Shape: {X_train.shape}\")\n","    print(f\"  Subjects: {df_train_meta['subject_id'].nunique()}\")\n","    print(f\"  Sessions: {df_train_meta.groupby(['subject_id', 'session_id']).ngroups}\")\n","    print(f\"  Average purity: {df_train_meta['label_purity'].mean()*100:.2f}%\")\n","\n","if test_windows:\n","    print(f\"\\nTest set:\")\n","    print(f\"  # windows: {len(test_windows):,}\")\n","    print(f\"  Shape: {X_test.shape}\")\n","    print(f\"  Subjects: {df_test_meta['subject_id'].nunique()}\")\n","    print(f\"  Sessions: {df_test_meta.groupby(['subject_id', 'session_id']).ngroups}\")\n","    print(f\"  Average purity: {df_test_meta['label_purity'].mean()*100:.2f}%\")\n","\n","print(f\"\\nOutputs:\")\n","print(f\"  Directory: {windows_dir}/\")\n","if train_windows:\n","    print(f\"  Train: X_train.npy (tensor), X_train.parquet (metadata), y_train.npy\")\n","    print(f\"         train_label_counts.csv\")\n","if test_windows:\n","    print(f\"  Test:  X_test.npy (tensor), X_test.parquet (metadata), y_test.npy\")\n","    print(f\"         test_label_counts.csv\")\n","print(f\"  Config: {window_config_file}\")\n","\n","print(\"\\nKey fixes:\")\n","print(\"  1. ✓ Sort by time column (time_sec preferred, otherwise index)\")\n","print(\"  2. ✓ Metadata filenames aligned to X_train/test.parquet\")\n","print(\"  3. ✓ Safe variable initialization (avoid NameError)\")\n","print(\"  4. ✓ Export label-distribution snapshot CSV (for audit)\")\n","print(\"  5. ✓ Time continuity check (allow 10% jitter)\")\n","print(\"  6. ✓ Slice by session (avoid cross-session windows)\")\n","print(\"  7. ✓ Dominant label proportion ≥ 80%\")\n","print(\"  8. ✓ Discard windows containing NaN\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gV14cdQ2rXYJ","executionInfo":{"status":"ok","timestamp":1763117594148,"user_tz":0,"elapsed":698,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"d01825a6-aa89-45c8-a88d-5ed0157c0497"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 9: Sliding-window slicing\n","============================================================\n","\n","Using train fold: FOLD_ID=3\n","Output directory: data/lara/mbientlab/proc/windows/fold_03\n","\n","Sliding-window parameters:\n","  Window length: 3.0 s = 150 samples @ 50.0 Hz\n","  Step size: 75 samples (overlap 50%)\n","  Dominant label threshold: 80%\n","\n","============================================================\n","1. Load cleaned data\n","============================================================\n","Loading data: data/lara/mbientlab/proc/labeled/\n","Data shape: (556504, 16)\n","Number of subjects: 8\n","Number of label classes: 6\n","\n","Feature columns: ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","Time column: time_sec\n","\n","============================================================\n","2. Load train/test split\n","============================================================\n","Train subjects: 7\n","Test subjects: 1\n","\n","Train set: 473,845 samples\n","Test set: 82,659 samples\n","\n","============================================================\n","4. Extract training-set windows\n","============================================================\n","Processing train set (473,845 samples)...\n","✓ Extracted training windows: 4,858\n","  X_train shape: (4858, 150, 8)\n","  Feature dimensions: 8 channels × 150 timesteps\n","\n","Train-set label distribution:\n","  Label 1:    791 windows (16.28%)\n","  Label 2:    852 windows (17.54%)\n","  Label 3:    439 windows ( 9.04%)\n","  Label 4:     55 windows ( 1.13%)\n","  Label 5:  2,327 windows (47.90%)\n","  Label 6:    394 windows ( 8.11%)\n","\n","Train-set label purity:\n","  Mean: 98.47%\n","  Min: 80.00%\n","\n","Saving train set...\n","  ✓ data/lara/mbientlab/proc/windows/fold_03/X_train.npy (feature tensor)\n","  ✓ data/lara/mbientlab/proc/windows/fold_03/X_train.parquet (metadata)\n","  ✓ data/lara/mbientlab/proc/windows/fold_03/y_train.npy\n","  ✓ train_label_counts.csv\n","\n","============================================================\n","5. Extract test-set windows\n","============================================================\n","Processing test set (82,659 samples)...\n","✓ Extracted test windows: 873\n","  X_test shape: (873, 150, 8)\n","\n","Test-set label distribution:\n","  Label 1:    113 windows (12.94%)\n","  Label 2:    142 windows (16.27%)\n","  Label 3:    164 windows (18.79%)\n","  Label 4:     11 windows ( 1.26%)\n","  Label 5:    378 windows (43.30%)\n","  Label 6:     65 windows ( 7.45%)\n","\n","Test-set label purity:\n","  Mean: 98.47%\n","  Min: 80.00%\n","\n","Saving test set...\n","  ✓ data/lara/mbientlab/proc/windows/fold_03/X_test.npy (feature tensor)\n","  ✓ data/lara/mbientlab/proc/windows/fold_03/X_test.parquet (metadata)\n","  ✓ data/lara/mbientlab/proc/windows/fold_03/y_test.npy\n","  ✓ test_label_counts.csv\n","\n","============================================================\n","6. Save window configuration\n","============================================================\n","✓ Saved config: configs/windows.yaml\n","✓ Saved config: configs/windows.json\n","\n","============================================================\n","Step 9 complete - Sliding-window slicing (revised)\n","============================================================\n","\n","Window parameters:\n","  Window length: 3.0 s = 150 samples\n","  Step size: 75 samples (overlap 50%)\n","  Dominant threshold: 80%\n","  Feature dimension: 8 channels\n","  Sort order: time_sec\n","\n","Train set:\n","  # windows: 4,858\n","  Shape: (4858, 150, 8)\n","  Subjects: 7\n","  Sessions: 82\n","  Average purity: 98.47%\n","\n","Test set:\n","  # windows: 873\n","  Shape: (873, 150, 8)\n","  Subjects: 1\n","  Sessions: 14\n","  Average purity: 98.47%\n","\n","Outputs:\n","  Directory: data/lara/mbientlab/proc/windows/fold_03/\n","  Train: X_train.npy (tensor), X_train.parquet (metadata), y_train.npy\n","         train_label_counts.csv\n","  Test:  X_test.npy (tensor), X_test.parquet (metadata), y_test.npy\n","         test_label_counts.csv\n","  Config: configs/windows.yaml\n","\n","Key fixes:\n","  1. ✓ Sort by time column (time_sec preferred, otherwise index)\n","  2. ✓ Metadata filenames aligned to X_train/test.parquet\n","  3. ✓ Safe variable initialization (avoid NameError)\n","  4. ✓ Export label-distribution snapshot CSV (for audit)\n","  5. ✓ Time continuity check (allow 10% jitter)\n","  6. ✓ Slice by session (avoid cross-session windows)\n","  7. ✓ Dominant label proportion ≥ 80%\n","  8. ✓ Discard windows containing NaN\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\n","\"\"\"\n","Step 10: LOSO Split (top-conf/journal grade)\n","Leave-One-Subject-Out: 1 subject for test per fold, the rest for training\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import json\n","import yaml\n","from collections import defaultdict\n","\n","print(\"=\"*60)\n","print(\"Step 10: LOSO split\")\n","print(\"=\"*60)\n","\n","# Path configuration\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","configs_dir.mkdir(parents=True, exist_ok=True)\n","\n","# ========== 1. Load data and get subject list ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Load data and get subject list\")\n","print(\"=\"*60)\n","\n","labeled_dir = proc_dir / \"labeled\"\n","print(f\"Loading data: {labeled_dir}/\")\n","\n","df = pd.read_parquet(labeled_dir)\n","\n","print(f\"Data shape: {df.shape}\")\n","print(f\"Total samples: {len(df):,}\")\n","\n","# Extract all subjects\n","all_subjects = sorted(df['subject_id'].unique().tolist())\n","n_subjects = len(all_subjects)\n","\n","print(f\"\\nSubject list:\")\n","print(f\"  Total: {n_subjects} subjects\")\n","print(f\"  IDs: {all_subjects}\")\n","\n","# Sample count per subject\n","subject_sample_counts = df['subject_id'].value_counts().sort_index()\n","print(f\"\\nSample count per subject:\")\n","for subj in all_subjects:\n","    count = subject_sample_counts.get(subj, 0)\n","    pct = count / len(df) * 100\n","    print(f\"  {subj}: {count:8,} samples ({pct:5.2f}%)\")\n","\n","# ========== 2. Generate LOSO split ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Generate LOSO split\")\n","print(\"=\"*60)\n","\n","print(f\"\\nLOSO strategy: Leave-One-Subject-Out\")\n","print(f\"  #folds = #subjects = {n_subjects}\")\n","print(f\"  Per fold: 1 subject for test, {n_subjects-1} subjects for train\")\n","\n","# Create split dict\n","splits = {}\n","\n","for fold_id, test_subject in enumerate(all_subjects):\n","    # Test set: current subject\n","    test_subjects = [test_subject]\n","\n","    # Train set: all other subjects\n","    train_subjects = [s for s in all_subjects if s != test_subject]\n","\n","    # Save split\n","    splits[str(fold_id)] = {\n","        \"fold_id\": fold_id,\n","        \"test_subject\": test_subject,\n","        \"test_subjects\": test_subjects,  # list for compatibility\n","        \"train_subjects\": train_subjects,\n","        \"n_train\": len(train_subjects),\n","        \"n_test\": len(test_subjects),\n","    }\n","\n","    print(f\"  Fold {fold_id}: test {test_subject}, train {len(train_subjects)} subjects\")\n","\n","print(f\"\\n✓ Generated {len(splits)} LOSO folds\")\n","\n","# ========== 3. Validate split integrity ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Validate split integrity\")\n","print(\"=\"*60)\n","\n","# Check 1: each subject appears exactly once in the test set\n","test_subject_appearances = defaultdict(int)\n","for fold_id, fold_info in splits.items():\n","    for subj in fold_info['test_subjects']:\n","        test_subject_appearances[subj] += 1\n","\n","print(f\"\\nCheck 1: times each subject appears as test\")\n","all_once = True\n","for subj in all_subjects:\n","    count = test_subject_appearances[subj]\n","    status = \"✓\" if count == 1 else \"✗\"\n","    print(f\"  {status} {subj}: {count} time(s)\")\n","    if count != 1:\n","        all_once = False\n","\n","if all_once:\n","    print(f\"  ✓ All subjects appear exactly once\")\n","else:\n","    raise RuntimeError(\"Split validation failed: subject test appearances not equal to 1\")\n","\n","# Check 2: train and test sets are disjoint\n","print(f\"\\nCheck 2: train and test sets are disjoint\")\n","all_disjoint = True\n","for fold_id, fold_info in splits.items():\n","    train_set = set(fold_info['train_subjects'])\n","    test_set = set(fold_info['test_subjects'])\n","    overlap = train_set & test_set\n","\n","    if overlap:\n","        print(f\"  ✗ Fold {fold_id}: overlap exists {overlap}\")\n","        all_disjoint = False\n","\n","if all_disjoint:\n","    print(f\"  ✓ Train/test sets are completely disjoint for all folds\")\n","else:\n","    raise RuntimeError(\"Split validation failed: train and test sets have overlap\")\n","\n","# Check 3: all subjects covered\n","print(f\"\\nCheck 3: all subjects covered\")\n","covered_subjects = set()\n","for fold_id, fold_info in splits.items():\n","    covered_subjects.update(fold_info['train_subjects'])\n","    covered_subjects.update(fold_info['test_subjects'])\n","\n","missing = set(all_subjects) - covered_subjects\n","extra = covered_subjects - set(all_subjects)\n","\n","if not missing and not extra:\n","    print(f\"  ✓ All subjects are covered; no missing or extra subjects\")\n","else:\n","    if missing:\n","        print(f\"  ✗ Missing subjects: {missing}\")\n","    if extra:\n","        print(f\"  ✗ Extra subjects: {extra}\")\n","    raise RuntimeError(\"Split validation failed: subject coverage incomplete\")\n","\n","# Check 4: sample count stats\n","print(f\"\\nCheck 4: per-fold sample counts\")\n","fold_sample_stats = []\n","for fold_id, fold_info in splits.items():\n","    train_subjects = fold_info['train_subjects']\n","    test_subjects = fold_info['test_subjects']\n","\n","    n_train_samples = df[df['subject_id'].isin(train_subjects)].shape[0]\n","    n_test_samples = df[df['subject_id'].isin(test_subjects)].shape[0]\n","\n","    fold_sample_stats.append({\n","        'fold_id': int(fold_id),\n","        'test_subject': fold_info['test_subject'],\n","        'n_train_samples': n_train_samples,\n","        'n_test_samples': n_test_samples,\n","        'train_ratio': round(n_train_samples / len(df), 4),\n","        'test_ratio': round(n_test_samples / len(df), 4),\n","    })\n","\n","df_fold_stats = pd.DataFrame(fold_sample_stats)\n","\n","print(f\"\\nPer-fold sample distribution:\")\n","print(df_fold_stats.to_string(index=False))\n","\n","# Summary\n","print(f\"\\nSample distribution summary:\")\n","print(f\"  Train sample count: {df_fold_stats['n_train_samples'].min():,} ~ {df_fold_stats['n_train_samples'].max():,}\")\n","print(f\"  Test sample count: {df_fold_stats['n_test_samples'].min():,} ~ {df_fold_stats['n_test_samples'].max():,}\")\n","print(f\"  Average train ratio: {df_fold_stats['train_ratio'].mean()*100:.2f}%\")\n","print(f\"  Average test ratio: {df_fold_stats['test_ratio'].mean()*100:.2f}%\")\n","\n","print(f\"\\n✓ All validations passed\")\n","\n","# ========== 4. Save split configuration ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"4. Save split configuration\")\n","print(\"=\"*60)\n","\n","# Save splits.json\n","splits_file = configs_dir / \"splits.json\"\n","with open(splits_file, 'w', encoding='utf-8') as f:\n","    json.dump(splits, f, indent=2)\n","\n","print(f\"✓ Saved: {splits_file}\")\n","\n","# Save detailed config (with metadata)\n","loso_config = {\n","    'strategy': 'LOSO (Leave-One-Subject-Out)',\n","    'description': 'One subject for test in each fold; remaining subjects for training',\n","    'n_folds': n_subjects,\n","    'n_subjects': n_subjects,\n","    'all_subjects': all_subjects,\n","    'fold_statistics': {\n","        'train_samples_min': int(df_fold_stats['n_train_samples'].min()),\n","        'train_samples_max': int(df_fold_stats['n_train_samples'].max()),\n","        'train_samples_mean': int(df_fold_stats['n_train_samples'].mean()),\n","        'test_samples_min': int(df_fold_stats['n_test_samples'].min()),\n","        'test_samples_max': int(df_fold_stats['n_test_samples'].max()),\n","        'test_samples_mean': int(df_fold_stats['n_test_samples'].mean()),\n","        'avg_train_ratio': round(float(df_fold_stats['train_ratio'].mean()), 4),\n","        'avg_test_ratio': round(float(df_fold_stats['test_ratio'].mean()), 4),\n","    },\n","    'validation': {\n","        'no_subject_overlap': True,\n","        'all_subjects_covered': True,\n","        'each_subject_tested_once': True,\n","    },\n","    'anti_leakage_principles': [\n","        'Train and test sets are completely separated by subject',\n","        'Window slicing is performed after splitting to ensure no cross-fold leakage',\n","        'Statistics (mean/std) are computed from the training fold only',\n","        'Feature engineering is performed independently within each fold',\n","        'Hyperparameter tuning uses training-fold data only (nested CV optional)',\n","        'Final model evaluation is strictly based on the corresponding fold’s test set',\n","        'When aggregating results across folds, use metrics from independent test sets',\n","    ],\n","    'notes': [\n","        f'LOSO split: {n_subjects} folds; 1 subject per fold for test',\n","        'Ensure each subject appears exactly once in the test set',\n","        'Train/test sets are mutually exclusive with no subject overlap',\n","        'Suitable for small-sample settings with large inter-subject variability',\n","        'Report mean and standard deviation across all folds',\n","    ]\n","}\n","\n","loso_config_file = configs_dir / \"loso.yaml\"\n","with open(loso_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(loso_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","print(f\"✓ Saved: {loso_config_file}\")\n","\n","loso_config_json = configs_dir / \"loso.json\"\n","with open(loso_config_json, 'w', encoding='utf-8') as f:\n","    json.dump(loso_config, f, indent=2)\n","\n","print(f\"✓ Saved: {loso_config_json}\")\n","\n","# Save per-fold sample stats\n","fold_stats_file = configs_dir / \"loso_fold_stats.csv\"\n","df_fold_stats.to_csv(fold_stats_file, index=False)\n","print(f\"✓ Saved: {fold_stats_file}\")\n","\n","# ========== 5. Generate usage example ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Generate usage example\")\n","print(\"=\"*60)\n","\n","example_code = '''\n","# ========== LOSO Usage Example ==========\n","\n","import json\n","from pathlib import Path\n","\n","# 1. Load splits\n","with open(\"configs/splits.json\", \"r\") as f:\n","    splits = json.load(f)\n","\n","# 2. Iterate over folds\n","for fold_id in range(len(splits)):\n","    print(f\"\\\\n========== Fold {fold_id} ==========\")\n","\n","    # Get current fold split\n","    fold = splits[str(fold_id)]\n","    train_subjects = fold[\"train_subjects\"]\n","    test_subject = fold[\"test_subject\"]\n","\n","    print(f\"Train: {len(train_subjects)} subjects\")\n","    print(f\"Test: {test_subject}\")\n","\n","    # 3. Set environment variable (used by later steps)\n","    import os\n","    os.environ[\"FOLD_ID\"] = str(fold_id)\n","\n","    # 4. Run training pipeline\n","    # - Step 6: per-fold clipping (statistics from train only)\n","    # - Step 7: per-fold standardization (statistics from train only)\n","    # - Step 9: per-fold windowing\n","    # - Train model (training windows only)\n","    # - Evaluate model (test windows only)\n","\n","    # 5. Save results of current fold\n","    # results[fold_id] = {\"accuracy\": acc, \"f1\": f1, ...}\n","\n","# 6. Aggregate results across folds\n","# mean_acc = np.mean([r[\"accuracy\"] for r in results.values()])\n","# std_acc = np.std([r[\"accuracy\"] for r in results.values()])\n","# print(f\"Mean accuracy: {mean_acc:.4f} ± {std_acc:.4f}\")\n","\n","# ========== Anti-leakage Checklist ==========\n","# ✓ Train/test separated by subject\n","# ✓ Statistics (mean/std) computed from training set only\n","# ✓ Feature scaling uses parameters from training set\n","# ✓ Windowing performed after splitting\n","# ✓ Hyperparameter tuning uses training data only\n","# ✓ Test set used strictly for final evaluation\n","'''\n","\n","example_file = configs_dir / \"loso_usage_example.py\"\n","with open(example_file, 'w', encoding='utf-8') as f:\n","    f.write(example_code)\n","\n","print(f\"✓ Generated usage example: {example_file}\")\n","\n","print(\"\\nHow to use:\")\n","print(\"  1. export FOLD_ID=0  # set current fold\")\n","print(\"  2. Run steps 6–9 (they will use the corresponding fold automatically)\")\n","print(\"  3. Train the model and evaluate\")\n","print(\"  4. Repeat steps 1–3 for all folds\")\n","print(\"  5. Aggregate results (mean ± std)\")\n","\n","# ========== 6. Split visualization info ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Split visualization info\")\n","print(\"=\"*60)\n","\n","print(f\"\\nLOSO split matrix (first 5 folds):\")\n","print(f\"{'Fold':<6} {'TestSubject':<12} {'#TrainSubs':<12} {'#TestSamples':<12} {'#TrainSamples':<12}\")\n","print(\"-\" * 60)\n","\n","for i in range(min(5, len(splits))):\n","    fold = splits[str(i)]\n","    stats = df_fold_stats[df_fold_stats['fold_id'] == i].iloc[0]\n","    print(f\"{i:<6} {fold['test_subject']:<12} {fold['n_train']:<12} \"\n","          f\"{stats['n_test_samples']:<12} {stats['n_train_samples']:<12}\")\n","\n","if len(splits) > 5:\n","    print(f\"... (total {len(splits)} folds)\")\n","\n","# ========== 7. Summary ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 10 complete - LOSO split\")\n","print(\"=\"*60)\n","\n","print(f\"\\nSplit strategy:\")\n","print(f\"  Method: LOSO (Leave-One-Subject-Out)\")\n","print(f\"  #folds: {n_subjects}\")\n","print(f\"  #subjects: {n_subjects}\")\n","print(f\"  Train per fold: {n_subjects-1} subjects\")\n","print(f\"  Test per fold: 1 subject\")\n","\n","print(f\"\\nData distribution:\")\n","print(f\"  Total samples: {len(df):,}\")\n","print(f\"  Train ratio (avg): {df_fold_stats['train_ratio'].mean()*100:.2f}%\")\n","print(f\"  Test ratio (avg): {df_fold_stats['test_ratio'].mean()*100:.2f}%\")\n","\n","print(f\"\\nValidation results:\")\n","print(f\"  ✓ No subject overlap\")\n","print(f\"  ✓ All subjects covered\")\n","print(f\"  ✓ Each subject tested exactly once\")\n","print(f\"  ✓ Train/test sets are disjoint\")\n","\n","print(f\"\\nOutput files:\")\n","print(f\"  Main config: {splits_file}\")\n","print(f\"  Detailed config: {loso_config_file}\")\n","print(f\"  Fold stats: {fold_stats_file}\")\n","print(f\"  Usage example: {example_file}\")\n","\n","print(\"\\nAnti-leakage principles:\")\n","print(\"  1. ✓ Fully separated by subject\")\n","print(\"  2. ✓ Statistics computed from training fold only\")\n","print(\"  3. ✓ Feature engineering is fold-internal\")\n","print(\"  4. ✓ Window slicing performed after splitting\")\n","print(\"  5. ✓ Hyperparameter tuning limited to training data\")\n","print(\"  6. ✓ Test set used strictly for independent evaluation\")\n","print(\"  7. ✓ Cross-fold aggregation uses independent metrics\")\n","\n","print(\"\\nNext steps:\")\n","print(\"  - Set export FOLD_ID=<fold_id>\")\n","print(\"  - Re-run steps 6–9 (per-fold processing)\")\n","print(\"  - Train and evaluate models\")\n","print(\"  - Iterate all folds and aggregate results\")\n","\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G425YZGyr8SM","executionInfo":{"status":"ok","timestamp":1763117594487,"user_tz":0,"elapsed":339,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"12c15b35-8e36-47f0-c613-b44bf7f6fd8b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 10: LOSO split\n","============================================================\n","\n","============================================================\n","1. Load data and get subject list\n","============================================================\n","Loading data: data/lara/mbientlab/proc/labeled/\n","Data shape: (556504, 16)\n","Total samples: 556,504\n","\n","Subject list:\n","  Total: 8 subjects\n","  IDs: ['S07', 'S08', 'S09', 'S10', 'S11', 'S12', 'S13', 'S14']\n","\n","Sample count per subject:\n","  S07:   76,522 samples (13.75%)\n","  S08:   64,857 samples (11.65%)\n","  S09:   77,701 samples (13.96%)\n","  S10:   82,659 samples (14.85%)\n","  S11:   70,410 samples (12.65%)\n","  S12:   30,923 samples ( 5.56%)\n","  S13:   82,335 samples (14.80%)\n","  S14:   71,097 samples (12.78%)\n","\n","============================================================\n","2. Generate LOSO split\n","============================================================\n","\n","LOSO strategy: Leave-One-Subject-Out\n","  #folds = #subjects = 8\n","  Per fold: 1 subject for test, 7 subjects for train\n","  Fold 0: test S07, train 7 subjects\n","  Fold 1: test S08, train 7 subjects\n","  Fold 2: test S09, train 7 subjects\n","  Fold 3: test S10, train 7 subjects\n","  Fold 4: test S11, train 7 subjects\n","  Fold 5: test S12, train 7 subjects\n","  Fold 6: test S13, train 7 subjects\n","  Fold 7: test S14, train 7 subjects\n","\n","✓ Generated 8 LOSO folds\n","\n","============================================================\n","3. Validate split integrity\n","============================================================\n","\n","Check 1: times each subject appears as test\n","  ✓ S07: 1 time(s)\n","  ✓ S08: 1 time(s)\n","  ✓ S09: 1 time(s)\n","  ✓ S10: 1 time(s)\n","  ✓ S11: 1 time(s)\n","  ✓ S12: 1 time(s)\n","  ✓ S13: 1 time(s)\n","  ✓ S14: 1 time(s)\n","  ✓ All subjects appear exactly once\n","\n","Check 2: train and test sets are disjoint\n","  ✓ Train/test sets are completely disjoint for all folds\n","\n","Check 3: all subjects covered\n","  ✓ All subjects are covered; no missing or extra subjects\n","\n","Check 4: per-fold sample counts\n","\n","Per-fold sample distribution:\n"," fold_id test_subject  n_train_samples  n_test_samples  train_ratio  test_ratio\n","       0          S07           479982           76522       0.8625      0.1375\n","       1          S08           491647           64857       0.8835      0.1165\n","       2          S09           478803           77701       0.8604      0.1396\n","       3          S10           473845           82659       0.8515      0.1485\n","       4          S11           486094           70410       0.8735      0.1265\n","       5          S12           525581           30923       0.9444      0.0556\n","       6          S13           474169           82335       0.8520      0.1480\n","       7          S14           485407           71097       0.8722      0.1278\n","\n","Sample distribution summary:\n","  Train sample count: 473,845 ~ 525,581\n","  Test sample count: 30,923 ~ 82,659\n","  Average train ratio: 87.50%\n","  Average test ratio: 12.50%\n","\n","✓ All validations passed\n","\n","============================================================\n","4. Save split configuration\n","============================================================\n","✓ Saved: configs/splits.json\n","✓ Saved: configs/loso.yaml\n","✓ Saved: configs/loso.json\n","✓ Saved: configs/loso_fold_stats.csv\n","\n","============================================================\n","5. Generate usage example\n","============================================================\n","✓ Generated usage example: configs/loso_usage_example.py\n","\n","How to use:\n","  1. export FOLD_ID=0  # set current fold\n","  2. Run steps 6–9 (they will use the corresponding fold automatically)\n","  3. Train the model and evaluate\n","  4. Repeat steps 1–3 for all folds\n","  5. Aggregate results (mean ± std)\n","\n","============================================================\n","6. Split visualization info\n","============================================================\n","\n","LOSO split matrix (first 5 folds):\n","Fold   TestSubject  #TrainSubs   #TestSamples #TrainSamples\n","------------------------------------------------------------\n","0      S07          7            76522        479982      \n","1      S08          7            64857        491647      \n","2      S09          7            77701        478803      \n","3      S10          7            82659        473845      \n","4      S11          7            70410        486094      \n","... (total 8 folds)\n","\n","============================================================\n","Step 10 complete - LOSO split\n","============================================================\n","\n","Split strategy:\n","  Method: LOSO (Leave-One-Subject-Out)\n","  #folds: 8\n","  #subjects: 8\n","  Train per fold: 7 subjects\n","  Test per fold: 1 subject\n","\n","Data distribution:\n","  Total samples: 556,504\n","  Train ratio (avg): 87.50%\n","  Test ratio (avg): 12.50%\n","\n","Validation results:\n","  ✓ No subject overlap\n","  ✓ All subjects covered\n","  ✓ Each subject tested exactly once\n","  ✓ Train/test sets are disjoint\n","\n","Output files:\n","  Main config: configs/splits.json\n","  Detailed config: configs/loso.yaml\n","  Fold stats: configs/loso_fold_stats.csv\n","  Usage example: configs/loso_usage_example.py\n","\n","Anti-leakage principles:\n","  1. ✓ Fully separated by subject\n","  2. ✓ Statistics computed from training fold only\n","  3. ✓ Feature engineering is fold-internal\n","  4. ✓ Window slicing performed after splitting\n","  5. ✓ Hyperparameter tuning limited to training data\n","  6. ✓ Test set used strictly for independent evaluation\n","  7. ✓ Cross-fold aggregation uses independent metrics\n","\n","Next steps:\n","  - Set export FOLD_ID=<fold_id>\n","  - Re-run steps 6–9 (per-fold processing)\n","  - Train and evaluate models\n","  - Iterate all folds and aggregate results\n","============================================================\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","\n","# ========== Manually set FOLD_ID (if the environment variable is not set) ==========\n","if \"FOLD_ID\" not in os.environ:\n","    print(\"⚠️ Environment variable FOLD_ID is not set; please specify it manually:\")\n","    print(\"Hint: if your LOSO has N folds, FOLD_ID should be 0 to N-1\")\n","    fold_input = input(\"Please enter FOLD_ID (press Enter to default to 0): \").strip()\n","    os.environ[\"FOLD_ID\"] = fold_input if fold_input else \"0\"\n","    print(f\"✓ FOLD_ID has been set to {os.environ['FOLD_ID']}\")\n","\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","\n","\"\"\"\n","Step 11: Feature Engineering (KNN/RF) - Final Version\n","Compute time- + frequency- + correlation-domain features per window, with magnitudes & energy\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import json\n","import yaml\n","import os\n","from scipy import stats, fft\n","from sklearn.preprocessing import StandardScaler\n","import pickle\n","\n","# ========== Config ==========\n","\n","SAMPLING_RATE_HZ = 50.0\n","\n","print(\"=\"*60)\n","print(\"Step 11: Feature Engineering (KNN/RF)\")\n","print(\"=\"*60)\n","\n","# Path config\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","\n","# Persist per fold (avoid overwrite)\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","fold_tag = f\"fold_{FOLD_ID:02d}\" if FOLD_ID >= 0 else \"all\"\n","\n","# ========== Change 1: force using this-fold directory, disable fallback ==========\n","win_dir_fold = proc_dir / \"windows\" / fold_tag\n","assert win_dir_fold.exists(), f\"Not found {win_dir_fold}; please run this fold steps 6→7→8→9 first\"\n","windows_dir = win_dir_fold\n","\n","features_dir = proc_dir / \"features\" / fold_tag\n","features_dir.mkdir(parents=True, exist_ok=True)\n","\n","if FOLD_ID >= 0:\n","    print(f\"Using train fold: FOLD_ID={FOLD_ID}\")\n","    print(f\"Windows dir: {windows_dir}\")\n","    print(f\"Output dir: {features_dir}\")\n","else:\n","    print(\"FOLD_ID not specified; using all data\")\n","    print(f\"Windows dir: {windows_dir}\")\n","    print(f\"Output dir: {features_dir}\")\n","\n","# ========== 1. Load window data & metadata ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"1. Load window data & metadata\")\n","print(\"=\"*60)\n","\n","# Load train windows\n","X_train_file = windows_dir / \"X_train.npy\"\n","y_train_file = windows_dir / \"y_train.npy\"\n","train_meta_file = windows_dir / \"X_train.parquet\"\n","\n","if not X_train_file.exists() or not y_train_file.exists():\n","    raise FileNotFoundError(f\"Window data not found; please run Step 9 first\")\n","\n","X_train = np.load(X_train_file)\n","y_train = np.load(y_train_file)\n","df_train_meta = pd.read_parquet(train_meta_file)\n","\n","# Consistency assertion\n","assert X_train.shape[0] == len(df_train_meta) == len(y_train), \\\n","    f\"Inconsistent train data: X={X_train.shape[0]}, meta={len(df_train_meta)}, y={len(y_train)}\"\n","\n","print(f\"\\nTrain windows:\")\n","print(f\"  X_train shape: {X_train.shape}\")\n","print(f\"  y_train shape: {y_train.shape}\")\n","print(f\"  metadata: {len(df_train_meta)} rows\")\n","print(f\"  ✓ Consistency check passed\")\n","\n","# Load test windows (if exist)\n","X_test_file = windows_dir / \"X_test.npy\"\n","y_test_file = windows_dir / \"y_test.npy\"\n","test_meta_file = windows_dir / \"X_test.parquet\"\n","\n","if X_test_file.exists() and y_test_file.exists():\n","    X_test = np.load(X_test_file)\n","    y_test = np.load(y_test_file)\n","    df_test_meta = pd.read_parquet(test_meta_file)\n","\n","    # Consistency assertion\n","    assert X_test.shape[0] == len(df_test_meta) == len(y_test), \\\n","        f\"Inconsistent test data: X={X_test.shape[0]}, meta={len(df_test_meta)}, y={len(y_test)}\"\n","\n","    print(f\"\\nTest windows:\")\n","    print(f\"  X_test shape: {X_test.shape}\")\n","    print(f\"  y_test shape: {y_test.shape}\")\n","    print(f\"  metadata: {len(df_test_meta)} rows\")\n","    print(f\"  ✓ Consistency check passed\")\n","    has_test = True\n","else:\n","    print(f\"\\nTest windows not found\")\n","    X_test = None\n","    y_test = None\n","    df_test_meta = None\n","    has_test = False\n","\n","# ========== Change 2: anti-leakage assertions ==========\n","train_subs = set(df_train_meta[\"subject_id\"].unique())\n","test_subs = set(df_test_meta[\"subject_id\"].unique()) if has_test else set()\n","assert train_subs.isdisjoint(test_subs), f\"Train/Test subjects overlap: {train_subs & test_subs}\"\n","if has_test:\n","    assert len(test_subs) == 1, f\"Test set contains multiple subjects: {test_subs}\"\n","print(f\"\\n✓ Anti-leakage check passed:\")\n","print(f\"  Train subjects: {len(train_subs)}\")\n","print(f\"  Test subjects: {len(test_subs)}\")\n","\n","# Channel names\n","channel_names = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","# ========== 2. Define feature extraction functions (with robustness fixes) ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"2. Define feature extraction functions\")\n","print(\"=\"*60)\n","\n","def extract_time_features(x):\n","    \"\"\"Extract time-domain features (with robustness)\"\"\"\n","    features = {}\n","\n","    # Basic stats\n","    features['mean'] = np.mean(x)\n","    features['std'] = np.std(x)\n","    features['min'] = np.min(x)\n","    features['max'] = np.max(x)\n","    features['range'] = features['max'] - features['min']\n","\n","    # Quantiles\n","    features['q25'] = np.percentile(x, 25)\n","    features['q50'] = np.median(x)\n","    features['q75'] = np.percentile(x, 75)\n","    features['iqr'] = features['q75'] - features['q25']\n","\n","    # Robust stats\n","    features['mad'] = np.median(np.abs(x - features['q50']))\n","\n","    # Higher-order stats (unbiased)\n","    features['skew'] = stats.skew(x, bias=False, nan_policy='omit')\n","    features['kurtosis'] = stats.kurtosis(x, bias=False, fisher=True, nan_policy='omit')\n","\n","    # Energy-related\n","    features['rms'] = np.sqrt(np.mean(x**2))\n","    features['energy'] = np.sum(x**2)\n","\n","    # Zero crossing rate\n","    zero_crossings = np.sum(np.diff(np.sign(x)) != 0)\n","    features['zero_crossing_rate'] = zero_crossings / len(x)\n","\n","    # Mean absolute difference\n","    features['mean_abs_diff'] = np.mean(np.abs(np.diff(x)))\n","\n","    return features\n","\n","def extract_freq_features(x, fs=50.0):\n","    \"\"\"Extract frequency-domain features (skip DC)\"\"\"\n","    features = {}\n","\n","    # FFT (Hann window to reduce spectral leakage)\n","    window = np.hanning(len(x))\n","    x_windowed = x * window\n","\n","    fft_vals = fft.fft(x_windowed)\n","    fft_mag = np.abs(fft_vals[:len(x)//2])\n","    fft_freq = fft.fftfreq(len(x), 1/fs)[:len(x)//2]\n","\n","    # Normalized power spectrum\n","    psd = fft_mag**2\n","    psd_norm = psd / np.sum(psd) if np.sum(psd) > 0 else psd\n","\n","    # Spectral energy\n","    features['spectral_energy'] = np.sum(psd)\n","\n","    # Spectral entropy\n","    psd_norm_pos = psd_norm[psd_norm > 0]\n","    features['spectral_entropy'] = -np.sum(psd_norm_pos * np.log2(psd_norm_pos)) if len(psd_norm_pos) > 0 else 0\n","\n","    # Peak frequency (skip DC to avoid 0 Hz dominating)\n","    fft_mag_no_dc = fft_mag.copy()\n","    fft_mag_no_dc[0] = 0.0\n","    peak_idx = np.argmax(fft_mag_no_dc)\n","    features['peak_frequency'] = fft_freq[peak_idx]\n","\n","    # Spectral centroid\n","    features['spectral_centroid'] = np.sum(fft_freq * psd_norm) if np.sum(psd_norm) > 0 else 0\n","\n","    # Spectral bandwidth (std)\n","    features['spectral_bandwidth'] = np.sqrt(np.sum(((fft_freq - features['spectral_centroid'])**2) * psd_norm))\n","\n","    # Spectral rolloff (85% energy)\n","    cumsum_psd = np.cumsum(psd_norm)\n","    rolloff_idx = np.where(cumsum_psd >= 0.85)[0]\n","    features['spectral_rolloff'] = fft_freq[rolloff_idx[0]] if len(rolloff_idx) > 0 else fft_freq[-1]\n","\n","    # Low-frequency energy ratio (0–5 Hz / total energy)\n","    low_freq_mask = fft_freq <= 5.0\n","    features['low_freq_energy_ratio'] = np.sum(psd[low_freq_mask]) / np.sum(psd) if np.sum(psd) > 0 else 0\n","\n","    return features\n","\n","def extract_window_features(window, channel_names, fs=50.0):\n","    \"\"\"Extract all features for a single window (with deduplicated cross-correlation)\"\"\"\n","    all_features = {}\n","    n_channels = window.shape[1]\n","\n","    # Time & frequency features per channel\n","    for ch_idx, ch_name in enumerate(channel_names):\n","        signal_data = window[:, ch_idx]\n","\n","        # Time-domain\n","        time_feats = extract_time_features(signal_data)\n","        for feat_name, feat_val in time_feats.items():\n","            all_features[f'{ch_name}_{feat_name}'] = feat_val\n","\n","        # Frequency-domain\n","        freq_feats = extract_freq_features(signal_data, fs)\n","        for feat_name, feat_val in freq_feats.items():\n","            all_features[f'{ch_name}_{feat_name}'] = feat_val\n","\n","    # Correlation features (deduplicated)\n","    # Correlation matrix across channels\n","    corr_mat = np.corrcoef(window, rowvar=False)\n","\n","    # Autocorrelation (lag=1) for each channel\n","    for ch_idx, ch_name in enumerate(channel_names):\n","        signal_data = window[:, ch_idx]\n","        if len(signal_data) > 1:\n","            autocorr = np.corrcoef(signal_data[:-1], signal_data[1:])[0, 1]\n","            all_features[f'{ch_name}_autocorr_lag1'] = autocorr if not np.isnan(autocorr) else 0\n","        else:\n","            all_features[f'{ch_name}_autocorr_lag1'] = 0\n","\n","    # Cross-correlation (upper triangle only; deduplicated): C(8,2)=28 pairs\n","    for i in range(n_channels):\n","        for j in range(i + 1, n_channels):\n","            val = corr_mat[i, j]\n","            key = f'crosscorr_{channel_names[i]}_{channel_names[j]}'\n","            all_features[key] = 0 if np.isnan(val) else val\n","\n","    return all_features\n","\n","print(\"Feature types:\")\n","print(\"  Time-domain: mean, std, min, max, range, q25, q50, q75, iqr, mad,\")\n","print(\"               skew, kurtosis, rms, energy, zero_crossing_rate, mean_abs_diff\")\n","print(\"  Frequency-domain: spectral_energy, spectral_entropy, peak_frequency (skip DC),\")\n","print(\"                    spectral_centroid, spectral_bandwidth, spectral_rolloff,\")\n","print(\"                    low_freq_energy_ratio\")\n","print(\"  Correlation: autocorr_lag1 (8 dims), crosscorr_* (28 dims, deduplicated)\")\n","print(f\"  Expected total dimension: 16×8 + 7×8 + 8 + 28 = 220 dims\")\n","\n","# ========== 3. Extract training-set features ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"3. Extract training-set features\")\n","print(\"=\"*60)\n","\n","print(f\"Processing {X_train.shape[0]:,} training windows...\")\n","\n","train_features_list = []\n","for i in range(X_train.shape[0]):\n","    window = X_train[i]\n","    features = extract_window_features(window, channel_names, SAMPLING_RATE_HZ)\n","    train_features_list.append(features)\n","\n","    if (i + 1) % 1000 == 0:\n","        print(f\"  Processed {i+1:,} / {X_train.shape[0]:,} windows\")\n","\n","# To DataFrame\n","df_train_features = pd.DataFrame(train_features_list)\n","\n","print(f\"\\n✓ Training-set features:\")\n","print(f\"  #samples: {len(df_train_features):,}\")\n","print(f\"  feature dimension: {df_train_features.shape[1]}\")\n","\n","# Check NaN & Inf\n","nan_count = df_train_features.isna().sum().sum()\n","inf_count = np.isinf(df_train_features.values).sum()\n","\n","if nan_count > 0 or inf_count > 0:\n","    print(f\"\\n⚠️ Found invalid values:\")\n","    print(f\"  NaN: {nan_count}\")\n","    print(f\"  Inf: {inf_count}\")\n","\n","    # Fill invalids\n","    df_train_features = df_train_features.replace([np.inf, -np.inf], np.nan)\n","    df_train_features = df_train_features.fillna(0)\n","    print(f\"  ✓ Filled with 0\")\n","\n","# Save feature names\n","feature_names = df_train_features.columns.tolist()\n","print(f\"\\nSample feature names (first 10):\")\n","for name in feature_names[:10]:\n","    print(f\"  - {name}\")\n","\n","# Check low-variance features (on raw features, before standardization)\n","print(f\"\\nChecking low-variance features (raw):\")\n","raw_variance = df_train_features.var()\n","low_var_features = raw_variance[raw_variance < 1e-8].index.tolist()\n","\n","if low_var_features:\n","    print(f\"  ⚠️ Found {len(low_var_features)} low-variance features (var < 1e-8):\")\n","    for feat in low_var_features[:5]:\n","        print(f\"    - {feat}: var={raw_variance[feat]:.2e}\")\n","    if len(low_var_features) > 5:\n","        print(f\"    ... (total {len(low_var_features)} features)\")\n","\n","    # Save list of low-variance features\n","    low_var_file = features_dir / \"low_variance_features.txt\"\n","    pd.Series(low_var_features).to_csv(low_var_file, index=False, header=False)\n","    print(f\"  ✓ Saved: {low_var_file}\")\n","else:\n","    print(f\"  ✓ All feature variances look OK\")\n","\n","# ========== 4. Extract test-set features ==========\n","\n","if has_test:\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"4. Extract test-set features\")\n","    print(\"=\"*60)\n","\n","    print(f\"Processing {X_test.shape[0]:,} test windows...\")\n","\n","    test_features_list = []\n","    for i in range(X_test.shape[0]):\n","        window = X_test[i]\n","        features = extract_window_features(window, channel_names, SAMPLING_RATE_HZ)\n","        test_features_list.append(features)\n","\n","        if (i + 1) % 1000 == 0:\n","            print(f\"  Processed {i+1:,} / {X_test.shape[0]:,} windows\")\n","\n","    # To DataFrame\n","    df_test_features = pd.DataFrame(test_features_list)\n","\n","    print(f\"\\n✓ Test-set features:\")\n","    print(f\"  #samples: {len(df_test_features):,}\")\n","    print(f\"  feature dimension: {df_test_features.shape[1]}\")\n","\n","    # Check NaN & Inf\n","    nan_count = df_test_features.isna().sum().sum()\n","    inf_count = np.isinf(df_test_features.values).sum()\n","\n","    if nan_count > 0 or inf_count > 0:\n","        print(f\"\\n⚠️ Found invalid values:\")\n","        print(f\"  NaN: {nan_count}\")\n","        print(f\"  Inf: {inf_count}\")\n","\n","        df_test_features = df_test_features.replace([np.inf, -np.inf], np.nan)\n","        df_test_features = df_test_features.fillna(0)\n","        print(f\"  ✓ Filled with 0\")\n","\n","# ========== 5. Feature standardization (train-only stats) ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"5. Feature standardization (train-only stats)\")\n","print(\"=\"*60)\n","\n","# Fit scaler (train set only)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(df_train_features)\n","\n","print(f\"Scaler parameters (train set):\")\n","print(f\"  mean range: [{scaler.mean_.min():.4f}, {scaler.mean_.max():.4f}]\")\n","print(f\"  std range:  [{scaler.scale_.min():.4f}, {scaler.scale_.max():.4f}]\")\n","\n","# To DataFrame and cast dtype\n","df_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names).astype('float32')\n","\n","# Apply to test set (using train stats)\n","if has_test:\n","    X_test_scaled = scaler.transform(df_test_features)\n","    df_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_names).astype('float32')\n","    print(f\"\\n✓ Standardized test set using train-set statistics\")\n","\n","print(f\"✓ Features cast to: float32\")\n","\n","# ========== 6. Save feature data (with metadata) ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"6. Save feature data (with metadata)\")\n","print(\"=\"*60)\n","\n","# Save train set\n","train_X_file = features_dir / \"train_X.parquet\"\n","train_y_file = features_dir / \"train_y.parquet\"\n","train_meta_output = features_dir / \"train_meta.parquet\"\n","\n","df_train_scaled.to_parquet(train_X_file, index=False)\n","pd.DataFrame({'label': y_train}).to_parquet(train_y_file, index=False)\n","\n","# Save metadata (traceable to window_id/session/time)\n","df_train_meta[['window_id', 'subject_id', 'session_id', 'placement',\n","               'time_range', 'label', 'label_purity']].to_parquet(train_meta_output, index=False)\n","\n","print(f\"✓ Train set:\")\n","print(f\"  features: {train_X_file}\")\n","print(f\"  labels:   {train_y_file}\")\n","print(f\"  metadata: {train_meta_output}\")\n","\n","# Save test set\n","if has_test:\n","    test_X_file = features_dir / \"test_X.parquet\"\n","    test_y_file = features_dir / \"test_y.parquet\"\n","    test_meta_output = features_dir / \"test_meta.parquet\"\n","\n","    df_test_scaled.to_parquet(test_X_file, index=False)\n","    pd.DataFrame({'label': y_test}).to_parquet(test_y_file, index=False)\n","    df_test_meta[['window_id', 'subject_id', 'session_id', 'placement',\n","                  'time_range', 'label', 'label_purity']].to_parquet(test_meta_output, index=False)\n","\n","    print(f\"✓ Test set:\")\n","    print(f\"  features: {test_X_file}\")\n","    print(f\"  labels:   {test_y_file}\")\n","    print(f\"  metadata: {test_meta_output}\")\n","\n","# Save scaler\n","scaler_file = features_dir / \"scaler.pkl\"\n","with open(scaler_file, 'wb') as f:\n","    pickle.dump(scaler, f)\n","\n","print(f\"✓ Scaler: {scaler_file}\")\n","\n","# ========== 7. Feature type stats (exact counts, revised) ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"7. Feature type stats (exact counts, revised)\")\n","print(\"=\"*60)\n","\n","# Frequency-domain features (exact match)\n","freq_features = [f for f in feature_names\n","                 if 'spectral_' in f\n","                 or f.endswith('_peak_frequency')\n","                 or f.endswith('_low_freq_energy_ratio')]\n","\n","# Correlation features\n","corr_features = [f for f in feature_names\n","                 if 'autocorr' in f or 'crosscorr' in f]\n","\n","# Time-domain features (everything else not in frequency/correlation)\n","time_features = [f for f in feature_names\n","                 if f not in freq_features and f not in corr_features]\n","\n","print(f\"\\nFeature counts (exact):\")\n","print(f\"  Total: {len(feature_names)}\")\n","print(f\"  Time-domain: {len(time_features)}\")\n","print(f\"  Frequency-domain: {len(freq_features)}\")\n","print(f\"  Correlation: {len(corr_features)}\")\n","print(f\"    - Autocorr: {len([f for f in corr_features if 'autocorr' in f])}\")\n","print(f\"    - Crosscorr: {len([f for f in corr_features if 'crosscorr' in f])} (deduplicated)\")\n","\n","# Verify total\n","total_check = len(time_features) + len(freq_features) + len(corr_features)\n","assert total_check == len(feature_names), \\\n","    f\"Feature count mismatch: {total_check} ≠ {len(feature_names)}\"\n","print(f\"\\n✓ Count verification: {len(time_features)} + {len(freq_features)} + {len(corr_features)} = {len(feature_names)}\")\n","\n","# ========== 8. Save feature config (named by fold) ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"8. Save feature config (named by fold)\")\n","print(\"=\"*60)\n","\n","# Build config\n","feature_config = {\n","    'feature_extraction': {\n","        'method': 'handcrafted (time + frequency + correlation)',\n","        'sampling_rate_hz': SAMPLING_RATE_HZ,\n","        'n_channels': len(channel_names),\n","        'channel_names': channel_names,\n","    },\n","    'feature_types': {\n","        'time_domain': [\n","            'mean', 'std', 'min', 'max', 'range',\n","            'q25', 'q50', 'q75', 'iqr', 'mad',\n","            'skew (bias=False)', 'kurtosis (bias=False, fisher=True)',\n","            'rms', 'energy',\n","            'zero_crossing_rate', 'mean_abs_diff'\n","        ],\n","        'frequency_domain': [\n","            'spectral_energy', 'spectral_entropy',\n","            'peak_frequency (skip DC)', 'spectral_centroid',\n","            'spectral_bandwidth', 'spectral_rolloff',\n","            'low_freq_energy_ratio'\n","        ],\n","        'correlation': [\n","            'autocorr_lag1 (8 dims: per channel)',\n","            'crosscorr_* (28 dims: C(8,2) pairs, deduplicated)'\n","        ]\n","    },\n","    'feature_dimensions': {\n","        'total': len(feature_names),\n","        'time_domain': len(time_features),\n","        'frequency_domain': len(freq_features),\n","        'correlation': len(corr_features),\n","        'correlation_breakdown': {\n","            'autocorr': len([f for f in corr_features if 'autocorr' in f]),\n","            'crosscorr': len([f for f in corr_features if 'crosscorr' in f]),\n","        }\n","    },\n","    'preprocessing': {\n","        'scaler': 'StandardScaler',\n","        'fit_on': 'training set only',\n","        'nan_inf_handling': 'fill with 0',\n","        'dtype': 'float32',\n","    },\n","    'dataset': {\n","        'fold_id': FOLD_ID if FOLD_ID >= 0 else None,\n","        'fold_tag': fold_tag,\n","        'train_samples': int(len(df_train_scaled)),\n","        'test_samples': int(len(df_test_scaled)) if has_test else 0,\n","    },\n","    'feature_names': feature_names,\n","    'improvements': [\n","        'Cross-correlation deduplication: keep upper triangle only, C(8,2)=28 pairs',\n","        'Higher-order stats: bias=False, fisher=True, nan_policy=omit',\n","        'Feature counting: exact (time = not frequency and not correlation)',\n","        'Frequency robustness: Hann window + skip DC for peak frequency',\n","        'Data type: float32 to save space',\n","        'Traceability: save window_id/session/time metadata',\n","        'Per-fold output: avoid overwriting different folds',\n","        'Consistency assertions: ensure X/y/meta alignment',\n","        'Low-variance check: on raw features (before standardization)',\n","        'Anti-leakage assertions: train/test subjects disjoint + single test subject under LOSO',\n","    ],\n","    'notes': [\n","        f'Total feature dimension: {len(feature_names)} (currently 220)',\n","        f'Time-domain: {len(time_features)} dims (16 types × 8 channels)',\n","        f'Frequency-domain: {len(freq_features)} dims (7 types × 8 channels)',\n","        f'Correlation: {len(corr_features)} dims (8 autocorr + 28 crosscorr)',\n","        'Standardization uses train set statistics only (anti-leakage)',\n","        'NaN/Inf filled with 0',\n","        'Suitable for KNN/RF/SVM and other classical ML models',\n","    ]\n","}\n","\n","# Save config (named by fold)\n","feature_config_file = configs_dir / f\"features_{fold_tag}.yaml\"\n","with open(feature_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(feature_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n","\n","print(f\"✓ Saved config: {feature_config_file}\")\n","\n","feature_config_json = configs_dir / f\"features_{fold_tag}.json\"\n","with open(feature_config_json, 'w', encoding='utf-8') as f:\n","    json.dump(feature_config, f, indent=2)\n","\n","print(f\"✓ Saved config: {feature_config_json}\")\n","\n","# Save feature name list\n","feature_names_file = features_dir / \"feature_names.txt\"\n","with open(feature_names_file, 'w') as f:\n","    for name in feature_names:\n","        f.write(f\"{name}\\n\")\n","\n","print(f\"✓ Saved feature names: {feature_names_file}\")\n","\n","# ========== 9. Feature statistics ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"9. Feature statistics\")\n","print(\"=\"*60)\n","\n","# Train feature stats\n","print(f\"\\nTraining-set feature stats (standardized):\")\n","print(df_train_scaled.describe().T[['mean', 'std', 'min', 'max']].head(10))\n","\n","# ========== 10. Summary ==========\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Step 11 complete - Feature Engineering (final version)\")\n","print(\"=\"*60)\n","\n","print(f\"\\nFeature extraction:\")\n","print(f\"  Method: handcrafted (time + frequency + correlation)\")\n","print(f\"  Total dims: {len(feature_names)}\")\n","print(f\"  Time-domain: {len(time_features)} dims\")\n","print(f\"  Frequency-domain: {len(freq_features)} dims\")\n","print(f\"  Correlation: {len(corr_features)} dims (8 autocorr + 28 crosscorr)\")\n","\n","print(f\"\\nDataset:\")\n","print(f\"  Fold: {fold_tag}\")\n","print(f\"  Train samples: {len(df_train_scaled):,}\")\n","if has_test:\n","    print(f\"  Test samples: {len(df_test_scaled):,}\")\n","\n","print(f\"\\nPreprocessing:\")\n","print(f\"  Standardization: StandardScaler (fit on train only)\")\n","print(f\"  Invalid values: NaN/Inf filled with 0\")\n","print(f\"  Dtype: float32\")\n","\n","print(f\"\\nOutputs:\")\n","print(f\"  Dir: {features_dir}/\")\n","print(f\"  Train: train_X.parquet, train_y.parquet, train_meta.parquet\")\n","if has_test:\n","    print(f\"  Test:  test_X.parquet, test_y.parquet, test_meta.parquet\")\n","print(f\"  Scaler: scaler.pkl\")\n","print(f\"  Config: {feature_config_file.name}\")\n","print(f\"  Feature list: feature_names.txt\")\n","if low_var_features:\n","    print(f\"  Low variance: low_variance_features.txt\")\n","\n","print(\"\\nFinal fixes:\")\n","print(\"  1. ✓ Feature counting fix (time = not frequency and not correlation)\")\n","print(\"  2. ✓ Skip DC in frequency (avoid 0 Hz dominance)\")\n","print(\"  3. ✓ Config named per fold (avoid overwrite)\")\n","print(\"  4. ✓ Low-variance check on raw features\")\n","print(\"  5. ✓ Consistency assertions (X/y/meta alignment)\")\n","print(\"  6. ✓ Hann window in FFT (reduce spectral leakage)\")\n","print(\"  7. ✓ Force this-fold directory (disable fallback)\")\n","print(\"  8. ✓ Anti-leakage assertions (train/test disjoint + single test subject)\")\n","\n","print(\"\\nNext steps:\")\n","print(\"  - Use train_X.parquet to train KNN/RF/SVM, etc.\")\n","print(\"  - Use test_X.parquet to evaluate the model\")\n","print(\"  - Optional: feature selection (based on train set)\")\n","\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E66mw05_sBt1","executionInfo":{"status":"ok","timestamp":1763117693334,"user_tz":0,"elapsed":98822,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"e09ce35b-d861-403a-e479-4f570cf36985"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 11: Feature Engineering (KNN/RF)\n","============================================================\n","Using train fold: FOLD_ID=3\n","Windows dir: data/lara/mbientlab/proc/windows/fold_03\n","Output dir: data/lara/mbientlab/proc/features/fold_03\n","\n","============================================================\n","1. Load window data & metadata\n","============================================================\n","\n","Train windows:\n","  X_train shape: (4858, 150, 8)\n","  y_train shape: (4858,)\n","  metadata: 4858 rows\n","  ✓ Consistency check passed\n","\n","Test windows:\n","  X_test shape: (873, 150, 8)\n","  y_test shape: (873,)\n","  metadata: 873 rows\n","  ✓ Consistency check passed\n","\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test subjects: 1\n","\n","============================================================\n","2. Define feature extraction functions\n","============================================================\n","Feature types:\n","  Time-domain: mean, std, min, max, range, q25, q50, q75, iqr, mad,\n","               skew, kurtosis, rms, energy, zero_crossing_rate, mean_abs_diff\n","  Frequency-domain: spectral_energy, spectral_entropy, peak_frequency (skip DC),\n","                    spectral_centroid, spectral_bandwidth, spectral_rolloff,\n","                    low_freq_energy_ratio\n","  Correlation: autocorr_lag1 (8 dims), crosscorr_* (28 dims, deduplicated)\n","  Expected total dimension: 16×8 + 7×8 + 8 + 28 = 220 dims\n","\n","============================================================\n","3. Extract training-set features\n","============================================================\n","Processing 4,858 training windows...\n","  Processed 1,000 / 4,858 windows\n","  Processed 2,000 / 4,858 windows\n","  Processed 3,000 / 4,858 windows\n","  Processed 4,000 / 4,858 windows\n","\n","✓ Training-set features:\n","  #samples: 4,858\n","  feature dimension: 220\n","\n","Sample feature names (first 10):\n","  - ax_mean\n","  - ax_std\n","  - ax_min\n","  - ax_max\n","  - ax_range\n","  - ax_q25\n","  - ax_q50\n","  - ax_q75\n","  - ax_iqr\n","  - ax_mad\n","\n","Checking low-variance features (raw):\n","  ✓ All feature variances look OK\n","\n","============================================================\n","4. Extract test-set features\n","============================================================\n","Processing 873 test windows...\n","\n","✓ Test-set features:\n","  #samples: 873\n","  feature dimension: 220\n","\n","============================================================\n","5. Feature standardization (train-only stats)\n","============================================================\n","Scaler parameters (train set):\n","  mean range: [-2.4468, 5166.6672]\n","  std range:  [0.0317, 5969.1228]\n","\n","✓ Standardized test set using train-set statistics\n","✓ Features cast to: float32\n","\n","============================================================\n","6. Save feature data (with metadata)\n","============================================================\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/features/fold_03/train_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_03/train_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_03/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/features/fold_03/test_X.parquet\n","  labels:   data/lara/mbientlab/proc/features/fold_03/test_y.parquet\n","  metadata: data/lara/mbientlab/proc/features/fold_03/test_meta.parquet\n","✓ Scaler: data/lara/mbientlab/proc/features/fold_03/scaler.pkl\n","\n","============================================================\n","7. Feature type stats (exact counts, revised)\n","============================================================\n","\n","Feature counts (exact):\n","  Total: 220\n","  Time-domain: 128\n","  Frequency-domain: 56\n","  Correlation: 36\n","    - Autocorr: 8\n","    - Crosscorr: 28 (deduplicated)\n","\n","✓ Count verification: 128 + 56 + 36 = 220\n","\n","============================================================\n","8. Save feature config (named by fold)\n","============================================================\n","✓ Saved config: configs/features_fold_03.yaml\n","✓ Saved config: configs/features_fold_03.json\n","✓ Saved feature names: data/lara/mbientlab/proc/features/fold_03/feature_names.txt\n","\n","============================================================\n","9. Feature statistics\n","============================================================\n","\n","Training-set feature stats (standardized):\n","                  mean       std        min       max\n","ax_mean   3.926202e-09  1.000103 -14.713995  6.117965\n","ax_std   -1.256384e-08  1.000103  -1.761580  4.688210\n","ax_min    0.000000e+00  1.000104  -1.319906  2.049749\n","ax_max    1.256384e-08  1.000103  -1.924394  1.374414\n","ax_range  0.000000e+00  1.000103  -2.132680  1.447773\n","ax_q25    0.000000e+00  1.000104  -7.787620  1.744398\n","ax_q50    7.852403e-10  1.000103 -13.990578  6.562627\n","ax_q75    0.000000e+00  1.000103  -4.510034  9.377128\n","ax_iqr   -1.256384e-08  1.000102  -1.450560  8.272748\n","ax_mad   -1.256384e-08  1.000102  -1.453880  8.145727\n","\n","============================================================\n","Step 11 complete - Feature Engineering (final version)\n","============================================================\n","\n","Feature extraction:\n","  Method: handcrafted (time + frequency + correlation)\n","  Total dims: 220\n","  Time-domain: 128 dims\n","  Frequency-domain: 56 dims\n","  Correlation: 36 dims (8 autocorr + 28 crosscorr)\n","\n","Dataset:\n","  Fold: fold_03\n","  Train samples: 4,858\n","  Test samples: 873\n","\n","Preprocessing:\n","  Standardization: StandardScaler (fit on train only)\n","  Invalid values: NaN/Inf filled with 0\n","  Dtype: float32\n","\n","Outputs:\n","  Dir: data/lara/mbientlab/proc/features/fold_03/\n","  Train: train_X.parquet, train_y.parquet, train_meta.parquet\n","  Test:  test_X.parquet, test_y.parquet, test_meta.parquet\n","  Scaler: scaler.pkl\n","  Config: features_fold_03.yaml\n","  Feature list: feature_names.txt\n","\n","Final fixes:\n","  1. ✓ Feature counting fix (time = not frequency and not correlation)\n","  2. ✓ Skip DC in frequency (avoid 0 Hz dominance)\n","  3. ✓ Config named per fold (avoid overwrite)\n","  4. ✓ Low-variance check on raw features\n","  5. ✓ Consistency assertions (X/y/meta alignment)\n","  6. ✓ Hann window in FFT (reduce spectral leakage)\n","  7. ✓ Force this-fold directory (disable fallback)\n","  8. ✓ Anti-leakage assertions (train/test disjoint + single test subject)\n","\n","Next steps:\n","  - Use train_X.parquet to train KNN/RF/SVM, etc.\n","  - Use test_X.parquet to evaluate the model\n","  - Optional: feature selection (based on train set)\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 12: Normalization (feature/deep) - top-conf/journal grade\n","\n","Classical models: features already standardized in Step 11\n","Deep models: per-channel z-score on windowed data (train-set statistics only)\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import pickle\n","import json\n","import yaml\n","import os\n","\n","# ========== Config ==========\n","EPSILON = 1e-8  # avoid divide-by-zero\n","\n","print(\"=\" * 60)\n","print(\"Step 12: Normalization (feature/deep)\")\n","print(\"=\" * 60)\n","\n","# Path config\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","\n","# Per-fold processing (strict LOSO: forbid 'all' mode)\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","if FOLD_ID < 0:\n","    raise RuntimeError(\n","        \"Step 12 requires strict LOSO; you must explicitly set FOLD_ID (0 to N-1)\\n\"\n","        \"Run: export FOLD_ID=0  or set os.environ['FOLD_ID']='0' in code\"\n","    )\n","\n","fold_tag = f\"fold_{FOLD_ID:02d}\"\n","windows_dir = proc_dir / \"windows\" / fold_tag\n","scalers_dir = proc_dir / \"scalers\" / fold_tag\n","scalers_dir.mkdir(parents=True, exist_ok=True)\n","\n","print(f\"\\nUsing train fold: FOLD_ID={FOLD_ID}\")\n","print(f\"Input dir:  {windows_dir}\")\n","print(f\"Output dir: {scalers_dir}\")\n","\n","# ========== 0. Load channel config (avoid hard-coding) ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"0. Load channel config\")\n","print(\"=\" * 60)\n","\n","channels_config_file = configs_dir / \"channels.yaml\"\n","if channels_config_file.exists():\n","    with open(channels_config_file, 'r', encoding='utf-8') as f:\n","        channels_config = yaml.safe_load(f)\n","    channel_names = channels_config['final_channels']\n","    print(f\"✓ Channels read from config: {channel_names}\")\n","else:\n","    # Fallback default\n","    channel_names = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","    print(f\"⚠️ channels.yaml not found; using default: {channel_names}\")\n","\n","# ========== 1. Classical models: confirm features already standardized ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"1. Classical models: confirm features already standardized\")\n","print(\"=\" * 60)\n","\n","features_dir = proc_dir / \"features\" / fold_tag\n","feature_scaler_file = features_dir / \"scaler.pkl\"\n","\n","if feature_scaler_file.exists():\n","    with open(feature_scaler_file, 'rb') as f:\n","        feature_scaler = pickle.load(f)\n","    print(f\"✓ Feature scaler was generated in Step 11: {feature_scaler_file}\")\n","    print(f\"  StandardScaler parameters:\")\n","    print(f\"  - #features: {len(feature_scaler.mean_)}\")\n","    print(f\"  - mean range: [{feature_scaler.mean_.min():.4f}, {feature_scaler.mean_.max():.4f}]\")\n","    print(f\"  - std  range: [{feature_scaler.scale_.min():.4f}, {feature_scaler.scale_.max():.4f}]\")\n","else:\n","    print(f\"⚠️ Feature scaler not found: {feature_scaler_file}\")\n","    print(f\"   Please run Step 11 first\")\n","\n","# ========== 2. Deep models: load window data ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"2. Deep models: load window data\")\n","print(\"=\" * 60)\n","\n","# Load train set\n","X_train_file = windows_dir / \"X_train.npy\"\n","y_train_file = windows_dir / \"y_train.npy\"\n","train_meta_file = windows_dir / \"X_train.parquet\"\n","\n","if not X_train_file.exists():\n","    raise FileNotFoundError(f\"Training windows not found: {X_train_file}; please run Step 9 first\")\n","\n","X_train = np.load(X_train_file)\n","y_train = np.load(y_train_file)\n","df_train_meta = pd.read_parquet(train_meta_file)\n","\n","print(f\"\\nTrain set:\")\n","print(f\"  X_train shape: {X_train.shape}\")\n","print(f\"  dtype: {X_train.dtype}\")\n","\n","# Load test set\n","X_test_file = windows_dir / \"X_test.npy\"\n","y_test_file = windows_dir / \"y_test.npy\"\n","test_meta_file = windows_dir / \"X_test.parquet\"\n","\n","has_test = X_test_file.exists()\n","if has_test:\n","    X_test = np.load(X_test_file)\n","    y_test = np.load(y_test_file)\n","    df_test_meta = pd.read_parquet(test_meta_file)\n","    print(f\"\\nTest set:\")\n","    print(f\"  X_test shape: {X_test.shape}\")\n","    print(f\"  dtype: {X_test.dtype}\")\n","else:\n","    print(f\"\\nTest set not found\")\n","    X_test = None\n","    y_test = None\n","    df_test_meta = None\n","\n","# ========== 2.1. Numerical robustness self-check (NaN/Inf) ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"2.1. Numerical robustness self-check\")\n","print(\"=\" * 60)\n","\n","def _check_finite(name, arr):\n","    \"\"\"Check array for NaN/Inf\"\"\"\n","    if not np.isfinite(arr).all():\n","        bad = np.logical_not(np.isfinite(arr))\n","        nbad = int(bad.sum())\n","        raise ValueError(f\"{name} contains NaN/Inf, total: {nbad}\")\n","\n","_check_finite(\"X_train\", X_train)\n","print(f\"✓ X_train has no NaN/Inf\")\n","\n","if has_test:\n","    _check_finite(\"X_test\", X_test)\n","    print(f\"✓ X_test has no NaN/Inf\")\n","\n","# ========== 2.2. Anti-leakage & consistency checks ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"2.2. Anti-leakage & consistency checks\")\n","print(\"=\" * 60)\n","\n","# Check channel count\n","n_channels = X_train.shape[2]\n","assert n_channels == len(channel_names), \\\n","    f\"Channel count mismatch: data {n_channels} vs config {len(channel_names)}\"\n","print(f\"✓ Channel count consistent: {n_channels}\")\n","\n","# Validate channel order (from sidecar file)\n","channels_sidecar = windows_dir / \"channels.json\"\n","if channels_sidecar.exists():\n","    with open(channels_sidecar, \"r\", encoding=\"utf-8\") as f:\n","        side = json.load(f)[\"channel_names\"]\n","    assert side == channel_names, \\\n","        f\"Window channel order inconsistent with config:\\n  windows={side}\\n  config ={channel_names}\"\n","    print(f\"✓ Channel order consistent: {channel_names}\")\n","else:\n","    print(f\"⚠️ channels.json sidecar not found; only checking count\")\n","\n","# Check train/test subjects are disjoint\n","train_subjects = set(df_train_meta['subject_id'].unique())\n","if has_test:\n","    test_subjects = set(df_test_meta['subject_id'].unique())\n","    assert train_subjects.isdisjoint(test_subjects), \\\n","        f\"Train/test subjects overlap {train_subjects & test_subjects}, violates LOSO!\"\n","    assert len(test_subjects) == 1, \\\n","        f\"LOSO test set should contain exactly 1 subject; got: {len(test_subjects)}\"\n","    print(f\"✓ Anti-leakage check passed:\")\n","    print(f\"  Train subjects: {len(train_subjects)}\")\n","    print(f\"  Test  subjects: {len(test_subjects)} {test_subjects}\")\n","    print(f\"  Subject sets disjoint: True\")\n","else:\n","    test_subjects = set()\n","    print(f\"✓ Train-only mode (train subjects: {len(train_subjects)})\")\n","\n","# ========== 3. Compute channel-wise statistics (train only) ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"3. Compute channel-wise statistics (train only)\")\n","print(\"=\" * 60)\n","\n","print(f\"\\n#channels: {n_channels}\")\n","print(f\"Channel names: {channel_names}\")\n","\n","# ========== 3.1. Detect if already standardized (avoid double normalization) ==========\n","probe_mean = np.mean(X_train, axis=(0, 1))\n","probe_std = np.std(X_train, axis=(0, 1))\n","already_z = (np.all(np.abs(probe_mean) < 1e-3) and\n","             np.all(np.abs(probe_std - 1.0) < 1e-2))\n","\n","if already_z:\n","    print(f\"\\n⚠️ Detected X_train appears already z-scored (mean≈0, std≈1); skipping normalization.\")\n","    print(f\"  Probed mean range: [{probe_mean.min():.4f}, {probe_mean.max():.4f}]\")\n","    print(f\"  Probed std  range: [{probe_std.min():.4f}, {probe_std.max():.4f}]\")\n","\n","    # Copy directly without re-normalization\n","    X_train_scaled = X_train.astype('float32', copy=True)\n","    if has_test:\n","        X_test_scaled = X_test.astype('float32', copy=True)\n","\n","    # Save original statistics (for record)\n","    channel_mean = probe_mean\n","    channel_std = probe_std\n","\n","    skip_normalization = True\n","    print(f\"  ✓ Skipped normalization; copied as float32\")\n","else:\n","    skip_normalization = False\n","\n","    # Compute per-channel mean/std (global, over train set)\n","    channel_mean = np.mean(X_train, axis=(0, 1))  # shape: (n_channels,)\n","    channel_std = np.std(X_train, axis=(0, 1))    # shape: (n_channels,)\n","\n","    # Guard against too-small std (lower bound EPSILON)\n","    channel_std = np.maximum(channel_std, EPSILON)\n","\n","    print(f\"\\nTrain-set channel statistics:\")\n","    for i, ch_name in enumerate(channel_names):\n","        print(f\"  {ch_name}: mean={channel_mean[i]:7.4f}, std={channel_std[i]:7.4f}\")\n","\n","# ========== 4. Apply channel-wise z-score normalization ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"4. Apply channel-wise z-score normalization\")\n","print(\"=\" * 60)\n","\n","if not skip_normalization:\n","    # Normalize train set\n","    X_train_scaled = (X_train - channel_mean) / channel_std\n","    print(f\"\\nTrain normalization:\")\n","    print(f\"  input:  {X_train.shape}, {X_train.dtype}\")\n","    print(f\"  output: {X_train_scaled.shape}, {X_train_scaled.dtype}\")\n","\n","    # Verify effect on train set\n","    train_scaled_mean = np.mean(X_train_scaled, axis=(0, 1))\n","    train_scaled_std = np.std(X_train_scaled, axis=(0, 1))\n","    print(f\"\\nTrain-set stats after normalization (should be ~0 and ~1):\")\n","    for i, ch_name in enumerate(channel_names):\n","        print(f\"  {ch_name}: mean={train_scaled_mean[i]:7.4f}, std={train_scaled_std[i]:7.4f}\")\n","\n","    # Normalize test set (using train statistics)\n","    if has_test:\n","        X_test_scaled = (X_test - channel_mean) / channel_std\n","        print(f\"\\nTest normalization (using train statistics):\")\n","        print(f\"  input:  {X_test.shape}, {X_test.dtype}\")\n","        print(f\"  output: {X_test_scaled.shape}, {X_test_scaled.dtype}\")\n","\n","        # Test-set stats (should not be exactly 0/1)\n","        test_scaled_mean = np.mean(X_test_scaled, axis=(0, 1))\n","        test_scaled_std = np.std(X_test_scaled, axis=(0, 1))\n","        print(f\"\\nTest-set stats after normalization (train params; not 0/1):\")\n","        for i, ch_name in enumerate(channel_names):\n","            print(f\"  {ch_name}: mean={test_scaled_mean[i]:7.4f}, std={test_scaled_std[i]:7.4f}\")\n","else:\n","    print(f\"\\n✓ Skipped normalization (data already normalized)\")\n","    print(f\"  Train: {X_train_scaled.shape}\")\n","    if has_test:\n","        print(f\"  Test:  {X_test_scaled.shape}\")\n","\n","# ========== 5. Save normalized data ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"5. Save normalized data\")\n","print(\"=\" * 60)\n","\n","# Save train set\n","X_train_scaled_file = scalers_dir / \"X_train_scaled.npy\"\n","y_train_scaled_file = scalers_dir / \"y_train.npy\"\n","train_meta_scaled_file = scalers_dir / \"train_meta.parquet\"\n","\n","np.save(X_train_scaled_file, X_train_scaled.astype('float32'))\n","np.save(y_train_scaled_file, y_train)\n","df_train_meta.to_parquet(train_meta_scaled_file, index=False)\n","\n","print(f\"✓ Train set:\")\n","print(f\"  features: {X_train_scaled_file}\")\n","print(f\"  labels:   {y_train_scaled_file}\")\n","print(f\"  metadata: {train_meta_scaled_file}\")\n","\n","# Save test set\n","if has_test:\n","    X_test_scaled_file = scalers_dir / \"X_test_scaled.npy\"\n","    y_test_scaled_file = scalers_dir / \"y_test.npy\"\n","    test_meta_scaled_file = scalers_dir / \"test_meta.parquet\"\n","\n","    np.save(X_test_scaled_file, X_test_scaled.astype('float32'))\n","    np.save(y_test_scaled_file, y_test)\n","    df_test_meta.to_parquet(test_meta_scaled_file, index=False)\n","\n","    print(f\"✓ Test set:\")\n","    print(f\"  features: {X_test_scaled_file}\")\n","    print(f\"  labels:   {y_test_scaled_file}\")\n","    print(f\"  metadata: {test_meta_scaled_file}\")\n","\n","# ========== 6. Save scaler parameters (with traceability) ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"6. Save scaler parameters (with traceability)\")\n","print(\"=\" * 60)\n","\n","# Compute input file SHA256 (traceability)\n","import hashlib\n","def _sha256(p):\n","    \"\"\"Compute file SHA256\"\"\"\n","    h = hashlib.sha256()\n","    with open(p, \"rb\") as f:\n","        h.update(f.read())\n","    return h.hexdigest()\n","\n","print(f\"\\nComputing input file SHA256...\")\n","x_train_sha = _sha256(X_train_file)\n","print(f\"  X_train: {x_train_sha[:16]}...\")\n","if has_test:\n","    x_test_sha = _sha256(X_test_file)\n","    print(f\"  X_test:  {x_test_sha[:16]}...\")\n","else:\n","    x_test_sha = None\n","\n","# Build scaler dict\n","channel_scaler = {\n","    'fold_id': FOLD_ID,\n","    'fold_tag': fold_tag,\n","    'epsilon': EPSILON,\n","    'n_channels': n_channels,\n","    'channel_names': channel_names,\n","    'channel_mean': channel_mean.tolist(),\n","    'channel_std': channel_std.tolist(),\n","    'train_subjects': sorted(list(train_subjects)),\n","    'test_subjects': sorted(list(test_subjects)) if has_test else [],\n","    'formula': '(x - channel_mean) / max(channel_std, ε)' if not skip_normalization else 'identity (already normalized)',\n","    'method': 'channel-wise z-score with ε floor' if not skip_normalization else 'skip (data already normalized)',\n","    'skip_normalization': skip_normalization,\n","    # Traceability\n","    'input_files': {\n","        'x_train_file': str(X_train_file.name),\n","        'x_train_sha256': x_train_sha,\n","        'x_test_file': str(X_test_file.name) if has_test else None,\n","        'x_test_sha256': x_test_sha,\n","    },\n","    'data_shape': {\n","        'window_size': int(X_train.shape[1]),\n","        'n_channels': int(X_train.shape[2]),\n","        'n_windows_train': int(X_train.shape[0]),\n","        'n_windows_test': int(X_test.shape[0]) if has_test else 0,\n","    },\n","    'notes': [\n","        'Per-channel normalization (8 channels independently compute mean/std)',\n","        'Statistics computed from train set only',\n","        'Test set normalized using train-set statistics',\n","        'Use np.maximum(std, ε) to avoid divide-by-zero',\n","        'Auto-detect double-standardization and skip if needed',\n","        'Consistent with Step 7 anti-leakage principles',\n","        'Suitable for CNN/LSTM/Transformer and other deep models',\n","        'Data shape: (n_windows, window_size, n_channels)',\n","        'Includes input file SHA256 for traceability',\n","    ]\n","}\n","\n","# Save as pickle\n","channel_scaler_pkl = scalers_dir / \"channel_scaler.pkl\"\n","with open(channel_scaler_pkl, 'wb') as f:\n","    pickle.dump(channel_scaler, f)\n","print(f\"✓ Saved scaler (pkl): {channel_scaler_pkl}\")\n","\n","# Save as JSON (human-readable)\n","channel_scaler_json = scalers_dir / \"channel_scaler.json\"\n","with open(channel_scaler_json, 'w') as f:\n","    json.dump(channel_scaler, f, indent=2)\n","print(f\"✓ Saved scaler (json): {channel_scaler_json}\")\n","\n","# ========== 7. Save config ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"7. Save config\")\n","print(\"=\" * 60)\n","\n","normalization_config = {\n","    'traditional_models': {\n","        'method': 'feature-wise StandardScaler',\n","        'scaler_file': str(feature_scaler_file.relative_to(proc_dir)) if feature_scaler_file.exists() else None,\n","        'description': 'Feature-level z-score standardization (completed in Step 11)',\n","        'input': 'features/*.parquet',\n","        'output': 'features/*.parquet (already standardized)',\n","    },\n","    'deep_learning_models': {\n","        'method': 'channel-wise z-score',\n","        'scaler_file': str(channel_scaler_pkl.relative_to(proc_dir)),\n","        'description': 'Channel-wise z-score standardization (per-channel)',\n","        'input': 'windows/*/X_*.npy',\n","        'output': 'scalers/*/X_*_scaled.npy',\n","        'formula': '(x - channel_mean) / max(channel_std, ε)',\n","        'epsilon': EPSILON,\n","    },\n","    'fold_info': {\n","        'fold_id': FOLD_ID,\n","        'fold_tag': fold_tag,\n","    },\n","    'anti_leakage': [\n","        'All statistics (mean/std) computed from train set only',\n","        'Test set uses train-set statistics for normalization',\n","        'Fully consistent with Step 7 principles',\n","        'Each fold computes and saves its own scaler',\n","        'Strict LOSO: FOLD_ID=-1 mode is forbidden',\n","    ],\n","    'notes': [\n","        'Classical models use feature-level standardization (Step 11)',\n","        'Deep models use channel-level standardization (this step)',\n","        'The two normalization routes are independent',\n","        'Post-normalization data cast to float32 to save space',\n","        f'Use np.maximum(std, {EPSILON}) to avoid divide-by-zero',\n","        'Channel names read from channels.yaml to avoid hard-coding',\n","        'Anti-leakage assertion: train/test subjects are disjoint',\n","        'Auto-detect double-standardization and skip (risk mitigation)',\n","        'Numerical robustness check: NaN/Inf self-check',\n","        'Channel order validation: compare with channels.json sidecar',\n","        'Enhanced traceability: record input file SHA256 and window size',\n","    ]\n","}\n","\n","norm_config_file = configs_dir / f\"normalization_{fold_tag}.yaml\"\n","with open(norm_config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(normalization_config, f, default_flow_style=False,\n","              allow_unicode=True, sort_keys=False)\n","print(f\"✓ Saved config: {norm_config_file}\")\n","\n","# ========== 8. Summary ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"Step 12 complete - Normalization (feature/deep)\")\n","print(\"=\" * 60)\n","\n","print(f\"\\nClassical models path:\")\n","print(f\"  Method: feature-level StandardScaler\")\n","print(f\"  Status: {'✓ completed (Step 11)' if feature_scaler_file.exists() else '⚠️ missing'}\")\n","\n","print(f\"\\nDeep-learning path:\")\n","print(f\"  Method: channel-wise z-score\")\n","print(f\"  Formula: (x - mean) / max(std, ε)\")\n","print(f\"  Skip normalization: {'Yes (data already normalized)' if skip_normalization else 'No'}\")\n","print(f\"  #channels: {n_channels}\")\n","print(f\"  Train windows: {X_train_scaled.shape[0]:,}\")\n","if has_test:\n","    print(f\"  Test  windows: {X_test_scaled.shape[0]:,}\")\n","\n","print(f\"\\nOutputs:\")\n","print(f\"  Normalized data: {scalers_dir}/\")\n","print(f\"  Scaler params:   {channel_scaler_pkl.name}\")\n","print(f\"  Config file:     {norm_config_file.name}\")\n","\n","print(f\"\\nAnti-leakage checks:\")\n","print(f\"  ✓ Statistics from train set only\")\n","print(f\"  ✓ Test set normalized with train parameters\")\n","print(f\"  ✓ Train/Test subjects disjoint\")\n","print(f\"  ✓ Single LOSO test subject\")\n","print(f\"  ✓ Consistent with Step 7\")\n","print(f\"  ✓ Strictly per-fold\")\n","\n","print(f\"\\nEnhancements:\")\n","print(f\"  ✓ Formula matches implementation: max(std, ε)\")\n","print(f\"  ✓ Strict LOSO: FOLD_ID=-1 forbidden\")\n","print(f\"  ✓ Channel names read from config\")\n","print(f\"  ✓ Double-normalization detection: {'skipped' if skip_normalization else 'not triggered'}\")\n","print(f\"  ✓ NaN/Inf robustness check\")\n","print(f\"  ✓ Channel order validation\")\n","print(f\"  ✓ SHA256 traceability records\")\n","\n","print(f\"\\nNext steps:\")\n","print(f\"  Classical: train with features/{fold_tag}/*.parquet\")\n","print(f\"  Deep:      train with scalers/{fold_tag}/X_*_scaled.npy\")\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UClolf4SsDhe","executionInfo":{"status":"ok","timestamp":1763117693747,"user_tz":0,"elapsed":394,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"ded2b6f6-33d5-41cb-b7ed-f82cafeee0ce"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 12: Normalization (feature/deep)\n","============================================================\n","\n","Using train fold: FOLD_ID=3\n","Input dir:  data/lara/mbientlab/proc/windows/fold_03\n","Output dir: data/lara/mbientlab/proc/scalers/fold_03\n","\n","============================================================\n","0. Load channel config\n","============================================================\n","✓ Channels read from config: ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","============================================================\n","1. Classical models: confirm features already standardized\n","============================================================\n","✓ Feature scaler was generated in Step 11: data/lara/mbientlab/proc/features/fold_03/scaler.pkl\n","  StandardScaler parameters:\n","  - #features: 220\n","  - mean range: [-2.4468, 5166.6672]\n","  - std  range: [0.0317, 5969.1228]\n","\n","============================================================\n","2. Deep models: load window data\n","============================================================\n","\n","Train set:\n","  X_train shape: (4858, 150, 8)\n","  dtype: float32\n","\n","Test set:\n","  X_test shape: (873, 150, 8)\n","  dtype: float32\n","\n","============================================================\n","2.1. Numerical robustness self-check\n","============================================================\n","✓ X_train has no NaN/Inf\n","✓ X_test has no NaN/Inf\n","\n","============================================================\n","2.2. Anti-leakage & consistency checks\n","============================================================\n","✓ Channel count consistent: 8\n","⚠️ channels.json sidecar not found; only checking count\n","✓ Anti-leakage check passed:\n","  Train subjects: 7\n","  Test  subjects: 1 {'S10'}\n","  Subject sets disjoint: True\n","\n","============================================================\n","3. Compute channel-wise statistics (train only)\n","============================================================\n","\n","#channels: 8\n","Channel names: ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'acc_mag', 'gyr_mag']\n","\n","Train-set channel statistics:\n","  ax: mean= 0.0024, std= 0.9584\n","  ay: mean=-0.0006, std= 0.9556\n","  az: mean=-0.0000, std= 0.9822\n","  gx: mean= 0.0063, std= 0.9569\n","  gy: mean=-0.0056, std= 0.9838\n","  gz: mean=-0.0074, std= 0.9365\n","  acc_mag: mean=-0.0449, std= 0.9681\n","  gyr_mag: mean=-0.0464, std= 0.9683\n","\n","============================================================\n","4. Apply channel-wise z-score normalization\n","============================================================\n","\n","Train normalization:\n","  input:  (4858, 150, 8), float32\n","  output: (4858, 150, 8), float32\n","\n","Train-set stats after normalization (should be ~0 and ~1):\n","  ax: mean=-0.0000, std= 1.0000\n","  ay: mean= 0.0000, std= 1.0000\n","  az: mean=-0.0000, std= 1.0000\n","  gx: mean=-0.0000, std= 1.0000\n","  gy: mean=-0.0000, std= 1.0000\n","  gz: mean= 0.0000, std= 1.0000\n","  acc_mag: mean=-0.0000, std= 1.0000\n","  gyr_mag: mean= 0.0000, std= 1.0000\n","\n","Test normalization (using train statistics):\n","  input:  (873, 150, 8), float32\n","  output: (873, 150, 8), float32\n","\n","Test-set stats after normalization (train params; not 0/1):\n","  ax: mean= 0.0018, std= 0.9063\n","  ay: mean=-0.0053, std= 0.9226\n","  az: mean= 0.0022, std= 0.9781\n","  gx: mean= 0.0094, std= 0.9051\n","  gy: mean= 0.0124, std= 0.9621\n","  gz: mean= 0.0037, std= 0.9538\n","  acc_mag: mean=-0.0491, std= 0.8965\n","  gyr_mag: mean=-0.0207, std= 0.9012\n","\n","============================================================\n","5. Save normalized data\n","============================================================\n","✓ Train set:\n","  features: data/lara/mbientlab/proc/scalers/fold_03/X_train_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_03/y_train.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_03/train_meta.parquet\n","✓ Test set:\n","  features: data/lara/mbientlab/proc/scalers/fold_03/X_test_scaled.npy\n","  labels:   data/lara/mbientlab/proc/scalers/fold_03/y_test.npy\n","  metadata: data/lara/mbientlab/proc/scalers/fold_03/test_meta.parquet\n","\n","============================================================\n","6. Save scaler parameters (with traceability)\n","============================================================\n","\n","Computing input file SHA256...\n","  X_train: 6735aede3f3fd42e...\n","  X_test:  19b905104258598a...\n","✓ Saved scaler (pkl): data/lara/mbientlab/proc/scalers/fold_03/channel_scaler.pkl\n","✓ Saved scaler (json): data/lara/mbientlab/proc/scalers/fold_03/channel_scaler.json\n","\n","============================================================\n","7. Save config\n","============================================================\n","✓ Saved config: configs/normalization_fold_03.yaml\n","\n","============================================================\n","Step 12 complete - Normalization (feature/deep)\n","============================================================\n","\n","Classical models path:\n","  Method: feature-level StandardScaler\n","  Status: ✓ completed (Step 11)\n","\n","Deep-learning path:\n","  Method: channel-wise z-score\n","  Formula: (x - mean) / max(std, ε)\n","  Skip normalization: No\n","  #channels: 8\n","  Train windows: 4,858\n","  Test  windows: 873\n","\n","Outputs:\n","  Normalized data: data/lara/mbientlab/proc/scalers/fold_03/\n","  Scaler params:   channel_scaler.pkl\n","  Config file:     normalization_fold_03.yaml\n","\n","Anti-leakage checks:\n","  ✓ Statistics from train set only\n","  ✓ Test set normalized with train parameters\n","  ✓ Train/Test subjects disjoint\n","  ✓ Single LOSO test subject\n","  ✓ Consistent with Step 7\n","  ✓ Strictly per-fold\n","\n","Enhancements:\n","  ✓ Formula matches implementation: max(std, ε)\n","  ✓ Strict LOSO: FOLD_ID=-1 forbidden\n","  ✓ Channel names read from config\n","  ✓ Double-normalization detection: not triggered\n","  ✓ NaN/Inf robustness check\n","  ✓ Channel order validation\n","  ✓ SHA256 traceability records\n","\n","Next steps:\n","  Classical: train with features/fold_03/*.parquet\n","  Deep:      train with scalers/fold_03/X_*_scaled.npy\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 13: Model Configuration & Training — top-conf/journal grade\n","\n","Deep model: InceptionTime (native 1D convolution)\n","Classical models: RandomForest, KNN\n","Training: bf16/fp32, early stopping, single random seed\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import pickle\n","import json\n","import yaml\n","import os\n","import time\n","from datetime import datetime, timezone\n","\n","# PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.cuda.amp import GradScaler\n","\n","# Classical models\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n","\n","# ========== Config ==========\n","RANDOM_SEED = 42\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Auto-select mixed precision\n","USE_AMP = False\n","AMP_DTYPE = None\n","USE_SCALER = False  # GradScaler only for fp16\n","if torch.cuda.is_available():\n","    # Prefer bf16 (more stable, no GradScaler)\n","    if torch.cuda.is_bf16_supported():\n","        USE_AMP = True\n","        AMP_DTYPE = torch.bfloat16\n","        USE_SCALER = False\n","    else:\n","        # Fall back to fp16 (needs GradScaler)\n","        USE_AMP = True\n","        AMP_DTYPE = torch.float16\n","        USE_SCALER = True\n","\n","# Deep model hyperparameters\n","DEEP_CONFIG = {\n","    'batch_size': 64,\n","    'epochs': 100,\n","    'learning_rate': 1e-3,\n","    'weight_decay': 1e-4,\n","    'patience': 10,  # early stopping\n","    'min_delta': 1e-4,\n","    'val_split': 0.15,  # validation ratio (split from train set)\n","    'num_workers': 4,\n","}\n","\n","# InceptionTime hyperparameters\n","INCEPTION_CONFIG = {\n","    'n_filters': 32,\n","    'kernel_sizes': [9, 19, 39],\n","    'bottleneck_channels': 32,\n","    'use_residual': True,\n","    'depth': 6,\n","}\n","\n","# Classical model hyperparameters\n","RF_CONFIG = {\n","    'n_estimators': 200,\n","    'max_depth': 30,\n","    'min_samples_split': 5,\n","    'min_samples_leaf': 2,\n","    'random_state': RANDOM_SEED,\n","    'n_jobs': -1,\n","}\n","\n","KNN_CONFIG = {\n","    'n_neighbors': 5,\n","    'weights': 'distance',\n","    'metric': 'euclidean',\n","    'n_jobs': -1,\n","}\n","\n","print(\"=\" * 60)\n","print(\"Step 13: Model Configuration & Training\")\n","print(\"=\" * 60)\n","\n","# Paths\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","\n","# Per-fold processing (strict LOSO)\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","if FOLD_ID < 0:\n","    raise RuntimeError(\n","        \"Step 13 requires strict LOSO; you must explicitly set FOLD_ID (0 to N-1)\\n\"\n","        \"Please run: export FOLD_ID=0\"\n","    )\n","\n","fold_tag = f\"fold_{FOLD_ID:02d}\"\n","models_dir = Path(\"models\") / fold_tag\n","models_dir.mkdir(parents=True, exist_ok=True)\n","\n","print(f\"\\nUsing train fold: FOLD_ID={FOLD_ID}\")\n","print(f\"Output dir: {models_dir}\")\n","print(f\"Device: {DEVICE}\")\n","if USE_AMP:\n","    dtype_str = 'bf16' if AMP_DTYPE == torch.bfloat16 else 'fp16'\n","    scaler_str = ' (using GradScaler)' if USE_SCALER else ' (no GradScaler)'\n","    print(f\"Mixed precision: {dtype_str}{scaler_str}\")\n","else:\n","    print(f\"Mixed precision: OFF (CPU)\")\n","\n","# Set random seed\n","torch.manual_seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(RANDOM_SEED)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","print(f\"Random seed: {RANDOM_SEED}\")\n","\n","# ========== InceptionTime definition ==========\n","class InceptionModule(nn.Module):\n","    \"\"\"InceptionTime base module\"\"\"\n","    def __init__(self, in_channels, n_filters, kernel_sizes, bottleneck_channels):\n","        super().__init__()\n","        self.bottleneck = nn.Conv1d(in_channels, bottleneck_channels, 1, bias=False)\n","\n","        self.conv_list = nn.ModuleList([\n","            nn.Conv1d(bottleneck_channels, n_filters, k, padding=k//2, bias=False)\n","            for k in kernel_sizes\n","        ])\n","\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool1d(3, stride=1, padding=1),\n","            nn.Conv1d(in_channels, n_filters, 1, bias=False)\n","        )\n","\n","        out_channels = n_filters * (len(kernel_sizes) + 1)\n","        self.bn = nn.BatchNorm1d(out_channels)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        bottleneck = self.bottleneck(x)\n","        conv_outputs = [conv(bottleneck) for conv in self.conv_list]\n","        maxpool_output = self.maxpool_conv(x)\n","\n","        out = torch.cat([*conv_outputs, maxpool_output], dim=1)\n","        out = self.bn(out)\n","        out = self.relu(out)\n","        return out\n","\n","class InceptionTime(nn.Module):\n","    \"\"\"InceptionTime classifier\"\"\"\n","    def __init__(self, n_channels, n_classes, config):\n","        super().__init__()\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","\n","        n_filters = config['n_filters']\n","        kernel_sizes = config['kernel_sizes']\n","        bottleneck_channels = config['bottleneck_channels']\n","        depth = config['depth']\n","        use_residual = config['use_residual']\n","\n","        # Inception blocks\n","        self.inception_modules = nn.ModuleList()\n","        in_ch = n_channels\n","        out_ch = n_filters * (len(kernel_sizes) + 1)\n","\n","        for i in range(depth):\n","            self.inception_modules.append(\n","                InceptionModule(in_ch, n_filters, kernel_sizes, bottleneck_channels)\n","            )\n","            in_ch = out_ch\n","\n","        self.use_residual = use_residual\n","        if use_residual:\n","            self.residual_conv = nn.ModuleList([\n","                nn.Conv1d(n_channels if i == 0 else out_ch, out_ch, 1, bias=False)\n","                for i in range(depth)\n","            ])\n","\n","        # GAP + classifier\n","        self.gap = nn.AdaptiveAvgPool1d(1)\n","        self.fc = nn.Linear(out_ch, n_classes)\n","\n","    def forward(self, x):\n","        # x: (batch, time, channels) -> (batch, channels, time)\n","        x = x.transpose(1, 2)\n","\n","        for i, inception in enumerate(self.inception_modules):\n","            residual = x\n","            x = inception(x)\n","\n","            if self.use_residual:\n","                residual = self.residual_conv[i](residual)\n","                x = x + residual\n","\n","        x = self.gap(x)  # (batch, channels, 1)\n","        x = x.squeeze(-1)  # (batch, channels)\n","        x = self.fc(x)\n","        return x\n","\n","# ========== Dataset ==========\n","class WindowDataset(Dataset):\n","    \"\"\"Windowed dataset\"\"\"\n","    def __init__(self, X, y):\n","        self.X = torch.FloatTensor(X)\n","        self.y = torch.LongTensor(y)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","# ========== Train / Eval helpers ==========\n","def train_epoch(model, loader, criterion, optimizer, scaler, device, use_amp, amp_dtype, use_scaler):\n","    \"\"\"Train one epoch\"\"\"\n","    model.train()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for X, y in loader:\n","        X, y = X.to(device), y.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        if use_amp:\n","            with torch.amp.autocast('cuda', dtype=amp_dtype):\n","                outputs = model(X)\n","                loss = criterion(outputs, y)\n","\n","            if use_scaler:\n","                # fp16: use GradScaler\n","                scaler.scale(loss).backward()\n","                scaler.step(optimizer)\n","                scaler.update()\n","            else:\n","                # bf16: no GradScaler\n","                loss.backward()\n","                optimizer.step()\n","        else:\n","            # fp32 / CPU\n","            outputs = model(X)\n","            loss = criterion(outputs, y)\n","            loss.backward()\n","            optimizer.step()\n","\n","        total_loss += loss.item() * X.size(0)\n","        _, predicted = outputs.max(1)\n","        total += y.size(0)\n","        correct += predicted.eq(y).sum().item()\n","\n","    return total_loss / total, correct / total\n","\n","def evaluate(model, loader, criterion, device):\n","    \"\"\"Evaluate model\"\"\"\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for X, y in loader:\n","            X, y = X.to(device), y.to(device)\n","            outputs = model(X)\n","            loss = criterion(outputs, y)\n","\n","            total_loss += loss.item() * X.size(0)\n","            _, predicted = outputs.max(1)\n","            total += y.size(0)\n","            correct += predicted.eq(y).sum().item()\n","\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(y.cpu().numpy())\n","\n","    return total_loss / total, correct / total, np.array(all_preds), np.array(all_labels)\n","\n","# ========== 1. Train deep model (InceptionTime) ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"1. Train deep model (InceptionTime)\")\n","print(\"=\" * 60)\n","\n","# Load data\n","scalers_dir = proc_dir / \"scalers\" / fold_tag\n","X_train_file = scalers_dir / \"X_train_scaled.npy\"\n","y_train_file = scalers_dir / \"y_train.npy\"\n","train_meta_file = scalers_dir / \"train_meta.parquet\"\n","X_test_file = scalers_dir / \"X_test_scaled.npy\"\n","y_test_file = scalers_dir / \"y_test.npy\"\n","\n","if not X_train_file.exists():\n","    raise FileNotFoundError(f\"Standardized data not found: {X_train_file}, please run Step 12 first\")\n","\n","X_train_full = np.load(X_train_file)\n","y_train_full = np.load(y_train_file)\n","df_train_meta = pd.read_parquet(train_meta_file)\n","X_test = np.load(X_test_file)\n","y_test = np.load(y_test_file)\n","\n","print(f\"\\nData loaded:\")\n","print(f\"  X_train_full: {X_train_full.shape}\")\n","print(f\"  y_train_full: {y_train_full.shape}\")\n","print(f\"  X_test: {X_test.shape}\")\n","print(f\"  y_test: {y_test.shape}\")\n","\n","# ========== 1.0. Remap labels to contiguous IDs (prevent IndexError) ==========\n","print(f\"\\nLabel remapping (ensure contiguous 0..n_classes-1):\")\n","unique_labels_train = np.unique(y_train_full)\n","unique_labels_test = np.unique(y_test)\n","all_unique_labels = np.unique(np.concatenate([unique_labels_train, unique_labels_test]))\n","\n","print(f\"  Original label set: {all_unique_labels.tolist()}\")\n","\n","# Mapping dict\n","label_map = {old_label: new_label for new_label, old_label in enumerate(sorted(all_unique_labels))}\n","label_map_inverse = {v: k for k, v in label_map.items()}\n","\n","print(f\"  Mapping: {label_map}\")\n","\n","# Apply mapping\n","y_train_full_mapped = np.array([label_map[y] for y in y_train_full], dtype=np.int64)\n","y_test_mapped = np.array([label_map[y] for y in y_test], dtype=np.int64)\n","\n","print(f\"  Mapped label range: {np.unique(y_train_full_mapped).tolist()}\")\n","print(f\"  #classes: {len(all_unique_labels)}\")\n","\n","# ========== 1.1. Subject-exclusive validation split (from train set) ==========\n","print(f\"\\nValidation split by subject from train (subject-exclusive):\")\n","\n","# Get all train subjects\n","train_subjects = df_train_meta['subject_id'].unique()\n","n_train_subjects = len(train_subjects)\n","print(f\"  #train subjects: {n_train_subjects}\")\n","\n","# Randomly choose validation subjects (~15%)\n","np.random.seed(RANDOM_SEED)\n","n_val_subjects = max(1, int(n_train_subjects * DEEP_CONFIG['val_split']))\n","val_subjects = np.random.choice(train_subjects, size=n_val_subjects, replace=False)\n","train_subjects_final = [s for s in train_subjects if s not in val_subjects]\n","\n","print(f\"  #val subjects: {n_val_subjects} ({n_val_subjects/n_train_subjects*100:.1f}%)\")\n","print(f\"  Val subjects: {sorted(val_subjects.tolist())}\")\n","print(f\"  Final #train subjects: {len(train_subjects_final)}\")\n","\n","# Split by subject\n","val_mask = df_train_meta['subject_id'].isin(val_subjects)\n","train_mask = ~val_mask\n","\n","X_train = X_train_full[train_mask]\n","y_train = y_train_full_mapped[train_mask]\n","X_val = X_train_full[val_mask]\n","y_val = y_train_full_mapped[val_mask]\n","\n","print(f\"\\nAfter split:\")\n","print(f\"  Train: {X_train.shape}\")\n","print(f\"  Val:   {X_val.shape}\")\n","print(f\"  Test:  {X_test.shape}\")\n","\n","# Ensure subject exclusivity\n","assert set(df_train_meta[train_mask]['subject_id'].unique()).isdisjoint(\n","    set(df_train_meta[val_mask]['subject_id'].unique())\n","), \"Train/Val subjects overlap!\"\n","print(f\"  ✓ Train/Val subjects disjoint\")\n","\n","n_channels = X_train.shape[2]\n","n_classes = len(all_unique_labels)\n","print(f\"\\nModel params:\")\n","print(f\"  Input channels: {n_channels}\")\n","print(f\"  #classes: {n_classes}\")\n","\n","# Datasets & loaders\n","train_dataset = WindowDataset(X_train, y_train)\n","val_dataset = WindowDataset(X_val, y_val)\n","test_dataset = WindowDataset(X_test, y_test_mapped)\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=DEEP_CONFIG['batch_size'],\n","    shuffle=True,\n","    num_workers=DEEP_CONFIG['num_workers'],\n","    pin_memory=True if torch.cuda.is_available() else False\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=DEEP_CONFIG['batch_size'],\n","    shuffle=False,\n","    num_workers=DEEP_CONFIG['num_workers'],\n","    pin_memory=True if torch.cuda.is_available() else False\n",")\n","\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=DEEP_CONFIG['batch_size'],\n","    shuffle=False,\n","    num_workers=DEEP_CONFIG['num_workers'],\n","    pin_memory=True if torch.cuda.is_available() else False\n",")\n","\n","# Model\n","model = InceptionTime(n_channels, n_classes, INCEPTION_CONFIG).to(DEVICE)\n","print(f\"\\nModel summary:\")\n","print(f\"  #params: {sum(p.numel() for p in model.parameters()):,}\")\n","\n","# Optimizer & loss\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(\n","    model.parameters(),\n","    lr=DEEP_CONFIG['learning_rate'],\n","    weight_decay=DEEP_CONFIG['weight_decay']\n",")\n","\n","# GradScaler only for fp16\n","scaler = GradScaler() if USE_SCALER else None\n","\n","# Early stopping\n","best_val_loss = float('inf')\n","patience_counter = 0\n","best_epoch = 0\n","history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n","\n","print(f\"\\nStart training:\")\n","print(f\"  Epochs: {DEEP_CONFIG['epochs']}\")\n","print(f\"  Batch size: {DEEP_CONFIG['batch_size']}\")\n","print(f\"  Learning rate: {DEEP_CONFIG['learning_rate']}\")\n","print(f\"  Early stopping patience: {DEEP_CONFIG['patience']}\")\n","if USE_AMP:\n","    dtype_str = 'bf16' if AMP_DTYPE == torch.bfloat16 else 'fp16'\n","    scaler_str = ' (using GradScaler)' if USE_SCALER else ' (no GradScaler)'\n","    print(f\"  Mixed precision: {dtype_str}{scaler_str}\")\n","else:\n","    print(f\"  Mixed precision: OFF\")\n","\n","start_time = time.time()\n","\n","for epoch in range(DEEP_CONFIG['epochs']):\n","    # Train\n","    train_loss, train_acc = train_epoch(\n","        model, train_loader, criterion, optimizer, scaler, DEVICE,\n","        USE_AMP, AMP_DTYPE, USE_SCALER\n","    )\n","\n","    # Validate (on validation set, not test set)\n","    val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, DEVICE)\n","\n","    history['train_loss'].append(train_loss)\n","    history['train_acc'].append(train_acc)\n","    history['val_loss'].append(val_loss)\n","    history['val_acc'].append(val_acc)\n","\n","    print(f\"Epoch {epoch+1:3d}/{DEEP_CONFIG['epochs']}: \"\n","          f\"Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n","          f\"Val Loss={val_loss:.4f}, Acc={val_acc:.4f}\")\n","\n","    # Early stopping (based on validation loss)\n","    if val_loss < best_val_loss - DEEP_CONFIG['min_delta']:\n","        best_val_loss = val_loss\n","        best_epoch = epoch\n","        patience_counter = 0\n","\n","        # Save best model\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'val_loss': val_loss,\n","            'val_acc': val_acc,\n","            'val_subjects': sorted(val_subjects.tolist()),\n","        }, models_dir / \"inception_time_best.pt\")\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= DEEP_CONFIG['patience']:\n","            print(f\"\\nEarly stopping triggered, best epoch: {best_epoch+1}\")\n","            break\n","\n","train_time = time.time() - start_time\n","print(f\"\\nTraining finished, time: {train_time:.2f}s\")\n","\n","# Load best model and evaluate on test set (final)\n","print(f\"\\nFinal evaluation (test set):\")\n","checkpoint = torch.load(models_dir / \"inception_time_best.pt\")\n","model.load_state_dict(checkpoint['model_state_dict'])\n","test_loss, test_acc, test_preds, test_labels = evaluate(model, test_loader, criterion, DEVICE)\n","\n","print(f\"  Test Loss: {test_loss:.4f}\")\n","print(f\"  Test Accuracy: {test_acc:.4f}\")\n","print(f\"  Test F1 (macro): {f1_score(test_labels, test_preds, average='macro'):.4f}\")\n","\n","# Save training history\n","deep_results = {\n","    'model': 'InceptionTime',\n","    'fold_id': FOLD_ID,\n","    'random_seed': RANDOM_SEED,\n","    'best_epoch': int(best_epoch + 1),\n","    'train_time': train_time,\n","    'label_mapping': {\n","        'original_labels': sorted(all_unique_labels.tolist()),\n","        'mapped_labels': list(range(len(all_unique_labels))),\n","        'label_map': {int(k): int(v) for k, v in label_map.items()},\n","        'label_map_inverse': {int(k): int(v) for k, v in label_map_inverse.items()},\n","    },\n","    'validation': {\n","        'val_subjects': sorted(val_subjects.tolist()),\n","        'n_val_subjects': int(n_val_subjects),\n","        'n_train_subjects': len(train_subjects_final),\n","        'best_val_loss': float(best_val_loss),\n","        'best_val_acc': float(checkpoint['val_acc']),\n","    },\n","    'test': {\n","        'test_accuracy': float(test_acc),\n","        'test_loss': float(test_loss),\n","        'test_f1_macro': float(f1_score(test_labels, test_preds, average='macro')),\n","    },\n","    'history': history,\n","    'config': {**DEEP_CONFIG, **INCEPTION_CONFIG},\n","    'mixed_precision': {\n","        'enabled': USE_AMP,\n","        'dtype': 'bf16' if AMP_DTYPE == torch.bfloat16 else 'fp16' if USE_AMP else 'None',\n","        'use_grad_scaler': USE_SCALER,\n","        'note': 'GradScaler is only used for fp16; bf16 does not need it',\n","    },\n","    'notes': [\n","        'Validation split by subject from training set (subject-exclusive)',\n","        'Early stopping based on validation set to avoid test leakage',\n","        'Test set used only for final evaluation',\n","        f'Mixed precision: {\"bf16 (no GradScaler)\" if AMP_DTYPE == torch.bfloat16 else \"fp16 (with GradScaler)\" if USE_AMP else \"OFF\"}',\n","        'Labels remapped to contiguous 0..n_classes-1',\n","    ]\n","}\n","\n","with open(models_dir / \"inception_time_results.json\", 'w') as f:\n","    json.dump(deep_results, f, indent=2)\n","\n","print(f\"✓ Saved: inception_time_best.pt\")\n","print(f\"✓ Saved: inception_time_results.json\")\n","\n","# ========== 2. Train classical models (RF/KNN) ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"2. Train classical models (RF/KNN)\")\n","print(\"=\" * 60)\n","\n","# Load feature data\n","features_dir = proc_dir / \"features\" / fold_tag\n","train_X_file = features_dir / \"train_X.parquet\"\n","train_y_file = features_dir / \"train_y.parquet\"\n","test_X_file = features_dir / \"test_X.parquet\"\n","test_y_file = features_dir / \"test_y.parquet\"\n","\n","if not train_X_file.exists():\n","    raise FileNotFoundError(f\"Feature data not found: {train_X_file}, please run Step 11 first\")\n","\n","df_train_X = pd.read_parquet(train_X_file)\n","df_train_y = pd.read_parquet(train_y_file)\n","df_test_X = pd.read_parquet(test_X_file)\n","df_test_y = pd.read_parquet(test_y_file)\n","\n","X_train_feat = df_train_X.values\n","y_train_feat_raw = df_train_y['label'].values\n","X_test_feat = df_test_X.values\n","y_test_feat_raw = df_test_y['label'].values\n","\n","# Apply the same label mapping\n","y_train_feat = np.array([label_map[y] for y in y_train_feat_raw], dtype=np.int64)\n","y_test_feat = np.array([label_map[y] for y in y_test_feat_raw], dtype=np.int64)\n","\n","print(f\"\\nFeature data:\")\n","print(f\"  X_train: {X_train_feat.shape}\")\n","print(f\"  X_test:  {X_test_feat.shape}\")\n","print(f\"  Labels remapped\")\n","\n","# ========== 2.1. RandomForest ==========\n","print(f\"\\nTraining RandomForest...\")\n","print(f\"  Config: {RF_CONFIG}\")\n","\n","rf_start = time.time()\n","rf_model = RandomForestClassifier(**RF_CONFIG)\n","rf_model.fit(X_train_feat, y_train_feat)\n","rf_train_time = time.time() - rf_start\n","\n","# Predict\n","rf_train_pred = rf_model.predict(X_train_feat)\n","rf_test_pred = rf_model.predict(X_test_feat)\n","\n","rf_train_acc = accuracy_score(y_train_feat, rf_train_pred)\n","rf_test_acc = accuracy_score(y_test_feat, rf_test_pred)\n","rf_test_f1 = f1_score(y_test_feat, rf_test_pred, average='macro')\n","\n","print(f\"✓ RandomForest training complete:\")\n","print(f\"  Train time: {rf_train_time:.2f}s\")\n","print(f\"  Train Acc:  {rf_train_acc:.4f}\")\n","print(f\"  Test  Acc:  {rf_test_acc:.4f}\")\n","print(f\"  Test  F1 (macro): {rf_test_f1:.4f}\")\n","\n","# Save model\n","with open(models_dir / \"random_forest.pkl\", 'wb') as f:\n","    pickle.dump(rf_model, f)\n","\n","rf_results = {\n","    'model': 'RandomForest',\n","    'fold_id': FOLD_ID,\n","    'random_state': RF_CONFIG['random_state'],\n","    'train_time': rf_train_time,\n","    'train_accuracy': float(rf_train_acc),\n","    'test_accuracy': float(rf_test_acc),\n","    'test_f1_macro': float(rf_test_f1),\n","    'config': RF_CONFIG,\n","    'label_mapping': {\n","        'original_labels': sorted(all_unique_labels.tolist()),\n","        'mapped_labels': list(range(len(all_unique_labels))),\n","        'label_map': {int(k): int(v) for k, v in label_map.items()},\n","    },\n","}\n","\n","with open(models_dir / \"random_forest_results.json\", 'w') as f:\n","    json.dump(rf_results, f, indent=2)\n","\n","print(f\"✓ Saved: random_forest.pkl\")\n","print(f\"✓ Saved: random_forest_results.json\")\n","\n","# ========== 2.2. KNN ==========\n","print(f\"\\nTraining KNN...\")\n","print(f\"  Config: {KNN_CONFIG}\")\n","\n","knn_start = time.time()\n","knn_model = KNeighborsClassifier(**KNN_CONFIG)\n","knn_model.fit(X_train_feat, y_train_feat)\n","knn_train_time = time.time() - knn_start\n","\n","# Predict\n","knn_train_pred = knn_model.predict(X_train_feat)\n","knn_test_pred = knn_model.predict(X_test_feat)\n","\n","knn_train_acc = accuracy_score(y_train_feat, knn_train_pred)\n","knn_test_acc = accuracy_score(y_test_feat, knn_test_pred)\n","knn_test_f1 = f1_score(y_test_feat, knn_test_pred, average='macro')\n","\n","print(f\"✓ KNN training complete:\")\n","print(f\"  Train time: {knn_train_time:.2f}s\")\n","print(f\"  Train Acc:  {knn_train_acc:.4f}\")\n","print(f\"  Test  Acc:  {knn_test_acc:.4f}\")\n","print(f\"  Test  F1 (macro): {knn_test_f1:.4f}\")\n","\n","# Save model\n","with open(models_dir / \"knn.pkl\", 'wb') as f:\n","    pickle.dump(knn_model, f)\n","\n","knn_results = {\n","    'model': 'KNN',\n","    'fold_id': FOLD_ID,\n","    'train_time': knn_train_time,\n","    'train_accuracy': float(knn_train_acc),\n","    'test_accuracy': float(knn_test_acc),\n","    'test_f1_macro': float(knn_test_f1),\n","    'config': KNN_CONFIG,\n","    'label_mapping': {\n","        'original_labels': sorted(all_unique_labels.tolist()),\n","        'mapped_labels': list(range(len(all_unique_labels))),\n","        'label_map': {int(k): int(v) for k, v in label_map.items()},\n","    },\n","}\n","\n","with open(models_dir / \"knn_results.json\", 'w') as f:\n","    json.dump(knn_results, f, indent=2)\n","\n","print(f\"✓ Saved: knn.pkl\")\n","print(f\"✓ Saved: knn_results.json\")\n","\n","# ========== 3. Save training config ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"3. Save training config\")\n","print(\"=\" * 60)\n","\n","training_config = {\n","    'fold_id': FOLD_ID,\n","    'fold_tag': fold_tag,\n","    'random_seed': RANDOM_SEED,\n","    'device': str(DEVICE),\n","    'timestamp': datetime.now(timezone.utc).isoformat(),\n","    'mixed_precision': {\n","        'enabled': USE_AMP,\n","        'dtype': 'bf16' if AMP_DTYPE == torch.bfloat16 else 'fp16' if USE_AMP else 'None',\n","        'use_grad_scaler': USE_SCALER,\n","        'auto_selection': 'bf16 first (no GradScaler), fp16 second (with GradScaler), OFF on CPU',\n","        'note': 'GradScaler is only used for fp16 to prevent underflow; bf16 has sufficient dynamic range',\n","    },\n","    'models': {\n","        'inception_time': {\n","            'architecture': 'InceptionTime (1D Conv)',\n","            'config': {**DEEP_CONFIG, **INCEPTION_CONFIG},\n","            'validation': {\n","                'val_subjects': sorted(val_subjects.tolist()),\n","                'n_val_subjects': int(n_val_subjects),\n","                'split_ratio': DEEP_CONFIG['val_split'],\n","                'subject_exclusive': True,\n","            },\n","            'test_accuracy': float(test_acc),\n","            'test_f1_macro': float(f1_score(test_labels, test_preds, average='macro')),\n","            'train_time': train_time,\n","            'best_epoch': int(best_epoch + 1),\n","        },\n","        'random_forest': {\n","            'config': RF_CONFIG,\n","            'test_accuracy': float(rf_test_acc),\n","            'test_f1_macro': float(rf_test_f1),\n","            'train_time': rf_train_time,\n","        },\n","        'knn': {\n","            'config': KNN_CONFIG,\n","            'test_accuracy': float(knn_test_acc),\n","            'test_f1_macro': float(knn_test_f1),\n","            'train_time': knn_train_time,\n","        },\n","    },\n","    'notes': [\n","        'Deep model uses InceptionTime (native 1D convolution)',\n","        'Validation subjects split from train (subject-exclusive)',\n","        'Early stopping based on validation set (no test leakage)',\n","        'Test set only for final evaluation',\n","        f'Mixed precision: {\"bf16 (no GradScaler)\" if AMP_DTYPE == torch.bfloat16 else \"fp16 (with GradScaler)\" if USE_AMP else \"OFF (CPU)\"}',\n","        'GradScaler only used for fp16',\n","        'Single random seed for reproducibility',\n","        'RF records random_state for reproducibility',\n","        'Labels remapped to contiguous 0..n_classes-1 (prevent IndexError)',\n","        'All models use the same label mapping',\n","        'All models evaluated on the same fold’s independent test set',\n","        'Strict LOSO: train/val/test subjects fully disjoint',\n","    ]\n","}\n","\n","config_file = models_dir / \"training_config.yaml\"\n","with open(config_file, 'w', encoding='utf-8') as f:\n","    yaml.dump(training_config, f, default_flow_style=False, allow_unicode=True)\n","\n","print(f\"✓ Saved: {config_file}\")\n","\n","# ========== 4. Summary ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"Step 13 complete - Model Configuration & Training\")\n","print(\"=\" * 60)\n","\n","print(f\"\\nFold: FOLD_ID={FOLD_ID}\")\n","print(f\"Random seed: {RANDOM_SEED}\")\n","if USE_AMP:\n","    dtype_str = 'bf16' if AMP_DTYPE == torch.bfloat16 else 'fp16'\n","    scaler_str = ' (using GradScaler)' if USE_SCALER else ' (no GradScaler)'\n","    print(f\"Mixed precision: {dtype_str}{scaler_str}\")\n","else:\n","    print(f\"Mixed precision: OFF (CPU)\")\n","\n","print(f\"\\nDeep model (InceptionTime):\")\n","print(f\"  #Val subjects: {n_val_subjects} / {n_train_subjects} ({n_val_subjects/n_train_subjects*100:.1f}%)\")\n","print(f\"  Test Acc: {test_acc:.4f}\")\n","print(f\"  Test F1:  {f1_score(test_labels, test_preds, average='macro'):.4f}\")\n","print(f\"  Train time: {train_time:.2f}s\")\n","print(f\"  Best epoch: {best_epoch+1}\")\n","\n","print(f\"\\nClassical models:\")\n","print(f\"  RandomForest: Acc={rf_test_acc:.4f}, F1={rf_test_f1:.4f}\")\n","print(f\"  KNN:          Acc={knn_test_acc:.4f}, F1={knn_test_f1:.4f}\")\n","\n","print(f\"\\nOutput files:\")\n","print(f\"  {models_dir}/\")\n","print(f\"  - inception_time_best.pt\")\n","print(f\"  - inception_time_results.json\")\n","print(f\"  - random_forest.pkl\")\n","print(f\"  - random_forest_results.json\")\n","print(f\"  - knn.pkl\")\n","print(f\"  - knn_results.json\")\n","print(f\"  - training_config.yaml\")\n","\n","print(f\"\\nRigor guarantees:\")\n","print(f\"  ✓ Labels remapped to contiguous IDs\")\n","print(f\"  ✓ Val subjects disjoint from train (no leakage)\")\n","print(f\"  ✓ Early stopping uses validation set\")\n","print(f\"  ✓ Test set for final evaluation only\")\n","print(f\"  ✓ Smart mixed precision (bf16 preferred)\")\n","print(f\"  ✓ Single random seed: {RANDOM_SEED}\")\n","print(f\"  ✓ RF records random_state\")\n","print(f\"  ✓ Strict LOSO evaluation\")\n","print(f\"  ✓ Per-fold outputs saved independently\")\n","\n","print(f\"\\nFixes:\")\n","print(f\"  ✓ Label remapping (original {all_unique_labels.tolist()} → contiguous 0..{len(all_unique_labels)-1})\")\n","print(f\"  ✓ Validation subjects split from training set\")\n","print(f\"  ✓ Train/Val/Test subjects mutually exclusive\")\n","print(f\"  ✓ Early stopping does not use test set\")\n","print(f\"  ✓ Mixed precision: bf16(no Scaler) > fp16(with Scaler) > OFF\")\n","print(f\"  ✓ GradScaler only for fp16\")\n","print(f\"  ✓ Using torch.amp.autocast (silences warnings)\")\n","\n","print(f\"\\nNext steps:\")\n","print(f\"  - Repeat training for all folds (FOLD_ID=0 .. N-1)\")\n","print(f\"  - Aggregate cross-fold results (mean ± std)\")\n","print(f\"  - Produce final evaluation report\")\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QRKLPPgzsFha","executionInfo":{"status":"ok","timestamp":1763117720977,"user_tz":0,"elapsed":27200,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"dd4b9b7d-7fe3-46e3-f7f7-9c82c7157686"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 13: Model Configuration & Training\n","============================================================\n","\n","Using train fold: FOLD_ID=3\n","Output dir: models/fold_03\n","Device: cuda\n","Mixed precision: bf16 (no GradScaler)\n","Random seed: 42\n","\n","============================================================\n","1. Train deep model (InceptionTime)\n","============================================================\n","\n","Data loaded:\n","  X_train_full: (4858, 150, 8)\n","  y_train_full: (4858,)\n","  X_test: (873, 150, 8)\n","  y_test: (873,)\n","\n","Label remapping (ensure contiguous 0..n_classes-1):\n","  Original label set: [1, 2, 3, 4, 5, 6]\n","  Mapping: {np.int32(1): 0, np.int32(2): 1, np.int32(3): 2, np.int32(4): 3, np.int32(5): 4, np.int32(6): 5}\n","  Mapped label range: [0, 1, 2, 3, 4, 5]\n","  #classes: 6\n","\n","Validation split by subject from train (subject-exclusive):\n","  #train subjects: 7\n","  #val subjects: 1 (14.3%)\n","  Val subjects: ['S07']\n","  Final #train subjects: 6\n","\n","After split:\n","  Train: (4092, 150, 8)\n","  Val:   (766, 150, 8)\n","  Test:  (873, 150, 8)\n","  ✓ Train/Val subjects disjoint\n","\n","Model params:\n","  Input channels: 8\n","  #classes: 6\n","\n","Model summary:\n","  #params: 538,374\n","\n","Start training:\n","  Epochs: 100\n","  Batch size: 64\n","  Learning rate: 0.001\n","  Early stopping patience: 10\n","  Mixed precision: bf16 (no GradScaler)\n","Epoch   1/100: Train Loss=1.1404, Acc=0.6002 | Val Loss=1.0986, Acc=0.5979\n","Epoch   2/100: Train Loss=0.9396, Acc=0.6711 | Val Loss=0.9972, Acc=0.6201\n","Epoch   3/100: Train Loss=0.8995, Acc=0.6877 | Val Loss=0.9802, Acc=0.6345\n","Epoch   4/100: Train Loss=0.8448, Acc=0.7050 | Val Loss=0.9202, Acc=0.6762\n","Epoch   5/100: Train Loss=0.8140, Acc=0.7160 | Val Loss=1.0249, Acc=0.6462\n","Epoch   6/100: Train Loss=0.7636, Acc=0.7373 | Val Loss=1.0306, Acc=0.6436\n","Epoch   7/100: Train Loss=0.7379, Acc=0.7466 | Val Loss=0.9786, Acc=0.6606\n","Epoch   8/100: Train Loss=0.7023, Acc=0.7593 | Val Loss=0.9531, Acc=0.6723\n","Epoch   9/100: Train Loss=0.6684, Acc=0.7698 | Val Loss=1.0305, Acc=0.6697\n","Epoch  10/100: Train Loss=0.6504, Acc=0.7703 | Val Loss=1.0451, Acc=0.6305\n","Epoch  11/100: Train Loss=0.6082, Acc=0.7874 | Val Loss=1.0236, Acc=0.6684\n","Epoch  12/100: Train Loss=0.5454, Acc=0.8030 | Val Loss=1.0424, Acc=0.6658\n","Epoch  13/100: Train Loss=0.5377, Acc=0.8003 | Val Loss=1.4101, Acc=0.5836\n","Epoch  14/100: Train Loss=0.4897, Acc=0.8248 | Val Loss=1.7196, Acc=0.4582\n","\n","Early stopping triggered, best epoch: 4\n","\n","Training finished, time: 20.76s\n","\n","Final evaluation (test set):\n","  Test Loss: 1.0441\n","  Test Accuracy: 0.5911\n","  Test F1 (macro): 0.5332\n","✓ Saved: inception_time_best.pt\n","✓ Saved: inception_time_results.json\n","\n","============================================================\n","2. Train classical models (RF/KNN)\n","============================================================\n","\n","Feature data:\n","  X_train: (4858, 220)\n","  X_test:  (873, 220)\n","  Labels remapped\n","\n","Training RandomForest...\n","  Config: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'random_state': 42, 'n_jobs': -1}\n","✓ RandomForest training complete:\n","  Train time: 2.63s\n","  Train Acc:  0.9883\n","  Test  Acc:  0.5979\n","  Test  F1 (macro): 0.4785\n","✓ Saved: random_forest.pkl\n","✓ Saved: random_forest_results.json\n","\n","Training KNN...\n","  Config: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'n_jobs': -1}\n","✓ KNN training complete:\n","  Train time: 0.00s\n","  Train Acc:  1.0000\n","  Test  Acc:  0.5567\n","  Test  F1 (macro): 0.5035\n","✓ Saved: knn.pkl\n","✓ Saved: knn_results.json\n","\n","============================================================\n","3. Save training config\n","============================================================\n","✓ Saved: models/fold_03/training_config.yaml\n","\n","============================================================\n","Step 13 complete - Model Configuration & Training\n","============================================================\n","\n","Fold: FOLD_ID=3\n","Random seed: 42\n","Mixed precision: bf16 (no GradScaler)\n","\n","Deep model (InceptionTime):\n","  #Val subjects: 1 / 7 (14.3%)\n","  Test Acc: 0.5911\n","  Test F1:  0.5332\n","  Train time: 20.76s\n","  Best epoch: 4\n","\n","Classical models:\n","  RandomForest: Acc=0.5979, F1=0.4785\n","  KNN:          Acc=0.5567, F1=0.5035\n","\n","Output files:\n","  models/fold_03/\n","  - inception_time_best.pt\n","  - inception_time_results.json\n","  - random_forest.pkl\n","  - random_forest_results.json\n","  - knn.pkl\n","  - knn_results.json\n","  - training_config.yaml\n","\n","Rigor guarantees:\n","  ✓ Labels remapped to contiguous IDs\n","  ✓ Val subjects disjoint from train (no leakage)\n","  ✓ Early stopping uses validation set\n","  ✓ Test set for final evaluation only\n","  ✓ Smart mixed precision (bf16 preferred)\n","  ✓ Single random seed: 42\n","  ✓ RF records random_state\n","  ✓ Strict LOSO evaluation\n","  ✓ Per-fold outputs saved independently\n","\n","Fixes:\n","  ✓ Label remapping (original [1, 2, 3, 4, 5, 6] → contiguous 0..5)\n","  ✓ Validation subjects split from training set\n","  ✓ Train/Val/Test subjects mutually exclusive\n","  ✓ Early stopping does not use test set\n","  ✓ Mixed precision: bf16(no Scaler) > fp16(with Scaler) > OFF\n","  ✓ GradScaler only for fp16\n","  ✓ Using torch.amp.autocast (silences warnings)\n","\n","Next steps:\n","  - Repeat training for all folds (FOLD_ID=0 .. N-1)\n","  - Aggregate cross-fold results (mean ± std)\n","  - Produce final evaluation report\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 14: Inner-layer Tuning (warning-fix version)\n","Fix autocast deprecation warning by using the new API\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import json\n","import yaml\n","import os\n","import time\n","from itertools import product\n","from functools import lru_cache\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import GroupKFold\n","from sklearn.metrics import f1_score\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","# Fix: use the new API\n","from torch.amp import autocast\n","\n","# ========== Global config ==========\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","\n","RANDOM_SEED = 42\n","INNER_CV_FOLDS = 5\n","USE_AMP_BF16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n","NUM_WORKERS = min(4, os.cpu_count() or 2)\n","\n","TUNE_MODELS = ['rf', 'knn', 'inception']\n","\n","RF_PARAM_GRID = {\n","    'n_estimators': [100, 200],\n","    'max_depth': [20, 30],\n","    'min_samples_split': [2, 5],\n","}\n","\n","KNN_PARAM_GRID = {\n","    'n_neighbors': [5, 7],\n","    'weights': ['uniform', 'distance'],\n","    'metric': ['euclidean', 'manhattan'],\n","}\n","\n","INCEPTION_PARAM_TRIALS = [\n","    {'learning_rate': 1e-3, 'n_filters': 32, 'depth': 6},\n","    {'learning_rate': 5e-4, 'n_filters': 32, 'depth': 6},\n","]\n","\n","print(\"=\" * 60)\n","print(\"Step 14: Inner-layer Tuning (warning-fix version)\")\n","print(\"=\" * 60)\n","\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","if FOLD_ID < 0:\n","    raise RuntimeError(\"FOLD_ID must be set (0 to N-1)\")\n","\n","fold_tag = f\"fold_{FOLD_ID:02d}\"\n","tuning_dir = Path(\"tuning\") / fold_tag\n","tuning_dir.mkdir(parents=True, exist_ok=True)\n","\n","print(f\"\\nOuter fold: FOLD_ID={FOLD_ID}\")\n","print(f\"BF16 mixed precision: {USE_AMP_BF16}\")\n","\n","np.random.seed(RANDOM_SEED)\n","\n","# ========== 1. Load global class set ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"1. Load global class set\")\n","print(\"=\" * 60)\n","\n","scalers_dir = proc_dir / \"scalers\" / fold_tag\n","train_meta_file = scalers_dir / \"train_meta.parquet\"\n","\n","df_train_meta = pd.read_parquet(train_meta_file)\n","train_subjects = sorted(df_train_meta['subject_id'].unique().tolist())\n","n_train_subjects = len(train_subjects)\n","\n","all_train_y = np.load(scalers_dir / \"y_train.npy\")\n","CLASS_LIST = sorted(np.unique(all_train_y).tolist())\n","N_CLASSES = len(CLASS_LIST)\n","\n","CLASS_TO_INDEX = {c: i for i, c in enumerate(CLASS_LIST)}\n","INDEX_LABELS = list(range(N_CLASSES))\n","\n","print(f\"#Train subjects: {n_train_subjects}\")\n","print(f\"Global class set: {CLASS_LIST} -> {INDEX_LABELS}\")\n","\n","n_inner = min(INNER_CV_FOLDS, n_train_subjects)\n","assert n_inner >= 2\n","if n_inner < INNER_CV_FOLDS:\n","    print(f\"⚠️ Adjust inner folds: {INNER_CV_FOLDS} -> {n_inner}\")\n","\n","# ========== 2. Inner CV splitting ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"2. Inner CV splitting (GroupKFold)\")\n","print(\"=\" * 60)\n","\n","df_train_meta = df_train_meta.sort_values(\n","    ['subject_id', 'session_id', 'window_id'],\n","    kind='mergesort'\n",").reset_index(drop=True)\n","\n","window_subjects = df_train_meta['subject_id'].values\n","window_indices = np.arange(len(df_train_meta))\n","\n","gkf = GroupKFold(n_splits=n_inner)\n","inner_cv_splits = []\n","\n","for fold_idx, (train_idx, val_idx) in enumerate(gkf.split(window_indices, groups=window_subjects)):\n","    val_subjects = sorted(set(window_subjects[val_idx]))\n","    train_subjects_inner = sorted(set(window_subjects[train_idx]))\n","\n","    inner_cv_splits.append({\n","        'fold': fold_idx,\n","        'train_subjects': train_subjects_inner,\n","        'val_subjects': val_subjects,\n","        'train_window_indices': train_idx.tolist(),\n","        'val_window_indices': val_idx.tolist(),\n","    })\n","    print(f\"  Inner fold {fold_idx}: train {len(train_subjects_inner)} subs, val {len(val_subjects)} subs\")\n","\n","for split in inner_cv_splits:\n","    assert set(split['train_subjects']).isdisjoint(set(split['val_subjects']))\n","print(\"✓ Disjointness check passed\")\n","\n","inner_splits_file = tuning_dir / \"inner_cv_splits.json\"\n","with open(inner_splits_file, \"w\", encoding=\"utf-8\") as f:\n","    splits_to_save = [{\n","        'fold': s['fold'],\n","        'train_subjects': s['train_subjects'],\n","        'val_subjects': s['val_subjects'],\n","        'n_train_windows': len(s['train_window_indices']),\n","        'n_val_windows': len(s['val_window_indices']),\n","    } for s in inner_cv_splits]\n","    json.dump(splits_to_save, f, ensure_ascii=False, indent=2)\n","print(f\"✓ Saved: {inner_splits_file}\")\n","\n","# ========== 3. Data loading functions (with cache) ==========\n","@lru_cache(maxsize=32)\n","def cached_window_subset(train_key, val_key):\n","    \"\"\"Cache standardized window subsets (force using raw, unstandardized data)\"\"\"\n","    train_subjects = list(train_key)\n","    val_subjects = list(val_key)\n","\n","    # Fix 1: force using unstandardized windows only to prevent inner-layer leakage of outer statistics\n","    candidates = [\n","        proc_dir / \"windows\" / fold_tag / \"X_train.npy\",\n","        scalers_dir / \"X_train.npy\",\n","    ]\n","    for p in candidates:\n","        if p.exists():\n","            X_full = np.load(p, mmap_mode='r')\n","            break\n","    else:\n","        raise FileNotFoundError(\n","            \"Need the unstandardized X_train.npy for inner-layer normalization; \"\n","            \"having only X_train_scaled.npy would introduce validation-subject statistics. \"\n","            \"Please persist the unstandardized window array in Steps 9/12.\"\n","        )\n","\n","    y_full = np.load(scalers_dir / \"y_train.npy\", mmap_mode='r')\n","    df_meta = pd.read_parquet(train_meta_file)\n","\n","    train_mask = df_meta['subject_id'].isin(train_subjects)\n","    val_mask = df_meta['subject_id'].isin(val_subjects)\n","\n","    X_train_raw = X_full[train_mask].copy().astype(np.float32)\n","    X_val_raw = X_full[val_mask].copy().astype(np.float32)\n","    y_train = y_full[train_mask].copy()\n","    y_val = y_full[val_mask].copy()\n","\n","    # Inner-layer independent normalization\n","    channel_mean = np.mean(X_train_raw, axis=(0, 1))\n","    channel_std = np.maximum(np.std(X_train_raw, axis=(0, 1)), 1e-8)\n","    X_train = (X_train_raw - channel_mean) / channel_std\n","    X_val = (X_val_raw - channel_mean) / channel_std\n","\n","    y_train = np.array([CLASS_TO_INDEX[int(v)] for v in y_train], dtype=np.int64)\n","    y_val = np.array([CLASS_TO_INDEX[int(v)] for v in y_val], dtype=np.int64)\n","\n","    return X_train, y_train, X_val, y_val\n","\n","def load_feature_data_subset(train_subjects, val_subjects):\n","    \"\"\"Load feature data (inner-layer normalization)\"\"\"\n","    features_dir = proc_dir / \"features\" / fold_tag\n","    df_X = pd.read_parquet(features_dir / \"train_X.parquet\")\n","    df_y = pd.read_parquet(features_dir / \"train_y.parquet\")\n","    df_meta = pd.read_parquet(features_dir / \"train_meta.parquet\")\n","\n","    train_mask = df_meta['subject_id'].isin(train_subjects)\n","    val_mask = df_meta['subject_id'].isin(val_subjects)\n","\n","    X_train_raw = df_X.values[train_mask]\n","    X_val_raw = df_X.values[val_mask]\n","\n","    # Fix 3: ensure label types are consistent and mapped to integer indices\n","    y_col = df_y['label']\n","    y_train = np.array([CLASS_TO_INDEX[int(v)] for v in y_col.values[train_mask]], dtype=np.int64)\n","    y_val = np.array([CLASS_TO_INDEX[int(v)] for v in y_col.values[val_mask]], dtype=np.int64)\n","\n","    scaler = StandardScaler()\n","    X_train = scaler.fit_transform(X_train_raw)\n","    X_val = scaler.transform(X_val_raw)\n","\n","    return X_train, y_train, X_val, y_val\n","\n","# ========== 4. Evaluation functions ==========\n","def evaluate_rf(params, inner_splits):\n","    f1_scores = []\n","    for split in inner_splits:\n","        X_train, y_train, X_val, y_val = load_feature_data_subset(\n","            split['train_subjects'], split['val_subjects']\n","        )\n","\n","        model = RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1, **params)\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_val)\n","\n","        f1 = f1_score(y_val, y_pred, average='macro', labels=INDEX_LABELS, zero_division=0)\n","        f1_scores.append(f1)\n","\n","    return np.mean(f1_scores), np.std(f1_scores)\n","\n","def evaluate_knn(params, inner_splits):\n","    f1_scores = []\n","    for split in inner_splits:\n","        X_train, y_train, X_val, y_val = load_feature_data_subset(\n","            split['train_subjects'], split['val_subjects']\n","        )\n","\n","        model = KNeighborsClassifier(n_jobs=-1, **params)\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_val)\n","\n","        f1 = f1_score(y_val, y_pred, average='macro', labels=INDEX_LABELS, zero_division=0)\n","        f1_scores.append(f1)\n","\n","    return np.mean(f1_scores), np.std(f1_scores)\n","\n","def evaluate_inception(params, inner_splits, max_epochs=20):\n","    \"\"\"InceptionTime evaluation (using the new autocast API)\"\"\"\n","    torch.manual_seed(RANDOM_SEED)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(RANDOM_SEED)\n","\n","    class InceptionModule(nn.Module):\n","        def __init__(self, in_channels, n_filters, kernel_sizes, bottleneck_channels):\n","            super().__init__()\n","            self.bottleneck = nn.Conv1d(in_channels, bottleneck_channels, 1, bias=False)\n","            self.conv_list = nn.ModuleList([\n","                nn.Conv1d(bottleneck_channels, n_filters, k, padding=k//2, bias=False)\n","                for k in kernel_sizes\n","            ])\n","            self.maxpool_conv = nn.Sequential(\n","                nn.MaxPool1d(3, stride=1, padding=1),\n","                nn.Conv1d(in_channels, n_filters, 1, bias=False)\n","            )\n","            out_channels = n_filters * (len(kernel_sizes) + 1)\n","            self.bn = nn.BatchNorm1d(out_channels)\n","            self.relu = nn.ReLU()\n","\n","        def forward(self, x):\n","            bottleneck = self.bottleneck(x)\n","            conv_outputs = [conv(bottleneck) for conv in self.conv_list]\n","            maxpool_output = self.maxpool_conv(x)\n","            out = torch.cat([*conv_outputs, maxpool_output], dim=1)\n","            return self.relu(self.bn(out))\n","\n","    class InceptionTime(nn.Module):\n","        def __init__(self, n_channels, n_classes, n_filters, depth):\n","            super().__init__()\n","            kernel_sizes = [9, 19, 39]\n","            bottleneck_channels = 32\n","\n","            self.inception_modules = nn.ModuleList()\n","            in_ch = n_channels\n","            out_ch = n_filters * (len(kernel_sizes) + 1)\n","\n","            for i in range(depth):\n","                self.inception_modules.append(\n","                    InceptionModule(in_ch, n_filters, kernel_sizes, bottleneck_channels)\n","                )\n","                in_ch = out_ch\n","\n","            self.gap = nn.AdaptiveAvgPool1d(1)\n","            self.fc = nn.Linear(out_ch, n_classes)\n","\n","        def forward(self, x):\n","            x = x.transpose(1, 2).contiguous()\n","            for inception in self.inception_modules:\n","                x = inception(x)\n","            x = self.gap(x).squeeze(-1)\n","            return self.fc(x)\n","\n","    class WindowDataset(Dataset):\n","        def __init__(self, X, y):\n","            self.X = torch.FloatTensor(X)\n","            self.y = torch.LongTensor(y)\n","        def __len__(self):\n","            return len(self.X)\n","        def __getitem__(self, idx):\n","            return self.X[idx], self.y[idx]\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    # Fix 4: enable pin_memory only when CUDA is available\n","    pin = torch.cuda.is_available()\n","    f1_scores = []\n","\n","    # Fix 2: use the function parameter inner_splits, not a global variable\n","    for split in inner_splits:\n","        X_train, y_train, X_val, y_val = cached_window_subset(\n","            tuple(split['train_subjects']),\n","            tuple(split['val_subjects'])\n","        )\n","\n","        train_loader = DataLoader(\n","            WindowDataset(X_train, y_train),\n","            batch_size=64,\n","            shuffle=True,\n","            num_workers=NUM_WORKERS,\n","            pin_memory=pin\n","        )\n","        val_loader = DataLoader(\n","            WindowDataset(X_val, y_val),\n","            batch_size=64,\n","            shuffle=False,\n","            num_workers=NUM_WORKERS,\n","            pin_memory=pin\n","        )\n","\n","        n_channels = X_train.shape[2]\n","        model = InceptionTime(n_channels, N_CLASSES, params['n_filters'], params['depth']).to(device)\n","\n","        optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=1e-4)\n","        criterion = nn.CrossEntropyLoss()\n","\n","        best_f1 = 0\n","        patience_counter = 0\n","\n","        for epoch in range(max_epochs):\n","            model.train()\n","            for X_batch, y_batch in train_loader:\n","                X_batch = X_batch.to(device, non_blocking=True)\n","                y_batch = y_batch.to(device, non_blocking=True)\n","\n","                optimizer.zero_grad(set_to_none=True)\n","\n","                # Fix: use the new autocast API\n","                if USE_AMP_BF16:\n","                    with autocast(device_type='cuda', dtype=torch.bfloat16):\n","                        outputs = model(X_batch)\n","                        loss = criterion(outputs, y_batch)\n","                else:\n","                    outputs = model(X_batch)\n","                    loss = criterion(outputs, y_batch)\n","\n","                loss.backward()\n","                optimizer.step()\n","\n","            model.eval()\n","            all_preds, all_labels = [], []\n","            with torch.no_grad():\n","                for X_batch, y_batch in val_loader:\n","                    X_batch = X_batch.to(device, non_blocking=True)\n","\n","                    if USE_AMP_BF16:\n","                        with autocast(device_type='cuda', dtype=torch.bfloat16):\n","                            outputs = model(X_batch)\n","                    else:\n","                        outputs = model(X_batch)\n","\n","                    _, preds = outputs.max(1)\n","                    all_preds.extend(preds.cpu().numpy())\n","                    all_labels.extend(y_batch.numpy())\n","\n","            f1 = f1_score(all_labels, all_preds, average='macro', labels=INDEX_LABELS, zero_division=0)\n","\n","            if f1 > best_f1 + 1e-4:\n","                best_f1 = f1\n","                patience_counter = 0\n","            else:\n","                patience_counter += 1\n","                if patience_counter >= 5:\n","                    break\n","\n","        f1_scores.append(best_f1)\n","\n","    return np.mean(f1_scores), np.std(f1_scores)\n","\n","# ========== 5. Tune RandomForest ==========\n","if 'rf' in TUNE_MODELS:\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"5. Tune RandomForest\")\n","    print(\"=\" * 60)\n","\n","    param_names = list(RF_PARAM_GRID.keys())\n","    param_values = [RF_PARAM_GRID[k] for k in param_names]\n","    rf_param_combinations = list(product(*param_values))\n","\n","    print(f\"Number of parameter combinations: {len(rf_param_combinations)}\")\n","\n","    rf_trials = []\n","    best_rf_f1 = 0\n","    best_rf_params = None\n","\n","    for i, combo in enumerate(rf_param_combinations):\n","        params = dict(zip(param_names, combo))\n","        print(f\"\\nTrial {i+1}/{len(rf_param_combinations)}: {params}\")\n","\n","        start_time = time.time()\n","        mean_f1, std_f1 = evaluate_rf(params, inner_cv_splits)\n","        elapsed = time.time() - start_time\n","\n","        print(f\"  Macro-F1: {mean_f1:.4f} ± {std_f1:.4f} ({elapsed:.2f}s)\")\n","\n","        rf_trials.append({\n","            'trial': i,\n","            'params': params,\n","            'mean_f1': float(mean_f1),\n","            'std_f1': float(std_f1),\n","            'time': float(elapsed),\n","        })\n","\n","        if mean_f1 > best_rf_f1:\n","            best_rf_f1 = mean_f1\n","            best_rf_params = params\n","            print(f\"  ✓ New best!\")\n","\n","    print(f\"\\n✓ Best RandomForest: {best_rf_params} (F1={best_rf_f1:.4f})\")\n","\n","    pd.DataFrame(rf_trials).to_csv(tuning_dir / \"rf_trials.csv\", index=False)\n","    with open(tuning_dir / \"rf_best_params.json\", 'w') as f:\n","        json.dump({'params': best_rf_params, 'mean_f1': float(best_rf_f1)}, f, indent=2)\n","\n","# ========== 6. Tune KNN ==========\n","if 'knn' in TUNE_MODELS:\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"6. Tune KNN\")\n","    print(\"=\" * 60)\n","\n","    param_names = list(KNN_PARAM_GRID.keys())\n","    param_values = [KNN_PARAM_GRID[k] for k in param_names]\n","    knn_param_combinations = list(product(*param_values))\n","\n","    print(f\"Number of parameter combinations: {len(knn_param_combinations)}\")\n","\n","    knn_trials = []\n","    best_knn_f1 = 0\n","    best_knn_params = None\n","\n","    for i, combo in enumerate(knn_param_combinations):\n","        params = dict(zip(param_names, combo))\n","        print(f\"\\nTrial {i+1}/{len(knn_param_combinations)}: {params}\")\n","\n","        start_time = time.time()\n","        mean_f1, std_f1 = evaluate_knn(params, inner_cv_splits)\n","        elapsed = time.time() - start_time\n","\n","        print(f\"  Macro-F1: {mean_f1:.4f} ± {std_f1:.4f} ({elapsed:.2f}s)\")\n","\n","        knn_trials.append({\n","            'trial': i,\n","            'params': params,\n","            'mean_f1': float(mean_f1),\n","            'std_f1': float(std_f1),\n","            'time': float(elapsed),\n","        })\n","\n","        if mean_f1 > best_knn_f1:\n","            best_knn_f1 = mean_f1\n","            best_knn_params = params\n","            print(f\"  ✓ New best!\")\n","\n","    print(f\"\\n✓ Best KNN: {best_knn_params} (F1={best_knn_f1:.4f})\")\n","\n","    pd.DataFrame(knn_trials).to_csv(tuning_dir / \"knn_trials.csv\", index=False)\n","    with open(tuning_dir / \"knn_best_params.json\", 'w') as f:\n","        json.dump({'params': best_knn_params, 'mean_f1': float(best_knn_f1)}, f, indent=2)\n","\n","# ========== 7. Tune InceptionTime ==========\n","if 'inception' in TUNE_MODELS:\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"7. Tune InceptionTime\")\n","    print(\"=\" * 60)\n","\n","    print(f\"#Trials: {len(INCEPTION_PARAM_TRIALS)}\")\n","\n","    inception_trials = []\n","    best_inception_f1 = 0\n","    best_inception_params = None\n","\n","    for i, params in enumerate(INCEPTION_PARAM_TRIALS):\n","        print(f\"\\nTrial {i+1}/{len(INCEPTION_PARAM_TRIALS)}: {params}\")\n","\n","        start_time = time.time()\n","        mean_f1, std_f1 = evaluate_inception(params, inner_cv_splits, max_epochs=20)\n","        elapsed = time.time() - start_time\n","\n","        print(f\"  Macro-F1: {mean_f1:.4f} ± {std_f1:.4f} ({elapsed:.2f}s)\")\n","\n","        inception_trials.append({\n","            'trial': i,\n","            'params': params,\n","            'mean_f1': float(mean_f1),\n","            'std_f1': float(std_f1),\n","            'time': float(elapsed),\n","        })\n","\n","        if mean_f1 > best_inception_f1:\n","            best_inception_f1 = mean_f1\n","            best_inception_params = params\n","            print(f\"  ✓ New best!\")\n","\n","    print(f\"\\n✓ Best InceptionTime: {best_inception_params} (F1={best_inception_f1:.4f})\")\n","\n","    pd.DataFrame(inception_trials).to_csv(tuning_dir / \"inception_trials.csv\", index=False)\n","    with open(tuning_dir / \"inception_best_params.json\", 'w') as f:\n","        json.dump({'params': best_inception_params, 'mean_f1': float(best_inception_f1)}, f, indent=2)\n","\n","# ========== 8. Save config ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"8. Save config\")\n","print(\"=\" * 60)\n","\n","tuning_config = {\n","    'fold_id': FOLD_ID,\n","    'random_seed': RANDOM_SEED,\n","    'n_inner_folds': n_inner,\n","    'tuned_models': TUNE_MODELS,\n","    'best_params': {},\n","}\n","\n","if 'rf' in TUNE_MODELS:\n","    tuning_config['best_params']['rf'] = {'params': best_rf_params, 'mean_f1': float(best_rf_f1)}\n","\n","if 'knn' in TUNE_MODELS:\n","    tuning_config['best_params']['knn'] = {'params': best_knn_params, 'mean_f1': float(best_knn_f1)}\n","\n","if 'inception' in TUNE_MODELS:\n","    tuning_config['best_params']['inception'] = {'params': best_inception_params, 'mean_f1': float(best_inception_f1)}\n","\n","with open(tuning_dir / \"tuning_config.yaml\", 'w') as f:\n","    yaml.dump(tuning_config, f, default_flow_style=False, allow_unicode=True)\n","\n","print(f\"✓ Saved: tuning_config.yaml\")\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"Step 14 complete\")\n","print(\"=\" * 60)\n","print(f\"\\nOutput dir: {tuning_dir}/\")\n","print(f\"  - inner_cv_splits.json\")\n","print(f\"  - rf_trials.csv, rf_best_params.json\")\n","print(f\"  - knn_trials.csv, knn_best_params.json\")\n","print(f\"  - inception_trials.csv, inception_best_params.json\")\n","print(f\"  - tuning_config.yaml\")\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q9N0aljNsH9l","executionInfo":{"status":"ok","timestamp":1763117960745,"user_tz":0,"elapsed":239702,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"7a5f6ce4-2b74-4e73-b195-e6a0c94887ca"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 14: Inner-layer Tuning (warning-fix version)\n","============================================================\n","\n","Outer fold: FOLD_ID=3\n","BF16 mixed precision: True\n","\n","============================================================\n","1. Load global class set\n","============================================================\n","#Train subjects: 7\n","Global class set: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","============================================================\n","2. Inner CV splitting (GroupKFold)\n","============================================================\n","  Inner fold 0: train 6 subs, val 1 subs\n","  Inner fold 1: train 6 subs, val 1 subs\n","  Inner fold 2: train 6 subs, val 1 subs\n","  Inner fold 3: train 5 subs, val 2 subs\n","  Inner fold 4: train 5 subs, val 2 subs\n","✓ Disjointness check passed\n","✓ Saved: tuning/fold_03/inner_cv_splits.json\n","\n","============================================================\n","5. Tune RandomForest\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4221 ± 0.0535 (5.58s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4525 ± 0.0421 (5.41s)\n","  ✓ New best!\n","\n","Trial 3/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4245 ± 0.0541 (5.83s)\n","\n","Trial 4/8: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4499 ± 0.0508 (5.66s)\n","\n","Trial 5/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 2}\n","  Macro-F1: 0.4294 ± 0.0519 (10.35s)\n","\n","Trial 6/8: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5}\n","  Macro-F1: 0.4549 ± 0.0409 (10.20s)\n","  ✓ New best!\n","\n","Trial 7/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 2}\n","  Macro-F1: 0.4505 ± 0.0507 (11.03s)\n","\n","Trial 8/8: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5}\n","  Macro-F1: 0.4461 ± 0.0491 (10.86s)\n","\n","✓ Best RandomForest: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5} (F1=0.4549)\n","\n","============================================================\n","6. Tune KNN\n","============================================================\n","Number of parameter combinations: 8\n","\n","Trial 1/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.4695 ± 0.0569 (0.32s)\n","  ✓ New best!\n","\n","Trial 2/8: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.4593 ± 0.0575 (1.79s)\n","\n","Trial 3/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.4762 ± 0.0556 (0.27s)\n","  ✓ New best!\n","\n","Trial 4/8: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.4715 ± 0.0565 (1.41s)\n","\n","Trial 5/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean'}\n","  Macro-F1: 0.4743 ± 0.0594 (0.27s)\n","\n","Trial 6/8: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan'}\n","  Macro-F1: 0.4692 ± 0.0519 (1.79s)\n","\n","Trial 7/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean'}\n","  Macro-F1: 0.4745 ± 0.0645 (0.27s)\n","\n","Trial 8/8: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan'}\n","  Macro-F1: 0.4738 ± 0.0614 (1.40s)\n","\n","✓ Best KNN: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'} (F1=0.4762)\n","\n","============================================================\n","7. Tune InceptionTime\n","============================================================\n","#Trials: 2\n","\n","Trial 1/2: {'learning_rate': 0.001, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5686 ± 0.0390 (80.59s)\n","  ✓ New best!\n","\n","Trial 2/2: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","  Macro-F1: 0.5719 ± 0.0428 (86.60s)\n","  ✓ New best!\n","\n","✓ Best InceptionTime: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6} (F1=0.5719)\n","\n","============================================================\n","8. Save config\n","============================================================\n","✓ Saved: tuning_config.yaml\n","\n","============================================================\n","Step 14 complete\n","============================================================\n","\n","Output dir: tuning/fold_03/\n","  - inner_cv_splits.json\n","  - rf_trials.csv, rf_best_params.json\n","  - knn_trials.csv, knn_best_params.json\n","  - inception_trials.csv, inception_best_params.json\n","  - tuning_config.yaml\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 15: Training & Inference (top-conf/journal grade)\n","Fit on the full training fold with best hyperparameters, run inference on the test subject, and save per-window predictions\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import json\n","import yaml\n","import os\n","import pickle\n","import time\n","from datetime import datetime, timezone\n","\n","# PyTorch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.amp import autocast\n","\n","# Classical models\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n","\n","# ========== Config ==========\n","RANDOM_SEED = 42\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Inference config\n","INFERENCE_CONFIG = {\n","    'batch_size': 128,\n","    'num_workers': 4,\n","}\n","\n","# Mixed precision\n","USE_AMP_BF16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n","\n","print(\"=\" * 60)\n","print(\"Step 15: Training & Inference\")\n","print(\"=\" * 60)\n","\n","# Path config\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","configs_dir = Path(\"configs\")\n","\n","# Get FOLD_ID\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","if FOLD_ID < 0:\n","    raise RuntimeError(\"FOLD_ID must be set (0 to N-1)\")\n","\n","fold_tag = f\"fold_{FOLD_ID:02d}\"\n","tuning_dir = Path(\"tuning\") / fold_tag\n","models_dir = Path(\"models\") / fold_tag\n","predictions_dir = Path(\"predictions\") / fold_tag\n","\n","models_dir.mkdir(parents=True, exist_ok=True)\n","predictions_dir.mkdir(parents=True, exist_ok=True)\n","\n","print(f\"\\nOuter fold: FOLD_ID={FOLD_ID}\")\n","print(f\"BF16 mixed precision: {USE_AMP_BF16}\")\n","print(f\"Inference batch size: {INFERENCE_CONFIG['batch_size']}\")\n","print(f\"Num workers: {INFERENCE_CONFIG['num_workers']}\")\n","\n","# Set seeds\n","torch.manual_seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(RANDOM_SEED)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# ========== 1. Load best hyperparameters ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"1. Load best hyperparameters\")\n","print(\"=\" * 60)\n","\n","best_params = {}\n","\n","# RF\n","rf_params_file = tuning_dir / \"rf_best_params.json\"\n","if rf_params_file.exists():\n","    with open(rf_params_file, 'r') as f:\n","        best_params['rf'] = json.load(f)\n","    print(f\"✓ RF: {best_params['rf']['params']}\")\n","\n","# KNN\n","knn_params_file = tuning_dir / \"knn_best_params.json\"\n","if knn_params_file.exists():\n","    with open(knn_params_file, 'r') as f:\n","        best_params['knn'] = json.load(f)\n","    print(f\"✓ KNN: {best_params['knn']['params']}\")\n","\n","# InceptionTime\n","inception_params_file = tuning_dir / \"inception_best_params.json\"\n","if inception_params_file.exists():\n","    with open(inception_params_file, 'r') as f:\n","        best_params['inception'] = json.load(f)\n","    print(f\"✓ InceptionTime: {best_params['inception']['params']}\")\n","\n","if not best_params:\n","    raise FileNotFoundError(\"Best hyperparameters not found; please run Step 14 first\")\n","\n","# ========== 2. Load data and label mapping ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"2. Load data and label mapping\")\n","print(\"=\" * 60)\n","\n","scalers_dir = proc_dir / \"scalers\" / fold_tag\n","features_dir = proc_dir / \"features\" / fold_tag\n","\n","# Load metadata to build label mapping\n","train_meta_file = scalers_dir / \"train_meta.parquet\"\n","df_train_meta = pd.read_parquet(train_meta_file)\n","all_train_y = np.load(scalers_dir / \"y_train.npy\")\n","\n","CLASS_LIST = sorted(np.unique(all_train_y).tolist())\n","N_CLASSES = len(CLASS_LIST)\n","CLASS_TO_INDEX = {c: i for i, c in enumerate(CLASS_LIST)}\n","INDEX_TO_CLASS = {i: c for c, i in CLASS_TO_INDEX.items()}\n","\n","print(f\"#classes: {N_CLASSES}\")\n","print(f\"Class mapping: {CLASS_LIST} -> {list(range(N_CLASSES))}\")\n","\n","# Read two meta tables, one for deep and one for feature models\n","df_test_meta_deep = pd.read_parquet(scalers_dir / \"test_meta.parquet\")\n","df_test_meta_feat = pd.read_parquet(features_dir / \"test_meta.parquet\")\n","\n","# Deep model data\n","X_train_deep = np.load(scalers_dir / \"X_train_scaled.npy\")\n","y_train_deep = np.array([CLASS_TO_INDEX[int(y)] for y in all_train_y], dtype=np.int64)\n","\n","X_test_deep = np.load(scalers_dir / \"X_test_scaled.npy\")\n","y_test_raw = np.load(scalers_dir / \"y_test.npy\")\n","y_test_deep = np.array([CLASS_TO_INDEX[int(y)] for y in y_test_raw], dtype=np.int64)\n","\n","# Alignment check\n","assert len(df_test_meta_deep) == X_test_deep.shape[0], \"Deep-model metadata and data are misaligned\"\n","\n","print(f\"\\nDeep-model data:\")\n","print(f\"  Train: {X_train_deep.shape}\")\n","print(f\"  Test:  {X_test_deep.shape}\")\n","\n","# Feature-based data\n","df_train_X = pd.read_parquet(features_dir / \"train_X.parquet\")\n","df_train_y = pd.read_parquet(features_dir / \"train_y.parquet\")\n","df_test_X = pd.read_parquet(features_dir / \"test_X.parquet\")\n","df_test_y = pd.read_parquet(features_dir / \"test_y.parquet\")\n","\n","X_train_feat = df_train_X.values\n","y_train_feat = np.array([CLASS_TO_INDEX[int(y)] for y in df_train_y['label'].values], dtype=np.int64)\n","\n","X_test_feat = df_test_X.values\n","y_test_feat = np.array([CLASS_TO_INDEX[int(y)] for y in df_test_y['label'].values], dtype=np.int64)\n","\n","# Alignment check\n","assert len(df_test_meta_feat) == X_test_feat.shape[0], \"Feature-model metadata and data are misaligned\"\n","\n","print(f\"\\nFeature-model data:\")\n","print(f\"  Train: {X_train_feat.shape}\")\n","print(f\"  Test:  {X_test_feat.shape}\")\n","\n","# Feature standardization (prefer this fold, fallback to features directory)\n","from sklearn.preprocessing import StandardScaler\n","import shutil\n","\n","scaler_candidates = [\n","    scalers_dir / \"feature_scaler.pkl\",   # Preferred: per-fold\n","    features_dir / \"scaler.pkl\",          # Fallback: saved in Step 11\n","]\n","feat_scaler = None\n","for p in scaler_candidates:\n","    if p.exists():\n","        with open(p, 'rb') as f:\n","            feat_scaler = pickle.load(f)\n","        # Remove feature_names_in_ to avoid sklearn warnings if present\n","        if hasattr(feat_scaler, 'feature_names_in_'):\n","            delattr(feat_scaler, 'feature_names_in_')\n","        if p != scaler_candidates[0]:\n","            shutil.copy(p, scaler_candidates[0])  # archive to scalers dir\n","        print(f\"\\n✓ Loaded feature scaler: {p.name}\")\n","        break\n","\n","if feat_scaler is None:\n","    print(\"\\n⚠️ Feature scaler not found; refitting on this fold's training set\")\n","    feat_scaler = StandardScaler().fit(X_train_feat)\n","    with open(scalers_dir / \"feature_scaler.pkl\", 'wb') as f:\n","        pickle.dump(feat_scaler, f)\n","\n","# Always transform (whether loaded or refit)\n","X_train_feat = feat_scaler.transform(X_train_feat)\n","X_test_feat = feat_scaler.transform(X_test_feat)\n","print(\"  Features standardized (consistent with Step 14)\")\n","\n","# ========== 3. Define InceptionTime model ==========\n","class InceptionModule(nn.Module):\n","    def __init__(self, in_channels, n_filters, kernel_sizes, bottleneck_channels):\n","        super().__init__()\n","        self.bottleneck = nn.Conv1d(in_channels, bottleneck_channels, 1, bias=False)\n","        self.conv_list = nn.ModuleList([\n","            nn.Conv1d(bottleneck_channels, n_filters, k, padding=k//2, bias=False)\n","            for k in kernel_sizes\n","        ])\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool1d(3, stride=1, padding=1),\n","            nn.Conv1d(in_channels, n_filters, 1, bias=False)\n","        )\n","        out_channels = n_filters * (len(kernel_sizes) + 1)\n","        self.bn = nn.BatchNorm1d(out_channels)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        bottleneck = self.bottleneck(x)\n","        conv_outputs = [conv(bottleneck) for conv in self.conv_list]\n","        maxpool_output = self.maxpool_conv(x)\n","        out = torch.cat([*conv_outputs, maxpool_output], dim=1)\n","        return self.relu(self.bn(out))\n","\n","class InceptionTime(nn.Module):\n","    def __init__(self, n_channels, n_classes, n_filters, depth):\n","        super().__init__()\n","        kernel_sizes = [9, 19, 39]\n","        bottleneck_channels = 32\n","        self.inception_modules = nn.ModuleList()\n","        in_ch = n_channels\n","        out_ch = n_filters * (len(kernel_sizes) + 1)\n","        for _ in range(depth):\n","            self.inception_modules.append(\n","                InceptionModule(in_ch, n_filters, kernel_sizes, bottleneck_channels)\n","            )\n","            in_ch = out_ch\n","        self.gap = nn.AdaptiveAvgPool1d(1)\n","        self.fc = nn.Linear(out_ch, n_classes)\n","\n","    def forward(self, x):\n","        x = x.transpose(1, 2).contiguous()\n","        for inception in self.inception_modules:\n","            x = inception(x)\n","        x = self.gap(x).squeeze(-1)\n","        return self.fc(x)\n","\n","class WindowDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = torch.FloatTensor(X)\n","        self.y = torch.LongTensor(y)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","# ========== 4. Train RandomForest ==========\n","if 'rf' in best_params:\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"4. Train RandomForest (full training set)\")\n","    print(\"=\" * 60)\n","\n","    rf_params = best_params['rf']['params']\n","    print(f\"Hyperparameters: {rf_params}\")\n","\n","    rf_start = time.time()\n","    rf_model = RandomForestClassifier(\n","        random_state=RANDOM_SEED,\n","        n_jobs=-1,\n","        **rf_params\n","    )\n","    rf_model.fit(X_train_feat, y_train_feat)\n","    rf_train_time = time.time() - rf_start\n","\n","    # Inference\n","    print(\"\\nInferencing...\")\n","    rf_test_proba = rf_model.predict_proba(X_test_feat)\n","    rf_test_pred = rf_model.predict(X_test_feat)\n","\n","    # Guardrail: verify probability column order\n","    assert list(rf_model.classes_) == list(range(N_CLASSES)), \\\n","        f\"RF classes order mismatch: {rf_model.classes_} != {list(range(N_CLASSES))}\"\n","\n","    # Save model\n","    with open(models_dir / \"rf_final.pkl\", 'wb') as f:\n","        pickle.dump(rf_model, f)\n","\n","    # Save predictions (using feature metadata)\n","    df_rf_pred = pd.DataFrame({\n","        'window_id': df_test_meta_feat['window_id'].values,\n","        'subject_id': df_test_meta_feat['subject_id'].values,\n","        'pred_label': [INDEX_TO_CLASS[int(p)] for p in rf_test_pred],\n","        'true_label': [INDEX_TO_CLASS[int(y)] for y in y_test_feat],\n","    })\n","\n","    # Add probability columns\n","    for i in range(N_CLASSES):\n","        df_rf_pred[f'proba_class_{CLASS_LIST[i]}'] = rf_test_proba[:, i]\n","\n","    df_rf_pred.to_parquet(predictions_dir / \"rf_predictions.parquet\", index=False)\n","\n","    # Evaluation\n","    rf_acc = accuracy_score(y_test_feat, rf_test_pred)\n","    rf_f1 = f1_score(y_test_feat, rf_test_pred, average='macro', zero_division=0)\n","\n","    print(f\"\\n✓ RF training complete:\")\n","    print(f\"  Train time: {rf_train_time:.2f}s\")\n","    print(f\"  Test Accuracy: {rf_acc:.4f}\")\n","    print(f\"  Test F1: {rf_f1:.4f}\")\n","    print(f\"  Saved: rf_final.pkl, rf_predictions.parquet\")\n","\n","# ========== 5. Train KNN ==========\n","if 'knn' in best_params:\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"5. Train KNN (full training set)\")\n","    print(\"=\" * 60)\n","\n","    knn_params = best_params['knn']['params']\n","    print(f\"Hyperparameters: {knn_params}\")\n","\n","    knn_start = time.time()\n","    knn_model = KNeighborsClassifier(n_jobs=-1, **knn_params)\n","    knn_model.fit(X_train_feat, y_train_feat)\n","    knn_train_time = time.time() - knn_start\n","\n","    # Inference\n","    print(\"\\nInferencing...\")\n","    knn_test_proba = knn_model.predict_proba(X_test_feat)\n","    knn_test_pred = knn_model.predict(X_test_feat)\n","\n","    # Guardrail: verify probability column order\n","    assert list(knn_model.classes_) == list(range(N_CLASSES)), \\\n","        f\"KNN classes order mismatch: {knn_model.classes_} != {list(range(N_CLASSES))}\"\n","\n","    # Save model\n","    with open(models_dir / \"knn_final.pkl\", 'wb') as f:\n","        pickle.dump(knn_model, f)\n","\n","    # Save predictions (using feature metadata)\n","    df_knn_pred = pd.DataFrame({\n","        'window_id': df_test_meta_feat['window_id'].values,\n","        'subject_id': df_test_meta_feat['subject_id'].values,\n","        'pred_label': [INDEX_TO_CLASS[int(p)] for p in knn_test_pred],\n","        'true_label': [INDEX_TO_CLASS[int(y)] for y in y_test_feat],\n","    })\n","\n","    # Add probability columns\n","    for i in range(N_CLASSES):\n","        df_knn_pred[f'proba_class_{CLASS_LIST[i]}'] = knn_test_proba[:, i]\n","\n","    df_knn_pred.to_parquet(predictions_dir / \"knn_predictions.parquet\", index=False)\n","\n","    # Evaluation\n","    knn_acc = accuracy_score(y_test_feat, knn_test_pred)\n","    knn_f1 = f1_score(y_test_feat, knn_test_pred, average='macro', zero_division=0)\n","\n","    print(f\"\\n✓ KNN training complete:\")\n","    print(f\"  Train time: {knn_train_time:.2f}s\")\n","    print(f\"  Test Accuracy: {knn_acc:.4f}\")\n","    print(f\"  Test F1: {knn_f1:.4f}\")\n","    print(f\"  Saved: knn_final.pkl, knn_predictions.parquet\")\n","\n","# ========== 6. Train InceptionTime ==========\n","if 'inception' in best_params:\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"6. Train InceptionTime (full training set)\")\n","    print(\"=\" * 60)\n","\n","    inception_params = best_params['inception']['params']\n","    print(f\"Hyperparameters: {inception_params}\")\n","\n","    # Create loaders\n","    pin = torch.cuda.is_available()\n","    train_loader = DataLoader(\n","        WindowDataset(X_train_deep, y_train_deep),\n","        batch_size=INFERENCE_CONFIG['batch_size'],\n","        shuffle=True,\n","        num_workers=INFERENCE_CONFIG['num_workers'],\n","        pin_memory=pin\n","    )\n","\n","    test_loader = DataLoader(\n","        WindowDataset(X_test_deep, y_test_deep),\n","        batch_size=INFERENCE_CONFIG['batch_size'],\n","        shuffle=False,\n","        num_workers=INFERENCE_CONFIG['num_workers'],\n","        pin_memory=pin\n","    )\n","\n","    # Build model\n","    n_channels = X_train_deep.shape[2]\n","    model = InceptionTime(\n","        n_channels=n_channels,\n","        n_classes=N_CLASSES,\n","        n_filters=inception_params['n_filters'],\n","        depth=inception_params['depth']\n","    ).to(DEVICE)\n","\n","    optimizer = torch.optim.Adam(\n","        model.parameters(),\n","        lr=inception_params['learning_rate'],\n","        weight_decay=1e-4\n","    )\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # Train\n","    print(\"\\nStart training...\")\n","    inception_start = time.time()\n","    max_epochs = 50\n","    patience = 10\n","    best_train_loss = float('inf')\n","    patience_counter = 0\n","\n","    for epoch in range(max_epochs):\n","        model.train()\n","        total_loss = 0\n","        correct = 0\n","        total = 0\n","\n","        for X_batch, y_batch in train_loader:\n","            X_batch = X_batch.to(DEVICE, non_blocking=True)\n","            y_batch = y_batch.to(DEVICE, non_blocking=True)\n","\n","            optimizer.zero_grad(set_to_none=True)\n","\n","            if USE_AMP_BF16:\n","                with autocast(device_type='cuda', dtype=torch.bfloat16):\n","                    outputs = model(X_batch)\n","                    loss = criterion(outputs, y_batch)\n","            else:\n","                outputs = model(X_batch)\n","                loss = criterion(outputs, y_batch)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item() * X_batch.size(0)\n","            _, preds = outputs.max(1)\n","            total += y_batch.size(0)\n","            correct += preds.eq(y_batch).sum().item()\n","\n","        train_loss = total_loss / total\n","        train_acc = correct / total\n","\n","        print(f\"Epoch {epoch+1}/{max_epochs}: Loss={train_loss:.4f}, Acc={train_acc:.4f}\")\n","\n","        # Early stopping\n","        if train_loss < best_train_loss - 1e-4:\n","            best_train_loss = train_loss\n","            patience_counter = 0\n","            # Save best model\n","            torch.save(model.state_dict(), models_dir / \"inception_final.pt\")\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n","                break\n","\n","    inception_train_time = time.time() - inception_start\n","\n","    # Load best model\n","    model.load_state_dict(torch.load(models_dir / \"inception_final.pt\"))\n","\n","    # Inference\n","    print(\"\\nInferencing...\")\n","    model.eval()\n","    all_logits = []\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for X_batch, y_batch in test_loader:\n","            X_batch = X_batch.to(DEVICE, non_blocking=True)\n","\n","            if USE_AMP_BF16:\n","                with autocast(device_type='cuda', dtype=torch.bfloat16):\n","                    outputs = model(X_batch)\n","            else:\n","                outputs = model(X_batch)\n","\n","            # Fix: BF16 needs cast to float32 before numpy\n","            all_logits.append(outputs.float().cpu().numpy())\n","            _, preds = outputs.max(1)\n","            all_preds.extend(preds.cpu().numpy())\n","            # Guardrail: move labels to cpu().numpy() as well\n","            all_labels.extend(y_batch.cpu().numpy())\n","\n","    # Merge results\n","    all_logits = np.vstack(all_logits)\n","    all_proba = torch.softmax(torch.FloatTensor(all_logits), dim=1).numpy()\n","    all_preds = np.array(all_preds)\n","    all_labels = np.array(all_labels)\n","\n","    # Save predictions (using deep-model metadata)\n","    df_inception_pred = pd.DataFrame({\n","        'window_id': df_test_meta_deep['window_id'].values,\n","        'subject_id': df_test_meta_deep['subject_id'].values,\n","        'pred_label': [INDEX_TO_CLASS[int(p)] for p in all_preds],\n","        'true_label': [INDEX_TO_CLASS[int(y)] for y in all_labels],\n","    })\n","\n","    # Add logits\n","    for i in range(N_CLASSES):\n","        df_inception_pred[f'logit_class_{CLASS_LIST[i]}'] = all_logits[:, i]\n","\n","    # Add probabilities\n","    for i in range(N_CLASSES):\n","        df_inception_pred[f'proba_class_{CLASS_LIST[i]}'] = all_proba[:, i]\n","\n","    df_inception_pred.to_parquet(predictions_dir / \"inception_predictions.parquet\", index=False)\n","\n","    # Evaluation\n","    inception_acc = accuracy_score(all_labels, all_preds)\n","    inception_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n","\n","    print(f\"\\n✓ InceptionTime training complete:\")\n","    print(f\"  Train time: {inception_train_time:.2f}s\")\n","    print(f\"  Test Accuracy: {inception_acc:.4f}\")\n","    print(f\"  Test F1: {inception_f1:.4f}\")\n","    print(f\"  Saved: inception_final.pt, inception_predictions.parquet\")\n","\n","# ========== 7. Save training config/results ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"7. Save training config/results\")\n","print(\"=\" * 60)\n","\n","final_results = {\n","    'fold_id': FOLD_ID,\n","    'fold_tag': fold_tag,\n","    'random_seed': RANDOM_SEED,\n","    'timestamp': datetime.now(timezone.utc).isoformat(),\n","    'inference_config': INFERENCE_CONFIG,\n","    'class_mapping': {\n","        'original_classes': CLASS_LIST,\n","        'mapped_indices': list(range(N_CLASSES)),\n","        'class_to_index': CLASS_TO_INDEX,\n","        'index_to_class': INDEX_TO_CLASS,\n","    },\n","    'models': {},\n","    'notes': [\n","        'Train final models with best hyperparameters from Step 14',\n","        'Fit on the full training set',\n","        'Run full inference on the test set',\n","        'Save per-window predictions (window_id → logits/proba)',\n","        'Supports later calibration/curve analyses',\n","        'RF/KNN use metadata from the features directory (ensures alignment)',\n","        'Deep model uses metadata from the scalers directory',\n","        'Feature standardization consistent with Step 14 (fit on full train)',\n","        'Added metadata-alignment assertions (prevent order mismatch)',\n","    ]\n","}\n","\n","if 'rf' in best_params:\n","    final_results['models']['rf'] = {\n","        'params': rf_params,\n","        'train_time': rf_train_time,\n","        'test_accuracy': float(rf_acc),\n","        'test_f1': float(rf_f1),\n","    }\n","\n","if 'knn' in best_params:\n","    final_results['models']['knn'] = {\n","        'params': knn_params,\n","        'train_time': knn_train_time,\n","        'test_accuracy': float(knn_acc),\n","        'test_f1': float(knn_f1),\n","    }\n","\n","if 'inception' in best_params:\n","    final_results['models']['inception'] = {\n","        'params': inception_params,\n","        'train_time': inception_train_time,\n","        'test_accuracy': float(inception_acc),\n","        'test_f1': float(inception_f1),\n","    }\n","\n","with open(predictions_dir / \"final_results.yaml\", 'w', encoding='utf-8') as f:\n","    yaml.dump(final_results, f, default_flow_style=False, allow_unicode=True)\n","\n","with open(predictions_dir / \"final_results.json\", 'w', encoding='utf-8') as f:\n","    json.dump(final_results, f, indent=2)\n","\n","print(f\"✓ Saved: final_results.yaml\")\n","print(f\"✓ Saved: final_results.json\")\n","\n","# ========== 8. Summary ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"Step 15 complete — Training & Inference\")\n","print(\"=\" * 60)\n","\n","print(f\"\\nFold: FOLD_ID={FOLD_ID}\")\n","print(f\"Random seed: {RANDOM_SEED}\")\n","print(f\"Inference batch size: {INFERENCE_CONFIG['batch_size']}\")\n","\n","print(f\"\\nFinal results:\")\n","if 'rf' in best_params:\n","    print(f\"  RF: Acc={rf_acc:.4f}, F1={rf_f1:.4f}\")\n","if 'knn' in best_params:\n","    print(f\"  KNN: Acc={knn_acc:.4f}, F1={knn_f1:.4f}\")\n","if 'inception' in best_params:\n","    print(f\"  InceptionTime: Acc={inception_acc:.4f}, F1={inception_f1:.4f}\")\n","\n","print(f\"\\nOutput files:\")\n","print(f\"  {predictions_dir}/\")\n","if 'rf' in best_params:\n","    print(f\"    - rf_final.pkl\")\n","    print(f\"    - rf_predictions.parquet\")\n","if 'knn' in best_params:\n","    print(f\"    - knn_final.pkl\")\n","    print(f\"    - knn_predictions.parquet\")\n","if 'inception' in best_params:\n","    print(f\"    - inception_final.pt\")\n","    print(f\"    - inception_predictions.parquet\")\n","print(f\"    - final_results.yaml\")\n","print(f\"    - final_results.json\")\n","\n","print(f\"\\nPrediction files include:\")\n","print(f\"  - window_id: traceable window ID\")\n","print(f\"  - subject_id: subject ID\")\n","print(f\"  - pred_label: predicted label\")\n","print(f\"  - true_label: ground truth label\")\n","print(f\"  - proba_class_*: per-class probabilities (RF/KNN/InceptionTime)\")\n","print(f\"  - logit_class_*: per-class logits (InceptionTime)\")\n","\n","print(f\"\\nImprovements:\")\n","print(f\"  ✓ RF/KNN use features metadata (alignment ensured)\")\n","print(f\"  ✓ Deep model uses scalers metadata (alignment ensured)\")\n","print(f\"  ✓ Added alignment assertions (prevent order mismatch)\")\n","print(f\"  ✓ Feature standardization: prefer per-fold, fallback to features dir\")\n","print(f\"  ✓ Always apply transform (whether loaded or refit)\")\n","print(f\"  ✓ Probability column order assertions (verify RF/KNN classes)\")\n","print(f\"  ✓ Deep inference labels via .cpu().numpy() (more robust)\")\n","print(f\"  ✓ BF16 logits cast to float32 before numpy\")\n","\n","print(f\"\\nNext steps:\")\n","print(f\"  - Repeat Steps 14–15 for all folds (FOLD_ID=0 .. N-1)\")\n","print(f\"  - Aggregate cross-fold prediction results\")\n","print(f\"  - Generate confusion matrices, ROC curves, and other evaluation plots\")\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BCia5M-psJx5","executionInfo":{"status":"ok","timestamp":1763118001193,"user_tz":0,"elapsed":40455,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"33f5a6e7-7936-457a-ccdf-66cf51529b06"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 15: Training & Inference\n","============================================================\n","\n","Outer fold: FOLD_ID=3\n","BF16 mixed precision: True\n","Inference batch size: 128\n","Num workers: 4\n","\n","============================================================\n","1. Load best hyperparameters\n","============================================================\n","✓ RF: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5}\n","✓ KNN: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n","✓ InceptionTime: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","\n","============================================================\n","2. Load data and label mapping\n","============================================================\n","#classes: 6\n","Class mapping: [1, 2, 3, 4, 5, 6] -> [0, 1, 2, 3, 4, 5]\n","\n","Deep-model data:\n","  Train: (4858, 150, 8)\n","  Test:  (873, 150, 8)\n","\n","Feature-model data:\n","  Train: (4858, 220)\n","  Test:  (873, 220)\n","\n","✓ Loaded feature scaler: scaler.pkl\n","  Features standardized (consistent with Step 14)\n","\n","============================================================\n","4. Train RandomForest (full training set)\n","============================================================\n","Hyperparameters: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5}\n","\n","Inferencing...\n","\n","✓ RF training complete:\n","  Train time: 2.44s\n","  Test Accuracy: 0.6002\n","  Test F1: 0.4875\n","  Saved: rf_final.pkl, rf_predictions.parquet\n","\n","============================================================\n","5. Train KNN (full training set)\n","============================================================\n","Hyperparameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean'}\n","\n","Inferencing...\n","\n","✓ KNN training complete:\n","  Train time: 0.00s\n","  Test Accuracy: 0.5521\n","  Test F1: 0.5078\n","  Saved: knn_final.pkl, knn_predictions.parquet\n","\n","============================================================\n","6. Train InceptionTime (full training set)\n","============================================================\n","Hyperparameters: {'learning_rate': 0.0005, 'n_filters': 32, 'depth': 6}\n","\n","Start training...\n","Epoch 1/50: Loss=1.2179, Acc=0.5982\n","Epoch 2/50: Loss=0.9073, Acc=0.7001\n","Epoch 3/50: Loss=0.8040, Acc=0.7242\n","Epoch 4/50: Loss=0.7278, Acc=0.7505\n","Epoch 5/50: Loss=0.6682, Acc=0.7709\n","Epoch 6/50: Loss=0.6201, Acc=0.7863\n","Epoch 7/50: Loss=0.5480, Acc=0.8152\n","Epoch 8/50: Loss=0.4724, Acc=0.8454\n","Epoch 9/50: Loss=0.3942, Acc=0.8732\n","Epoch 10/50: Loss=0.3476, Acc=0.8888\n","Epoch 11/50: Loss=0.3057, Acc=0.9012\n","Epoch 12/50: Loss=0.2546, Acc=0.9164\n","Epoch 13/50: Loss=0.2344, Acc=0.9255\n","Epoch 14/50: Loss=0.1918, Acc=0.9378\n","Epoch 15/50: Loss=0.1422, Acc=0.9603\n","Epoch 16/50: Loss=0.1400, Acc=0.9574\n","Epoch 17/50: Loss=0.1171, Acc=0.9667\n","Epoch 18/50: Loss=0.1158, Acc=0.9664\n","Epoch 19/50: Loss=0.1321, Acc=0.9594\n","Epoch 20/50: Loss=0.1276, Acc=0.9623\n","Epoch 21/50: Loss=0.0990, Acc=0.9712\n","Epoch 22/50: Loss=0.0761, Acc=0.9804\n","Epoch 23/50: Loss=0.0648, Acc=0.9827\n","Epoch 24/50: Loss=0.0701, Acc=0.9811\n","Epoch 25/50: Loss=0.0574, Acc=0.9831\n","Epoch 26/50: Loss=0.0670, Acc=0.9809\n","Epoch 27/50: Loss=0.0660, Acc=0.9798\n","Epoch 28/50: Loss=0.0626, Acc=0.9819\n","Epoch 29/50: Loss=0.0733, Acc=0.9786\n","Epoch 30/50: Loss=0.0490, Acc=0.9862\n","Epoch 31/50: Loss=0.0412, Acc=0.9881\n","Epoch 32/50: Loss=0.0539, Acc=0.9831\n","Epoch 33/50: Loss=0.0514, Acc=0.9848\n","Epoch 34/50: Loss=0.0595, Acc=0.9819\n","Epoch 35/50: Loss=0.1000, Acc=0.9646\n","Epoch 36/50: Loss=0.0982, Acc=0.9671\n","Epoch 37/50: Loss=0.0773, Acc=0.9757\n","Epoch 38/50: Loss=0.0629, Acc=0.9788\n","Epoch 39/50: Loss=0.0576, Acc=0.9804\n","Epoch 40/50: Loss=0.0374, Acc=0.9897\n","Epoch 41/50: Loss=0.0315, Acc=0.9914\n","Epoch 42/50: Loss=0.0257, Acc=0.9930\n","Epoch 43/50: Loss=0.0248, Acc=0.9930\n","Epoch 44/50: Loss=0.0192, Acc=0.9949\n","Epoch 45/50: Loss=0.0158, Acc=0.9957\n","Epoch 46/50: Loss=0.0130, Acc=0.9969\n","Epoch 47/50: Loss=0.0182, Acc=0.9938\n","Epoch 48/50: Loss=0.0222, Acc=0.9932\n","Epoch 49/50: Loss=0.0296, Acc=0.9918\n","Epoch 50/50: Loss=0.0244, Acc=0.9911\n","\n","Inferencing...\n","\n","✓ InceptionTime training complete:\n","  Train time: 37.27s\n","  Test Accuracy: 0.6060\n","  Test F1: 0.5937\n","  Saved: inception_final.pt, inception_predictions.parquet\n","\n","============================================================\n","7. Save training config/results\n","============================================================\n","✓ Saved: final_results.yaml\n","✓ Saved: final_results.json\n","\n","============================================================\n","Step 15 complete — Training & Inference\n","============================================================\n","\n","Fold: FOLD_ID=3\n","Random seed: 42\n","Inference batch size: 128\n","\n","Final results:\n","  RF: Acc=0.6002, F1=0.4875\n","  KNN: Acc=0.5521, F1=0.5078\n","  InceptionTime: Acc=0.6060, F1=0.5937\n","\n","Output files:\n","  predictions/fold_03/\n","    - rf_final.pkl\n","    - rf_predictions.parquet\n","    - knn_final.pkl\n","    - knn_predictions.parquet\n","    - inception_final.pt\n","    - inception_predictions.parquet\n","    - final_results.yaml\n","    - final_results.json\n","\n","Prediction files include:\n","  - window_id: traceable window ID\n","  - subject_id: subject ID\n","  - pred_label: predicted label\n","  - true_label: ground truth label\n","  - proba_class_*: per-class probabilities (RF/KNN/InceptionTime)\n","  - logit_class_*: per-class logits (InceptionTime)\n","\n","Improvements:\n","  ✓ RF/KNN use features metadata (alignment ensured)\n","  ✓ Deep model uses scalers metadata (alignment ensured)\n","  ✓ Added alignment assertions (prevent order mismatch)\n","  ✓ Feature standardization: prefer per-fold, fallback to features dir\n","  ✓ Always apply transform (whether loaded or refit)\n","  ✓ Probability column order assertions (verify RF/KNN classes)\n","  ✓ Deep inference labels via .cpu().numpy() (more robust)\n","  ✓ BF16 logits cast to float32 before numpy\n","\n","Next steps:\n","  - Repeat Steps 14–15 for all folds (FOLD_ID=0 .. N-1)\n","  - Aggregate cross-fold prediction results\n","  - Generate confusion matrices, ROC curves, and other evaluation plots\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 16: Evaluation (Primary: Macro-F1)\n","Compute Macro-F1, Balanced Acc, Per-class F1, Confusion Matrix\n","BCa bootstrap 95% CI (1,000 iterations, window-level sampling)\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import json\n","import os\n","from collections import defaultdict\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import (\n","    f1_score, balanced_accuracy_score, confusion_matrix,\n","    classification_report\n",")\n","from scipy import stats\n","\n","# ========== Config ==========\n","RANDOM_SEED = 42\n","N_BOOTSTRAP = 1000\n","\n","print(\"=\" * 60)\n","print(\"Step 16: Evaluation (Primary: Macro-F1)\")\n","print(\"=\" * 60)\n","\n","# Path config\n","predictions_dir = Path(\"predictions\")\n","metrics_dir = Path(\"metrics\")\n","metrics_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Get FOLD_ID\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","if FOLD_ID < 0:\n","    raise RuntimeError(\"FOLD_ID must be set (0 to N-1)\")\n","\n","fold_tag = f\"fold_{FOLD_ID:02d}\"\n","pred_fold_dir = predictions_dir / fold_tag\n","\n","print(f\"\\nCurrent fold: FOLD_ID={FOLD_ID}\")\n","print(f\"Bootstrap: {N_BOOTSTRAP} iterations, window-level sampling\")\n","\n","np.random.seed(RANDOM_SEED)\n","\n","# ========== 1. Helper functions ==========\n","def mean_ci_t(values, alpha=0.05):\n","    \"\"\"\n","    Fold-level t-interval 95% CI for the mean (more stable for small n)\n","\n","    Returns: (mean, ci_lower, ci_upper)\n","    \"\"\"\n","    arr = np.asarray(values, dtype=float)\n","    n = len(arr)\n","    mean = float(np.mean(arr))\n","    if n < 2:\n","        return mean, None, None\n","    se = float(np.std(arr, ddof=1)) / np.sqrt(n)\n","    tcrit = stats.t.ppf(1 - alpha / 2, df=n - 1)\n","    ci_lower = mean - tcrit * se\n","    ci_upper = mean + tcrit * se\n","    return mean, ci_lower, ci_upper\n","\n","# ========== 2. BCa Bootstrap function ==========\n","def bootstrap_ci(y_true, y_pred, metric_func, n_bootstrap=1000, alpha=0.05):\n","    \"\"\"\n","    BCa Bootstrap 95% CI\n","    Sample at window level (with replacement)\n","\n","    Returns: (mean, lower, upper)\n","    \"\"\"\n","    n_samples = len(y_true)\n","    bootstrap_scores = []\n","\n","    # Bootstrap sampling\n","    for _ in range(n_bootstrap):\n","        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n","        y_true_boot = y_true[indices]\n","        y_pred_boot = y_pred[indices]\n","        score = metric_func(y_true_boot, y_pred_boot)\n","        bootstrap_scores.append(score)\n","\n","    bootstrap_scores = np.array(bootstrap_scores)\n","\n","    # Original score\n","    original_score = metric_func(y_true, y_pred)\n","\n","    # BCa correction\n","    # 1. bias correction (z0)\n","    n_less = np.sum(bootstrap_scores < original_score)\n","    p_less = n_less / n_bootstrap\n","    from scipy import stats\n","    z0 = stats.norm.ppf(max(min(p_less, 0.9999), 0.0001))\n","\n","    # 2. acceleration (a) - jackknife\n","    jackknife_scores = []\n","    for i in range(n_samples):\n","        mask = np.ones(n_samples, dtype=bool)\n","        mask[i] = False\n","        jack_score = metric_func(y_true[mask], y_pred[mask])\n","        jackknife_scores.append(jack_score)\n","\n","    jackknife_scores = np.array(jackknife_scores)\n","    jack_mean = jackknife_scores.mean()\n","    numerator = np.sum((jack_mean - jackknife_scores) ** 3)\n","    denominator = 6 * (np.sum((jack_mean - jackknife_scores) ** 2) ** 1.5)\n","    a = numerator / denominator if denominator != 0 else 0\n","\n","    # 3. adjusted percentiles\n","    z_alpha_lower = stats.norm.ppf(alpha / 2)\n","    z_alpha_upper = stats.norm.ppf(1 - alpha / 2)\n","\n","    p_lower = stats.norm.cdf(z0 + (z0 + z_alpha_lower) / (1 - a * (z0 + z_alpha_lower)))\n","    p_upper = stats.norm.cdf(z0 + (z0 + z_alpha_upper) / (1 - a * (z0 + z_alpha_upper)))\n","\n","    p_lower = max(min(p_lower, 0.9999), 0.0001)\n","    p_upper = max(min(p_upper, 0.9999), 0.0001)\n","\n","    lower = np.percentile(bootstrap_scores, p_lower * 100)\n","    upper = np.percentile(bootstrap_scores, p_upper * 100)\n","\n","    return original_score, lower, upper\n","\n","# ========== 3. Evaluate a single model ==========\n","def evaluate_model(pred_file, model_name):\n","    \"\"\"Evaluate a single model\"\"\"\n","    print(f\"\\n{'=' * 60}\")\n","    print(f\"Evaluate {model_name}\")\n","    print(f\"{'=' * 60}\")\n","\n","    # Load predictions\n","    df_pred = pd.read_parquet(pred_file)\n","    print(f\"Loaded predictions: {len(df_pred)} windows\")\n","\n","    # Extract ground truth and predictions\n","    y_true = df_pred['true_label'].values\n","    y_pred = df_pred['pred_label'].values\n","\n","    # Unique classes\n","    unique_labels = sorted(set(y_true) | set(y_pred))\n","    n_classes = len(unique_labels)\n","    print(f\"#classes: {n_classes}\")\n","    print(f\"classes: {unique_labels}\")\n","\n","    # ========== 2.1. Basic metrics ==========\n","    print(f\"\\nBasic metrics:\")\n","\n","    # Macro-F1\n","    macro_f1 = f1_score(y_true, y_pred, average='macro', labels=unique_labels, zero_division=0)\n","    print(f\"  Macro-F1: {macro_f1:.4f}\")\n","\n","    # Balanced Accuracy\n","    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n","    print(f\"  Balanced Acc: {balanced_acc:.4f}\")\n","\n","    # Per-class F1\n","    per_class_f1 = f1_score(y_true, y_pred, average=None, labels=unique_labels, zero_division=0)\n","    print(f\"\\n  Per-class F1:\")\n","    for label, f1 in zip(unique_labels, per_class_f1):\n","        print(f\"    Class {label}: {f1:.4f}\")\n","\n","    # ========== 2.2. Bootstrap 95% CI ==========\n","    print(f\"\\nBootstrap 95% CI ({N_BOOTSTRAP} iterations):\")\n","\n","    # Macro-F1 CI\n","    macro_f1_func = lambda yt, yp: f1_score(yt, yp, average='macro', labels=unique_labels, zero_division=0)\n","    macro_f1_mean, macro_f1_lower, macro_f1_upper = bootstrap_ci(\n","        y_true, y_pred, macro_f1_func, N_BOOTSTRAP\n","    )\n","    print(f\"  Macro-F1: {macro_f1_mean:.4f} [{macro_f1_lower:.4f}, {macro_f1_upper:.4f}]\")\n","\n","    # Balanced Acc CI\n","    bal_acc_func = lambda yt, yp: balanced_accuracy_score(yt, yp)\n","    bal_acc_mean, bal_acc_lower, bal_acc_upper = bootstrap_ci(\n","        y_true, y_pred, bal_acc_func, N_BOOTSTRAP\n","    )\n","    print(f\"  Balanced Acc: {bal_acc_mean:.4f} [{bal_acc_lower:.4f}, {bal_acc_upper:.4f}]\")\n","\n","    # ========== 2.3. Confusion matrix ==========\n","    cm = confusion_matrix(y_true, y_pred, labels=unique_labels)\n","\n","    # Plot confusion matrix\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=unique_labels, yticklabels=unique_labels)\n","    plt.title(f'{model_name} - Confusion Matrix (Fold {FOLD_ID})')\n","    plt.ylabel('True Label')\n","    plt.xlabel('Predicted Label')\n","    plt.tight_layout()\n","\n","    cm_file = metrics_dir / f\"confusion_{model_name.lower()}_{fold_tag}.png\"\n","    plt.savefig(cm_file, dpi=150, bbox_inches='tight')\n","    plt.close()\n","    print(f\"\\n✓ Saved confusion matrix: {cm_file.name}\")\n","\n","    # ========== 2.4. Subject-level Macro-F1 ==========\n","    print(f\"\\nSubject-level evaluation:\")\n","    subject_f1_scores = []\n","\n","    for subject_id in df_pred['subject_id'].unique():\n","        mask = df_pred['subject_id'] == subject_id\n","        y_true_subj = df_pred.loc[mask, 'true_label'].values\n","        y_pred_subj = df_pred.loc[mask, 'pred_label'].values\n","\n","        if len(y_true_subj) > 0:\n","            f1_subj = f1_score(y_true_subj, y_pred_subj, average='macro',\n","                              labels=unique_labels, zero_division=0)\n","            subject_f1_scores.append(f1_subj)\n","\n","    subject_macro_f1_mean = np.mean(subject_f1_scores)\n","    subject_macro_f1_std = np.std(subject_f1_scores)\n","    print(f\"  Subject-level Macro-F1: {subject_macro_f1_mean:.4f} ± {subject_macro_f1_std:.4f}\")\n","    print(f\"  #test subjects: {len(subject_f1_scores)}\")\n","\n","    # ========== 2.5. Aggregate results ==========\n","    results = {\n","        'model': model_name,\n","        'fold_id': FOLD_ID,\n","        'n_windows': int(len(df_pred)),\n","        'n_classes': n_classes,\n","        'classes': [int(c) for c in unique_labels],\n","        'window_level': {\n","            'macro_f1': {\n","                'value': float(macro_f1),\n","                'ci_lower': float(macro_f1_lower),\n","                'ci_upper': float(macro_f1_upper),\n","            },\n","            'balanced_accuracy': {\n","                'value': float(balanced_acc),\n","                'ci_lower': float(bal_acc_lower),\n","                'ci_upper': float(bal_acc_upper),\n","            },\n","            'per_class_f1': {\n","                int(label): float(f1)\n","                for label, f1 in zip(unique_labels, per_class_f1)\n","            },\n","        },\n","        'subject_level': {\n","            'macro_f1_mean': float(subject_macro_f1_mean),\n","            'macro_f1_std': float(subject_macro_f1_std),\n","            'n_subjects': len(subject_f1_scores),\n","        },\n","        'confusion_matrix': cm.tolist(),\n","        'bootstrap': {\n","            'n_iterations': N_BOOTSTRAP,\n","            'method': 'BCa',\n","            'sampling': 'window-level with replacement',\n","        },\n","    }\n","\n","    return results\n","\n","# ========== 4. Evaluate all models ==========\n","print(f\"\\n{'=' * 60}\")\n","print(f\"Evaluate all models\")\n","print(f\"{'=' * 60}\")\n","\n","models_to_eval = {\n","    'RF': pred_fold_dir / \"rf_predictions.parquet\",\n","    'KNN': pred_fold_dir / \"knn_predictions.parquet\",\n","    'InceptionTime': pred_fold_dir / \"inception_predictions.parquet\",\n","}\n","\n","all_results = {}\n","\n","for model_name, pred_file in models_to_eval.items():\n","    if not pred_file.exists():\n","        print(f\"\\n⚠️ Skip {model_name}: not found {pred_file}\")\n","        continue\n","\n","    results = evaluate_model(pred_file, model_name)\n","    all_results[model_name] = results\n","\n","    # Save per-model results\n","    model_metrics_file = metrics_dir / f\"{model_name.lower()}_{fold_tag}.json\"\n","    with open(model_metrics_file, 'w') as f:\n","        json.dump(results, f, indent=2)\n","    print(f\"✓ Saved metrics: {model_metrics_file.name}\")\n","\n","# ========== 5. Save aggregated results ==========\n","print(f\"\\n{'=' * 60}\")\n","print(f\"Save aggregated results\")\n","print(f\"{'=' * 60}\")\n","\n","summary = {\n","    'fold_id': FOLD_ID,\n","    'fold_tag': fold_tag,\n","    'models': list(all_results.keys()),\n","    'results': all_results,\n","    'summary_table': {},\n","}\n","\n","# Build summary table\n","for model_name, results in all_results.items():\n","    wl = results['window_level']\n","    sl = results['subject_level']\n","    summary['summary_table'][model_name] = {\n","        'window_macro_f1': f\"{wl['macro_f1']['value']:.4f} [{wl['macro_f1']['ci_lower']:.4f}, {wl['macro_f1']['ci_upper']:.4f}]\",\n","        'window_balanced_acc': f\"{wl['balanced_accuracy']['value']:.4f} [{wl['balanced_accuracy']['ci_lower']:.4f}, {wl['balanced_accuracy']['ci_upper']:.4f}]\",\n","        'subject_macro_f1': f\"{sl['macro_f1_mean']:.4f} ± {sl['macro_f1_std']:.4f}\",\n","    }\n","\n","summary_file = metrics_dir / f\"summary_{fold_tag}.json\"\n","with open(summary_file, 'w') as f:\n","    json.dump(summary, f, indent=2)\n","print(f\"✓ Saved summary: {summary_file.name}\")\n","\n","# ========== 6. Display results table ==========\n","print(f\"\\n{'=' * 60}\")\n","print(f\"Summary (Fold {FOLD_ID})\")\n","print(f\"{'=' * 60}\")\n","\n","print(f\"\\n{'Model':<15} {'Window Macro-F1 (95% CI)':<40} {'Subject Macro-F1':<25} {'Balanced Acc (95% CI)':<40}\")\n","print(\"-\" * 120)\n","\n","for model_name in all_results.keys():\n","    st = summary['summary_table'][model_name]\n","    print(f\"{model_name:<15} {st['window_macro_f1']:<40} {st['subject_macro_f1']:<25} {st['window_balanced_acc']:<40}\")\n","\n","# ========== 7. Cross-fold aggregation (if multiple folds exist) ==========\n","print(f\"\\n{'=' * 60}\")\n","print(f\"Check cross-fold results\")\n","print(f\"{'=' * 60}\")\n","\n","# Collect all fold results\n","all_folds_results = defaultdict(lambda: defaultdict(list))\n","\n","for fold_file in sorted(metrics_dir.glob(\"summary_fold_*.json\")):\n","    with open(fold_file, 'r') as f:\n","        fold_data = json.load(f)\n","\n","    for model_name, model_results in fold_data['results'].items():\n","        wl = model_results['window_level']\n","        sl = model_results['subject_level']\n","\n","        all_folds_results[model_name]['window_macro_f1'].append(wl['macro_f1']['value'])\n","        all_folds_results[model_name]['balanced_acc'].append(wl['balanced_accuracy']['value'])\n","        all_folds_results[model_name]['subject_macro_f1'].append(sl['macro_f1_mean'])\n","\n","if len(all_folds_results) > 0:\n","    print(f\"\\nCross-fold summary ({len(list(metrics_dir.glob('summary_fold_*.json')))} folds):\")\n","    print(f\"\\n{'Model':<15} {'Window Macro-F1 [95% CI]':<35} {'Subject Macro-F1 [95% CI]':<35} {'Balanced Acc [95% CI]':<35}\")\n","    print(\"-\" * 120)\n","\n","    cross_fold_summary = {}\n","    for model_name, metrics in all_folds_results.items():\n","        # Cross-fold mean and 95% CI (t-distribution)\n","        w_mean, w_lo, w_hi = mean_ci_t(metrics['window_macro_f1'])\n","        s_mean, s_lo, s_hi = mean_ci_t(metrics['subject_macro_f1'])\n","        b_mean, b_lo, b_hi = mean_ci_t(metrics['balanced_acc'])\n","\n","        # Format\n","        w_str = f\"{w_mean:.4f} [{w_lo:.4f}, {w_hi:.4f}]\" if w_lo is not None else f\"{w_mean:.4f}\"\n","        s_str = f\"{s_mean:.4f} [{s_lo:.4f}, {s_hi:.4f}]\" if s_lo is not None else f\"{s_mean:.4f}\"\n","        b_str = f\"{b_mean:.4f} [{b_lo:.4f}, {b_hi:.4f}]\" if b_lo is not None else f\"{b_mean:.4f}\"\n","\n","        print(f\"{model_name:<15} {w_str:<35} {s_str:<35} {b_str:<35}\")\n","\n","        cross_fold_summary[model_name] = {\n","            'window_macro_f1': {\n","                'mean': w_mean,\n","                'ci_lower': w_lo,\n","                'ci_upper': w_hi,\n","            },\n","            'subject_macro_f1': {\n","                'mean': s_mean,\n","                'ci_lower': s_lo,\n","                'ci_upper': s_hi,\n","            },\n","            'balanced_acc': {\n","                'mean': b_mean,\n","                'ci_lower': b_lo,\n","                'ci_upper': b_hi,\n","            },\n","            'n_folds': len(metrics['window_macro_f1']),\n","            'ci_method': 't-interval (fold-level)',\n","        }\n","\n","    # Save cross-fold summary\n","    cross_fold_file = metrics_dir / \"cross_fold_summary.json\"\n","    with open(cross_fold_file, 'w') as f:\n","        json.dump(cross_fold_summary, f, indent=2)\n","    print(f\"\\n✓ Saved cross-fold summary: {cross_fold_file.name}\")\n","else:\n","    print(\"\\nOnly single-fold results available; run more folds to produce cross-fold summary\")\n","\n","# ========== 8. Summary ==========\n","print(f\"\\n{'=' * 60}\")\n","print(f\"Step 16 complete — Evaluation\")\n","print(f\"{'=' * 60}\")\n","\n","print(f\"\\nCurrent fold: FOLD_ID={FOLD_ID}\")\n","print(f\"Bootstrap: {N_BOOTSTRAP} iterations (BCa)\")\n","print(f\"Evaluated models: {len(all_results)}\")\n","\n","print(f\"\\nOutput files:\")\n","print(f\"  {metrics_dir}/\")\n","for model_name in all_results.keys():\n","    print(f\"    - {model_name.lower()}_{fold_tag}.json\")\n","    print(f\"    - confusion_{model_name.lower()}_{fold_tag}.png\")\n","print(f\"    - summary_{fold_tag}.json\")\n","if (metrics_dir / \"cross_fold_summary.json\").exists():\n","    print(f\"    - cross_fold_summary.json\")\n","\n","print(f\"\\nEvaluation metrics:\")\n","print(f\"  ✓ Window-level Macro-F1 (95% CI - BCa bootstrap)\")\n","print(f\"  ✓ Balanced Accuracy (95% CI - BCa bootstrap)\")\n","print(f\"  ✓ Per-class F1\")\n","print(f\"  ✓ Subject-level Macro-F1 (mean ± std)\")\n","print(f\"  ✓ Confusion Matrix\")\n","print(f\"  ✓ Cross-fold means (95% CI - t-distribution)\")\n","\n","print(f\"\\nNext steps:\")\n","print(f\"  - Run Steps 14–16 for all folds\")\n","print(f\"  - See cross_fold_summary.json for final aggregated results\")\n","print(f\"  - Produce paper figures (ROC/PR curves, etc.)\")\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sSUR7DL1sLXv","executionInfo":{"status":"ok","timestamp":1763118018155,"user_tz":0,"elapsed":16952,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"8815b68a-c816-4554-883d-0f815338ebac"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 16: Evaluation (Primary: Macro-F1)\n","============================================================\n","\n","Current fold: FOLD_ID=3\n","Bootstrap: 1000 iterations, window-level sampling\n","\n","============================================================\n","Evaluate all models\n","============================================================\n","\n","============================================================\n","Evaluate RF\n","============================================================\n","Loaded predictions: 873 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.4875\n","  Balanced Acc: 0.5069\n","\n","  Per-class F1:\n","    Class 1: 0.4550\n","    Class 2: 0.8244\n","    Class 3: 0.0000\n","    Class 4: 0.9524\n","    Class 5: 0.6933\n","    Class 6: 0.0000\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.4875 [0.4549, 0.5083]\n","  Balanced Acc: 0.5069 [0.4498, 0.5299]\n","\n","✓ Saved confusion matrix: confusion_rf_fold_03.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.4875 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: rf_fold_03.json\n","\n","============================================================\n","Evaluate KNN\n","============================================================\n","Loaded predictions: 873 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.5078\n","  Balanced Acc: 0.5227\n","\n","  Per-class F1:\n","    Class 1: 0.3985\n","    Class 2: 0.8161\n","    Class 3: 0.0503\n","    Class 4: 0.9524\n","    Class 5: 0.6499\n","    Class 6: 0.1798\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.5078 [0.4753, 0.5383]\n","  Balanced Acc: 0.5227 [0.4659, 0.5492]\n","\n","✓ Saved confusion matrix: confusion_knn_fold_03.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.5078 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: knn_fold_03.json\n","\n","============================================================\n","Evaluate InceptionTime\n","============================================================\n","Loaded predictions: 873 windows\n","#classes: 6\n","classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n","\n","Basic metrics:\n","  Macro-F1: 0.5937\n","  Balanced Acc: 0.6002\n","\n","  Per-class F1:\n","    Class 1: 0.4306\n","    Class 2: 0.8512\n","    Class 3: 0.1159\n","    Class 4: 1.0000\n","    Class 5: 0.6890\n","    Class 6: 0.4754\n","\n","Bootstrap 95% CI (1000 iterations):\n","  Macro-F1: 0.5937 [0.5654, 0.6238]\n","  Balanced Acc: 0.6002 [0.5745, 0.6301]\n","\n","✓ Saved confusion matrix: confusion_inceptiontime_fold_03.png\n","\n","Subject-level evaluation:\n","  Subject-level Macro-F1: 0.5937 ± 0.0000\n","  #test subjects: 1\n","✓ Saved metrics: inceptiontime_fold_03.json\n","\n","============================================================\n","Save aggregated results\n","============================================================\n","✓ Saved summary: summary_fold_03.json\n","\n","============================================================\n","Summary (Fold 3)\n","============================================================\n","\n","Model           Window Macro-F1 (95% CI)                 Subject Macro-F1          Balanced Acc (95% CI)                   \n","------------------------------------------------------------------------------------------------------------------------\n","RF              0.4875 [0.4549, 0.5083]                  0.4875 ± 0.0000           0.5069 [0.4498, 0.5299]                 \n","KNN             0.5078 [0.4753, 0.5383]                  0.5078 ± 0.0000           0.5227 [0.4659, 0.5492]                 \n","InceptionTime   0.5937 [0.5654, 0.6238]                  0.5937 ± 0.0000           0.6002 [0.5745, 0.6301]                 \n","\n","============================================================\n","Check cross-fold results\n","============================================================\n","\n","Cross-fold summary (4 folds):\n","\n","Model           Window Macro-F1 [95% CI]            Subject Macro-F1 [95% CI]           Balanced Acc [95% CI]              \n","------------------------------------------------------------------------------------------------------------------------\n","RF              0.4225 [0.3141, 0.5308]             0.4225 [0.3141, 0.5308]             0.4436 [0.3053, 0.5819]            \n","KNN             0.4476 [0.3372, 0.5579]             0.4476 [0.3372, 0.5579]             0.4604 [0.3251, 0.5957]            \n","InceptionTime   0.5264 [0.4035, 0.6494]             0.5264 [0.4035, 0.6494]             0.5377 [0.4199, 0.6554]            \n","\n","✓ Saved cross-fold summary: cross_fold_summary.json\n","\n","============================================================\n","Step 16 complete — Evaluation\n","============================================================\n","\n","Current fold: FOLD_ID=3\n","Bootstrap: 1000 iterations (BCa)\n","Evaluated models: 3\n","\n","Output files:\n","  metrics/\n","    - rf_fold_03.json\n","    - confusion_rf_fold_03.png\n","    - knn_fold_03.json\n","    - confusion_knn_fold_03.png\n","    - inceptiontime_fold_03.json\n","    - confusion_inceptiontime_fold_03.png\n","    - summary_fold_03.json\n","    - cross_fold_summary.json\n","\n","Evaluation metrics:\n","  ✓ Window-level Macro-F1 (95% CI - BCa bootstrap)\n","  ✓ Balanced Accuracy (95% CI - BCa bootstrap)\n","  ✓ Per-class F1\n","  ✓ Subject-level Macro-F1 (mean ± std)\n","  ✓ Confusion Matrix\n","  ✓ Cross-fold means (95% CI - t-distribution)\n","\n","Next steps:\n","  - Run Steps 14–16 for all folds\n","  - See cross_fold_summary.json for final aggregated results\n","  - Produce paper figures (ROC/PR curves, etc.)\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 17: Statistical Comparison (Top-tier Conference/Journal Level)\n","\n","Friedman→Nemenyi; Cliff's δ/Â12; α=0.05\n","Cross-fold Macro-F1, with fold as block\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import json\n","from scipy import stats\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as mpatches\n","\n","# ========== Configuration ==========\n","ALPHA = 0.05\n","MODELS = ['RF', 'KNN', 'InceptionTime']\n","\n","print(\"=\" * 60)\n","print(\"Step 17: Statistical Comparison\")\n","print(\"=\" * 60)\n","\n","# Path configuration\n","metrics_dir = Path(\"metrics\")\n","stats_dir = Path(\"stats\")\n","stats_dir.mkdir(parents=True, exist_ok=True)\n","\n","# ========== 1. Load Cross-fold Macro-F1 ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"1. Load Cross-fold Macro-F1\")\n","print(\"=\" * 60)\n","\n","# Collect results from all folds\n","fold_results = []\n","for fold_file in sorted(metrics_dir.glob(\"summary_fold_*.json\")):\n","    with open(fold_file, 'r') as f:\n","        data = json.load(f)\n","\n","    fold_id = data['fold_id']\n","    fold_scores = {'fold_id': fold_id}\n","\n","    for model in MODELS:\n","        if model in data['results']:\n","            score = data['results'][model]['window_level']['macro_f1']['value']\n","            fold_scores[model] = score\n","        else:\n","            fold_scores[model] = np.nan\n","\n","    fold_results.append(fold_scores)\n","\n","df_folds = pd.DataFrame(fold_results).sort_values('fold_id')\n","\n","# Check completeness\n","n_folds = len(df_folds)\n","print(f\"\\nLoaded folds: {n_folds}\")\n","print(f\"Models: {MODELS}\")\n","\n","if n_folds < 2:\n","    raise ValueError(f\"At least 2 folds required for statistical comparison, got only {n_folds}\")\n","\n","print(\"\\nData matrix (row=fold, column=model):\")\n","print(df_folds.to_string(index=False))\n","\n","# Check missing values\n","for model in MODELS:\n","    n_missing = df_folds[model].isna().sum()\n","    if n_missing > 0:\n","        print(f\"\\n⚠️ {model}: {n_missing} folds with missing data\")\n","\n","# Remove folds with any missing model\n","df_complete = df_folds.dropna(subset=MODELS)\n","if len(df_complete) < len(df_folds):\n","    print(f\"\\n⚠️ Removed {len(df_folds) - len(df_complete)} folds (containing missing values)\")\n","    df_folds = df_complete\n","\n","n_folds = len(df_folds)\n","if n_folds < 2:\n","    raise ValueError(\"Complete data < 2 folds, cannot perform statistical test\")\n","\n","# ========== 2. Friedman Test ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"2. Friedman Test (α=0.05)\")\n","print(\"=\" * 60)\n","\n","# Construct data matrix\n","data_matrix = df_folds[MODELS].values  # shape: (n_folds, n_models)\n","\n","# Friedman test\n","friedman_stat, friedman_p = stats.friedmanchisquare(*data_matrix.T)\n","\n","print(f\"\\nFriedman statistic: {friedman_stat:.4f}\")\n","print(f\"p-value: {friedman_p:.4f}\")\n","print(f\"Significance: {'Significant' if friedman_p < ALPHA else 'Not significant'} (α={ALPHA})\")\n","\n","# ========== 3. Calculate Average Ranks and Critical Distance ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"3. Calculate Average Ranks and Critical Distance (Nemenyi)\")\n","print(\"=\" * 60)\n","\n","def nemenyi_test(data_matrix, alpha=0.05):\n","    \"\"\"\n","    Nemenyi post-hoc test\n","    data_matrix: (n_blocks, n_treatments)\n","    Returns: ranks, critical_distance, pairwise_p\n","    \"\"\"\n","    n_blocks, k = data_matrix.shape\n","\n","    # Calculate ranks (rank within each row independently)\n","    ranks = np.zeros_like(data_matrix, dtype=float)\n","    for i in range(n_blocks):\n","        ranks[i] = stats.rankdata(-data_matrix[i])  # Descending rank (higher score = smaller rank)\n","\n","    # Average ranks\n","    mean_ranks = ranks.mean(axis=0)\n","\n","    # Critical distance (Nemenyi)\n","    from scipy.stats import studentized_range\n","    q_alpha = studentized_range.ppf(1 - alpha, k, np.inf)  # Studentized range quantile\n","    cd = q_alpha * np.sqrt(k * (k + 1) / (6 * n_blocks))\n","\n","    # Pairwise comparison p-values (using z-test on rank differences)\n","    pairwise_p = np.ones((k, k))\n","    for i in range(k):\n","        for j in range(i + 1, k):\n","            rank_diff = abs(mean_ranks[i] - mean_ranks[j])\n","            # Nemenyi test statistic\n","            z = rank_diff / np.sqrt(k * (k + 1) / (6 * n_blocks))\n","            # Two-tailed p-value (normal approximation)\n","            p = 2 * (1 - stats.norm.cdf(z))\n","            pairwise_p[i, j] = p\n","            pairwise_p[j, i] = p\n","\n","    return mean_ranks, cd, pairwise_p\n","\n","# Always calculate (regardless of Friedman significance)\n","mean_ranks, cd, pairwise_p = nemenyi_test(data_matrix, ALPHA)\n","\n","print(f\"\\nAverage ranks (smaller is better):\")\n","for model, rank in zip(MODELS, mean_ranks):\n","    print(f\"  {model:15s}: {rank:.2f}\")\n","\n","print(f\"\\nCritical Distance (CD): {cd:.3f}\")\n","print(f\"Note: Rank difference > CD indicates significant difference\")\n","\n","if friedman_p < ALPHA:\n","    print(f\"\\nFriedman significant → Proceed with Nemenyi post-hoc test\")\n","    print(f\"\\nPairwise comparison (p-values):\")\n","    print(f\"{'':15s}\", end='')\n","    for model in MODELS:\n","        print(f\"{model:15s}\", end='')\n","    print()\n","\n","    for i, model_i in enumerate(MODELS):\n","        print(f\"{model_i:15s}\", end='')\n","        for j, model_j in enumerate(MODELS):\n","            if i == j:\n","                print(f\"{'—':>15s}\", end='')\n","            else:\n","                sig = ' *' if pairwise_p[i, j] < ALPHA else ''\n","                print(f\"{pairwise_p[i, j]:>14.4f}{sig}\", end='')\n","        print()\n","\n","    print(f\"\\n* indicates p < {ALPHA}\")\n","else:\n","    print(f\"\\nFriedman not significant (p={friedman_p:.4f}) → Pairwise comparisons are for reference only\")\n","\n","# ========== 4. Effect Size Calculation ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"4. Effect Sizes (Cliff's δ & Â12)\")\n","print(\"=\" * 60)\n","\n","def cliffs_delta(x, y):\n","    \"\"\"\n","    Cliff's δ: Measures non-overlap between two distributions\n","    Range: [-1, 1]\n","    \"\"\"\n","    n_x, n_y = len(x), len(y)\n","    dom = sum(1 for xi in x for yi in y if xi > yi)\n","    dom += 0.5 * sum(1 for xi in x for yi in y if xi == yi)\n","    delta = (dom / (n_x * n_y)) * 2 - 1\n","    return delta\n","\n","def a12_score(x, y):\n","    \"\"\"\n","    Â12 (Vargha-Delaney A): Non-parametric effect size\n","    Range: [0, 1], 0.5 indicates no difference\n","    \"\"\"\n","    delta = cliffs_delta(x, y)\n","    return (delta + 1) / 2\n","\n","# Calculate effect sizes for all pairs\n","effect_sizes = {}\n","print(f\"\\nPairwise effect sizes:\")\n","print(f\"{'Comparison':30s} {'Cliff\\'s δ':>12s} {'Â12':>12s} {'Interpretation':>20s}\")\n","print(\"-\" * 80)\n","\n","for i, model_i in enumerate(MODELS):\n","    for j, model_j in enumerate(MODELS):\n","        if i < j:  # Only calculate upper triangle\n","            x = df_folds[model_i].values\n","            y = df_folds[model_j].values\n","\n","            delta = cliffs_delta(x, y)\n","            a12 = a12_score(x, y)\n","\n","            # Effect size interpretation (Cohen's guideline for δ)\n","            if abs(delta) < 0.147:\n","                interpretation = \"Negligible\"\n","            elif abs(delta) < 0.33:\n","                interpretation = \"Small\"\n","            elif abs(delta) < 0.474:\n","                interpretation = \"Medium\"\n","            else:\n","                interpretation = \"Large\"\n","\n","            pair_key = f\"{model_i}_vs_{model_j}\"\n","            effect_sizes[pair_key] = {\n","                'cliffs_delta': float(delta),\n","                'a12': float(a12),\n","                'interpretation': interpretation\n","            }\n","\n","            print(f\"{model_i} vs {model_j:15s} {delta:>12.4f} {a12:>12.4f} {interpretation:>20s}\")\n","\n","# ========== 5. Plot Critical Distance Diagram (Always Generated) ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"5. Plot Critical Distance Diagram (CD Plot)\")\n","print(\"=\" * 60)\n","\n","fig, ax = plt.subplots(figsize=(10, 3))\n","\n","# Sort models by average rank\n","sorted_indices = np.argsort(mean_ranks)\n","sorted_models = [MODELS[i] for i in sorted_indices]\n","sorted_ranks = mean_ranks[sorted_indices]\n","\n","# Plot model positions\n","y_pos = np.arange(len(sorted_models))\n","ax.barh(y_pos, sorted_ranks, color='skyblue', edgecolor='black')\n","\n","# Only draw connection lines when Friedman is significant\n","if friedman_p < ALPHA:\n","    for i in range(len(sorted_models)):\n","        for j in range(i + 1, len(sorted_models)):\n","            rank_diff = abs(sorted_ranks[j] - sorted_ranks[i])\n","            if rank_diff <= cd:\n","                # No significant difference, draw connection line\n","                y_mid = (y_pos[i] + y_pos[j]) / 2\n","                ax.plot([sorted_ranks[i], sorted_ranks[j]],\n","                       [y_pos[i], y_pos[j]],\n","                       'r-', linewidth=2, alpha=0.6)\n","\n","ax.set_yticks(y_pos)\n","ax.set_yticklabels(sorted_models)\n","ax.set_xlabel('Average Rank (smaller is better)')\n","\n","# Adjust title based on Friedman result\n","if friedman_p < ALPHA:\n","    title = f'Nemenyi Critical Distance Plot (CD={cd:.3f}, α={ALPHA})\\nFriedman Significant (p={friedman_p:.4f})'\n","else:\n","    title = f'Average Ranks and Critical Distance (CD={cd:.3f}, α={ALPHA})\\nFriedman Not Significant (p={friedman_p:.4f}, reference only)'\n","\n","ax.set_title(title)\n","ax.axvline(x=sorted_ranks.min() + cd, color='red', linestyle='--',\n","           linewidth=1, alpha=0.5, label=f'CD line ({cd:.3f})')\n","ax.legend()\n","ax.grid(axis='x', alpha=0.3)\n","\n","plt.tight_layout()\n","cd_plot_file = stats_dir / \"cd_plot.png\"\n","plt.savefig(cd_plot_file, dpi=150, bbox_inches='tight')\n","plt.close()\n","\n","print(f\"\\n✓ Saved: {cd_plot_file}\")\n","\n","# ========== 6. Save Results ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"6. Save Statistical Results\")\n","print(\"=\" * 60)\n","\n","results = {\n","    'alpha': ALPHA,\n","    'n_folds': int(n_folds),\n","    'models': MODELS,\n","    'data_matrix': df_folds[MODELS].to_dict(orient='list'),\n","    'friedman': {\n","        'statistic': float(friedman_stat),\n","        'p_value': float(friedman_p),\n","        'significant': bool(friedman_p < ALPHA),\n","    },\n","    'nemenyi': {\n","        'mean_ranks': {model: float(rank) for model, rank in zip(MODELS, mean_ranks)},\n","        'critical_distance': float(cd),\n","        'pairwise_p_values': {\n","            f\"{MODELS[i]}_vs_{MODELS[j]}\": float(pairwise_p[i, j])\n","            for i in range(len(MODELS))\n","            for j in range(i + 1, len(MODELS))\n","        },\n","        'significantly_different_pairs': [\n","            f\"{MODELS[i]} vs {MODELS[j]}\"\n","            for i in range(len(MODELS))\n","            for j in range(i + 1, len(MODELS))\n","            if pairwise_p[i, j] < ALPHA\n","        ] if friedman_p < ALPHA else [],\n","        'note': 'Nemenyi pairwise comparisons are only valid when Friedman is significant, otherwise for reference only'\n","    },\n","    'effect_sizes': effect_sizes,\n","    'notes': [\n","        'Friedman test: Non-parametric test for paired samples',\n","        'Nemenyi post-hoc test: Always calculated, but only valid when Friedman is significant',\n","        'CD plot: Always generated, title indicates Friedman significance',\n","        'Cliff\\'s δ: [-1,1], 0 indicates no difference',\n","        'Â12: [0,1], 0.5 indicates no difference',\n","        'Effect size interpretation: |δ|<0.147(negligible), <0.33(small), <0.474(medium), ≥0.474(large)',\n","        f'Cross-{n_folds}-fold comparison, using fold as block',\n","    ]\n","}\n","\n","tests_file = stats_dir / \"tests.json\"\n","with open(tests_file, 'w') as f:\n","    json.dump(results, f, indent=2)\n","\n","print(f\"✓ Saved: {tests_file}\")\n","\n","# ========== 7. Generate Summary ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"Step 17 Complete - Statistical Comparison\")\n","print(\"=\" * 60)\n","\n","print(f\"\\nTest results:\")\n","print(f\"  Friedman: χ²={friedman_stat:.4f}, p={friedman_p:.4f} ({'Significant' if friedman_p < ALPHA else 'Not significant'})\")\n","\n","print(f\"\\n  Nemenyi:\")\n","print(f\"  Critical distance: {cd:.3f}\")\n","print(f\"  Best model (smallest average rank): {sorted_models[0]} (rank={sorted_ranks[0]:.2f})\")\n","\n","if friedman_p < ALPHA:\n","    n_sig_pairs = len(results['nemenyi']['significantly_different_pairs'])\n","    print(f\"  Significantly different model pairs: {n_sig_pairs}\")\n","    if n_sig_pairs > 0:\n","        for pair in results['nemenyi']['significantly_different_pairs']:\n","            print(f\"    - {pair}\")\n","else:\n","    print(f\"  Note: Friedman not significant, pairwise comparisons are for reference only\")\n","\n","print(f\"\\nEffect sizes:\")\n","for pair_key, es in effect_sizes.items():\n","    print(f\"  {pair_key.replace('_', ' ')}: δ={es['cliffs_delta']:.3f}, Â12={es['a12']:.3f} ({es['interpretation']})\")\n","\n","print(f\"\\nOutput files:\")\n","print(f\"  {stats_dir}/\")\n","print(f\"  - tests.json\")\n","print(f\"  - cd_plot.png ({'Friedman significant' if friedman_p < ALPHA else 'Friedman not significant, reference only'})\")\n","\n","print(f\"\\nInterpretation:\")\n","print(f\"  - Friedman test detects whether there are significant differences among models\")\n","print(f\"  - Nemenyi post-hoc test identifies which model pairs are significantly different (only valid when Friedman is significant)\")\n","print(f\"  - Cliff's δ and Â12 quantify effect magnitude (independent of statistical significance)\")\n","print(f\"  - CD plot is always generated, title indicates Friedman significance\")\n","\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZcoXaBkd37n5","executionInfo":{"status":"ok","timestamp":1763118018403,"user_tz":0,"elapsed":221,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"935899f5-d472-4b96-c6a0-c6aaf8e11402"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 17: Statistical Comparison\n","============================================================\n","\n","============================================================\n","1. Load Cross-fold Macro-F1\n","============================================================\n","\n","Loaded folds: 4\n","Models: ['RF', 'KNN', 'InceptionTime']\n","\n","Data matrix (row=fold, column=model):\n"," fold_id       RF      KNN  InceptionTime\n","       0 0.415438 0.452418       0.564242\n","       1 0.330447 0.349263       0.417156\n","       2 0.456530 0.480814       0.530587\n","       3 0.487510 0.507812       0.593691\n","\n","============================================================\n","2. Friedman Test (α=0.05)\n","============================================================\n","\n","Friedman statistic: 8.0000\n","p-value: 0.0183\n","Significance: Significant (α=0.05)\n","\n","============================================================\n","3. Calculate Average Ranks and Critical Distance (Nemenyi)\n","============================================================\n","\n","Average ranks (smaller is better):\n","  RF             : 3.00\n","  KNN            : 2.00\n","  InceptionTime  : 1.00\n","\n","Critical Distance (CD): 2.344\n","Note: Rank difference > CD indicates significant difference\n","\n","Friedman significant → Proceed with Nemenyi post-hoc test\n","\n","Pairwise comparison (p-values):\n","               RF             KNN            InceptionTime  \n","RF                           —        0.1573        0.0047 *\n","KNN                    0.1573              —        0.1573\n","InceptionTime          0.0047 *        0.1573              —\n","\n","* indicates p < 0.05\n","\n","============================================================\n","4. Effect Sizes (Cliff's δ & Â12)\n","============================================================\n","\n","Pairwise effect sizes:\n","Comparison                        Cliff's δ          Â12       Interpretation\n","--------------------------------------------------------------------------------\n","RF vs KNN                  -0.2500       0.3750                Small\n","RF vs InceptionTime        -0.7500       0.1250                Large\n","KNN vs InceptionTime        -0.6250       0.1875                Large\n","\n","============================================================\n","5. Plot Critical Distance Diagram (CD Plot)\n","============================================================\n","\n","✓ Saved: stats/cd_plot.png\n","\n","============================================================\n","6. Save Statistical Results\n","============================================================\n","✓ Saved: stats/tests.json\n","\n","============================================================\n","Step 17 Complete - Statistical Comparison\n","============================================================\n","\n","Test results:\n","  Friedman: χ²=8.0000, p=0.0183 (Significant)\n","\n","  Nemenyi:\n","  Critical distance: 2.344\n","  Best model (smallest average rank): InceptionTime (rank=1.00)\n","  Significantly different model pairs: 1\n","    - RF vs InceptionTime\n","\n","Effect sizes:\n","  RF vs KNN: δ=-0.250, Â12=0.375 (Small)\n","  RF vs InceptionTime: δ=-0.750, Â12=0.125 (Large)\n","  KNN vs InceptionTime: δ=-0.625, Â12=0.188 (Large)\n","\n","Output files:\n","  stats/\n","  - tests.json\n","  - cd_plot.png (Friedman significant)\n","\n","Interpretation:\n","  - Friedman test detects whether there are significant differences among models\n","  - Nemenyi post-hoc test identifies which model pairs are significantly different (only valid when Friedman is significant)\n","  - Cliff's δ and Â12 quantify effect magnitude (independent of statistical significance)\n","  - CD plot is always generated, title indicates Friedman significance\n","============================================================\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","Step 18: Probability Calibration (Full Version)\n","Copy and run this file as-is; do not partially update\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import json\n","import os\n","import matplotlib.pyplot as plt\n","from scipy.optimize import minimize\n","from scipy.special import softmax\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.amp import autocast\n","\n","# ========== Config ==========\n","RANDOM_SEED = 42\n","N_BINS = 15\n","MODELS = ['RF', 'KNN', 'InceptionTime']\n","\n","print(\"=\" * 60)\n","print(\"Step 18: Probability Calibration (Full Version)\")\n","print(\"=\" * 60)\n","\n","proc_dir = Path(\"data/lara/mbientlab/proc\")\n","predictions_dir = Path(\"predictions\")\n","models_dir = Path(\"models\")\n","calibration_dir = Path(\"calibration\")\n","calibration_dir.mkdir(parents=True, exist_ok=True)\n","\n","FOLD_ID = int(os.environ.get(\"FOLD_ID\", \"-1\"))\n","if FOLD_ID < 0:\n","    raise RuntimeError(\"FOLD_ID must be set (0 to N-1)\")\n","\n","fold_tag = f\"fold_{FOLD_ID:02d}\"\n","pred_fold_dir = predictions_dir / fold_tag\n","models_fold_dir = models_dir / fold_tag\n","calib_fold_dir = calibration_dir / fold_tag\n","calib_fold_dir.mkdir(parents=True, exist_ok=True)\n","\n","print(f\"\\nCurrent fold: FOLD_ID={FOLD_ID}, ECE bins: {N_BINS}\")\n","\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","\n","# ========== Load class list ==========\n","scalers_dir = proc_dir / \"scalers\" / fold_tag\n","y_train_full = np.load(scalers_dir / \"y_train.npy\")\n","CLASS_LIST = [int(c) for c in sorted(np.unique(y_train_full))]\n","CLASS_TO_INDEX = {c: i for i, c in enumerate(CLASS_LIST)}\n","N_CLASSES = len(CLASS_LIST)\n","\n","print(f\"\\n#classes: {N_CLASSES}, classes: {CLASS_LIST}\")\n","\n","# ========== Metric functions ==========\n","def expected_calibration_error(y_true, y_proba, n_bins=15):\n","    y_pred = np.argmax(y_proba, axis=1)\n","    conf = np.max(y_proba, axis=1)\n","    acc = (y_pred == y_true).astype(float)\n","    edges = np.linspace(0.0, 1.0, n_bins + 1)\n","    ece, N = 0.0, len(y_true)\n","    for i in range(n_bins):\n","        lo, hi = edges[i], edges[i+1]\n","        mask = (conf >= lo) & (conf < hi) if i < n_bins-1 else (conf >= lo) & (conf <= hi)\n","        if not np.any(mask):\n","            continue\n","        ece += abs(acc[mask].mean() - conf[mask].mean()) * (mask.sum() / N)\n","    return ece\n","\n","def brier_score(y_true, y_proba):\n","    n_classes = y_proba.shape[1]\n","    y_true_one_hot = np.eye(n_classes)[y_true]\n","    return np.mean(np.sum((y_proba - y_true_one_hot) ** 2, axis=1))\n","\n","def negative_log_likelihood(y_true, y_proba):\n","    n_samples = len(y_true)\n","    y_proba = np.clip(y_proba, 1e-15, 1 - 1e-15)\n","    return -np.mean(np.log(y_proba[np.arange(n_samples), y_true]))\n","\n","def reliability_diagram_data(y_true, y_proba, n_bins=15):\n","    y_pred = np.argmax(y_proba, axis=1)\n","    conf = np.max(y_proba, axis=1)\n","    acc = (y_pred == y_true).astype(float)\n","    edges = np.linspace(0.0, 1.0, n_bins + 1)\n","\n","    bin_centers, bin_accuracies, bin_confidences, bin_counts = [], [], [], []\n","    for i in range(n_bins):\n","        lo, hi = edges[i], edges[i+1]\n","        mask = (conf >= lo) & (conf < hi) if i < n_bins-1 else (conf >= lo) & (conf <= hi)\n","        if not np.any(mask):\n","            continue\n","        bin_centers.append((lo + hi) / 2)\n","        bin_accuracies.append(acc[mask].mean())\n","        bin_confidences.append(conf[mask].mean())\n","        bin_counts.append(int(mask.sum()))\n","    return bin_centers, bin_accuracies, bin_confidences, bin_counts\n","\n","def temperature_scale(logits, temperature):\n","    return softmax(logits / temperature, axis=1)\n","\n","def find_optimal_temperature(logits, y_true, temp_range=(0.05, 10.0)):\n","    candidates = np.logspace(np.log10(0.2), np.log10(5.0), 21)\n","    nll_vals = [negative_log_likelihood(y_true, temperature_scale(logits, t)) for t in candidates]\n","    best_T_init = candidates[np.argmin(nll_vals)]\n","\n","    def nll_loss(temp):\n","        temp_val = float(temp[0]) if hasattr(temp, '__len__') else float(temp)\n","        scaled_proba = temperature_scale(logits, temp_val)\n","        return negative_log_likelihood(y_true, scaled_proba)\n","\n","    result = minimize(nll_loss, x0=np.array([best_T_init]), bounds=[temp_range], method='L-BFGS-B')\n","\n","    if not result.success:\n","        print(f\"  ⚠️ Temperature optimization did not converge; using coarse-grid best T={best_T_init:.4f}\")\n","        return best_T_init\n","\n","    return float(result.x[0])\n","\n","# ========== Load test predictions ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"3. Load test predictions and compute uncalibrated metrics\")\n","print(\"=\" * 60)\n","\n","uncalibrated_results = {}\n","filename_map = {\n","    'RF': 'rf_predictions.parquet',\n","    'KNN': 'knn_predictions.parquet',\n","    'InceptionTime': 'inception_predictions.parquet'\n","}\n","\n","for model_name in MODELS:\n","    pred_file = pred_fold_dir / filename_map[model_name]\n","    if not pred_file.exists():\n","        print(f\"\\n⚠️ Skip {model_name}: not found {pred_file}\")\n","        continue\n","\n","    print(f\"\\nProcessing {model_name}...\")\n","    df_pred = pd.read_parquet(pred_file)\n","\n","    proba_cols = [f'proba_class_{label}' for label in CLASS_LIST]\n","    missing_cols = set(proba_cols) - set(df_pred.columns)\n","    if missing_cols:\n","        raise ValueError(f\"{model_name}: missing probability columns {missing_cols}\")\n","\n","    y_true = df_pred['true_label'].values\n","    y_true_idx = np.array([CLASS_TO_INDEX[int(y)] for y in y_true])\n","    y_proba = df_pred[proba_cols].values\n","\n","    ece = expected_calibration_error(y_true_idx, y_proba, N_BINS)\n","    brier = brier_score(y_true_idx, y_proba)\n","    nll = negative_log_likelihood(y_true_idx, y_proba)\n","\n","    print(f\"  ECE: {ece:.4f}, Brier: {brier:.4f}, NLL: {nll:.4f}\")\n","\n","    uncalibrated_results[model_name] = {\n","        'ece': float(ece), 'brier': float(brier), 'nll': float(nll),\n","        'y_true': y_true_idx, 'y_proba': y_proba\n","    }\n","\n","# ========== InceptionTime temperature scaling ==========\n","calibrated_results = {}\n","\n","if 'InceptionTime' in uncalibrated_results:\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"4. InceptionTime Temperature Scaling (No-Leakage Version)\")\n","    print(\"=\" * 60)\n","\n","    pred_file = pred_fold_dir / filename_map['InceptionTime']\n","    df_pred = pd.read_parquet(pred_file)\n","\n","    logit_cols = [f'logit_class_{label}' for label in CLASS_LIST]\n","    missing_logit_cols = set(logit_cols) - set(df_pred.columns)\n","    if missing_logit_cols:\n","        raise ValueError(f\"Missing logit columns {missing_logit_cols}\")\n","\n","    test_logits = df_pred[logit_cols].values\n","    test_y_true = uncalibrated_results['InceptionTime']['y_true']\n","\n","    print(\"\\nChecking logits validity...\")\n","    proba_from_logits = softmax(test_logits, axis=1)\n","    proba_from_file = df_pred[[f'proba_class_{c}' for c in CLASS_LIST]].values\n","    if np.allclose(proba_from_logits, proba_from_file, atol=1e-5):\n","        print(\"✓ Logits check passed (softmax(logits) ≈ proba_class_*)\")\n","    else:\n","        print(\"⚠️ logits and probabilities are not exactly matched\")\n","\n","    print(\"\\nPrepare validation set (no leakage: retrain model excluding validation subjects)...\")\n","\n","    X_train_full = np.load(scalers_dir / \"X_train_scaled.npy\")\n","    y_train_full_mapped = np.array([CLASS_TO_INDEX[int(y)] for y in y_train_full])\n","    df_train_meta = pd.read_parquet(scalers_dir / \"train_meta.parquet\")\n","\n","    train_subjects = df_train_meta['subject_id'].unique()\n","    S = len(train_subjects)\n","    n_val_subjects = min(max(2, int(np.ceil(0.2 * S))), max(1, S - 1))\n","\n","    np.random.seed(RANDOM_SEED)\n","    val_subjects = sorted(np.random.choice(train_subjects, n_val_subjects, replace=False).tolist())\n","    val_mask = df_train_meta['subject_id'].isin(val_subjects)\n","\n","    X_val = X_train_full[val_mask]\n","    y_val = y_train_full_mapped[val_mask]\n","    X_train_cal = X_train_full[~val_mask]\n","    y_train_cal = y_train_full_mapped[~val_mask]\n","\n","    print(f\"Validation set: {len(X_val)} windows, {n_val_subjects} subjects\")\n","    print(f\"Calibration train set: {len(X_train_cal)} windows (excluding validation subjects)\")\n","\n","    # Define model\n","    class InceptionModule(nn.Module):\n","        def __init__(self, in_channels, n_filters, kernel_sizes, bottleneck_channels):\n","            super().__init__()\n","            self.bottleneck = nn.Conv1d(in_channels, bottleneck_channels, 1, bias=False)\n","            self.conv_list = nn.ModuleList([\n","                nn.Conv1d(bottleneck_channels, n_filters, k, padding=k//2, bias=False)\n","                for k in kernel_sizes\n","            ])\n","            self.maxpool_conv = nn.Sequential(\n","                nn.MaxPool1d(3, stride=1, padding=1),\n","                nn.Conv1d(in_channels, n_filters, 1, bias=False)\n","            )\n","            out_channels = n_filters * (len(kernel_sizes) + 1)\n","            self.bn = nn.BatchNorm1d(out_channels)\n","            self.relu = nn.ReLU()\n","\n","        def forward(self, x):\n","            bottleneck = self.bottleneck(x)\n","            conv_outputs = [conv(bottleneck) for conv in self.conv_list]\n","            maxpool_output = self.maxpool_conv(x)\n","            out = torch.cat([*conv_outputs, maxpool_output], dim=1)\n","            return self.relu(self.bn(out))\n","\n","    class InceptionTime(nn.Module):\n","        def __init__(self, n_channels, n_classes, n_filters, depth):\n","            super().__init__()\n","            kernel_sizes = [9, 19, 39]\n","            bottleneck_channels = 32\n","            self.inception_modules = nn.ModuleList()\n","            in_ch = n_channels\n","            out_ch = n_filters * (len(kernel_sizes) + 1)\n","            for _ in range(depth):\n","                self.inception_modules.append(\n","                    InceptionModule(in_ch, n_filters, kernel_sizes, bottleneck_channels)\n","                )\n","                in_ch = out_ch\n","            self.gap = nn.AdaptiveAvgPool1d(1)\n","            self.fc = nn.Linear(out_ch, n_classes)\n","\n","        def forward(self, x):\n","            x = x.transpose(1, 2).contiguous()\n","            for inception in self.inception_modules:\n","                x = inception(x)\n","            x = self.gap(x).squeeze(-1)\n","            return self.fc(x)\n","\n","    n_channels = X_val.shape[2]\n","\n","    tuning_dir = Path(\"tuning\") / fold_tag\n","    inception_params_file = tuning_dir / \"inception_best_params.json\"\n","    if inception_params_file.exists():\n","        with open(inception_params_file, 'r') as f:\n","            best_params = json.load(f)['params']\n","    else:\n","        best_params = {'n_filters': 32, 'depth': 6}\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    print(\"\\nRetrain calibration model (excluding validation subjects)...\")\n","    model_cal = InceptionTime(n_channels, N_CLASSES, best_params['n_filters'], best_params['depth']).to(device)\n","\n","    class WindowDataset(Dataset):\n","        def __init__(self, X, y):\n","            self.X = torch.FloatTensor(X)\n","            self.y = torch.LongTensor(y)\n","        def __len__(self):\n","            return len(self.X)\n","        def __getitem__(self, idx):\n","            return self.X[idx], self.y[idx]\n","\n","    train_loader = DataLoader(WindowDataset(X_train_cal, y_train_cal), batch_size=256, shuffle=True, num_workers=4)\n","\n","    optimizer = torch.optim.AdamW(model_cal.parameters(), lr=1e-3, weight_decay=1e-4)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    use_amp = torch.cuda.is_available() and hasattr(torch.cuda, 'is_bf16_supported') and torch.cuda.is_bf16_supported()\n","\n","    for epoch in range(10):\n","        model_cal.train()\n","        for X_batch, y_batch in train_loader:\n","            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","            optimizer.zero_grad()\n","\n","            if use_amp:\n","                with autocast(device_type='cuda', dtype=torch.bfloat16):\n","                    outputs = model_cal(X_batch)\n","                    loss = criterion(outputs, y_batch)\n","            else:\n","                outputs = model_cal(X_batch)\n","                loss = criterion(outputs, y_batch)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","    print(\"✓ Calibration model training complete\")\n","\n","    print(\"\\nInfer on validation set...\")\n","    model_cal.eval()\n","    val_loader = DataLoader(WindowDataset(X_val, y_val), batch_size=256, shuffle=False, num_workers=4)\n","\n","    val_logits_list, val_labels_list = [], []\n","    with torch.no_grad():\n","        for X_batch, y_batch in val_loader:\n","            X_batch = X_batch.to(device, non_blocking=True)\n","            if use_amp:\n","                with autocast(device_type='cuda', dtype=torch.bfloat16):\n","                    outputs = model_cal(X_batch)\n","            else:\n","                outputs = model_cal(X_batch)\n","            val_logits_list.append(outputs.float().cpu().numpy())\n","            val_labels_list.append(y_batch.cpu().numpy())\n","\n","    val_logits = np.vstack(val_logits_list)\n","    val_y_true = np.concatenate(val_labels_list)\n","\n","    print(f\"Validation logits: {val_logits.shape}\")\n","\n","    print(\"\\nSearch best temperature (coarse grid warm-start)...\")\n","    optimal_temp = find_optimal_temperature(val_logits, val_y_true)\n","    print(f\"Best temperature: {optimal_temp:.4f}\")\n","\n","    val_proba_uncal = softmax(val_logits, axis=1)\n","    val_proba_cal = temperature_scale(val_logits, optimal_temp)\n","\n","    val_nll_uncal = negative_log_likelihood(val_y_true, val_proba_uncal)\n","    val_nll_cal = negative_log_likelihood(val_y_true, val_proba_cal)\n","    val_ece_uncal = expected_calibration_error(val_y_true, val_proba_uncal, N_BINS)\n","    val_ece_cal = expected_calibration_error(val_y_true, val_proba_cal, N_BINS)\n","\n","    print(f\"Validation NLL: {val_nll_uncal:.4f} → {val_nll_cal:.4f} (improvement {(1-val_nll_cal/val_nll_uncal)*100:.1f}%)\")\n","    print(f\"Validation ECE: {val_ece_uncal:.4f} → {val_ece_cal:.4f}\")\n","\n","    print(\"\\nGate decision based on validation set (no test usage)...\")\n","    print(f\"Gate rule: val_nll≤0.99*uncal && val_ece≤uncal+0.01\")\n","\n","    accept_calibration = bool(\n","        (val_nll_cal <= 0.99 * val_nll_uncal) and\n","        (val_ece_cal <= val_ece_uncal + 0.01)\n","    )\n","\n","    print(f\"Decision: {'✓ accept calibration' if accept_calibration else '⚠️ reject calibration (fallback T=1.0)'}\")\n","\n","    final_T = float(optimal_temp) if accept_calibration else 1.0\n","\n","    print(f\"\\nRecompute test logits with calibration model (ensure base-model consistency between fitted T and applied T)...\")\n","    test_windows_file = scalers_dir / \"X_test_scaled.npy\"\n","    test_labels_file = scalers_dir / \"y_test.npy\"\n","    base_model_consistent = False\n","\n","    if test_windows_file.exists() and test_labels_file.exists():\n","        X_test = np.load(test_windows_file)\n","        y_test = np.load(test_labels_file)\n","        y_test_idx = np.array([CLASS_TO_INDEX[int(y)] for y in y_test])\n","\n","        test_loader = DataLoader(\n","            WindowDataset(X_test, y_test_idx),\n","            batch_size=256, shuffle=False, num_workers=4\n","        )\n","\n","        test_logits_list = []\n","        with torch.no_grad():\n","            for X_batch, _ in test_loader:\n","                X_batch = X_batch.to(device, non_blocking=True)\n","                if use_amp:\n","                    with autocast(device_type='cuda', dtype=torch.bfloat16):\n","                        outputs = model_cal(X_batch)\n","                else:\n","                    outputs = model_cal(X_batch)\n","                test_logits_list.append(outputs.float().cpu().numpy())\n","\n","        test_logits = np.vstack(test_logits_list)\n","        test_y_true = y_test_idx\n","        base_model_consistent = True\n","        print(f\"✓ Generated test logits with calibration model: {test_logits.shape} (same base model as for T fitting)\")\n","\n","        test_proba_uncal = softmax(test_logits, axis=1)\n","        unc_ece_same = expected_calibration_error(test_y_true, test_proba_uncal, N_BINS)\n","        unc_brier_same = brier_score(test_y_true, test_proba_uncal)\n","        unc_nll_same = negative_log_likelihood(test_y_true, test_proba_uncal)\n","\n","        uncalibrated_results['InceptionTime'] = {\n","            'ece': float(unc_ece_same),\n","            'brier': float(unc_brier_same),\n","            'nll': float(unc_nll_same),\n","            'y_true': test_y_true,\n","            'y_proba': test_proba_uncal\n","        }\n","        print(f\"✓ Updated uncalibrated baseline (same model): ECE={unc_ece_same:.4f}, Brier={unc_brier_same:.4f}, NLL={unc_nll_same:.4f}\")\n","    else:\n","        print(f\"⚠️ Not found {test_windows_file} / {test_labels_file}\")\n","        print(\"⚠️ Fallback to test logits from original predictions (base-model mismatch risk disclosed)\")\n","        base_model_consistent = False\n","\n","    print(f\"\\nApply final temperature on test set (T={final_T:.4f})...\")\n","\n","    final_proba = temperature_scale(test_logits, final_T)\n","    final_ece = expected_calibration_error(test_y_true, final_proba, N_BINS)\n","    final_brier = brier_score(test_y_true, final_proba)\n","    final_nll = negative_log_likelihood(test_y_true, final_proba)\n","\n","    trial_proba = temperature_scale(test_logits, optimal_temp)\n","    trial_ece = expected_calibration_error(test_y_true, trial_proba, N_BINS)\n","    trial_brier = brier_score(test_y_true, trial_proba)\n","    trial_nll = negative_log_likelihood(test_y_true, trial_proba)\n","\n","    print(f\"\\nFinal test metrics (T={final_T:.4f}):\")\n","    print(f\"  ECE: {final_ece:.4f} (orig:{uncalibrated_results['InceptionTime']['ece']:.4f})\")\n","    print(f\"  Brier: {final_brier:.4f} (orig:{uncalibrated_results['InceptionTime']['brier']:.4f})\")\n","    print(f\"  NLL: {final_nll:.4f} (orig:{uncalibrated_results['InceptionTime']['nll']:.4f})\")\n","\n","    if not accept_calibration:\n","        print(f\"\\nWhat-if metrics (if applying T={optimal_temp:.4f}):\")\n","        print(f\"  ECE: {trial_ece:.4f}, Brier: {trial_brier:.4f}, NLL: {trial_nll:.4f}\")\n","        print(f\"  (for post-hoc analysis only; not used for any decision)\")\n","\n","    calibrated_results['InceptionTime'] = {\n","        'ece': float(final_ece),\n","        'brier': float(final_brier),\n","        'nll': float(final_nll),\n","        'temperature': float(optimal_temp),\n","        'final_temperature': float(final_T),\n","        'gate': {\n","            'source': 'val',\n","            'val_ece_uncal': float(val_ece_uncal),\n","            'val_ece_cal': float(val_ece_cal),\n","            'val_nll_uncal': float(val_nll_uncal),\n","            'val_nll_cal': float(val_nll_cal),\n","            'rule': 'val_nll<=0.99*uncal && val_ece<=uncal+0.01',\n","            'accepted': bool(accept_calibration)\n","        },\n","        'test_trial_if_applied': {\n","            'ece': float(trial_ece),\n","            'brier': float(trial_brier),\n","            'nll': float(trial_nll)\n","        },\n","        'y_true': test_y_true,\n","        'y_proba': final_proba,\n","        'val_subjects': val_subjects,\n","        'n_val_subjects': n_val_subjects,\n","        'n_val_windows': len(X_val),\n","        'n_cal_train_windows': len(X_train_cal),\n","        'base_model_consistent': base_model_consistent\n","    }\n","\n","# ========== Plot Reliability Diagram ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"5. Plot Reliability Diagram\")\n","print(\"=\" * 60)\n","\n","n_models = len(uncalibrated_results)\n","n_cols = min(3, n_models)\n","n_rows = (n_models + n_cols - 1) // n_cols\n","\n","fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))\n","if n_models == 1:\n","    axes = np.array([axes])\n","axes = axes.flatten()\n","\n","reliability_data = {}\n","\n","for idx, (model_name, results) in enumerate(uncalibrated_results.items()):\n","    ax = axes[idx]\n","\n","    bin_centers, bin_accuracies, bin_confidences, bin_counts = reliability_diagram_data(\n","        results['y_true'], results['y_proba'], N_BINS\n","    )\n","\n","    reliability_data[model_name] = {\n","        'uncalibrated': {\n","            'bin_centers': [float(x) for x in bin_centers],\n","            'bin_accuracies': [float(x) for x in bin_accuracies],\n","            'bin_confidences': [float(x) for x in bin_confidences],\n","            'bin_counts': bin_counts\n","        }\n","    }\n","\n","    ax.bar(bin_centers, bin_accuracies, width=1/N_BINS*0.8,\n","           alpha=0.6, label='Uncalibrated', color='skyblue', edgecolor='black')\n","\n","    if model_name in calibrated_results:\n","        cal_results = calibrated_results[model_name]\n","        bin_centers_cal, bin_accuracies_cal, bin_confidences_cal, bin_counts_cal = reliability_diagram_data(\n","            cal_results['y_true'], cal_results['y_proba'], N_BINS\n","        )\n","        reliability_data[model_name]['calibrated'] = {\n","            'bin_centers': [float(x) for x in bin_centers_cal],\n","            'bin_accuracies': [float(x) for x in bin_accuracies_cal],\n","            'bin_confidences': [float(x) for x in bin_confidences_cal],\n","            'bin_counts': bin_counts_cal\n","        }\n","        ax.bar(bin_centers_cal, bin_accuracies_cal, width=1/N_BINS*0.8,\n","               alpha=0.6, label='Calibrated', color='coral', edgecolor='black')\n","\n","    ax.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect calibration')\n","    ax.set_xlabel('Confidence')\n","    ax.set_ylabel('Accuracy')\n","    ax.set_xlim(0, 1)\n","    ax.set_ylim(0, 1)\n","    ax.set_title(f'{model_name}\\nECE={results[\"ece\"]:.4f}')\n","    ax.legend(loc='upper left')\n","    ax.grid(alpha=0.3)\n","\n","for idx in range(n_models, len(axes)):\n","    axes[idx].axis('off')\n","\n","plt.tight_layout()\n","diagram_file = calib_fold_dir / \"reliability_diagram.png\"\n","plt.savefig(diagram_file, dpi=150, bbox_inches='tight')\n","plt.close()\n","\n","print(f\"✓ Saved: {diagram_file}\")\n","\n","# ========== Save results ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"6. Save Results\")\n","print(\"=\" * 60)\n","\n","results_to_save = {\n","    'fold_id': FOLD_ID,\n","    'n_bins': N_BINS,\n","    'n_classes': N_CLASSES,\n","    'class_list': CLASS_LIST,\n","    'uncalibrated': {},\n","    'calibrated': {},\n","    'reliability_data': reliability_data,\n","    'notes': [\n","        'ECE: Expected Calibration Error (smaller is better)',\n","        'Brier Score: mean squared error (smaller is better)',\n","        'NLL: Negative Log-Likelihood (smaller is better)',\n","        'Temperature scaling: retrain model excluding validation subjects (eliminate leakage)',\n","        'Validation set ≥ 2 subjects; coarse-grid warm-start to avoid local minima',\n","        f'Reliability diagram: {N_BINS} bins (truly left-closed, right-open)',\n","        '✓ No data leakage: calibration model never sees validation samples',\n","        '✓ Gate strategy based on validation set: val_nll≤0.99*uncal && val_ece≤uncal+0.01',\n","        '✓ Test set is only reported and never used for any decision'\n","    ]\n","}\n","\n","results_to_save['bin_edges'] = np.linspace(0, 1, N_BINS + 1).tolist()\n","\n","if 'InceptionTime' in calibrated_results:\n","    if calibrated_results['InceptionTime']['base_model_consistent']:\n","        results_to_save['notes'].append('✓ Base-model consistent: calibration model produced both validation and test logits')\n","    else:\n","        results_to_save['notes'].append('⚠️ Base-model inconsistent: test logits came from final model (disclosed)')\n","\n","    if calibrated_results['InceptionTime']['gate']['accepted']:\n","        results_to_save['notes'].append(f\"✓ Calibration accepted: final T={calibrated_results['InceptionTime']['final_temperature']:.4f}\")\n","    else:\n","        results_to_save['notes'].append(f\"⚠️ Calibration rejected: fallback T=1.0 (validation-optimized T={calibrated_results['InceptionTime']['temperature']:.4f})\")\n","\n","for model_name, results in uncalibrated_results.items():\n","    results_to_save['uncalibrated'][model_name] = {\n","        'ece': results['ece'],\n","        'brier': results['brier'],\n","        'nll': results['nll']\n","    }\n","\n","for model_name, results in calibrated_results.items():\n","    results_to_save['calibrated'][model_name] = {\n","        'ece': results['ece'],\n","        'brier': results['brier'],\n","        'nll': results['nll'],\n","        'temperature': results['temperature'],\n","        'final_temperature': results['final_temperature'],\n","        'gate': results['gate'],\n","        'test_trial_if_applied': results['test_trial_if_applied'],\n","        'val_subjects': results['val_subjects'],\n","        'n_val_subjects': results['n_val_subjects'],\n","        'n_val_windows': results['n_val_windows'],\n","        'n_cal_train_windows': results['n_cal_train_windows'],\n","        'base_model_consistent': results['base_model_consistent']\n","    }\n","\n","results_file = calib_fold_dir / f\"calibration_{fold_tag}.json\"\n","with open(results_file, 'w') as f:\n","    json.dump(results_to_save, f, indent=2)\n","\n","print(f\"✓ Saved: {results_file}\")\n","\n","# ========== Summary ==========\n","print(\"\\n\" + \"=\" * 60)\n","print(\"Step 18 complete — Probability Calibration (Full Version)\")\n","print(\"=\" * 60)\n","\n","print(f\"\\nCurrent fold: FOLD_ID={FOLD_ID}, #classes: {N_CLASSES}\")\n","print(f\"\\nUncalibrated metrics:\")\n","print(f\"{'Model':<15} {'ECE':<10} {'Brier':<10} {'NLL':<10}\")\n","print(\"-\" * 45)\n","for model_name, results in uncalibrated_results.items():\n","    print(f\"{model_name:<15} {results['ece']:<10.4f} {results['brier']:<10.4f} {results['nll']:<10.4f}\")\n","\n","if calibrated_results:\n","    print(f\"\\nPost-calibration metrics (InceptionTime):\")\n","    print(f\"{'Metric':<15} {'Uncal':<10} {'Calib':<10} {'Improvement':<10}\")\n","    print(\"-\" * 45)\n","    uncal = uncalibrated_results['InceptionTime']\n","    cal = calibrated_results['InceptionTime']\n","    for metric in ['ece', 'brier', 'nll']:\n","        base_val = float(uncal[metric])\n","        after_val = float(cal[metric])\n","        if base_val == 0:\n","            improvement = \"—\"\n","        else:\n","            improvement = f\"{(base_val - after_val) / base_val * 100:>9.1f}%\"\n","        print(f\"{metric.upper():<15} {base_val:<10.4f} {after_val:<10.4f} {improvement}\")\n","\n","    print(f\"\\nTemperature:\")\n","    print(f\"  Val-optimized T: {cal['temperature']:.4f}\")\n","    print(f\"  Final applied T: {cal['final_temperature']:.4f}\")\n","\n","    gate = cal['gate']\n","    print(f\"\\nGate (based on validation set):\")\n","    print(f\"  Val ECE: {gate['val_ece_uncal']:.4f} → {gate['val_ece_cal']:.4f}\")\n","    print(f\"  Val NLL: {gate['val_nll_uncal']:.4f} → {gate['val_nll_cal']:.4f}\")\n","    print(f\"  Rule: {gate['rule']}\")\n","    print(f\"  Decision: {'✓ accept calibration' if gate['accepted'] else '⚠️ reject calibration (fallback T=1.0)'}\")\n","\n","    if not gate['accepted']:\n","        trial = cal['test_trial_if_applied']\n","        print(f\"\\nWhat-if metrics (if applying T={cal['temperature']:.4f}):\")\n","        print(f\"  Test ECE: {trial['ece']:.4f}, Brier: {trial['brier']:.4f}, NLL: {trial['nll']:.4f}\")\n","        print(f\"  (for post-hoc analysis only; not used for any decision)\")\n","\n","    print(f\"\\nValidation set: {cal['n_val_subjects']} subjects / {cal['n_val_windows']} windows (no leakage)\")\n","    print(f\"Calibration train set: {cal['n_cal_train_windows']} windows (validation subjects excluded)\")\n","    print(f\"Base-model consistency: {'✓ Yes' if cal['base_model_consistent'] else '⚠️ No (fallback to original predictions)'}\")\n","\n","print(f\"\\n✓ All critical fixes applied:\")\n","print(f\"  1. Validation set ≥2 subjects without emptying train set (boundary protection)\")\n","print(f\"  2. Retrain model excluding validation subjects (eliminate leakage)\")\n","print(f\"  3. Coarse-grid warm-start (avoid T=1.0 trap)\")\n","print(f\"  4. Truly left-closed, right-open binning\")\n","print(f\"  5. Logits validity check\")\n","print(f\"  6. Base-model consistency: same model for validation and test logits\")\n","print(f\"  7. Baseline self-consistency: uncalibrated and calibrated from the same model\")\n","print(f\"  8. Robust reporting: zero-division protection, bin_edges, consistency flags\")\n","print(f\"  9. Gate strategy on validation only (never uses test set for decisions)\")\n","\n","print(\"\\nNotes:\")\n","print(\"  - Gate decision on validation: val_nll≤0.99*uncal && val_ece≤uncal+0.01\")\n","print(\"  - Test set is only for reporting; not used for any decision\")\n","print(\"  - test_trial_if_applied: records 'what-if T applied' metrics (post-hoc only)\")\n","print(\"  - Future improvement: use Step 14 OOF logits to fit temperature (more stable)\")\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qKnuTSBD0GhJ","executionInfo":{"status":"ok","timestamp":1763118023945,"user_tz":0,"elapsed":5489,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"3f078738-5117-4b94-f965-1f8f632c0e4b"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Step 18: Probability Calibration (Full Version)\n","============================================================\n","\n","Current fold: FOLD_ID=3, ECE bins: 15\n","\n","#classes: 6, classes: [1, 2, 3, 4, 5, 6]\n","\n","============================================================\n","3. Load test predictions and compute uncalibrated metrics\n","============================================================\n","\n","Processing RF...\n","  ECE: 0.0492, Brier: 0.5589, NLL: 1.1236\n","\n","Processing KNN...\n","  ECE: 0.1448, Brier: 0.6482, NLL: 7.6340\n","\n","Processing InceptionTime...\n","  ECE: 0.2992, Brier: 0.6828, NLL: 2.3626\n","\n","============================================================\n","4. InceptionTime Temperature Scaling (No-Leakage Version)\n","============================================================\n","\n","Checking logits validity...\n","✓ Logits check passed (softmax(logits) ≈ proba_class_*)\n","\n","Prepare validation set (no leakage: retrain model excluding validation subjects)...\n","Validation set: 1425 windows, 2 subjects\n","Calibration train set: 3433 windows (excluding validation subjects)\n","\n","Retrain calibration model (excluding validation subjects)...\n","✓ Calibration model training complete\n","\n","Infer on validation set...\n","Validation logits: (1425, 6)\n","\n","Search best temperature (coarse grid warm-start)...\n","Best temperature: 1.9037\n","Validation NLL: 1.3395 → 1.0556 (improvement 21.2%)\n","Validation ECE: 0.1866 → 0.0651\n","\n","Gate decision based on validation set (no test usage)...\n","Gate rule: val_nll≤0.99*uncal && val_ece≤uncal+0.01\n","Decision: ✓ accept calibration\n","\n","Recompute test logits with calibration model (ensure base-model consistency between fitted T and applied T)...\n","✓ Generated test logits with calibration model: (873, 6) (same base model as for T fitting)\n","✓ Updated uncalibrated baseline (same model): ECE=0.2620, Brier=0.6573, NLL=1.6016\n","\n","Apply final temperature on test set (T=1.9037)...\n","\n","Final test metrics (T=1.9037):\n","  ECE: 0.1082 (orig:0.2620)\n","  Brier: 0.5799 (orig:0.6573)\n","  NLL: 1.1768 (orig:1.6016)\n","\n","============================================================\n","5. Plot Reliability Diagram\n","============================================================\n","✓ Saved: calibration/fold_03/reliability_diagram.png\n","\n","============================================================\n","6. Save Results\n","============================================================\n","✓ Saved: calibration/fold_03/calibration_fold_03.json\n","\n","============================================================\n","Step 18 complete — Probability Calibration (Full Version)\n","============================================================\n","\n","Current fold: FOLD_ID=3, #classes: 6\n","\n","Uncalibrated metrics:\n","Model           ECE        Brier      NLL       \n","---------------------------------------------\n","RF              0.0492     0.5589     1.1236    \n","KNN             0.1448     0.6482     7.6340    \n","InceptionTime   0.2620     0.6573     1.6016    \n","\n","Post-calibration metrics (InceptionTime):\n","Metric          Uncal      Calib      Improvement\n","---------------------------------------------\n","ECE             0.2620     0.1082          58.7%\n","BRIER           0.6573     0.5799          11.8%\n","NLL             1.6016     1.1768          26.5%\n","\n","Temperature:\n","  Val-optimized T: 1.9037\n","  Final applied T: 1.9037\n","\n","Gate (based on validation set):\n","  Val ECE: 0.1866 → 0.0651\n","  Val NLL: 1.3395 → 1.0556\n","  Rule: val_nll<=0.99*uncal && val_ece<=uncal+0.01\n","  Decision: ✓ accept calibration\n","\n","Validation set: 2 subjects / 1425 windows (no leakage)\n","Calibration train set: 3433 windows (validation subjects excluded)\n","Base-model consistency: ✓ Yes\n","\n","✓ All critical fixes applied:\n","  1. Validation set ≥2 subjects without emptying train set (boundary protection)\n","  2. Retrain model excluding validation subjects (eliminate leakage)\n","  3. Coarse-grid warm-start (avoid T=1.0 trap)\n","  4. Truly left-closed, right-open binning\n","  5. Logits validity check\n","  6. Base-model consistency: same model for validation and test logits\n","  7. Baseline self-consistency: uncalibrated and calibrated from the same model\n","  8. Robust reporting: zero-division protection, bin_edges, consistency flags\n","  9. Gate strategy on validation only (never uses test set for decisions)\n","\n","Notes:\n","  - Gate decision on validation: val_nll≤0.99*uncal && val_ece≤uncal+0.01\n","  - Test set is only for reporting; not used for any decision\n","  - test_trial_if_applied: records 'what-if T applied' metrics (post-hoc only)\n","  - Future improvement: use Step 14 OOF logits to fit temperature (more stable)\n","============================================================\n"]}]}]}