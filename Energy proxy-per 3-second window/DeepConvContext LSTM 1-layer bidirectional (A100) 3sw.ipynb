{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOHAC7ugAen2OZRBMy5ZzZo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# ================================================================\n","# DeepConvContext (official architecture-aligned, A100 v2.1) × Scheme 1:\n","# NVML GPU inference energy (mJ per 3-second window) on Google Colab — single cell, no prereqs\n","# ================================================================\n","# 0) System check & deps\n","!nvidia-smi\n","!pip -q install pynvml\n","\n","import os, json, time, math, pathlib, warnings, multiprocessing as mp\n","from pathlib import Path\n","warnings.filterwarnings(\"ignore\")\n","\n","import numpy as np\n","import pandas as pd\n","\n","# ---------------- NVML sampling + trapezoidal energy integration ----------------\n","import pynvml\n","\n","def _nvml_sampler(stop_event, q, dev_index=0, interval=0.02):\n","    \"\"\"Background process: sample GPU power (mW) every `interval` seconds, push (t, mW).\"\"\"\n","    import time, pynvml\n","    pynvml.nvmlInit()\n","    h = pynvml.nvmlDeviceGetHandleByIndex(dev_index)\n","    try:\n","        while not stop_event.is_set():\n","            q.put((time.perf_counter(), pynvml.nvmlDeviceGetPowerUsage(h)))\n","            time.sleep(interval)\n","    finally:\n","        pynvml.nvmlShutdown()\n","\n","def _integrate_mJ_between(samples, t0, t1):\n","    \"\"\"Trapezoidal integrate power (mW) over [t0, t1] → mJ.\"\"\"\n","    if not samples: return 0.0\n","    samples = sorted(samples, key=lambda x: x[0])\n","    ts = np.array([t for t,_ in samples], dtype=np.float64)\n","    ps = np.array([p for _,p in samples], dtype=np.float64)\n","    mask = (ts >= t0) & (ts <= t1)\n","    ts_w, ps_w = ts[mask], ps[mask]\n","    if ts_w.size == 0 or ts_w[0] > t0:\n","        p0 = np.interp(t0, ts, ps); ts_w = np.insert(ts_w, 0, t0); ps_w = np.insert(ps_w, 0, p0)\n","    if ts_w[-1] < t1:\n","        p1 = np.interp(t1, ts, ps); ts_w = np.append(ts_w, t1); ps_w = np.append(ps_w, p1)\n","    return float(np.trapz(ps_w, ts_w))  # mW*s = mJ\n","\n","def sample_idle_power_mW(duration_s=20.0, dev_index=0, interval=0.02, save_csv=None):\n","    \"\"\"Measure average idle power (mW) for `duration_s` seconds.\"\"\"\n","    q = mp.Queue(); stop = mp.Event()\n","    p = mp.Process(target=_nvml_sampler, args=(stop, q, dev_index, interval)); p.start()\n","    time.sleep(duration_s)\n","    stop.set(); p.join()\n","    samples = []\n","    while not q.empty(): samples.append(q.get())\n","    if not samples: raise RuntimeError(\"No NVML samples during idle.\")\n","    samples.sort(key=lambda x: x[0])\n","    t0, t1 = samples[0][0], samples[-1][0]\n","    E_idle_mJ = _integrate_mJ_between(samples, t0, t1)\n","    T_idle_s  = max(1e-9, t1 - t0)\n","    P_idle_mW = E_idle_mJ / T_idle_s\n","    if save_csv:\n","        pd.DataFrame(samples, columns=[\"t_abs_s\",\"power_mW\"]).to_csv(save_csv, index=False)\n","    return P_idle_mW, samples\n","\n","def measure_mJ_per_window(run_once, n_windows_per_call, repeats, P_idle_mW,\n","                          dev_index=0, interval=0.02, save_csv=None):\n","    \"\"\"Concurrent NVML sampling + integration + idle subtraction → energy/latency per window.\"\"\"\n","    q = mp.Queue(); stop = mp.Event()\n","    p = mp.Process(target=_nvml_sampler, args=(stop, q, dev_index, interval)); p.start()\n","    t0 = time.perf_counter()\n","    for _ in range(repeats):\n","        run_once()\n","    t1 = time.perf_counter()\n","    stop.set(); p.join()\n","    samples = []\n","    while not q.empty(): samples.append(q.get())\n","    if not samples: raise RuntimeError(\"No NVML samples during active measurement.\")\n","    E_total_mJ = _integrate_mJ_between(samples, t0, t1)\n","    T_total_s  = max(1e-9, t1 - t0)\n","    E_idle_mJ  = P_idle_mW * T_total_s\n","    n_windows  = max(1, repeats * n_windows_per_call)\n","    mJ_per_window = max(0.0, (E_total_mJ - E_idle_mJ) / n_windows)\n","    ms_per_window = (T_total_s / n_windows) * 1e3\n","    if save_csv:\n","        pd.DataFrame(samples, columns=[\"t_abs_s\",\"power_mW\"]).to_csv(save_csv, index=False)\n","    return {\n","        \"mJ_per_window\": mJ_per_window,\n","        \"ms_per_window\": ms_per_window,\n","        \"throughput_windows_per_s\": n_windows / T_total_s,\n","        \"n_windows\": n_windows,\n","        \"repeats\": repeats,\n","        \"T_total_s\": T_total_s,\n","        \"E_total_mJ\": E_total_mJ,\n","        \"E_idle_mJ\": E_idle_mJ,\n","        \"P_idle_mW\": P_idle_mW,\n","        \"t0_abs\": t0, \"t1_abs\": t1\n","    }\n","\n","def calibrate_repeats(run_once, target_s=8.0, min_rep=3, max_rep=5000):\n","    \"\"\"Estimate repeats so one measurement window lasts ~target_s seconds.\"\"\"\n","    run_once()\n","    t0 = time.perf_counter(); run_once(); t1 = time.perf_counter()\n","    dt = max(1e-4, t1 - t0)\n","    reps = int(np.ceil(target_s / dt))\n","    return int(np.clip(reps, min_rep, max_rep))\n","\n","def measure_with_bootstrap(name, run_once, n_windows, repeats, n_runs=5, n_boot=1000, logdir=Path(\"logs\")):\n","    \"\"\"Repeat n_runs, bootstrap the per-window mean with 95% CI; save traces and summary.\"\"\"\n","    logdir.mkdir(exist_ok=True, parents=True)\n","    runs = []\n","    for i in range(n_runs):\n","        print(f\"[Measure] {name} run {i+1}/{n_runs} ...\")\n","        r = measure_mJ_per_window(\n","            run_once, n_windows, repeats, P_idle_mW,\n","            dev_index=0, interval=0.02,\n","            save_csv=str(logdir/f\"power_trace_{name}_run{i+1}.csv\")\n","        )\n","        runs.append(r)\n","    mJ = np.array([r[\"mJ_per_window\"] for r in runs], dtype=np.float64)\n","    ms = np.array([r[\"ms_per_window\"] for r in runs], dtype=np.float64)\n","    rng = np.random.default_rng(123)\n","    boots_mJ = [float(np.mean(mJ[rng.integers(0, len(mJ), size=len(mJ))])) for _ in range(n_boot)]\n","    ci_low_mJ, ci_high_mJ = np.percentile(boots_mJ, [2.5, 97.5])\n","    summary = {\n","        \"model\": name,\n","        \"mean_mJ_per_window\": float(mJ.mean()),\n","        \"ci95_low_mJ\": float(ci_low_mJ),\n","        \"ci95_high_mJ\": float(ci_high_mJ),\n","        \"mean_ms_per_window\": float(ms.mean()),\n","        \"runs\": runs\n","    }\n","    with open(logdir/f\"energy_{name}.json\", \"w\") as f: json.dump(summary, f, indent=2)\n","    print(f\"[Result] {name}: {summary['mean_mJ_per_window']:.3f} mJ per window \"\n","          f\"(95% CI [{summary['ci95_low_mJ']:.3f}, {summary['ci95_high_mJ']:.3f}]); \"\n","          f\"{summary['mean_ms_per_window']:.3f} ms per window\")\n","    return summary\n","\n","# ---------------- Step 10 model (official architecture-aligned, A100 v2.1) — structure/params unchanged ----------------\n","# Environment variables that MUST be set before importing torch\n","os.environ[\"TORCHINDUCTOR_DISABLE_CUDA_GRAPHS\"] = \"1\"\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","os.environ.setdefault(\"OMP_NUM_THREADS\", \"16\")\n","os.environ.setdefault(\"MKL_NUM_THREADS\", \"16\")\n","os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"16\")\n","os.environ.setdefault(\"NUMEXPR_MAX_THREADS\", \"16\")\n","\n","import warnings as _warnings; _warnings.filterwarnings(\"ignore\", message=r\".*CUDA Graph is empty.*\")\n","\n","import json as _json_shadow\n","import numpy as _np_shadow\n","from pathlib import Path as _Path_shadow\n","from typing import List, Tuple\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch import amp\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Global A100 acceleration switches (unaltered)\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","torch.backends.cudnn.benchmark = True\n","torch.set_float32_matmul_precision('high')\n","\n","print(\"\\n\\nStep 10: DeepConvContext (official architecture-aligned, A100 80GB high-throughput v2.1)\")\n","print(\"=\" * 78)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Device: {device}\")\n","\n","# Configs (create minimal defaults if missing; does not change model structure/params)\n","CFG_DIR = Path('/content/configs'); CFG_DIR.mkdir(parents=True, exist_ok=True)\n","classes_json = CFG_DIR / 'classes.json'\n","splits_json  = CFG_DIR / 'splits.json'\n","if not classes_json.exists():\n","    classes_json.write_text(json.dumps({\"num_classes\": 8, \"standard_classes\": [f\"class{i}\" for i in range(8)]}, indent=2))\n","if not splits_json.exists():\n","    splits_json.write_text(json.dumps({\"folds\": [{\"fold\": 0, \"test_subject\": \"S01\"}]}, indent=2))\n","\n","with open(CFG_DIR / 'classes.json', 'r') as f:\n","    classes_cfg = json.load(f)\n","with open(CFG_DIR / 'splits.json', 'r') as f:\n","    splits_cfg = json.load(f)\n","\n","NUM_CLASSES    = int(classes_cfg['num_classes'])\n","STANDARD_NAMES = classes_cfg.get('standard_classes', [str(i) for i in range(NUM_CLASSES)])\n","\n","# Windowing & sequence settings (unaltered)\n","NUM_CHANNELS      = 6\n","SAMPLES_PER_WIN   = 150  # 3 s @ 50 Hz\n","STRIDE_SAMPLES    = 75\n","CONTEXT_LEN_WINS  = 100\n","WINDOW_SECONDS    = 3.0  # reporting only\n","\n","# Hyperparameters (unaltered)\n","EPOCHS        = 30\n","BASE_LR       = 1e-4\n","WEIGHT_DECAY  = 1e-6\n","DROPOUT_P     = 0.5\n","BIDIRECTIONAL = True\n","\n","USE_AMP     = True\n","AMP_DTYPE   = torch.float16\n","TRAIN_BATCH = 256\n","EVAL_BATCH  = 256\n","LEARNING_RATE = BASE_LR * (TRAIN_BATCH / 100.0)\n","\n","CPU = os.cpu_count() or 16\n","WORKERS  = min(32, max(8, CPU // 2))\n","PREFETCH = 8\n","PIN_DEV  = \"cuda\"\n","\n","print(f\"\\nConfig: num_classes={NUM_CLASSES}, channels={NUM_CHANNELS}, samples_per_window=150, sequence_length={CONTEXT_LEN_WINS}\")\n","print(f\"batch={EVAL_BATCH}, lr(base)={BASE_LR}, wd={WEIGHT_DECAY}, AMP(FP16)={USE_AMP}, TF32=on\\n\")\n","\n","# Official architecture (unaltered)\n","class DeepConvLSTM_Intra(nn.Module):\n","    def __init__(self, in_ch=6, conv_ch=64, kernel_size=9, lstm_units=128):\n","        super().__init__()\n","        pad = kernel_size // 2\n","        self.conv1 = nn.Conv1d(in_ch,   conv_ch, kernel_size, padding=pad)\n","        self.conv2 = nn.Conv1d(conv_ch, conv_ch, kernel_size, padding=pad)\n","        self.conv3 = nn.Conv1d(conv_ch, conv_ch, kernel_size, padding=pad)\n","        self.conv4 = nn.Conv1d(conv_ch, conv_ch, kernel_size, padding=pad)\n","        self.relu  = nn.ReLU(inplace=True)\n","        self.lstm  = nn.LSTM(input_size=conv_ch, hidden_size=lstm_units, num_layers=1, batch_first=True)\n","    def forward(self, x_win):\n","        x = self.relu(self.conv1(x_win))\n","        x = self.relu(self.conv2(x))\n","        x = self.relu(self.conv3(x))\n","        x = self.relu(self.conv4(x))\n","        x = x.permute(0, 2, 1)\n","        _, (h_n, _) = self.lstm(x)\n","        return h_n[-1]\n","\n","class DeepConvContext(nn.Module):\n","    def __init__(self, num_channels=6, num_classes=8,\n","                 conv_channels=64, intra_lstm_units=128,\n","                 inter_lstm_units=128, projection_dim=128,\n","                 dropout=0.5, bidirectional=True):\n","        super().__init__()\n","        self.intra = DeepConvLSTM_Intra(num_channels, conv_channels, 9, intra_lstm_units)\n","        self.proj  = nn.Linear(intra_lstm_units, projection_dim)\n","        self.inter = nn.LSTM(input_size=projection_dim, hidden_size=inter_lstm_units,\n","                             num_layers=1, batch_first=True, bidirectional=bidirectional)\n","        inter_out = inter_lstm_units * (2 if bidirectional else 1)\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(inter_out, num_classes)\n","    def forward(self, x):\n","        B, S, C, T = x.shape\n","        x2d = x.reshape(B*S, C, T)\n","        feats = self.intra(x2d).view(B, S, -1)\n","        proj  = self.proj(feats)\n","        inter_out, _ = self.inter(proj)\n","        inter_out = self.dropout(inter_out)\n","        logits = self.fc(inter_out)\n","        return logits\n","\n","# Dataset (unaltered logic)\n","class HARSequenceDataset(Dataset):\n","    def __init__(self, npz_file: Path, split='train', sequence_length=100):\n","        data = np.load(npz_file, allow_pickle=True)\n","        mask = data['splits'] == split\n","        labels = data['labels'][mask]\n","        subjects = data['subjects'][mask]\n","        channels = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']\n","        wins = np.stack([data[ch][mask] for ch in channels], axis=1).astype(np.float32)  # (N,C,T)\n","        self.sequences, self.seq_labels = [], []\n","        for subj in np.unique(subjects):\n","            m = subjects == subj\n","            w_subj, y_subj = wins[m], labels[m]\n","            for i in range(0, len(w_subj) - sequence_length + 1):\n","                self.sequences.append(w_subj[i:i+sequence_length])   # (S,C,T)\n","                self.seq_labels.append(y_subj[i:i+sequence_length])  # (S,)\n","        self.sequences = np.asarray(self.sequences)\n","        self.seq_labels = np.asarray(self.seq_labels)\n","    def __len__(self): return len(self.seq_labels)\n","    def __getitem__(self, idx):\n","        return (torch.from_numpy(self.sequences[idx]),\n","                torch.from_numpy(self.seq_labels[idx]).long())\n","\n","# Minimal feature bootstrap if missing (does not change model structure/params)\n","features_dir = Path('/content/features'); features_dir.mkdir(parents=True, exist_ok=True)\n","npz_path = features_dir / 'windows_normalized_fold0.npz'\n","if not npz_path.exists():\n","    N_train, N_test, T, C, K = 2000, 800, 150, 6, NUM_CLASSES\n","    rng = np.random.default_rng(2025)\n","    def make_axis(N): return rng.normal(0, 1, size=(N, T)).astype(np.float32)\n","    out = {\n","        'acc_x': make_axis(N_train+N_test), 'acc_y': make_axis(N_train+N_test), 'acc_z': make_axis(N_train+N_test),\n","        'gyro_x': make_axis(N_train+N_test), 'gyro_y': make_axis(N_train+N_test), 'gyro_z': make_axis(N_train+N_test),\n","        'labels': np.concatenate([rng.integers(0, K, size=N_train), rng.integers(0, K, size=N_test)]).astype(np.int64),\n","        'subjects': np.array([b'S01']*(N_train+N_test)),\n","        'splits': np.array(['train']*N_train + ['test']*N_test)\n","    }\n","    np.savez(npz_path, **out)\n","\n","# Build test loader (matching Step 10 batch size); fallback if some kwargs unsupported\n","test_ds  = HARSequenceDataset(npz_path, split='test', sequence_length=CONTEXT_LEN_WINS)\n","try:\n","    test_loader  = DataLoader(\n","        test_ds,\n","        batch_size=EVAL_BATCH,\n","        shuffle=False,\n","        drop_last=False,\n","        num_workers=WORKERS,\n","        pin_memory=True,\n","        pin_memory_device=PIN_DEV,\n","        persistent_workers=True,\n","        prefetch_factor=PREFETCH\n","    )\n","except TypeError:\n","    test_loader  = DataLoader(\n","        test_ds,\n","        batch_size=EVAL_BATCH,\n","        shuffle=False,\n","        drop_last=False,\n","        num_workers=WORKERS,\n","        pin_memory=True\n","    )\n","\n","# Instantiate model with exact hyperparameters; eval mode for inference\n","model = DeepConvContext(\n","    num_channels=NUM_CHANNELS, num_classes=NUM_CLASSES,\n","    conv_channels=64, intra_lstm_units=128,\n","    inter_lstm_units=128, projection_dim=128,\n","    dropout=DROPOUT_P, bidirectional=BIDIRECTIONAL\n",").to(device).eval()\n","\n","# Optional: torch.compile for max-autotune (no change to structure/params)\n","try:\n","    model = torch.compile(model, mode=\"max-autotune\", fullgraph=False, dynamic=False)\n","    print(\"✓ Enabled torch.compile(max-autotune)\")\n","except Exception as e:\n","    print(f\"⚠ torch.compile unavailable, continuing without: {e}\")\n","\n","# Optional: load trained weights if present (does not affect energy/FLOPs)\n","models_dir = Path('/content/models'); models_dir.mkdir(parents=True, exist_ok=True)\n","wpath = models_dir / 'deepconvcontext_fold0.pth'\n","if wpath.exists():\n","    try:\n","        sd = torch.load(wpath, map_location=device)\n","        if isinstance(sd, dict):\n","            model.load_state_dict(sd)\n","            print(f\"[Info] Loaded weights: {wpath.name}\")\n","    except Exception as e:\n","        print(f\"[Warn] Failed to load weights: {e}\")\n","\n","# Build run_once: one complete forward over the test loader (AMP dtype as in Step 10)\n","def make_runner(model: nn.Module, loader: DataLoader):\n","    n_sequences = len(loader.dataset)\n","    n_windows_per_call = n_sequences * CONTEXT_LEN_WINS  # total windows computed per full pass\n","    @torch.no_grad()\n","    def run_once():\n","        for x, _ in loader:\n","            x = x.to(device, non_blocking=True)  # (B,S,C,T)\n","            with amp.autocast('cuda', dtype=AMP_DTYPE, enabled=USE_AMP):\n","                _ = model(x)\n","        if torch.cuda.is_available():\n","            torch.cuda.synchronize()\n","    return run_once, n_windows_per_call\n","\n","run_once, N_windows_per_call = make_runner(model, test_loader)\n","\n","# Measure idle power\n","logs_dir = Path('/content/logs'); logs_dir.mkdir(parents=True, exist_ok=True)\n","print(\"\\n[Info] Sampling idle power for 20 s ...\")\n","P_idle_mW, _idle = sample_idle_power_mW(duration_s=20.0, dev_index=0, interval=0.02,\n","                                        save_csv=str(logs_dir/'power_idle_trace_dcc_official_v21.csv'))\n","print(f\"[Info] Mean idle power ~ {P_idle_mW:.1f} mW\")\n","\n","# Warmup & calibrate repeats (target ≥ 8 s per measurement)\n","print(\"\\n[Warmup] warmup ...\")\n","run_once(); run_once()\n","repeats = calibrate_repeats(run_once, target_s=8.0, min_rep=3, max_rep=5000)\n","print(f\"[Info] repeats = {repeats} (windows per call = {N_windows_per_call})\")\n","\n","# NVML measurement + bootstrap CI (per-window metrics)\n","summary = measure_with_bootstrap(\n","    name=\"deepconvcontext_official_v21_inference_per_window\",\n","    run_once=run_once,\n","    n_windows=N_windows_per_call,\n","    repeats=repeats,\n","    n_runs=5,\n","    n_boot=1000,\n","    logdir=logs_dir\n",")\n","\n","# Save a compact CSV summary (mJ/ms per 3-second window)\n","df = pd.DataFrame([{\n","    \"model\": \"DeepConvContext (official, A100 v2.1)\",\n","    \"window_seconds\": WINDOW_SECONDS,                   # 3.0 if 150 samples @ 50 Hz\n","    \"mJ_per_3s_window_mean\": summary[\"mean_mJ_per_window\"],\n","    \"ci95_low_mJ\": summary[\"ci95_low_mJ\"],\n","    \"ci95_high_mJ\": summary[\"ci95_high_mJ\"],\n","    \"ms_per_3s_window_mean\": summary[\"mean_ms_per_window\"],\n","    \"windows_per_call\": N_windows_per_call,\n","    \"repeats\": repeats,\n","    \"idle_mW\": P_idle_mW\n","}])\n","df.to_csv(logs_dir/\"energy_summary_deepconvcontext_official_v21_per_3s_window.csv\", index=False)\n","print(\"\\n=== Done: GPU inference energy — mJ per 3-second window (Scheme 1) ===\")\n","print(df)\n","print(\"\\nArtifacts:\")\n","print(\"- logs/power_idle_trace_dcc_official_v21.csv\")\n","print(\"- logs/power_trace_deepconvcontext_official_v21_inference_per_window_run*.csv\")\n","print(\"- logs/energy_deepconvcontext_official_v21_inference_per_window.json\")\n","print(\"- logs/energy_summary_deepconvcontext_official_v21_per_3s_window.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5QyPDOGCFDCU","executionInfo":{"status":"ok","timestamp":1763404301189,"user_tz":0,"elapsed":89058,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"0b9344cb-c30b-469e-eab7-1970497a8122"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Nov 17 18:30:12 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n","| N/A   33C    P0             60W /  400W |   17855MiB /  81920MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","+-----------------------------------------------------------------------------------------+\n","\n","\n","Step 10: DeepConvContext (official architecture-aligned, A100 80GB high-throughput v2.1)\n","==============================================================================\n","Device: cuda\n","\n","Config: num_classes=8, channels=6, samples_per_window=150, sequence_length=100\n","batch=256, lr(base)=0.0001, wd=1e-06, AMP(FP16)=True, TF32=on\n","\n","✓ Enabled torch.compile(max-autotune)\n","\n","[Info] Sampling idle power for 20 s ...\n","[Info] Mean idle power ~ 60598.6 mW\n","\n","[Warmup] warmup ...\n"]},{"output_type":"stream","name":"stderr","text":["AUTOTUNE addmm(25600x128, 25600x128, 128x128)\n","strides: [0, 1], [128, 1], [1, 128]\n","dtypes: torch.float16, torch.float16, torch.float16\n","  triton_mm_15 0.0174 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=8\n","  triton_mm_17 0.0174 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4\n","  triton_mm_16 0.0184 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4\n","  triton_mm_14 0.0195 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8\n","  bias_addmm 0.0205 ms 85.0% \n","  triton_mm_13 0.0205 ms 85.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4\n","  triton_mm_18 0.0215 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8\n","  triton_mm_12 0.0236 ms 73.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4\n","  triton_mm_8 0.0246 ms 70.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4\n","  triton_mm_11 0.0246 ms 70.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4\n","SingleProcess AUTOTUNE benchmarking takes 0.3808 seconds and 1.6786 seconds precompiling for 21 choices\n","AUTOTUNE addmm(18900x128, 18900x128, 128x128)\n","strides: [0, 1], [128, 1], [1, 128]\n","dtypes: torch.float16, torch.float16, torch.float16\n","  triton_mm_32 0.0154 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4\n","  triton_mm_28 0.0164 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4\n","  triton_mm_30 0.0164 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4\n","  triton_mm_33 0.0164 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8\n","  triton_mm_35 0.0164 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4\n","  triton_mm_29 0.0174 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8\n","  triton_mm_31 0.0174 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4\n","  triton_mm_34 0.0174 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=8\n","  triton_mm_26 0.0184 ms 83.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8\n","  triton_mm_27 0.0184 ms 83.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4\n","SingleProcess AUTOTUNE benchmarking takes 0.3560 seconds and 1.2282 seconds precompiling for 21 choices\n"]},{"output_type":"stream","name":"stdout","text":["[Info] repeats = 25 (windows per call = 70100)\n","[Measure] deepconvcontext_official_v21_inference_per_window run 1/5 ...\n","[Measure] deepconvcontext_official_v21_inference_per_window run 2/5 ...\n","[Measure] deepconvcontext_official_v21_inference_per_window run 3/5 ...\n","[Measure] deepconvcontext_official_v21_inference_per_window run 4/5 ...\n","[Measure] deepconvcontext_official_v21_inference_per_window run 5/5 ...\n","[Result] deepconvcontext_official_v21_inference_per_window: 0.561 mJ per window (95% CI [0.553, 0.570]); 0.004 ms per window\n","\n","=== Done: GPU inference energy — mJ per 3-second window (Scheme 1) ===\n","                                   model  window_seconds  \\\n","0  DeepConvContext (official, A100 v2.1)             3.0   \n","\n","   mJ_per_3s_window_mean  ci95_low_mJ  ci95_high_mJ  ms_per_3s_window_mean  \\\n","0               0.561497     0.553002      0.569882               0.004416   \n","\n","   windows_per_call  repeats       idle_mW  \n","0             70100       25  60598.627309  \n","\n","Artifacts:\n","- logs/power_idle_trace_dcc_official_v21.csv\n","- logs/power_trace_deepconvcontext_official_v21_inference_per_window_run*.csv\n","- logs/energy_deepconvcontext_official_v21_inference_per_window.json\n","- logs/energy_summary_deepconvcontext_official_v21_per_3s_window.csv\n"]}]}]}