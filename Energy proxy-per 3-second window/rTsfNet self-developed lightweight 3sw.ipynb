{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOsm+OJ4V3ixkLzBHoa/3HF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# ================================================================\n","# rTsfNet (lightweight in-house version) × Option 1:\n","# NVML-based GPU inference energy — mJ per window (auto-computed window_seconds)\n","# ================================================================\n","# 0) Basic environment and dependencies\n","!nvidia-smi\n","!pip -q install pynvml\n","\n","import os, math, json, time, pathlib, gc, warnings, multiprocessing as mp\n","from pathlib import Path\n","warnings.filterwarnings(\"ignore\")\n","\n","import numpy as np\n","import pandas as pd\n","\n","# ---------------- NVML sampling & energy integration utilities (per-window reporting) ----------------\n","import pynvml\n","\n","def _nvml_sampler(stop_event, q, dev_index=0, interval=0.02):\n","    \"\"\"Subprocess: sample power (mW) every `interval` and send back (t_abs, mW).\"\"\"\n","    import time, pynvml\n","    pynvml.nvmlInit()\n","    h = pynvml.nvmlDeviceGetHandleByIndex(dev_index)\n","    try:\n","        while not stop_event.is_set():\n","            q.put((time.perf_counter(), pynvml.nvmlDeviceGetPowerUsage(h)))\n","            time.sleep(interval)\n","    finally:\n","        pynvml.nvmlShutdown()\n","\n","def _integrate_mJ_between(samples, t0, t1):\n","    \"\"\"Trapezoidal integration of power (mW) over [t0, t1], returning mJ.\"\"\"\n","    if not samples:\n","        return 0.0\n","    samples = sorted(samples, key=lambda x: x[0])\n","    ts = np.array([t for t, _ in samples], dtype=np.float64)\n","    ps = np.array([p for _, p in samples], dtype=np.float64)\n","    m = (ts >= t0) & (ts <= t1)\n","    ts_w = ts[m]; ps_w = ps[m]\n","    if ts_w.size == 0 or ts_w[0] > t0:\n","        p0 = np.interp(t0, ts, ps); ts_w = np.insert(ts_w, 0, t0); ps_w = np.insert(ps_w, 0, p0)\n","    if ts_w[-1] < t1:\n","        p1 = np.interp(t1, ts, ps); ts_w = np.append(ts_w, t1); ps_w = np.append(ps_w, p1)\n","    return float(np.trapz(ps_w, ts_w))  # mW*s = mJ\n","\n","def sample_idle_power_mW(duration_s=20.0, dev_index=0, interval=0.02, save_csv=None):\n","    \"\"\"Measure mean idle power (mW), optionally saving the power trace.\"\"\"\n","    import time\n","    q = mp.Queue(); stop = mp.Event()\n","    p = mp.Process(target=_nvml_sampler, args=(stop, q, dev_index, interval)); p.start()\n","    time.sleep(duration_s)\n","    stop.set(); p.join()\n","    samples = []\n","    while not q.empty(): samples.append(q.get())\n","    if not samples:\n","        raise RuntimeError(\"NVML did not capture any power samples (idle).\")\n","    samples.sort(key=lambda x: x[0])\n","    t0, t1 = samples[0][0], samples[-1][0]\n","    E_idle_mJ = _integrate_mJ_between(samples, t0, t1)\n","    T_idle_s = max(1e-9, t1 - t0)\n","    P_idle_mW = E_idle_mJ / T_idle_s\n","    if save_csv:\n","        pd.DataFrame(samples, columns=[\"t_abs_s\", \"power_mW\"]).to_csv(save_csv, index=False)\n","    return P_idle_mW, samples\n","\n","def measure_mJ_per_window(run_once, n_windows_per_call, repeats, P_idle_mW,\n","                          dev_index=0, interval=0.02, save_csv=None):\n","    \"\"\"Concurrent NVML sampling + integration + idle subtraction; return per-window energy & latency.\"\"\"\n","    import time\n","    q = mp.Queue(); stop = mp.Event()\n","    p = mp.Process(target=_nvml_sampler, args=(stop, q, dev_index, interval)); p.start()\n","\n","    # Measurement window\n","    t0 = time.perf_counter()\n","    for _ in range(repeats):\n","        run_once()\n","    t1 = time.perf_counter()\n","\n","    stop.set(); p.join()\n","    samples = []\n","    while not q.empty(): samples.append(q.get())\n","    if not samples:\n","        raise RuntimeError(\"NVML did not capture any power samples (measurement).\")\n","\n","    E_total_mJ = _integrate_mJ_between(samples, t0, t1)\n","    T_total_s  = max(1e-9, t1 - t0)\n","    E_idle_mJ  = P_idle_mW * T_total_s\n","    n_windows  = max(1, repeats * n_windows_per_call)\n","\n","    if save_csv:\n","        pd.DataFrame(samples, columns=[\"t_abs_s\", \"power_mW\"]).to_csv(save_csv, index=False)\n","\n","    return {\n","        \"mJ_per_window\": max(0.0, (E_total_mJ - E_idle_mJ) / n_windows),\n","        \"ms_per_window\": (T_total_s / n_windows) * 1e3,\n","        \"throughput_windows_per_s\": n_windows / T_total_s,\n","        \"n_windows\": n_windows,\n","        \"repeats\": repeats,\n","        \"T_total_s\": T_total_s,\n","        \"E_total_mJ\": E_total_mJ,\n","        \"E_idle_mJ\": E_idle_mJ,\n","        \"P_idle_mW\": P_idle_mW,\n","        \"t0_abs\": t0, \"t1_abs\": t1,\n","    }\n","\n","def calibrate_repeats(run_once, target_s=8.0, min_rep=3, max_rep=5000):\n","    \"\"\"Estimate repeats so that one measurement lasts ≈ target_s to reduce sampling noise.\"\"\"\n","    import time\n","    run_once()\n","    t0 = time.perf_counter(); run_once(); t1 = time.perf_counter()\n","    dt = max(1e-4, t1 - t0)\n","    reps = int(np.ceil(target_s / dt))\n","    return int(np.clip(reps, min_rep, max_rep))\n","\n","def measure_with_bootstrap(name, run_once, n_windows, repeats, n_runs=5, n_boot=1000, logdir=Path(\"logs\")):\n","    \"\"\"Repeat n_runs, compute bootstrap 95% CI; save traces and a per-window summary JSON.\"\"\"\n","    logdir.mkdir(exist_ok=True)\n","    res_list = []\n","    for i in range(n_runs):\n","        print(f\"[Measure] {name} run {i+1}/{n_runs} ...\")\n","        r = measure_mJ_per_window(\n","            run_once, n_windows, repeats,\n","            P_idle_mW=P_idle_mW, dev_index=0, interval=0.02,\n","            save_csv=str(logdir / f\"power_trace_{name}_run{i+1}.csv\")\n","        )\n","        res_list.append(r)\n","\n","    mJ = np.array([r[\"mJ_per_window\"] for r in res_list], dtype=np.float64)\n","    ms = np.array([r[\"ms_per_window\"] for r in res_list], dtype=np.float64)\n","    rng = np.random.default_rng(123)\n","    boots_mJ = [float(np.mean(mJ[rng.integers(0, len(mJ), size=len(mJ))])) for _ in range(n_boot)]\n","    ci_low, ci_high = np.percentile(boots_mJ, [2.5, 97.5])\n","\n","    summary = {\n","        \"model\": name,\n","        \"mean_mJ_per_window\": float(mJ.mean()),\n","        \"ci95_low_mJ\": float(ci_low),\n","        \"ci95_high_mJ\": float(ci_high),\n","        \"mean_ms_per_window\": float(ms.mean()),\n","        \"runs\": res_list,\n","    }\n","    with open(logdir / f\"energy_{name}.json\", \"w\") as f:\n","        json.dump(summary, f, indent=2)\n","    print(\n","        f\"[Result] {name}: {summary['mean_mJ_per_window']:.3f} mJ per window  \"\n","        f\"(95% CI [{summary['ci95_low_mJ']:.3f}, {summary['ci95_high_mJ']:.3f}]);  \"\n","        f\"{summary['mean_ms_per_window']:.3f} ms per window\"\n","    )\n","    return summary\n","\n","# ---------------- rTsfNet (original architecture and hyperparameters — unchanged) ----------------\n","import tensorflow as tf\n","from tensorflow.keras import Input\n","from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, LeakyReLU, Layer, Lambda, Flatten, GlobalAveragePooling1D, Activation\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras import backend as K\n","\n","SEED = 42\n","tf.random.set_seed(SEED); np.random.seed(SEED)\n","FS = 50.0\n","IMU_ROT_HEADS = 2\n","MLP_BASE = 128\n","MLP_DEPTH = 3\n","DROPOUT = 0.5\n","LR = 1e-3\n","WEIGHT_DECAY = 1e-6\n","USE_ORIG_INPUT = True\n","\n","BASE = Path('/content')\n","features_dir = BASE / 'features'\n","models_dir = BASE / 'models'\n","logs_dir = BASE / 'logs'\n","models_dir.mkdir(parents=True, exist_ok=True)\n","logs_dir.mkdir(parents=True, exist_ok=True)\n","\n","def load_fold_data(fold_k, features_dir: Path):\n","    npz_file = features_dir / f'windows_normalized_fold{fold_k}.npz'\n","    data = np.load(npz_file, allow_pickle=True)\n","    X = np.stack([data['acc_x'], data['acc_y'], data['acc_z'], data['gyro_x'], data['gyro_y'], data['gyro_z']], axis=-1)  # [N,T,6]\n","    y = data['labels']; splits = data['splits']\n","    train_mask = splits == 'train'; test_mask = splits == 'test'\n","    return X[train_mask], y[train_mask], X[test_mask], y[test_mask]\n","\n","class TSFFeatureLayer(Layer):\n","    def __init__(self, fs=50.0, **kwargs):\n","        super().__init__(**kwargs); self.fs = float(fs); self.eps = 1e-8\n","    def get_config(self): cfg = super().get_config(); cfg.update({'fs': self.fs}); return cfg\n","    def call(self, x):\n","        mean = tf.reduce_mean(x, axis=1, keepdims=True)\n","        std  = tf.math.reduce_std(x, axis=1, keepdims=True) + self.eps\n","        maxv = tf.reduce_max(x, axis=1, keepdims=True); minv = tf.reduce_min(x, axis=1, keepdims=True)\n","        ptp  = maxv - minv; rms = tf.sqrt(tf.reduce_mean(tf.square(x), axis=1, keepdims=True))\n","        energy = tf.reduce_sum(tf.square(x), axis=1, keepdims=True)\n","        skew = tf.reduce_mean(tf.pow((x-mean)/std, 3), axis=1, keepdims=True)\n","        kurt = tf.reduce_mean(tf.pow((x-mean)/std, 4), axis=1, keepdims=True)\n","        signs = tf.sign(x); sign_changes = tf.abs(signs[:,1:,:] - signs[:,:-1,:]); zcr = tf.reduce_mean(sign_changes, axis=1, keepdims=True) / 2.0\n","        x_t1 = x[:,:-1,:]; x_tn1 = x[:,1:,:]\n","        ar1 = tf.reduce_sum(x_t1*x_tn1, axis=1, keepdims=True) / (tf.reduce_sum(tf.square(x_t1), axis=1, keepdims=True) + self.eps)\n","        x_t2 = x[:,:-2,:]; x_tn2 = x[:,2:,:]\n","        ar2 = tf.reduce_sum(x_t2*x_tn2, axis=1, keepdims=True) / (tf.reduce_sum(tf.square(x_t2), axis=1, keepdims=True) + self.eps)\n","        xc = x - mean; x_bc_t = tf.transpose(xc, [0,2,1]); fft = tf.signal.rfft(x_bc_t); power = tf.square(tf.abs(fft)) + self.eps; power = tf.transpose(power, [0,2,1])\n","        F = tf.shape(power)[1]; freqs = tf.linspace(0.0, tf.cast(self.fs, tf.float32)/2.0, F); freqs = tf.reshape(freqs, [1,F,1])\n","        p = power / (tf.reduce_sum(power, axis=1, keepdims=True) + self.eps)\n","        centroid = tf.reduce_sum(p * freqs, axis=1, keepdims=True)\n","        entropy  = -tf.reduce_sum(p * tf.math.log(p + self.eps), axis=1, keepdims=True) / (tf.math.log(tf.cast(F, tf.float32) + self.eps))\n","        geo = tf.exp(tf.reduce_mean(tf.math.log(power), axis=1, keepdims=True)); ari = tf.reduce_mean(power, axis=1, keepdims=True)\n","        flatness = geo / (ari + self.eps)\n","        w = tf.nn.softmax(power * 10.0, axis=1); soft_peak = tf.reduce_sum(w * freqs, axis=1, keepdims=True)\n","        def band(low, high):\n","            mask = tf.cast((freqs >= low) & (freqs < high), tf.float32)\n","            return tf.reduce_sum(power * mask, axis=1, keepdims=True) / (tf.reduce_sum(power, axis=1, keepdims=True) + self.eps)\n","        bp1 = band(0.5, 3.0); bp2 = band(3.0, 8.0); bp3 = band(8.0, 15.0)\n","        res = tf.concat([mean,std,maxv,minv,ptp,rms,energy,skew,kurt,zcr,ar1,ar2,centroid,entropy,flatness,soft_peak,bp1,bp2,bp3], axis=1)\n","        return tf.transpose(res, [0,2,1])\n","\n","class Multihead3DRotation(Layer):\n","    def __init__(self, head_nums=2, base_kn=64, param_depth=2, **kwargs):\n","        super().__init__(**kwargs); self.head_nums=head_nums; self.base_kn=base_kn; self.param_depth=param_depth; self.eps=1e-8\n","        self.gap = GlobalAveragePooling1D(); self.mlp=[Dense(self.base_kn, activation='relu') for _ in range(self.param_depth)]\n","        self.out_heads=[Dense(4, activation='tanh') for _ in range(self.head_nums)]\n","    def get_config(self): cfg = super().get_config(); cfg.update({'head_nums':self.head_nums,'base_kn':self.base_kn,'param_depth':self.param_depth}); return cfg\n","    def compute_output_shape(self, input_shape): return [tf.TensorShape(input_shape) for _ in range(self.head_nums)]\n","    def _axis_angle_to_R(self, axis_raw, angle_raw):\n","        axis = axis_raw / (tf.norm(axis_raw, axis=-1, keepdims=True) + self.eps); theta = angle_raw * math.pi\n","        B = tf.shape(axis)[0]; ux,uy,uz = axis[:,0],axis[:,1],axis[:,2]; z = tf.zeros_like(ux)\n","        Kmat = tf.stack([z,-uz,uy,uz,z,-ux,-uy,ux,z], axis=-1); Kmat = tf.reshape(Kmat,[B,3,3])\n","        I = tf.tile(tf.eye(3, dtype=axis.dtype)[None,...],[B,1,1]); u = tf.expand_dims(axis,-1); uuT = tf.matmul(u,u,transpose_b=True)\n","        cos = tf.reshape(tf.cos(theta),[-1,1,1]); sin = tf.reshape(tf.sin(theta),[-1,1,1])\n","        return cos*I + (1.0-cos)*uuT + sin*Kmat\n","    def call(self, x):\n","        acc, gyr = x[:,:,:3], x[:,:,3:6]; pooled = self.gap(x); h=pooled\n","        for layer in self.mlp: h = layer(h)\n","        out_list=[]\n","        for oh in self.out_heads:\n","            p=oh(h); axis=p[:,:3]; angle=tf.expand_dims(p[:,3],-1); R=self._axis_angle_to_R(axis,angle)\n","            acc_t=tf.transpose(acc,[0,2,1]); acc_rot_t=tf.matmul(R,acc_t); acc_rot=tf.transpose(acc_rot_t,[0,2,1])\n","            gyr_t=tf.transpose(gyr,[0,2,1]); gyr_rot_t=tf.matmul(R,gyr_t); gyr_rot=tf.transpose(gyr_rot_t,[0,2,1])\n","            out_list.append(tf.concat([acc_rot,gyr_rot],axis=-1))\n","        return out_list\n","\n","def add_l2_channels(x):\n","    acc = x[:,:,:3]; gyr = x[:,:,3:6]\n","    l2_acc = tf.sqrt(tf.reduce_sum(tf.square(acc), axis=-1, keepdims=True))\n","    l2_gyr = tf.sqrt(tf.reduce_sum(tf.square(gyr), axis=-1, keepdims=True))\n","    return tf.concat([x,l2_acc,l2_gyr],axis=-1)\n","\n","def r_tsf_net(x_shape, n_classes, learning_rate=LR, base_kn=MLP_BASE, depth=MLP_DEPTH, dropout_rate=DROPOUT,\n","              imu_rot_heads=IMU_ROT_HEADS, fs=FS, use_orig_input=USE_ORIG_INPUT):\n","    inputs = Input(shape=x_shape[1:]); x = inputs\n","    rot_layer = Multihead3DRotation(head_nums=imu_rot_heads, base_kn=64, param_depth=2, name='multihead_rot')\n","    rotated_list = rot_layer(x)\n","    streams=[]\n","    if use_orig_input: streams.append(Lambda(add_l2_channels,name='orig_plus_l2')(x))\n","    for i,xr in enumerate(rotated_list): streams.append(Lambda(add_l2_channels,name=f'rot{i}_plus_l2')(xr))\n","    concat_streams = Lambda(lambda lst: tf.concat(lst, axis=-1), name='concat_streams')(streams)\n","    tsf = TSFFeatureLayer(fs=fs, name='tsf')(concat_streams)\n","    z = Flatten(name='flatten')(tsf)\n","    for k in range(depth-1, -1, -1):\n","        z = Dense(base_kn*(2**k), kernel_regularizer=l2(WEIGHT_DECAY), name=f'fc_{k}')(z)\n","        z = LayerNormalization(epsilon=1e-7, name=f'ln_{k}')(z)\n","        z = LeakyReLU(name=f'lrelu_{k}')(z)\n","        z = Dropout(dropout_rate, name=f'drop_{k}')(z)\n","    logits = Dense(n_classes, kernel_regularizer=l2(WEIGHT_DECAY), name='logits')(z)\n","    probs  = Activation('softmax', dtype='float32', name='softmax')(logits)\n","    model = Model(inputs, probs, name='rTsfNet_officially_aligned_fixed')\n","    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate, amsgrad=True)\n","    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","    return model\n","\n","# ---------------- Data: prefer real features; otherwise generate demo data ----------------\n","def scan_available_folds(features_dir: Path):\n","    ks=[]\n","    for f in features_dir.glob(\"windows_normalized_fold*.npz\"):\n","        try: ks.append(int(f.stem.replace(\"windows_normalized_fold\",\"\")))\n","        except Exception: pass\n","    return sorted(set(ks))\n","\n","available_folds = scan_available_folds(features_dir)\n","ACTIVE_FOLDS = available_folds[:1] if available_folds else [0]\n","print(f\"[Info] Available folds: {available_folds}  |  Planned measurement: {ACTIVE_FOLDS}\")\n","\n","def make_synth_fold(n_train=4000, n_test=800, T=150, C=6, n_classes=8, seed=2025):\n","    rng = np.random.default_rng(seed)\n","    Xtr = rng.normal(0,1,size=(n_train,T,C)).astype(np.float32)\n","    Xte = rng.normal(0,1,size=(n_test, T,C)).astype(np.float32)\n","    ytr = rng.integers(0,n_classes,size=n_train).astype(np.int64)\n","    yte = rng.integers(0,n_classes,size=n_test).astype(np.int64)\n","    return Xtr,ytr,Xte,yte\n","\n","# ---------------- TF inference wrapper: one full pass (unit = window) ----------------\n","def make_tf_runner(model: tf.keras.Model, X_test_np: np.ndarray, bs: int = 256):\n","    device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n","    with tf.device(device):\n","        X_gpu = tf.convert_to_tensor(X_test_np.astype(np.float32))  # keep resident\n","    N = X_test_np.shape[0]\n","    @tf.function(jit_compile=False)\n","    def fwd(x): return model(x, training=False)\n","    def run_once():\n","        last=None\n","        for s in range(0, N, bs):\n","            e = min(N, s+bs)\n","            last = fwd(X_gpu[s:e])\n","        _ = tf.reduce_sum(last).numpy()  # device sync\n","    return run_once, N  # N windows per call\n","\n","# ---------------- Idle power ----------------\n","print(\"\\n[Info] Measuring idle power for 20 s ...\")\n","P_idle_mW, _idle = sample_idle_power_mW(duration_s=20.0, dev_index=0, interval=0.02,\n","                                        save_csv=str(logs_dir/'power_idle_trace_rtsfnet.csv'))\n","print(f\"[Info] Mean idle power ~ {P_idle_mW:.1f} mW\")\n","\n","# ---------------- Per-fold measurement ----------------\n","summary_rows = []\n","for k in ACTIVE_FOLDS:\n","    print(\"\\n\" + \"=\"*72)\n","    print(f\"[rTsfNet] Fold {k} — preparing data and model (original architecture/hyperparameters)\")\n","    if k in available_folds:\n","        X_train, y_train, X_test, y_test = load_fold_data(k, features_dir)\n","        n_classes = int(np.max(np.maximum(y_train.max(), y_test.max())) + 1)\n","    else:\n","        print(\"[Warn] Real features not found; using synthetic data to demonstrate the measurement pipeline.\")\n","        X_train, y_train, X_test, y_test = make_synth_fold()\n","        n_classes = int(np.max(np.maximum(y_train.max(), y_test.max())) + 1)\n","\n","    # Derive window_seconds from data length and FS (for explicit reporting)\n","    window_seconds = float(X_test.shape[1] / FS)\n","\n","    model = r_tsf_net(x_shape=X_train.shape, n_classes=n_classes,\n","                      learning_rate=LR, base_kn=MLP_BASE, depth=MLP_DEPTH, dropout_rate=DROPOUT,\n","                      imu_rot_heads=IMU_ROT_HEADS, fs=FS, use_orig_input=USE_ORIG_INPUT)\n","\n","    wpath = models_dir / f\"model_fold{k}.weights.h5\"\n","    if wpath.exists():\n","        try:\n","            model.load_weights(wpath); print(f\"[Info] Loaded weights: {wpath.name}\")\n","        except Exception as e:\n","            print(f\"[Warn] Failed to load weights: {e}\")\n","\n","    # Optional quick sanity check (not part of energy measurement)\n","    try:\n","        acc = (model.predict(X_test, batch_size=256, verbose=0).argmax(1) == y_test).mean()\n","        print(f\"[Check] Fold {k} quick accuracy: {acc:.3f}\")\n","    except Exception as e:\n","        print(f\"[Warn] Skipping accuracy check: {e}\")\n","\n","    run_once, N_windows_per_call = make_tf_runner(model, X_test, bs=256)\n","\n","    # Warmup\n","    for _ in range(3): run_once()\n","\n","    # Ensure ≥8 s effective window\n","    repeats = calibrate_repeats(run_once, target_s=8.0, min_rep=3, max_rep=5000)\n","    print(f\"[Info] repeats = {repeats}  (windows per call = {N_windows_per_call})\")\n","\n","    # Measure (per-window) + bootstrap CI\n","    tag = f\"rtsfnet_fold{k}_per_window\"\n","    summ = measure_with_bootstrap(\n","        name=tag, run_once=run_once, n_windows=N_windows_per_call,\n","        repeats=repeats, n_runs=5, n_boot=1000, logdir=logs_dir\n","    )\n","\n","    summary_rows.append({\n","        \"fold\": k,\n","        \"model\": f\"rTsfNet (fold {k})\",\n","        \"window_seconds\": window_seconds,\n","        \"mJ_per_window_mean\": summ[\"mean_mJ_per_window\"],\n","        \"ci95_low_mJ\": summ[\"ci95_low_mJ\"],\n","        \"ci95_high_mJ\": summ[\"ci95_high_mJ\"],\n","        \"ms_per_window_mean\": summ[\"mean_ms_per_window\"],\n","    })\n","\n","    K.clear_session(); gc.collect()\n","\n","# ---------------- Summary output ----------------\n","df_sum = pd.DataFrame(summary_rows).sort_values(\"fold\").reset_index(drop=True)\n","df_sum.to_csv(logs_dir / \"energy_summary_rtsfnet_per_window.csv\", index=False)\n","print(\"\\n=== Completed (rTsfNet GPU inference energy · per window · original architecture and hyperparameters) ===\")\n","print(df_sum)\n","print(\"\\nLog files:\")\n","print(\"- logs/power_idle_trace_rtsfnet.csv\")\n","print(\"- logs/power_trace_rtsfnet_fold*_per_window_run*.csv\")\n","print(\"- logs/energy_rtsfnet_fold*_per_window.json\")\n","print(\"- logs/energy_summary_rtsfnet_per_window.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IbdCD_VuF6og","executionInfo":{"status":"ok","timestamp":1763404631049,"user_tz":0,"elapsed":85832,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"6911832c-2814-4c4e-fd28-420dbea822fc"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Nov 17 18:35:45 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n","| N/A   32C    P0             60W /  400W |   26109MiB /  81920MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","+-----------------------------------------------------------------------------------------+\n","[Info] Available folds: [0]  |  Planned measurement: [0]\n","\n","[Info] Measuring idle power for 20 s ...\n","[Info] Mean idle power ~ 60541.0 mW\n","\n","========================================================================\n","[rTsfNet] Fold 0 — preparing data and model (original architecture/hyperparameters)\n","[Check] Fold 0 quick accuracy: 0.120\n","[Info] repeats = 704  (windows per call = 800)\n","[Measure] rtsfnet_fold0_per_window run 1/5 ...\n","[Measure] rtsfnet_fold0_per_window run 2/5 ...\n","[Measure] rtsfnet_fold0_per_window run 3/5 ...\n","[Measure] rtsfnet_fold0_per_window run 4/5 ...\n","[Measure] rtsfnet_fold0_per_window run 5/5 ...\n","[Result] rtsfnet_fold0_per_window: 0.447 mJ per window  (95% CI [0.436, 0.454]);  0.015 ms per window\n","\n","=== Completed (rTsfNet GPU inference energy · per window · original architecture and hyperparameters) ===\n","   fold             model  window_seconds  mJ_per_window_mean  ci95_low_mJ  \\\n","0     0  rTsfNet (fold 0)             3.0            0.447486     0.435941   \n","\n","   ci95_high_mJ  ms_per_window_mean  \n","0      0.454202            0.014552  \n","\n","Log files:\n","- logs/power_idle_trace_rtsfnet.csv\n","- logs/power_trace_rtsfnet_fold*_per_window_run*.csv\n","- logs/energy_rtsfnet_fold*_per_window.json\n","- logs/energy_summary_rtsfnet_per_window.csv\n"]}]}]}