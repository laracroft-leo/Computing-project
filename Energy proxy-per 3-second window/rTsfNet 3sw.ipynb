{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOV/MrpO9zBJ6xDN9BCPWOf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# ================================================================\n","# rTsfNet (Step 10 official architecture aligned · TSF-Mixer variant)\n","# × Option 1: NVML-based GPU inference energy — mJ per window (Colab one-click)\n","# ================================================================\n","# 0) Environment check & dependencies\n","!nvidia-smi\n","!pip -q install pynvml\n","\n","import os, math, json, time, pathlib, gc, warnings, multiprocessing as mp\n","from pathlib import Path\n","warnings.filterwarnings(\"ignore\")\n","\n","import numpy as np\n","import pandas as pd\n","\n","# ---------------- Option 1: NVML sampling + numerical integration + idle subtraction (per-window reporting) ----------------\n","import pynvml\n","\n","def _nvml_sampler(stop_event, q, dev_index=0, interval=0.02):\n","    \"\"\"Subprocess: sample power (mW) every `interval` and send back (t_abs, mW).\"\"\"\n","    import time, pynvml\n","    pynvml.nvmlInit()\n","    h = pynvml.nvmlDeviceGetHandleByIndex(dev_index)\n","    try:\n","        while not stop_event.is_set():\n","            q.put((time.perf_counter(), pynvml.nvmlDeviceGetPowerUsage(h)))\n","            time.sleep(interval)\n","    finally:\n","        pynvml.nvmlShutdown()\n","\n","def _integrate_mJ_between(samples, t0, t1):\n","    \"\"\"Trapezoidal integration of power (mW) over [t0, t1], returning mJ.\"\"\"\n","    if not samples:\n","        return 0.0\n","    samples = sorted(samples, key=lambda x: x[0])\n","    ts = np.array([t for t, _ in samples], dtype=np.float64)\n","    ps = np.array([p for _, p in samples], dtype=np.float64)\n","    m = (ts >= t0) & (ts <= t1)\n","    ts_w, ps_w = ts[m], ps[m]\n","    if ts_w.size == 0 or ts_w[0] > t0:\n","        p0 = np.interp(t0, ts, ps); ts_w = np.insert(ts_w, 0, t0); ps_w = np.insert(ps_w, 0, p0)\n","    if ts_w[-1] < t1:\n","        p1 = np.interp(t1, ts, ps); ts_w = np.append(ts_w, t1); ps_w = np.append(ps_w, p1)\n","    return float(np.trapz(ps_w, ts_w))  # mW*s = mJ\n","\n","def sample_idle_power_mW(duration_s=20.0, dev_index=0, interval=0.02, save_csv=None):\n","    \"\"\"Measure mean idle power (mW), optionally saving the power trace.\"\"\"\n","    import time\n","    q = mp.Queue(); stop = mp.Event()\n","    p = mp.Process(target=_nvml_sampler, args=(stop, q, dev_index, interval))\n","    p.start()\n","    time.sleep(duration_s)\n","    stop.set(); p.join()\n","    samples = []\n","    while not q.empty():\n","        samples.append(q.get())\n","    if not samples:\n","        raise RuntimeError(\"NVML captured no idle power samples.\")\n","    samples.sort(key=lambda x: x[0])\n","    t0, t1 = samples[0][0], samples[-1][0]\n","    E_idle_mJ = _integrate_mJ_between(samples, t0, t1)\n","    T_idle_s = max(1e-9, t1 - t0)\n","    P_idle_mW = E_idle_mJ / T_idle_s\n","    if save_csv:\n","        pd.DataFrame(samples, columns=[\"t_abs_s\", \"power_mW\"]).to_csv(save_csv, index=False)\n","    return P_idle_mW, samples\n","\n","def measure_mJ_per_window(run_once, n_windows_per_call, repeats, P_idle_mW,\n","                          dev_index=0, interval=0.02, save_csv=None):\n","    \"\"\"Concurrent NVML sampling + integration + idle subtraction; return per-window energy & latency.\"\"\"\n","    import time\n","    q = mp.Queue(); stop = mp.Event()\n","    p = mp.Process(target=_nvml_sampler, args=(stop, q, dev_index, interval))\n","    p.start()\n","\n","    t0 = time.perf_counter()\n","    for _ in range(repeats):\n","        run_once()\n","    t1 = time.perf_counter()\n","\n","    stop.set(); p.join()\n","    samples = []\n","    while not q.empty():\n","        samples.append(q.get())\n","    if not samples:\n","        raise RuntimeError(\"NVML captured no power samples (active phase).\")\n","\n","    E_total_mJ = _integrate_mJ_between(samples, t0, t1)\n","    T_total_s  = max(1e-9, t1 - t0)\n","    E_idle_mJ  = P_idle_mW * T_total_s\n","    n_windows  = max(1, repeats * n_windows_per_call)\n","\n","    if save_csv:\n","        pd.DataFrame(samples, columns=[\"t_abs_s\", \"power_mW\"]).to_csv(save_csv, index=False)\n","\n","    return {\n","        \"mJ_per_window\": max(0.0, (E_total_mJ - E_idle_mJ) / n_windows),\n","        \"ms_per_window\": (T_total_s / n_windows) * 1e3,\n","        \"throughput_windows_per_s\": n_windows / T_total_s,\n","        \"n_windows\": n_windows,\n","        \"repeats\": repeats,\n","        \"T_total_s\": T_total_s,\n","        \"E_total_mJ\": E_total_mJ,\n","        \"E_idle_mJ\": E_idle_mJ,\n","        \"P_idle_mW\": P_idle_mW,\n","        \"t0_abs\": t0, \"t1_abs\": t1\n","    }\n","\n","def calibrate_repeats(run_once, target_s=8.0, min_rep=3, max_rep=5000):\n","    \"\"\"Estimate repeats such that one measurement window ≈ target_s (warm-up to avoid first-call overhead).\"\"\"\n","    import time\n","    run_once()\n","    t0 = time.perf_counter(); run_once(); t1 = time.perf_counter()\n","    dt = max(1e-4, t1 - t0)\n","    reps = int(np.ceil(target_s / dt))\n","    return int(np.clip(reps, min_rep, max_rep))\n","\n","def measure_with_bootstrap(name, run_once, n_windows, repeats, n_runs=5, n_boot=1000, logdir=Path(\"logs\")):\n","    \"\"\"Repeat n_runs, compute bootstrap 95% CI, and save traces/summary (per-window metrics).\"\"\"\n","    logdir.mkdir(exist_ok=True)\n","    res_list = []\n","    for i in range(n_runs):\n","        print(f\"[Measure] {name} run {i+1}/{n_runs} ...\")\n","        r = measure_mJ_per_window(\n","            run_once, n_windows, repeats, P_idle_mW,\n","            dev_index=0, interval=0.02,\n","            save_csv=str(logdir / f\"power_trace_{name}_run{i+1}.csv\")\n","        )\n","        res_list.append(r)\n","\n","    mJ = np.array([r[\"mJ_per_window\"] for r in res_list], dtype=np.float64)\n","    ms = np.array([r[\"ms_per_window\"] for r in res_list], dtype=np.float64)\n","    rng = np.random.default_rng(123)\n","    boots = [float(np.mean(mJ[rng.integers(0, len(mJ), size=len(mJ))])) for _ in range(n_boot)]\n","    ci_low, ci_high = np.percentile(boots, [2.5, 97.5])\n","\n","    summary = {\n","        \"model\": name,\n","        \"mean_mJ_per_window\": float(mJ.mean()),\n","        \"ci95_low_mJ\": float(ci_low),\n","        \"ci95_high_mJ\": float(ci_high),\n","        \"mean_ms_per_window\": float(ms.mean()),\n","        \"runs\": res_list\n","    }\n","    with open(logdir / f\"energy_{name}.json\", \"w\") as f:\n","        json.dump(summary, f, indent=2)\n","    print(f\"[Result] {name}: {summary['mean_mJ_per_window']:.3f} mJ per window \"\n","          f\"(95% CI [{summary['ci95_low_mJ']:.3f}, {summary['ci95_high_mJ']:.3f}]); \"\n","          f\"{summary['mean_ms_per_window']:.3f} ms per window\")\n","    return summary\n","\n","# ---------------- rTsfNet (Step 10: official architecture aligned · TSF-Mixer etc.) [architecture/hyperparameters unchanged] ----------------\n","import tensorflow as tf\n","from tensorflow.keras import Input\n","from tensorflow.keras.layers import (\n","    Dense, Dropout, LayerNormalization, LeakyReLU,\n","    Layer, Activation, TimeDistributed, Flatten, Concatenate, GlobalAveragePooling1D\n",")\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras import backend as K\n","SEED = 42\n","tf.random.set_seed(SEED); np.random.seed(SEED)\n","FS = 50.0\n","IMU_ROT_HEADS = 2\n","MLP_BASE = 128\n","MLP_DEPTH = 3\n","DROPOUT = 0.5\n","LR = 1e-3\n","WEIGHT_DECAY = 1e-6\n","USE_ORIG_INPUT = True\n","USE_BINARY_SELECTION = True\n","LN_EPS = 1e-7\n","PAD_MODE = 'SYMMETRIC'\n","\n","class MLPStack(Layer):\n","    def __init__(self, base_kn=128, depth=3, drop=0.5, wd=0.0, ln_eps=1e-7, name=None):\n","        super().__init__(name=name)\n","        self.base_kn = int(base_kn); self.depth = int(depth)\n","        self.drop = float(drop); self.wd = float(wd); self.ln_eps = float(ln_eps)\n","        self.seq = []\n","        for k in range(self.depth - 1, -1, -1):\n","            self.seq.append(Dense(self.base_kn * (2**k), kernel_regularizer=l2(self.wd)))\n","            self.seq.append(LayerNormalization(epsilon=self.ln_eps))\n","            self.seq.append(LeakyReLU())\n","            self.seq.append(Dropout(self.drop))\n","    @property\n","    def out_dim(self): return self.base_kn\n","    def call(self, x, training=None):\n","        z = x\n","        for lyr in self.seq:\n","            z = lyr(z, training=training) if isinstance(lyr, Dropout) else lyr(z)\n","        return z\n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape([input_shape[0], self.out_dim])\n","\n","TIME_FEATS = 12\n","FREQ_FEATS = 7\n","\n","class TSFFeatureLayer(Layer):\n","    def __init__(self, fs=50.0, use_time=True, use_freq=True, **kwargs):\n","        super().__init__(**kwargs)\n","        self.fs = float(fs); self.use_time = bool(use_time); self.use_freq = bool(use_freq); self.eps = 1e-8\n","        self._feat_dim = (TIME_FEATS if self.use_time else 0) + (FREQ_FEATS if self.use_freq else 0)\n","    def get_config(self):\n","        c = super().get_config(); c.update({'fs': self.fs, 'use_time': self.use_time, 'use_freq': self.use_freq}); return c\n","    def call(self, x):  # [B,L,C]\n","        feats = []\n","        if self.use_time:\n","            mean = tf.reduce_mean(x, axis=1, keepdims=True)\n","            std  = tf.math.reduce_std(x, axis=1, keepdims=True) + self.eps\n","            maxv = tf.reduce_max(x, axis=1, keepdims=True); minv = tf.reduce_min(x, axis=1, keepdims=True)\n","            ptp  = maxv - minv; rms = tf.sqrt(tf.reduce_mean(tf.square(x), axis=1, keepdims=True))\n","            energy = tf.reduce_sum(tf.square(x), axis=1, keepdims=True)\n","            skew = tf.reduce_mean(tf.pow((x-mean)/std, 3), axis=1, keepdims=True)\n","            kurt = tf.reduce_mean(tf.pow((x-mean)/std, 4), axis=1, keepdims=True)\n","            signs = tf.sign(x); sign_changes = tf.abs(signs[:,1:,:] - signs[:,:-1,:]); zcr = tf.reduce_mean(sign_changes, axis=1, keepdims=True)/2.0\n","            x_t1 = x[:,:-1,:]; x_tn1 = x[:,1:,:]\n","            ar1 = tf.reduce_sum(x_t1*x_tn1, axis=1, keepdims=True) / (tf.reduce_sum(tf.square(x_t1), axis=1, keepdims=True) + self.eps)\n","            x_t2 = x[:,:-2,:]; x_tn2 = x[:,2:,:]\n","            ar2 = tf.reduce_sum(x_t2*x_tn2, axis=1, keepdims=True) / (tf.reduce_sum(tf.square(x_t2), axis=1, keepdims=True) + self.eps)\n","            feats += [mean,std,maxv,minv,ptp,rms,energy,skew,kurt,zcr,ar1,ar2]\n","        if self.use_freq:\n","            mean = tf.reduce_mean(x, axis=1, keepdims=True); xc = x - mean\n","            x_bc_t = tf.transpose(xc, [0,2,1]); fft = tf.signal.rfft(x_bc_t); power = tf.square(tf.abs(fft)) + self.eps\n","            power = tf.transpose(power, [0,2,1]); F = tf.shape(power)[1]\n","            freqs = tf.linspace(0.0, tf.cast(self.fs, tf.float32)/2.0, F); freqs = tf.reshape(freqs, [1,F,1])\n","            p = power / (tf.reduce_sum(power, axis=1, keepdims=True) + self.eps)\n","            centroid = tf.reduce_sum(p * freqs, axis=1, keepdims=True)\n","            entropy  = -tf.reduce_sum(p * tf.math.log(p + self.eps), axis=1, keepdims=True) / (tf.math.log(tf.cast(F, tf.float32) + self.eps))\n","            geo = tf.exp(tf.reduce_mean(tf.math.log(power), axis=1, keepdims=True)); ari = tf.reduce_mean(power, axis=1, keepdims=True)\n","            flatness = geo / (ari + self.eps)\n","            w = tf.nn.softmax(power * 10.0, axis=1); soft_peak = tf.reduce_sum(w * freqs, axis=1, keepdims=True)\n","            def band(low, high):\n","                mask = tf.cast((freqs >= low) & (freqs < high), tf.float32)\n","                return tf.reduce_sum(power * mask, axis=1, keepdims=True) / (tf.reduce_sum(power, axis=1, keepdims=True) + self.eps)\n","            bp1 = band(0.5,3.0); bp2 = band(3.0,8.0); bp3 = band(8.0,15.0)\n","            feats += [centroid,entropy,flatness,soft_peak,bp1,bp2,bp3]\n","        res = tf.concat(feats, axis=1)                 # [B,Fnum,C]\n","        return tf.transpose(res, [0,2,1])              # [B,C,Fnum]\n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape([input_shape[0], input_shape[2], (TIME_FEATS if self.use_time else 0) + (FREQ_FEATS if self.use_freq else 0)])\n","\n","class AddL2Channels(Layer):\n","    def call(self, x, training=None):\n","        acc = x[:,:,:3]; gyr = x[:,:,3:6]\n","        l2_acc = tf.sqrt(tf.reduce_sum(tf.square(acc), axis=-1, keepdims=True))\n","        l2_gyr = tf.sqrt(tf.reduce_sum(tf.square(gyr), axis=-1, keepdims=True))\n","        return tf.concat([x,l2_acc,l2_gyr], axis=-1)\n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape([input_shape[0], input_shape[1], 8])\n","\n","def _int_ceil_div(a,b):\n","    a = tf.cast(a, tf.int32); b = tf.cast(b, tf.int32)\n","    return tf.math.floordiv(a + b - 1, b)\n","\n","def frame_signal_with_padding(x, num_blocks, pad_mode='SYMMETRIC'):\n","    B = tf.shape(x)[0]; T = tf.shape(x)[1]; C = tf.shape(x)[2]\n","    nb = tf.cast(num_blocks, tf.int32); L = _int_ceil_div(T, nb)\n","    total = L * nb; pad_len = total - T\n","    pad_left = tf.math.floordiv(pad_len, 2); pad_right = pad_len - pad_left\n","    paddings = tf.stack([tf.constant([0,0],tf.int32), tf.stack([pad_left,pad_right]), tf.constant([0,0],tf.int32)], axis=0)\n","    x_pad = tf.pad(x, paddings, mode=pad_mode)\n","    return tf.reshape(x_pad, [B, nb, L, C])\n","\n","class BlockTSFExtractor(Layer):\n","    def __init__(self, num_blocks, fs, use_time, use_freq, tag_spec=None, pad_mode='SYMMETRIC', name=None, **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.num_blocks = int(num_blocks); self.tsf = TSFFeatureLayer(fs=fs, use_time=use_time, use_freq=use_freq)\n","        self.tag_spec = tag_spec; self.pad_mode = pad_mode\n","        self.tag_dim = 0 if (tag_spec is None or 'axis_tags' not in tag_spec) else int(tag_spec['axis_tags'].shape[1])\n","        self.base_feat_dim = (TIME_FEATS if use_time else 0) + (FREQ_FEATS if use_freq else 0)\n","        self.out_feat_dim = self.base_feat_dim + self.tag_dim\n","    def get_config(self):\n","        c = super().get_config(); c.update({'num_blocks':self.num_blocks, 'fs':self.tsf.fs,\n","                                            'use_time':self.tsf.use_time,'use_freq':self.tsf.use_freq,\n","                                            'pad_mode':self.pad_mode}); return c\n","    def call(self, x, training=None):\n","        xb = frame_signal_with_padding(x, self.num_blocks, pad_mode=self.pad_mode)  # [B,K,L,C]\n","        B = tf.shape(xb)[0]; K_ = tf.shape(xb)[1]; L = tf.shape(xb)[2]; C = tf.shape(xb)[3]\n","        xb2 = tf.reshape(xb, [B*K_, L, C])\n","        tsf_axis = self.tsf(xb2)\n","        tsf_axis = tf.reshape(tsf_axis, [B, K_, C, self.base_feat_dim])\n","        if self.tag_dim > 0:\n","            axis_tags = tf.convert_to_tensor(self.tag_spec['axis_tags'], dtype=tsf_axis.dtype)\n","            axis_tags = tf.reshape(axis_tags, [1,1,tf.shape(tsf_axis)[2],-1])\n","            axis_tags = tf.tile(axis_tags, [B, K_, 1, 1])\n","            tsf_axis = tf.concat([tsf_axis, axis_tags], axis=-1)\n","        return tsf_axis\n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape([input_shape[0], self.num_blocks, input_shape[2], self.out_feat_dim])\n","\n","class BinaryGate(Layer):\n","    def call(self, p, training=None):\n","        p = tf.clip_by_value(p, 0.0, 1.0)\n","        hard = tf.round(p)\n","        return hard + tf.stop_gradient(p - hard)\n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape(input_shape)\n","\n","class TSFMixerSubBlock(Layer):\n","    def __init__(self, axis_hidden=128, out_hidden=128, base_depth=2, drop=0.5, wd=0.0, ln_eps=1e-7, name=None):\n","        super().__init__(name=name)\n","        self.axis_hidden=int(axis_hidden); self.out_hidden=int(out_hidden); self.base_depth=int(base_depth)\n","        self.drop=float(drop); self.wd=float(wd); self.ln_eps=float(ln_eps)\n","        self.axis_mlp_layers=[]\n","        for k in range(self.base_depth-1, -1, -1):\n","            self.axis_mlp_layers.append(Dense(self.axis_hidden*(2**k), kernel_regularizer=l2(self.wd)))\n","            self.axis_mlp_layers.append(LayerNormalization(epsilon=self.ln_eps))\n","            self.axis_mlp_layers.append(LeakyReLU())\n","            self.axis_mlp_layers.append(Dropout(self.drop))\n","        self.out_stack = MLPStack(base_kn=self.out_hidden, depth=self.base_depth, drop=self.drop, wd=self.wd, ln_eps=self.ln_eps, name=f'{self.name}_out')\n","    def call(self, x, training=None, **kwargs):\n","        Bp = tf.shape(x)[0]; A = tf.shape(x)[1]; F = tf.shape(x)[2]\n","        x2 = tf.reshape(x, [Bp*A, F]); z = x2\n","        for lyr in self.axis_mlp_layers:\n","            z = lyr(z, training=training) if isinstance(lyr, Dropout) else lyr(z)\n","        z = tf.reshape(z, [Bp, A, self.axis_hidden]); z = tf.reshape(z, [Bp, A*self.axis_hidden])\n","        z = self.out_stack(z, training=training); return z\n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape([input_shape[0], self.out_stack.out_dim])\n","\n","class TSFMixerBlock(Layer):\n","    def __init__(self, feat_dim, axis_hidden=128, out_hidden=128, base_depth=2, drop=0.5, wd=0.0, ln_eps=1e-7, use_binary=True, name=None):\n","        super().__init__(name=name)\n","        self.use_binary=bool(use_binary)\n","        self.sub = TSFMixerSubBlock(axis_hidden, out_hidden, base_depth, drop, wd, ln_eps, name=f'{name}_sub')\n","        self.axis_gate_dense = Dense(1, activation='sigmoid', name=f'{name}_axis_gate')\n","        self.chan_gate_dense = Dense(int(feat_dim), activation='sigmoid', name=f'{name}_chan_gate')\n","        self.bin_gate = BinaryGate(name=f'{name}_bin')\n","        self.out_stack = MLPStack(base_kn=out_hidden, depth=base_depth, drop=drop, wd=wd, ln_eps=ln_eps, name=f'{name}_out')\n","    def call(self, x, training=None, **kwargs):\n","        Bp = tf.shape(x)[0]; A = tf.shape(x)[1]; F = tf.shape(x)[2]\n","        x_mean_axis = tf.reduce_mean(x, axis=1); p_chan = self.chan_gate_dense(x_mean_axis)\n","        p_chan = tf.reshape(p_chan, [Bp,1,F]); g_chan = self.bin_gate(p_chan, training=training) if self.use_binary else p_chan\n","        x = x * g_chan\n","        x2 = tf.reshape(x,[Bp*A,F]); z = x2\n","        for lyr in self.sub.axis_mlp_layers:\n","            z = lyr(z, training=training) if isinstance(lyr, Dropout) else lyr(z)\n","        z = tf.reshape(z,[Bp,A,self.sub.axis_hidden]); p_axis = self.axis_gate_dense(z)\n","        g_axis = self.bin_gate(p_axis, training=training) if self.use_binary else p_axis\n","        z = z * g_axis; z = tf.reshape(z,[Bp,A*self.sub.axis_hidden])\n","        z = self.out_stack(z, training=training); return z\n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape([input_shape[0], self.out_stack.out_dim])\n","\n","def _feat_dim_for_spec(use_time, use_freq, tag_dim):\n","    base = (TIME_FEATS if use_time else 0) + (FREQ_FEATS if use_freq else 0)\n","    return base + tag_dim\n","\n","BLOCK_SPECS = [dict(name='short', num_blocks=4, use_time=True,  use_freq=False),\n","               dict(name='long',  num_blocks=1, use_time=False, use_freq=True)]\n","\n","class RotationParamEstimator(Layer):\n","    def __init__(self, block_specs, fs, mlp_base=128, mlp_depth=2, drop=0.5, wd=0.0, ln_eps=1e-7, use_binary=True, pad_mode='SYMMETRIC', name=None):\n","        super().__init__(name=name)\n","        self.block_specs=block_specs; self.fs=fs; self.mlp_base=int(mlp_base); self.mlp_depth=int(mlp_depth)\n","        self.drop=float(drop); self.wd=float(wd); self.ln_eps=float(ln_eps); self.use_binary=bool(use_binary); self.pad_mode=pad_mode\n","        axis_tags=[];\n","        for i in range(8):\n","            axis_type=i+1; sensor_type=1 if (i<=2 or i==6) else 2\n","            axis_tags.append([axis_type, sensor_type])\n","        axis_tags=np.array(axis_tags,dtype=np.float32)\n","        self.tag_spec={'axis_tags':axis_tags}; tag_dim=axis_tags.shape[1]\n","        self.extractors=[]; self.td_mixers=[]; self.flatteners=[]\n","        for spec in block_specs:\n","            ext=BlockTSFExtractor(num_blocks=spec['num_blocks'], fs=fs, use_time=spec['use_time'], use_freq=spec['use_freq'], tag_spec=self.tag_spec, pad_mode=self.pad_mode, name=f'rot_ext_{spec[\"name\"]}')\n","            self.extractors.append(ext); feat_dim=_feat_dim_for_spec(spec['use_time'], spec['use_freq'], tag_dim)\n","            mix=TSFMixerBlock(feat_dim=feat_dim, axis_hidden=self.mlp_base, out_hidden=self.mlp_base, base_depth=max(1,self.mlp_depth-1), drop=self.drop, wd=self.wd, ln_eps=self.ln_eps, use_binary=self.use_binary, name=f'rot_mix_{spec[\"name\"]}')\n","            self.td_mixers.append(TimeDistributed(mix, name=f'rot_td_{spec[\"name\"]}')); self.flatteners.append(Flatten(name=f'rot_flat_{spec[\"name\"]}'))\n","        self.concat_sets=Concatenate(name='rot_concat_sets')\n","        self.post_stack=MLPStack(base_kn=self.mlp_base, depth=self.mlp_depth, drop=self.drop, wd=self.wd, ln_eps=self.ln_eps, name='rot_post')\n","        self.out_head=Dense(4, activation='tanh', name='rot4_tanh'); self.add_l2=AddL2Channels()\n","    def call(self, x, training=None, **kwargs):\n","        x8=self.add_l2(x); feats_all=[]\n","        for ext,td,flt in zip(self.extractors,self.td_mixers,self.flatteners):\n","            tsf_blocks=ext(x8, training=training); blk_feat=td(tsf_blocks, training=training); blk_feat=flt(blk_feat); feats_all.append(blk_feat)\n","        h=self.concat_sets(feats_all); h=self.post_stack(h, training=training); rot4=self.out_head(h); return rot4\n","    def compute_output_shape(self, input_shape): return tf.TensorShape([input_shape[0], 4])\n","\n","class Multihead3DRotationOfficial(Layer):\n","    def __init__(self, head_nums=2, fs=50.0, mlp_base=128, mlp_depth=2, drop=0.5, wd=0.0, ln_eps=1e-7, block_specs=None, use_binary=True, pad_mode='SYMMETRIC', name=None):\n","        super().__init__(name=name); block_specs = BLOCK_SPECS if block_specs is None else block_specs\n","        self.head_nums=int(head_nums)\n","        self.estimator=RotationParamEstimator(block_specs=block_specs, fs=fs, mlp_base=mlp_base, mlp_depth=mlp_depth, drop=drop, wd=wd, ln_eps=ln_eps, use_binary=use_binary, pad_mode=pad_mode, name='rot_estimator')\n","        self.eps=1e-8\n","    def compute_output_shape(self, input_shape): return [tf.TensorShape(input_shape) for _ in range(self.head_nums)]\n","    def _axis_angle_to_R(self, axis_raw, angle_raw):\n","        axis=axis_raw/(tf.norm(axis_raw,axis=-1,keepdims=True)+self.eps); theta=angle_raw*math.pi\n","        B=tf.shape(axis)[0]; ux,uy,uz=axis[:,0],axis[:,1],axis[:,2]; z=tf.zeros_like(ux)\n","        K=tf.stack([z,-uz,uy,uz,z,-ux,-uy,ux,z],axis=-1); K=tf.reshape(K,[B,3,3])\n","        I=tf.tile(tf.eye(3,dtype=axis.dtype)[None,...],[B,1,1]); u=tf.expand_dims(axis,-1); uuT=tf.matmul(u,u,transpose_b=True)\n","        cos=tf.reshape(tf.cos(theta),[-1,1,1]); sin=tf.reshape(tf.sin(theta),[-1,1,1]); R=cos*I+(1.0-cos)*uuT+sin*K; return R\n","    def call(self, x, training=None, **kwargs):\n","        acc,gyr=x[:,:,:3],x[:,:,3:6]; out_list=[]; prev_rot4=None\n","        for _ in range(self.head_nums):\n","            rot4=self.estimator(x, training=training)\n","            if prev_rot4 is not None: rot4=rot4+prev_rot4\n","            prev_rot4=rot4; axis=rot4[:,:3]; angle=tf.expand_dims(rot4[:,3],-1); R=self._axis_angle_to_R(axis,angle)\n","            acc_t=tf.transpose(acc,[0,2,1]); acc_rot=tf.transpose(tf.matmul(R,acc_t),[0,2,1])\n","            gyr_t=tf.transpose(gyr,[0,2,1]); gyr_rot=tf.transpose(tf.matmul(R,gyr_t),[0,2,1])\n","            out_list.append(tf.concat([acc_rot,gyr_rot],axis=-1))\n","        return out_list\n","\n","class AddL2ChannelsPublic(Layer):\n","    def call(self, x, training=None):\n","        acc=x[:,:,:3]; gyr=x[:,:,3:6]\n","        l2_acc=tf.sqrt(tf.reduce_sum(tf.square(acc),axis=-1,keepdims=True))\n","        l2_gyr=tf.sqrt(tf.reduce_sum(tf.square(gyr),axis=-1,keepdims=True))\n","        return tf.concat([x,l2_acc,l2_gyr],axis=-1)\n","\n","def r_tsf_net_official(x_shape, n_classes, learning_rate=LR, base_kn=MLP_BASE, depth=MLP_DEPTH, dropout_rate=DROPOUT,\n","                       imu_rot_heads=IMU_ROT_HEADS, fs=FS, use_orig_input=USE_ORIG_INPUT, use_binary_selection=USE_BINARY_SELECTION,\n","                       ln_eps=LN_EPS, pad_mode=PAD_MODE):\n","    inputs=Input(shape=x_shape[1:]); x=inputs\n","    rot_layer=Multihead3DRotationOfficial(head_nums=imu_rot_heads, fs=fs, mlp_base=base_kn, mlp_depth=max(1,depth-1),\n","                                          drop=dropout_rate, wd=WEIGHT_DECAY, ln_eps=ln_eps, block_specs=BLOCK_SPECS,\n","                                          use_binary=use_binary_selection, pad_mode=pad_mode, name='multihead_rot_official')\n","    rotated_list=rot_layer(x)\n","    streams=[]; add_l2=AddL2ChannelsPublic()\n","    if use_orig_input: streams.append(add_l2(x))\n","    for xr in rotated_list: streams.append(add_l2(xr))\n","    concat_streams=Concatenate(axis=-1,name='concat_streams')(streams)\n","    axis_tags_one_stream=[[i+1, 1 if (i<=2 or i==6) else 2] for i in range(8)]\n","    axis_tags_one_stream=np.array(axis_tags_one_stream,dtype=np.float32)\n","    num_streams=(1 if use_orig_input else 0)+imu_rot_heads\n","    axis_tags_all=np.concatenate([axis_tags_one_stream for _ in range(num_streams)],axis=0)\n","    tag_spec_main={'axis_tags':axis_tags_all}; tag_dim_main=axis_tags_all.shape[1]\n","    feats_all_sets=[]\n","    for spec in BLOCK_SPECS:\n","        ext=BlockTSFExtractor(num_blocks=spec['num_blocks'], fs=fs, use_time=spec['use_time'], use_freq=spec['use_freq'],\n","                              tag_spec=tag_spec_main, pad_mode=pad_mode, name=f'main_ext_{spec[\"name\"]}')\n","        feat_dim=_feat_dim_for_spec(spec['use_time'], spec['use_freq'], tag_dim_main)\n","        mix=TSFMixerBlock(feat_dim=feat_dim, axis_hidden=base_kn, out_hidden=base_kn, base_depth=max(1,depth-1),\n","                          drop=dropout_rate, wd=WEIGHT_DECAY, ln_eps=ln_eps, use_binary=use_binary_selection, name=f'main_mix_{spec[\"name\"]}')\n","        td=TimeDistributed(mix, name=f'main_td_{spec[\"name\"]}'); flt=Flatten(name=f'main_flat_{spec[\"name\"]}')\n","        tsf_blocks=ext(concat_streams); blk_feat=td(tsf_blocks); blk_feat=flt(blk_feat); feats_all_sets.append(blk_feat)\n","    z=Concatenate(name='main_concat_sets')(feats_all_sets)\n","    cls_stack=MLPStack(base_kn=base_kn, depth=depth, drop=dropout_rate, wd=WEIGHT_DECAY, ln_eps=ln_eps, name='cls')\n","    z=cls_stack(z); logits=Dense(n_classes, kernel_regularizer=l2(WEIGHT_DECAY), name='logits')(z)\n","    probs=Activation('softmax', dtype='float32', name='softmax')(logits)\n","    model=Model(inputs, probs, name='rTsfNet_official_aligned'); opt=Adam(learning_rate=learning_rate, amsgrad=True)\n","    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy']); return model\n","\n","# ---------------- Data: prefer /content/features; otherwise synthetic demo ----------------\n","BASE=Path('/content'); features_dir=BASE/'features'; models_dir=BASE/'models'; logs_dir=BASE/'logs'\n","models_dir.mkdir(parents=True, exist_ok=True); logs_dir.mkdir(parents=True, exist_ok=True)\n","\n","def load_fold_data(fold_k, features_dir: Path):\n","    npz_file=features_dir/f'windows_normalized_fold{fold_k}.npz'\n","    data=np.load(npz_file, allow_pickle=True)\n","    X=np.stack([data['acc_x'],data['acc_y'],data['acc_z'],data['gyro_x'],data['gyro_y'],data['gyro_z']],axis=-1)\n","    y=data['labels']; splits=data['splits']; train_mask=splits=='train'; test_mask=splits=='test'\n","    return X[train_mask],y[train_mask],X[test_mask],y[test_mask]\n","\n","def scan_available_folds(features_dir: Path):\n","    ks=[];\n","    for f in features_dir.glob(\"windows_normalized_fold*.npz\"):\n","        try: ks.append(int(f.stem.replace(\"windows_normalized_fold\",\"\")))\n","        except Exception: pass\n","    return sorted(set(ks))\n","\n","def make_synth_fold(n_train=4000, n_test=800, T=150, C=6, n_classes=8, seed=2025):\n","    rng=np.random.default_rng(seed)\n","    Xtr=rng.normal(0,1,size=(n_train,T,C)).astype(np.float32); Xte=rng.normal(0,1,size=(n_test,T,C)).astype(np.float32)\n","    ytr=rng.integers(0,n_classes,size=n_train).astype(np.int64); yte=rng.integers(0,n_classes,size=n_test).astype(np.int64)\n","    return Xtr,ytr,Xte,yte\n","\n","available_folds=scan_available_folds(features_dir)\n","ACTIVE_FOLDS=available_folds[:1] if available_folds else [0]\n","print(f\"[Info] Available folds: {available_folds} | Planned measurement: {ACTIVE_FOLDS}\")\n","\n","# ---------------- TF inference wrapper (full test set = one logical call; unit = window) ----------------\n","for g in tf.config.list_physical_devices('GPU'):\n","    try: tf.config.experimental.set_memory_growth(g, True)\n","    except Exception: pass\n","\n","def make_tf_runner(model: tf.keras.Model, X_test_np: np.ndarray, bs: int = 256):\n","    device=\"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n","    with tf.device(device): X_gpu=tf.convert_to_tensor(X_test_np.astype(np.float32))\n","    N=X_test_np.shape[0]\n","    @tf.function(jit_compile=False)\n","    def fwd(x): return model(x, training=False)\n","    def run_once():\n","        last=None\n","        for s in range(0,N,bs):\n","            e=min(N,s+bs); last=fwd(X_gpu[s:e])\n","        _=tf.reduce_sum(last).numpy()\n","    return run_once, N  # N windows per call\n","\n","# ---------------- Idle power ----------------\n","print(\"\\n[Info] Measuring idle power for 20 s ...\")\n","P_idle_mW,_idle=sample_idle_power_mW(duration_s=20.0, dev_index=0, interval=0.02,\n","                                     save_csv=str(logs_dir/'power_idle_trace_rtsfnet_official.csv'))\n","print(f\"[Info] Mean idle power ~ {P_idle_mW:.1f} mW\")\n","\n","# ---------------- Per-fold measurement (per-window metrics) ----------------\n","summary_rows=[]\n","for k in ACTIVE_FOLDS:\n","    print(\"\\n\"+\"=\"*72)\n","    print(f\"[rTsfNet-Official] Fold {k} — preparing data and model (original architecture/hyperparameters)\")\n","\n","    if k in available_folds:\n","        X_train,y_train,X_test,y_test=load_fold_data(k, features_dir); n_classes=int(max(y_train.max(), y_test.max())+1)\n","    else:\n","        print(\"[Warn] Real feature files not found; using synthetic data for demonstration.\")\n","        X_train,y_train,X_test,y_test=make_synth_fold(); n_classes=int(max(y_train.max(), y_test.max())+1)\n","\n","    # Derive window_seconds from data length and FS (explicit reporting)\n","    window_seconds=float(X_test.shape[1]/FS)\n","\n","    model=r_tsf_net_official(x_shape=X_train.shape, n_classes=n_classes,\n","                             learning_rate=LR, base_kn=MLP_BASE, depth=MLP_DEPTH, dropout_rate=DROPOUT,\n","                             imu_rot_heads=IMU_ROT_HEADS, fs=FS, use_orig_input=USE_ORIG_INPUT,\n","                             use_binary_selection=USE_BINARY_SELECTION, ln_eps=LN_EPS, pad_mode=PAD_MODE)\n","\n","    wpath=models_dir/f\"model_fold{k}.weights.h5\"\n","    if wpath.exists():\n","        try: model.load_weights(wpath); print(f\"[Info] Loaded weights: {wpath.name}\")\n","        except Exception as e: print(f\"[Warn] Failed to load weights: {e}\")\n","\n","    # Optional sanity check (excluded from energy measurement)\n","    try:\n","        y_prob=model.predict(X_test, batch_size=256, verbose=0); acc=(y_prob.argmax(1)==y_test).mean()\n","        print(f\"[Check] Fold {k} quick accuracy: {acc:.3f}\")\n","    except Exception as e:\n","        print(f\"[Warn] Skipping accuracy check: {e}\")\n","\n","    run_once,N_windows_per_call=make_tf_runner(model, X_test, bs=256)\n","\n","    for _ in range(3): run_once()\n","    repeats=calibrate_repeats(run_once, target_s=8.0, min_rep=3, max_rep=5000)\n","    print(f\"[Info] repeats = {repeats}  (windows per call = {N_windows_per_call})\")\n","\n","    tag=f\"rtsfnet_official_fold{k}_per_window\"\n","    summ=measure_with_bootstrap(name=tag, run_once=run_once, n_windows=N_windows_per_call,\n","                                repeats=repeats, n_runs=5, n_boot=1000, logdir=logs_dir)\n","\n","    summary_rows.append({\n","        \"fold\": k,\n","        \"model\": f\"rTsfNet-Official (fold {k})\",\n","        \"window_seconds\": window_seconds,\n","        \"mJ_per_window_mean\": summ[\"mean_mJ_per_window\"],\n","        \"ci95_low_mJ\": summ[\"ci95_low_mJ\"],\n","        \"ci95_high_mJ\": summ[\"ci95_high_mJ\"],\n","        \"ms_per_window_mean\": summ[\"mean_ms_per_window\"],\n","    })\n","\n","    K.clear_session(); gc.collect()\n","\n","# ---------------- Summary output ----------------\n","df_sum=pd.DataFrame(summary_rows).sort_values(\"fold\").reset_index(drop=True)\n","df_sum.to_csv(logs_dir/\"energy_summary_rtsfnet_official_per_window.csv\", index=False)\n","print(\"\\n=== Completed (rTsfNet official × Option 1 · per-window energy & latency) ===\")\n","print(df_sum)\n","print(\"\\nLog files:\")\n","print(\"- logs/power_idle_trace_rtsfnet_official.csv\")\n","print(\"- logs/power_trace_rtsfnet_official_fold*_per_window_run*.csv\")\n","print(\"- logs/energy_rtsfnet_official_fold*_per_window.json\")\n","print(\"- logs/energy_summary_rtsfnet_official_per_window.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cpmWUmtmHA8h","executionInfo":{"status":"ok","timestamp":1763404899539,"user_tz":0,"elapsed":122592,"user":{"displayName":"yu Wu","userId":"12692660435918028293"}},"outputId":"a690293d-9d4c-4baf-eb7d-e0a48e071e57"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Nov 17 18:39:37 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n","| N/A   32C    P0             60W /  400W |   26271MiB /  81920MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","+-----------------------------------------------------------------------------------------+\n","[Info] Available folds: [0] | Planned measurement: [0]\n","\n","[Info] Measuring idle power for 20 s ...\n","[Info] Mean idle power ~ 60541.4 mW\n","\n","========================================================================\n","[rTsfNet-Official] Fold 0 — preparing data and model (original architecture/hyperparameters)\n","[Check] Fold 0 quick accuracy: 0.149\n","[Info] repeats = 176  (windows per call = 800)\n","[Measure] rtsfnet_official_fold0_per_window run 1/5 ...\n","[Measure] rtsfnet_official_fold0_per_window run 2/5 ...\n","[Measure] rtsfnet_official_fold0_per_window run 3/5 ...\n","[Measure] rtsfnet_official_fold0_per_window run 4/5 ...\n","[Measure] rtsfnet_official_fold0_per_window run 5/5 ...\n","[Result] rtsfnet_official_fold0_per_window: 2.020 mJ per window (95% CI [1.979, 2.058]); 0.057 ms per window\n","\n","=== Completed (rTsfNet official × Option 1 · per-window energy & latency) ===\n","   fold                      model  window_seconds  mJ_per_window_mean  \\\n","0     0  rTsfNet-Official (fold 0)             3.0            2.020364   \n","\n","   ci95_low_mJ  ci95_high_mJ  ms_per_window_mean  \n","0     1.979468      2.057532            0.056782  \n","\n","Log files:\n","- logs/power_idle_trace_rtsfnet_official.csv\n","- logs/power_trace_rtsfnet_official_fold*_per_window_run*.csv\n","- logs/energy_rtsfnet_official_fold*_per_window.json\n","- logs/energy_summary_rtsfnet_official_per_window.csv\n"]}]}]}